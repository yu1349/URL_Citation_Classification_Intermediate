[
  {
    "id": 0,
    "name": "UUParser",
    "fullname": "N/A",
    "genericmention": [
      "the parser"
    ],
    "description": [
      "a near-SOTA model",
      "a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle"
    ],
    "citationtag": [
      "de Lhoneux et al. (2017b)"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/mdelhoneux/uuparser-composition",
    "section_title": "4 Composition in a K&G Parser",
    "add_info": "4 The code can be found at https://github.com/mdelhoneux/uuparser-composition",
    "text": "Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [Cite_Footnote_4] . The S WAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarse-grained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below the best system, which uses an ensemble system as well as ELMo embed-dings, as introduced by Peters et al. (2018). Note, however, that we use a slightly impoverished ver-sion of the model used for the shared task which is described in Smith et al. (2018a): we use a less ac-curate POS tagger (UDPipe) and we do not make use of multi-treebank models. In addition, Smith et al. (2018a) use the three top items of the stack as well as the first item of the buffer to represent the configuration, while we only use the two top items of the stack and the first item of the buffer. Smith et al. (2018a) also use an extended feature set as introduced by Kiperwasser and Goldberg (2016b) where they also use the rightmost and left-most children of the items of the stack and buffer that they consider. We do not use that extended feature set. This is to keep the parser settings as simple as possible and avoid adding confounding factors. It is still a near-SOTA model. We evaluate parsing models on the development sets and report the average of the 5 best results in 30 epochs and 5 runs with different random seeds."
  },
  {
    "id": 1,
    "name": "Universal Depen-dencies 2.0 treebanks",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Straka and Strakov, 2017",
      "Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88\u201399."
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://hdl.handle.net/11234/1-2364",
    "section_title": "5 What Correlates with Difficulty?",
    "add_info": "Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88\u201399. Documented models at http://hdl.handle.net/11234/1-2364.",
    "text": "Head-POS Entropy Dehouck and Denis (2018) propose an alternative measure of morphosyntactic complexity. Given a corpus of dependency graphs, they estimate the conditional entropy of the POS tag of a random token\u2019s parent, conditioned on the token\u2019s type. In a language where this HPE-mean metric is low, most tokens can predict the POS of their parent even without context. We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017)  ."
  },
  {
    "id": 2,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the re-versible language-agnostic tokenizer"
    ],
    "description": [
      "the re-versible language-agnostic tokenizer"
    ],
    "citationtag": [
      "Mielke and Eisner (2018)"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://sjmielke.com/papers/tokenize/",
    "section_title": "D Data selection: Europarl",
    "add_info": "31 http://sjmielke.com/papers/tokenize/",
    "text": "Finally, it should be said that the text in CoStEP itself contains some markup, marking reports, el-lipses, etc., but we strip this additional markup to obtain the raw text. We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [Cite_Footnote_31] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set. This way we ensure uniform division over sessions of the parliament and sizes of 2 / 3 , 1 / 6 , and 1 / 6 , respectively."
  },
  {
    "id": 3,
    "name": "Twitter API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://developer.twitter.com/en/docs.html",
    "section_title": "2 Problem Formulation 2.2 Data",
    "add_info": null,
    "text": "\u2022 Negative examples: We have col-lected 1% of tweets from Twitter\u2019s daily feed using the Twitter API (  https://developer.twitter.com/en/docs.html) to use as negative examples."
  },
  {
    "id": 4,
    "name": "MTurk",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Amazon, 2005",
      "Amazon. 2005"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.mturk.com/",
    "section_title": "5 User study",
    "add_info": "Amazon. 2005. MTurk. (https://www.mturk.com/).",
    "text": "To verify whether human evaluators are in agree-ment with our characterization model, we con-ducted a user study using MTurk (Amazon, 2005)  ."
  },
  {
    "id": 5,
    "name": "Fast-Text",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Facebook-Research, 2016",
      "Facebook-Research. 2016"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://research.fb.com/fasttext/",
    "section_title": "4 Approaches to authorship verification 4.4 Approach 4: Document embeddings",
    "add_info": "Facebook-Research. 2016. FastText. (https://research.fb.com/fasttext/).",
    "text": "1. We obtain representations of tweets as doc-ument embeddings. We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016)  (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768)."
  },
  {
    "id": 6,
    "name": "Google-n-Gram",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://books.google.com/ngrams",
    "section_title": "3 The Proposed Method 3.1 Automatic Seed Generation",
    "add_info": "1 Google-n-Gram (http://books.google.com/ngrams) is used as the background corpus.",
    "text": "The seed set consists of positive labeled examples (i.e. product features) and negative labeled exam-ples (i.e. noise terms). Intuitively, popular product features are frequently mentioned in reviews, so they can be extracted by simply mining frequently occurring nouns (Hu and Liu, 2004). However, this strategy will also find many noise terms (e.g., commonly used nouns like thing, one, etc.). To produce high quality seeds, we employ a Domain Relevance Measure (DRM) (Jiang and Tan, 2010), which combines term frequency with a domain-specific measuring metric called Likelihood Ratio Test (LRT) (Dunning, 1993). Let \u03bb(t) denotes the LRT score of a product feature candidate t, where k 1 and k 2 are the frequencies of t in the review corpus R and a background corpus [Cite_Footnote_1] B, n 1 and n 2 are the total number of terms in R and B, p = (k 1 + k 2 )/(n 1 + n 2 ), p 1 = k 1 /n 1 and p 2 = k 2 /n 2 . Then a modified DRM 2 is proposed, where tf(t) is the frequency of t in R and df(t) is the frequency of t in B."
  },
  {
    "id": 7,
    "name": "Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.wikipedia.org",
    "section_title": "3 The Proposed Method 3.2 Capturing Lexical Semantic Clue in a Semantic Similarity Graph 3.2.1 Learning Word Embedding for",
    "add_info": "3 Wikipedia(http://www.wikipedia.org) is used in practice.",
    "text": "To alleviate the data sparsity problem, EB is first trained on a very large corpus [Cite_Footnote_3] (denoted by C), and then fine-tuned on the target review cor-pus R. Particularly, for phrasal product features, a statistic-based method in (Zhu et al., 2009) is used to detect noun phrases in R. Then, an Unfold-ing Recursive Autoencoder (Socher et al., 2011) is trained on C to obtain embedding vectors for noun phrases. In this way, semantics of infrequent terms in R can be well captured. Finally, the phrase-based Skip-gram model in (Mikolov et al., 2013) is applied on R."
  },
  {
    "id": 8,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a benchmark dataset in Wang et al. (2011)",
      "The first one"
    ],
    "description": [
      "a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel)",
      "real world datasets"
    ],
    "citationtag": [
      "Wang et al. (2011)"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://timan.cs.uiuc.edu/downloads.html",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "5 http://timan.cs.uiuc.edu/downloads.html",
    "text": "Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) [Cite_Footnote_5] . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper."
  },
  {
    "id": 9,
    "name": "COAE 2008",
    "fullname": "Chinese Opinion Analysis Evalua-tion 2008",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "real world datasets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://ir-china.org.cn/coae2008.html",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "6 http://ir-china.org.cn/coae2008.html",
    "text": "Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) [Cite_Footnote_6] , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper."
  },
  {
    "id": 10,
    "name": "\u201cinterest\u201d, \u201cline\u201d",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://www.d.umn.edu/\u223ctpederse/data.html",
    "section_title": "4 Experiments and Results 4.1 Experiment Design",
    "add_info": "1 Available at http://www.d.umn.edu/\u223ctpederse/data.html",
    "text": "For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d [Cite_Footnote_1] and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3."
  },
  {
    "id": 11,
    "name": "the data in English lexical sample task of SENSEVAL-3",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://www.senseval.org/senseval3",
    "section_title": "4 Experiments and Results 4.1 Experiment Design",
    "add_info": "2 Available at http://www.senseval.org/senseval3",
    "text": "For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) [Cite_Footnote_2] . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3."
  },
  {
    "id": 12,
    "name": "SV M light",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://svmlight.joachims.org/",
    "section_title": "4 Experiments and Results 4.2 Experiment 1: LP vs. SVM",
    "add_info": "3 we SV M light ,used linear available at http://svmlight.joachims.org/.",
    "text": "Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [Cite_Footnote_3] systems in ELS task of SENSEVAL-3."
  },
  {
    "id": 13,
    "name": "Isomap",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://isomap.stanford.edu/",
    "section_title": "4 Experiments and Results 4.4 An Example: Word \u201cuse\u201d",
    "add_info": "5 We used Isomap to perform dimensionality reduction by computing two-dimensional, 39-nearest-neighbor-preserving embedding of 210-dimensional input. Isomap is available at http://isomap.stanford.edu/.",
    "text": "For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word \u201cuse\u201d in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set). For data visualization, we conducted unsupervised nonlinear dimensionality reduction [Cite_Footnote_5] on these 40 feature vec-tors with 210 dimensions. Figure 3 (a) shows the dimensionality reduced vectors in two-dimensional space. We randomly sampled only one labeled ex-ample for each sense of word \u201cuse\u201d as labeled data. The remaining data in training set and test set served as unlabeled data for bootstrapping and LP. All of these three algorithms are evaluated using accuracy on test set."
  },
  {
    "id": 14,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code and preprocessed datasets",
      "our proposed model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/GeneZC/ASGCN",
    "section_title": "References",
    "add_info": "1 Code and preprocessed datasets are available at https://github.com/GeneZC/ASGCN.",
    "text": "Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neu-ral Networks (CNNs) are widely applied for aspect-based sentiment classification. How-ever, these models lack a mechanism to ac-count for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we pro-pose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models [Cite_Footnote_1] , and further demon-strate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure."
  },
  {
    "id": 15,
    "name": "SVM light",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Joachims, 1999"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://svmlight.joachims.org",
    "section_title": "3 Problem Formulation",
    "add_info": "2 Available at http://svmlight.joachims.org.",
    "text": "The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Scho\u0308lkopf and Smola (2002). SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments. [Cite_Footnote_2]"
  },
  {
    "id": 16,
    "name": "N/A",
    "fullname": "Center for Research in Security Prices (CRSP) US Stocks Database",
    "genericmention": [
      "The text and volatility data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/10K",
    "section_title": "4 Dataset",
    "add_info": "4 The text and volatility data are publicly available at http://www.ark.cs.cmu.edu/10K.",
    "text": "In addition to the reports, we used the Center for Research in Security Prices (CRSP) US Stocks Database to obtain the price return series along with other firm characteristics. [Cite_Footnote_4] We proceeded to calcu-late two volatilities for each firm/report observation: the twelve months prior to the report (v (\u221212) ) and the twelve months after the report (v (+12) )."
  },
  {
    "id": 17,
    "name": "GMTG",
    "fullname": "Gener-alized multitext grammars",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Technical Report 04-003, NYU Proteus Project",
      "Dan Melamed, G. Satta, and B. Wellington. 2004"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/pubs/",
    "section_title": "1 Introduction",
    "add_info": "I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.",
    "text": "This paper begins with an informal description of GMTG. It continues with an investigation of this formalism\u2019s generative capacity. Next, we prove that in GMTG each component grammar retains its generative power, a requirement for synchronous formalisms that Rambow and Satta (1996) called the \u201cweak language preservation property.\u201d Lastly, we propose a synchronous generalization of Chom-sky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKY-style algorithm (Younger, 1967; Melamed, 2004  )."
  },
  {
    "id": 18,
    "name": "GMTG",
    "fullname": "Gener-alized multitext grammars",
    "genericmention": [
      "a GMTG"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Melamed, 2004",
      "Dan Melamed, G. Satta, and B. Wellington. 2004",
      "Technical Report 04-003, NYU Proteus Project"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/pubs/",
    "section_title": "6 Generalized Chomsky Normal Form 6.2 Step 4: Eliminate \u2019s",
    "add_info": "I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.",
    "text": "Grammars in GCNF cannot have \u2019s in their productions. Thus, GCNF is a more restrictive normal form than those used by Wu (1997) and Melamed (2003). The absence of \u2019s simplifies parsers for GMTG (Melamed, 2004)  . Given a GMTG u with in some productions, we give the construction of a weakly equivalent gram-mar u9O without any \u2019s. First, determine all nullable links and associated - strings in u . - A link * Z Z is nullable if < y is an ITV where at least one y\u2022bhg is . We say the link is nullable and the string at address in is nullable. For each nullable link, we create versions of the link, where is the number of nullable strings of that link. There is one version for each of the possible combinations of the nullable strings being present or absent. The version of the link with all strings present is its original version. Each non-original version of the link (except in the case of start links) gets a unique subscript, which is applied to all the nonterminals in the link, so that each link is unique in the grammar. We construct a new grammar u O whose set of productions w O is determined as follows: for each production, we identify the nullable links on the RHS and replace them with each combination of the non-original versions found earlier. If a string is left empty during this process, that string is removed from the RHS and the fan-out of the production component is reduced by one. The link on the LHS is replaced with its appropriate matching non-original link. There is one exception to the replacements. If a production consists of all nullable strings, do not include this case. Lastly, we remove all strings on the RHS of productions that have \u2019s, and reduce the fan-out of the productions accordingly. Once again, we replace the LHS link with the appropriate version. case and are nullable - so we create 54 a new version of both links: and . We then alter the productions. Pro-duction (31) gets replaced by (40). A new produc-tion based on (30) is Production (38). Lastly, Pro-duction (29) has two nullable strings on the RHS, so it gets altered to add three new productions, (34), (35) and (36). The altered set of productions are the following:"
  },
  {
    "id": 19,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our open-source code repository"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://hohocode.github.io/textSimilarityConvNet/",
    "section_title": "6 Experiments and Results",
    "add_info": "4 http://hohocode.github.io/textSimilarityConvNet/",
    "text": "Everything necessary to replicate our experimen-tal results can be found in our open-source code repository. [Cite_Footnote_4]"
  },
  {
    "id": 20,
    "name": "STAC corpus",
    "fullname": "N/A",
    "genericmention": [
      "the corpus",
      "the corpus",
      "the corpus"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.irit.fr/STAC/",
    "section_title": "1 Introduction",
    "add_info": "1 https://www.irit.fr/STAC/",
    "text": "In our study, we restrict the structure learning problem to predicting edges or attachments be-tween DU pairs in the dependency graph. After training a supervised deep learning algorithm to predict attachments on the STAC corpus [Cite_Footnote_1] , we then constructed a weakly supervised learning system in which we used 10% of the corpus as a develop-ment set. Experts on discourse structure wrote a set of attachment rules, Labeling Functions (LFs), with reference to this development set. Although the whole of the STAC corpus is annotated, we treated the remainder of the corpus as unseen/u-nannotated data in order to simulate the conditions in which the snorkel framework is meant to be used, i.e. where there is a large amount of unla-beled data but where it is only feasible to hand la-bel a relatively small portion of it. Accordingly, we applied the completed LFs to our \u201cunseen\u201d training set, 80% of the corpus, and used the final 10% as our test set."
  },
  {
    "id": 21,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a subset of the data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://tizirinagh.github.io/acl2019/",
    "section_title": "5 Results and Analysis",
    "add_info": "3 https://tizirinagh.github.io/acl2019/",
    "text": "We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [Cite_Footnote_3] . We then evaluated the generative model and the generative + discriminative model with the Snorkel architecture on the test set with the results in Table 2."
  },
  {
    "id": 22,
    "name": "ACE2005",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "https://catalog.ldc.upenn.edu/LDC2006T06",
    "section_title": "1 Introduction",
    "add_info": "1 https://catalog.ldc.upenn.edu/LDC2006T06",
    "text": "We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [Cite_Footnote_1] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community."
  },
  {
    "id": 23,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/liushulinle/event",
    "section_title": "1 Introduction",
    "add_info": "2 https://github.com/liushulinle/event detection without triggers",
    "text": "We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community. [Cite_Footnote_2]"
  },
  {
    "id": 24,
    "name": "Stanford CoreNLP tool-s",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Manning et al., 2014"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://stanfordnlp.github.io/CoreNLP",
    "section_title": "3 Methodology 3.1 Input Tokens",
    "add_info": "3 http://stanfordnlp.github.io/CoreNLP",
    "text": "Given a sentence, we use Stanford CoreNLP tool-s [Cite_Footnote_3] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b)."
  },
  {
    "id": 25,
    "name": "NYT corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2008T19",
    "section_title": "3 Methodology 3.2 Word/Entity Embeddings",
    "add_info": "4 https://catalog.ldc.upenn.edu/LDC2008T19",
    "text": "In this work, we use the Skip-gram mod-el(Mikolov et al., 2013) to learn word embeddings on the NYT corpus [Cite_Footnote_4] . Furthermore, we random-ly initialized an embedding table for each entity tags. All the input word tokens and entity tags will be transformed into low-dimensional vectors by looking up these embedding tables. In this work, we denote the dimension of word embeddings by d w , and that of entity embeddings by d e ."
  },
  {
    "id": 26,
    "name": "BIBTEX",
    "fullname": "N/A",
    "genericmention": [
      "The second dataset",
      "The dataset",
      "The dataset"
    ],
    "description": [
      "The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users.",
      "the BIBTEX dataset does not provide how many times each tag is annotated to a resource."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.kde.cs.uni-kassel.de/bibsonomy/dumps",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "2 The dataset can be obtained from http://www.kde.cs.uni-kassel.de/bibsonomy/dumps",
    "text": "The first dataset, denoted as BOOK, is obtained from a popular Chinese book review website www. douban.com, which contains the descriptions of books and the tags collaboratively annotated by users. The second dataset, denoted as BIBTEX, is obtained from an English online bibliography web-site www.bibsonomy.org [Cite_Footnote_2] . The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users. As shown in Table 2, the average length of descriptions in the BIBTEX dataset is much shorter than the BOOK dataset. Moreover, the BIBTEX dataset does not provide how many times each tag is annotated to a resource."
  },
  {
    "id": 27,
    "name": "Elasticsearch [Cite_Footnote_3] search engine",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Keyword search on tweets is powered by Elasticsearch which is cou-pled with querying the database to provide addi-tional filters."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.elastic.co/products/elasticsearch",
    "section_title": "2 System Architecture and Components 2.1 System Architecture",
    "add_info": "3 https://www.elastic.co/products/elasticsearch",
    "text": "ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [Cite_Footnote_3] search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers. The front-end presentation layer allows users to narrow down search results by applying multiple filters. Keyword search on tweets is powered by Elasticsearch which is cou-pled with querying the database to provide addi-tional filters. Additionally, it provides numerous visualized graphs. The back-end data collection and computation layer performs pre-processing of tweets, computing check-worthiness scores of tweets using the public ClaimBuster API (Hassan et al., 2017a), Elasticsearch batch insertion, de-tecting claim types of tweets, and finding similar fact-checked claims for each tweet, using Claim-Buster API. ClaimPortal stays up-to-date with current tweets by periodically calling the Twitter REST API."
  },
  {
    "id": 28,
    "name": "Twitter\u2019s card API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://developer.twitter.com/en/docs/tweets/optimize-with-cards",
    "section_title": "2 System Architecture and Components 2.2 Monitoring, Processing, and Storing Tweets",
    "add_info": "5 https://developer.twitter.com/en/docs/tweets/optimize-with-cards",
    "text": "ClaimPortal\u2019s back-end layer focuses on data processing and storage. The Twitter REST API provides us with the necessary data. However, the system does not require all of it. In fact, a lot of the API\u2019s response is discarded to keep our database small and yet sufficient enough to pro-vide all necessary information for the portal. This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters. Based on the keyword search and filters requested by a user, the API queries the database to find the resulting list of tweet IDs and returns the list as a JSON response. A tweet ID is a unique number assigned to a tweet by Twitter. By using Twitter\u2019s card API [Cite_Footnote_5] the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID."
  },
  {
    "id": 29,
    "name": "ClaimBuster API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a well-known fact-checking tool",
      "The Claim-Buster API returns a check-worthiness score for any given text.",
      "The score is on a scale from 0 to 1, ranging from least check-worthy to most check-worthy."
    ],
    "citationtag": [
      "Adair et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://idir.uta.edu/claimbuster/",
    "section_title": "2 System Architecture and Components 2.3 Claim Spotter",
    "add_info": "6 https://idir.uta.edu/claimbuster/",
    "text": "In ClaimPortal, each tweet is given a check-worthiness score which denotes whether the tweet has a factual claim of which the truthfulness is im-portant to the public. This score is obtained by probing the ClaimBuster API, [Cite_Footnote_6] a well-known fact-checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019). Claim-Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates. The Claim-Buster API returns a check-worthiness score for any given text. The score is on a scale from 0 to 1, ranging from least check-worthy to most check-worthy. The background task of probing Claim-Buster API for getting scores for tweets is another batch process, in parallel with the tweet collection and the Elasticsearch indexing processes."
  },
  {
    "id": 30,
    "name": "PolitiFact",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.politifact.com",
    "section_title": "2 System Architecture and Components 2.4 Detecting Claim Types 2.4.1 Frame detection",
    "add_info": "7 https://www.politifact.com",
    "text": "We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [Cite_Footnote_7] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it. Details of these purposely created new frames can be found in (Arslan et al., 2019). The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available."
  },
  {
    "id": 31,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The corpus of the newly-defined frames along with their annotated exemplary sentences"
    ],
    "description": [
      "the newly-defined frames along with their annotated exemplary sentences"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/idirlab/factframe",
    "section_title": "2 System Architecture and Components 2.4 Detecting Claim Types 2.4.1 Frame detection",
    "add_info": "8 https://github.com/idirlab/factframe",
    "text": "We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it. Details of these purposely created new frames can be found in (Arslan et al., 2019). The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available. [Cite_Footnote_8]"
  },
  {
    "id": 32,
    "name": "Share-the-facts [Cite_Footnote_10] fact checks",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.sharethefacts.org/",
    "section_title": "2 System Architecture and Components 2.5 Claim Matcher",
    "add_info": "10 http://www.sharethefacts.org/",
    "text": "ClaimPortal leverages the claim matching func-tion in the ClaimBuster API. The fact-check repository is composed of the Share-the-facts [Cite_Footnote_10] fact checks as well as fact checks collected from several fact-checking organizations like PolitiFact, Snopes, factcheck.org, Washington Post, etc. The system measures the similarity between a claim and a fact-check based on the similarity of their tokens. An Elasticsearch server is deployed for searching the repository based on token similarity."
  },
  {
    "id": 33,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our final parser",
      "Our implementation"
    ],
    "description": [
      "The default setting for our final parser is a 2-layer GNN model that uses hd \u25b7 h (Equation 8) aggregating function and \u201cH-first\u201d asynchronous update method (Equation 9)."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/AntNLP/gnn-dep-parsing",
    "section_title": "4 Experiments",
    "add_info": "6 Our implementation is publicly available at: https://github.com/AntNLP/gnn-dep-parsing",
    "text": "The default setting for our final parser is a 2-layer GNN model that uses hd \u25b7 h (Equation 8) aggregating function and \u201cH-first\u201d asynchronous update method (Equation 9). [Cite_Footnote_6]"
  },
  {
    "id": 34,
    "name": "FastText multilingual pretrained vectors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "multilingual pretrained vectors"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/facebookresearch/fastText",
    "section_title": "4 Experiments 4.1 Main Results",
    "add_info": "8 https://github.com/facebookresearch/fastText",
    "text": "Finally, we report the results of our model on partial UD treebanks on the CoNLL 2018 shared task (Table 5). Our model uses only word and XPOS tag (predict by UDPipe), without any cross lingual features. We use FastText multilingual pretrained vectors instead of Glove vectors. [Cite_Footnote_8] The results show that our GNN parser performs better on 10 UD 2.2 treebanks. For bg, our parser does not improve performance. For nl, our parser im-proves 0.22 UAS, although LAS is slightly lower than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser."
  },
  {
    "id": 35,
    "name": "UD 2.2",
    "fullname": "Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [Cite_Ref] tree-banks",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague",
      "Nivre et al., 2018"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://hdl.handle.net/11234/1-1983xxx",
    "section_title": "4 Experiments",
    "add_info": "Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.",
    "text": "We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018)  tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=\u00a72-21, dev=\u00a722, test=\u00a723), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy \u2248 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task\u2019s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016)."
  },
  {
    "id": 36,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/JiaweiSheng/FAAN",
    "section_title": "References",
    "add_info": null,
    "text": "Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a rela-tion given its few-shot reference entity pairs. Recent attempts solve this problem by learn-ing static representations of entities and refer-ences, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, en-tities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mecha-nism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acqui-sition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at  https://github.com/JiaweiSheng/FAAN."
  },
  {
    "id": 37,
    "name": "NELL and Wiki",
    "fullname": "N/A",
    "genericmention": [
      "both datasets",
      "both datasets"
    ],
    "description": [
      "re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks.",
      "There are 67 and 183 tasks in NELL and Wiki, re-spectively."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/xwhan/One-shot-Relational-Learning",
    "section_title": "5 Experiments 5.1 Datasets",
    "add_info": "1 https://github.com/xwhan/One-shot-Relational-Learning",
    "text": "We conduct experiments on two public benchmark datasets: NELL and Wiki [Cite_Footnote_1] . In both datasets, re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks. There are 67 and 183 tasks in NELL and Wiki, re-spectively. We use original 51/5/11 and 133/16/34 relations in NELL and Wiki, respectively, for train-ing/validation/testing as defined in Section 3. More-over, for each task relation, both datasets also pro-vide candidate entities, which are constructed based on the entity type constraint (Xiong et al., 2018). More details are shown in Table 1."
  },
  {
    "id": 38,
    "name": "OpenKE",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Han et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch",
    "section_title": "5 Experiments 5.3 Implementation Details",
    "add_info": "2 https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch",
    "text": "We perform 5-shot KG completion task for all the methods. Our implementation for KG embedding baselines is based on OpenKE [Cite_Footnote_2] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature. During training, all triples in back-ground KG G 0 and training set, as well as few-shot reference triples of validation and testing set are used to train models. For few-shot relational learn-ing baselines, we extend GMatching from original one-shot scenario to few-shot scenario by three set-tings: obtaining general reference representation by mean/max pooling (denoted as MeanP/MaxP) over references, or taking the reference that leads to the maximal similarity score to the query (denoted as Max). Because FSRL was reported in completely different experimental settings, we reimplement the model to make a fair comparison. We directly re-port the original results of MetaR with pre-trained embeddings to avoid re-implementation bias."
  },
  {
    "id": 39,
    "name": "Weibo",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Weibo [Cite_Footnote_2] , which contains massive multi-turn con-versation sessions and user identification informa-tion"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Produce",
    "url": "http://www.weibo.com/",
    "section_title": "4 Experiments 4.1 Dataset",
    "add_info": "2 http://www.weibo.com/",
    "text": "To evaluate the effectiveness of our proposed per-sonalized WAE model (PersonaWAE), we collect a dataset from an open online chatting forum, i.e., Weibo [Cite_Footnote_2] , which contains massive multi-turn con-versation sessions and user identification informa-tion. Overall, there are 31,128,520 utterances in the raw dataset with corresponded user identifica-tions. To construct the personalized conversation systems, we retrieve users with more than 14 utter-ances from the raw Weibo corpus. We also filtrate conversation sessions with less than 2 turns for training multi-turn conversation systems. We use a sliding window with a size of 3 to construct each dialogue session and there are 3 utterances in each dialogue session. By doing so, there are 336,342 conversation sessions in the cleaned corpus. We remove emojis in utterances and utilize NLTK for tokenization. Then, we randomly split the Weibo corpus into 335,342/5,000/5,000 sessions as train-ing/validation/testing sets. For each session, the last utterance is the target response for generation while other utterances are treated as context."
  },
  {
    "id": 40,
    "name": "word2vec vectors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/tmikolov/word2vec",
    "section_title": "4 Experiments 4.3 Settings",
    "add_info": "3 https://github.com/tmikolov/word2vec",
    "text": "The dimension of word embeddings is set to 200, which is initialized with pre-trained word2vec vectors [Cite_Footnote_3] . The vocabulary is comprised of the most frequent 31,000 words. The sentence encoder and the context encoder in our PersonaWAE model are two bi-directional RNN with the GRU cells, re-spectively. The decoder consists of a one-layer RNN with GRUs. The hidden state sizes of both GRU encoder and decoder are set to 256. Each user is allocated a user-level vector representation with dimension size 512. We set the mini-batch size to 100. The SGD optimizer is used to train the autoencoder module with the initial learning rate 1.0, and the learning rate decay strategy is employed. We use RMSprop optimizer (Hinton et al., 2012) to update the parameters of the gener-ator and the discriminator, where the initial learn-ing rates are set to 5e-5 and 1e-5, respectively. The gradient penalty is used for training discriminator (Gulrajani et al., 2017). The value of \u03c4 in Gumbel softmax is set to 0.1."
  },
  {
    "id": 41,
    "name": "smoothing 7",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "smoothing techniques"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.nltk.org/_modules/nltk/translate/bleu_score.html",
    "section_title": "4 Experiments 4.4 Evaluation Metrics",
    "add_info": "4 http://www.nltk.org/_modules/nltk/translate/bleu_score.html",
    "text": "Overlap-based Metric. We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated re-sponse. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [Cite_Footnote_4] . For each testing context, we sam-ple 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEU-Precision), n-gram recall (BLEU-Recall), and n-gram F1 (BLEU-F1)."
  },
  {
    "id": 42,
    "name": "Stan-ford Parser",
    "fullname": "N/A",
    "genericmention": [
      "the parser",
      "The parser"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/lex-parser.shtml",
    "section_title": "4 Experimentation 4.2 Experimental Settings",
    "add_info": "2 http://nlp.stanford.edu/software/lex-parser.shtml",
    "text": "In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [Cite_Footnote_2] . Specially, we train the parser on the GENIA Treebank 1.0 (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0."
  },
  {
    "id": 43,
    "name": "GENIA Treebank 1.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the GENIA Treebank 1.0 [Cite_Footnote_3] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus"
    ],
    "citationtag": [
      "Tateisi et al., 2005"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.geniaproject.org/genia-corpus/treebank",
    "section_title": "4 Experimentation 4.2 Experimental Settings",
    "add_info": "3 http://www.geniaproject.org/genia-corpus/treebank",
    "text": "In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser . Specially, we train the parser on the GENIA Treebank 1.0 [Cite_Footnote_3] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0."
  },
  {
    "id": 44,
    "name": "Word2Vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Mikolov et al., 2013"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "4 Experimentation 4.2 Experimental Settings",
    "add_info": "4 https://code.google.com/archive/p/word2vec/",
    "text": "For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, \u03bb=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [Cite_Footnote_4] (Mikolov et al., 2013)."
  },
  {
    "id": 45,
    "name": "CNN-based models",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu/",
    "section_title": "4 Experimentation 4.3 Experimental Results on Abstracts",
    "add_info": null,
    "text": "Table 2 illustrates that the performance of spec-ulation scope detection is higher than that of nega-tion (Best PCS: 85.75% vs 77.14%). It is mainly attributed to the shorter scopes of negation cues. Under the circumstances that the average length of negation sentences is almost as long as that of speculation ones (29.28 vs 29.77), shorter negation scopes mean that more tokens do not belong to the scopes, indicating more negative instances. The imbalance between positive and negative instances has negative effects on both the baseline and the 5  http://mallet.cs.umass.edu/ CNN-based models for negation scope detection."
  },
  {
    "id": 46,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code and trained models",
      "our model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/doug919/entity_based_narrative_graph",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/doug919/entity_based_narrative_graph",
    "text": "The evaluated downstream tasks include two challenging narrative analysis tasks, predicting characters\u2019 psychological states (Rashkin et al., 2018) and desire fulfilment (Rahimtoroghi et al., 2017). Results show that our model can outperform competitive transformer-based representations of the narrative text, suggesting that explicitly model-ing the relational structure of entities and events is beneficial. Our code and trained models are pub-licly available [Cite_Footnote_1] ."
  },
  {
    "id": 47,
    "name": "Twitter APIs",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://dev.twitter.com/",
    "section_title": "3 Experiments and Evaluations",
    "add_info": "3 https://dev.twitter.com/",
    "text": "The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [Cite_Footnote_3] . The statistics are shown in Table 1."
  },
  {
    "id": 48,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our source codes"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/luyaojie/text2event",
    "section_title": "1 Introduction",
    "add_info": "1 Our source codes are openly available at https://github.com/luyaojie/text2event",
    "text": "We conducted experiments [Cite_Footnote_1] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings. In summary, the contri-butions are as follows:"
  },
  {
    "id": 49,
    "name": "Wikipedia snapshot",
    "fullname": "N/A",
    "genericmention": [
      "The snapshot"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://archive.org/download/",
    "section_title": "3 Analysis Setup 3.1 Experiment Procedure",
    "add_info": "1 The snapshot is available at https://archive.org/download/enwiki-20181220. Wikipedia is licensed under CC BY-SA 3.0.",
    "text": "Our goal is to analyze the influence in downstream task performance brought by different masking policies g(.;\u03c6) during intermediate pre-training. Towards this goal, we ensure that the only vari-able is the masking policy, while all other aspects are controlled, so that the downstream performance reveal the influence we aim to study. We first initial-ize with a BART-base model (Lewis et al., 2020); then for each masking policy, we conduct experi-ments following a two-stage pipeline: Stage 1. Intermediate Pre-training. We per-form intermediate pre-training with a given mask-ing policy g(.; \u03c6). All intermediate pre-training is done with input sequence length of 128, batch size of 2048, learning rate of 0.0001, up to a total num-ber of 100, 000 updates, using Wikipedia snapshot from December 20, 2018 [Cite_Footnote_1] ."
  },
  {
    "id": 50,
    "name": "Cc-news",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sebastian Nagel. 2016",
      "Nagel, 2016"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://commoncrawl.org/2016/10/newsdatasetavailable",
    "section_title": "mixture of corpus 5 used to pre-train BART.",
    "add_info": "Sebastian Nagel. 2016. Cc-news. URL: http://web.archive. org/save/http://commoncrawl.org/2016/10/newsdatasetavailable.",
    "text": "5 Similar to RoBERTa, BART uses the combination of BookCorpus (Zhu et al., 2015), CC-News (Nagel, 2016)  , OpenWebText (Gokaslan and Cohen, 2019), and Stories (Trinh and Le, 2018) as pre-training corpus."
  },
  {
    "id": 51,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "closed-book QA datasets"
    ],
    "description": [
      "closed-book QA datasets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py",
    "section_title": "D Reproducibility D.1 Dataset Details",
    "add_info": null,
    "text": "We obtain closed-book QA datasets from  https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set."
  },
  {
    "id": 52,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "knowledge-intensive language tasks"
    ],
    "description": [
      "knowledge-intensive language tasks"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/facebookresearch/KILT/blob/master/scripts/donwload",
    "section_title": "D Reproducibility D.1 Dataset Details",
    "add_info": null,
    "text": "We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from  https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set."
  },
  {
    "id": 53,
    "name": "hug-gingface datasets",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://huggingface.co/datasets",
    "section_title": "D Reproducibility D.1 Dataset Details",
    "add_info": null,
    "text": "We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (  https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set."
  },
  {
    "id": 54,
    "name": "TextAttack",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a popular suite of NLP adversarial attacks"
    ],
    "citationtag": [
      "Morris et al., 2020"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py",
    "section_title": "3 Experiments",
    "add_info": "1 https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py",
    "text": "We present complete effectiveness graphs and details of human evaluation in Appendix B and C. BAE is implemented [Cite_Footnote_1] in TextAttack (Morris et al., 2020), a popular suite of NLP adversarial attacks."
  },
  {
    "id": 55,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "A dataset for classifying a sentence as objective or subjective"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences",
    "section_title": "A Experimental Reproducibility",
    "add_info": "2 https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences",
    "text": "\u2022 SUBJ: A dataset for classifying a sentence as objective or subjective. [Cite_Footnote_2]"
  },
  {
    "id": 56,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "A movie reviews dataset"
    ],
    "description": [
      "A movie reviews dataset based on sub-jective rating and sentiment polarity"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.cs.cornell.edu/people/pabo/movie-review-data/",
    "section_title": "A Experimental Reproducibility",
    "add_info": "3 https://www.cs.cornell.edu/people/pabo/movie-review-data/",
    "text": "\u2022 MR: A movie reviews dataset based on sub-jective rating and sentiment polarity [Cite_Footnote_3] ."
  },
  {
    "id": 57,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "An unbalanced dataset"
    ],
    "description": [
      "An unbalanced dataset for polarity detection of opinions"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mpqa.cs.pitt.edu/",
    "section_title": "A Experimental Reproducibility",
    "add_info": "4 http://mpqa.cs.pitt.edu/",
    "text": "\u2022 MPQA: An unbalanced dataset for polarity detection of opinions [Cite_Footnote_4] ."
  },
  {
    "id": 58,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "A dataset for classifying types of ques-tions with 6 classes"
    ],
    "description": [
      "A dataset for classifying types of ques-tions with 6 classes"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://cogcomp.org/Data/QA/QC/",
    "section_title": "A Experimental Reproducibility",
    "add_info": "5 http://cogcomp.org/Data/QA/QC/",
    "text": "\u2022 TREC: A dataset for classifying types of ques-tions with 6 classes [Cite_Footnote_5] ."
  },
  {
    "id": 59,
    "name": "SPRL",
    "fullname": "N/A",
    "genericmention": [
      "our model\u2019",
      "the proposed model",
      "Implementation"
    ],
    "description": [
      "an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/decomp-sem/neural-sprl",
    "section_title": "1 Introduction",
    "add_info": "2 Implementation available at https://github.com/decomp-sem/neural-sprl.",
    "text": "Figure 1 : BiLSTM sentence encoder with SPR de-coder. Semantic proto-role labeling is with respect to a specific predicate and argument within a sen-tence, so the decoder receives the two correspond-ing hidden states. achieves state-of-the-art performance for SPRL. [Cite_Footnote_2] As depicted in Figure 1, our model\u2019s architecture is an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training. We include a thorough quanti-tative analysis highlighting the contrasting errors between the proposed model and previous (non-neural) state-of-the-art."
  },
  {
    "id": 60,
    "name": "Fr-En corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Callison-Burch et al., 2009"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 \u201cNeural-Davidsonian\u201d Model",
    "add_info": "10 300-dimensional, uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/; 15,533 out-of-vocabulary words across all datasets were assigned a random embedding (uniformly from [\u2212 01 01]).. , . Embeddings remained fixed during training.",
    "text": "There are a few noteworthy differences between our neural model and the CRF of prior work. As an adapted BiLSTM, our model easily ex- 2017) trained on the [Cite_Footnote_10] Fr-En corpus (Callison-Burch et al., 2009) (Appendix A). ploits the benefits of large-scale pretraining, in the form of GloVe embeddings and MT pretrain-ing, both absent in the CRF. Ablation experiments (Appendix A) show the advantages conferred by these features. In contrast, the discrete-featured CRF model makes use of gold dependency labels, as well as joint modeling of SPR attribute pairs with explicit joint factors, both absent in our neu-ral model. Future SPRL work could explore the use of models like the LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016) to combine the advan-tages of both paradigms."
  },
  {
    "id": 61,
    "name": "SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/EducationalTestingService/skll",
    "section_title": "3 Models for Short Answer Scoring",
    "add_info": "2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.",
    "text": "Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Scho\u0308lkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [Cite_Footnote_2]"
  },
  {
    "id": 62,
    "name": "PropBank",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://verbs.colorado.edu/\u02dcmpalmer/projects/ace.html",
    "section_title": "3 Models for Short Answer Scoring 3.1 Response-based",
    "add_info": "3 http://verbs.colorado.edu/\u02dcmpalmer/projects/ace.html",
    "text": "\u2022 semantic roles in the form of PropBank [Cite_Footnote_3] style (e.g. say.01-A0-boy for \u201c(the) boy said\u201d)"
  },
  {
    "id": 63,
    "name": "ClearNLP parser",
    "fullname": "N/A",
    "genericmention": [
      "the parser"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.clearnlp.com,v2.0.2",
    "section_title": "3 Models for Short Answer Scoring 3.1 Response-based",
    "add_info": "4 http://www.clearnlp.com,v2.0.2",
    "text": "The syntactic and semantic features were extracted using the ClearNLP parser. [Cite_Footnote_4] We used the default models and options for the parser. We treat this model as a strong baseline to which we will add reference-based features."
  },
  {
    "id": 64,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset"
    ],
    "description": [
      "The dataset contains 2500 similarity judgements, provided by 25 participants"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt",
    "section_title": "5 Evaluation 5.1 Methodology",
    "add_info": "6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt",
    "text": "In order to evaluate the performance of our tensor-based factorization model of compositionality, we make use of the sentence similarity task for transi-tive sentences, defined in Grefenstette and Sadrzadeh (2011a). This is an extension of the similarity task for compositional models developed by Mitchell and Lapata (2008), and constructed according to the same guidelines. The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [Cite_Footnote_6]"
  },
  {
    "id": 65,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "official implementation [Cite_Footnote_3] of TDA"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/marziehf/DataAugmentationNMT",
    "section_title": "5 Experiments 5.5 Translation Result",
    "add_info": "3 https://github.com/marziehf/DataAugmentationNMT",
    "text": "We compare TCWR with six baselines, Word-Dropout, BPEDropout, SwitchOut, SCDA, TDA and DADA. For WordDropout and BPEDropout, we perform a range search on its dropout proba-bility from 0 to 1 and select the best one on de-velopment sets. Similarly, we choose the temper-ature with the highest score on development sets for SwitchOut. For SCDA, we search the replacing probability and set it to 0.15. We follow the official implementation [Cite_Footnote_3] of TDA. We reuse the hyperpa-rameters from Cheng et al. (2019) for DADA."
  },
  {
    "id": 66,
    "name": "VNTC",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/duyvuleo/VNTC",
    "section_title": "5 Experiments 5.6 Backtranslation Result",
    "add_info": "4 https://github.com/duyvuleo/VNTC",
    "text": "As backtranslation is a widely-used data augmenta-tion method by utilizing monolingual data to gener-ate new parallel pairs, we show how TCWR can be used with backtranslation. To perform backtransla-tion, we use the monolingual sequences from News Crawl 2017, News Crawl 2010 and VNTC [Cite_Footnote_4] for En-Tr, En-De and En-Vi, respectively. Then we per-form data augmentation on both training data and backtranslated data. As shown in Table 5, TCWR improves upon backtranslation, demonstrating that TCWR and backtranslation are not mutually exclu-sive, and TCWR can enhance the performance of backtranslation."
  },
  {
    "id": 67,
    "name": "Jieba",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "We use Jieba for segmentation",
      "a Chinese word segmentation system"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/fxsjy/jieba",
    "section_title": "3 Embeddings for Chinese Text",
    "add_info": "7 We use Jieba for segmentation: https://github.com/fxsjy/jieba",
    "text": "Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation system [Cite_Footnote_7] over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word. Since the NER system tags char-acters, we add the same word embedding features to each character in the word."
  },
  {
    "id": 68,
    "name": "CNN/Daily Mail dataset",
    "fullname": "N/A",
    "genericmention": [
      "the source documents and sum-mary sentences"
    ],
    "description": [
      "a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs"
    ],
    "citationtag": [
      "Her-mann et al., 2015"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/abisee/cnn-dailymail",
    "section_title": "4 Experiments 4.1 Dataset",
    "add_info": "1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail",
    "text": "We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [Cite_Footnote_1] and fairseq ."
  },
  {
    "id": 69,
    "name": "fairseq",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "section_title": "4 Experiments 4.1 Dataset",
    "add_info": "2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "text": "We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [Cite_Footnote_2] ."
  },
  {
    "id": 70,
    "name": "files2rouge",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/pltrdy/files2rouge",
    "section_title": "4 Experiments [Gold Summary]",
    "add_info": "3 files2rouge usage : https://github.com/pltrdy/files2rouge",
    "text": "About a dozen Native American actors walk off set of Adam Sandler comedy, says report . Actors say satirical Western\u2019s script is insulting to Native Americans and women . used files2rouge [Cite_Footnote_3] . Hie-BART was compared with LEAD-3 (Nallapati et al., 2017), PTGEN, PT-GEN+COV (See et al., 2017), B ERT S UM E XT A BS (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART with our environment, and BART with Lewis et al. (2020). The LEAD-3 method uses the first three sentences of the source document as a summary. PTGEN is a sequence-to-sequence model that incorporates a pointer generator net-work. PTGEN+COV introduces the coverage mechanism into PTGEN. B ERT S UM E XT A BS is a pre-training model that adapts BERT for sum-marization tasks. T5 is a generalized pre-training model for sequence-to-sequence tasks based on the Transformer model. The statistical signifi-cance test was performed by the Wilcoxon-Mann-Whitney test. In Table 1, * and ** indicate that the comparisons with BART (ours) are statistically significant at 5% significance level and 10% sig-nificance level, respectively."
  },
  {
    "id": 71,
    "name": "IMN",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "state-of-the-art baselines for extracting targets and expressions and predicting the polarity"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/ruidan/IMN-E2E-ABSA",
    "section_title": "4 Modeling 4.3 Baselines",
    "add_info": "6 IMN code available at https://github.com/ruidan/IMN-E2E-ABSA.",
    "text": "We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN [Cite_Footnote_6] , RACL , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models. Note, however, that our model does not update the mBERT representa-tions, putting it at a disadvantage to RACL-BERT. We also compare with previously reported extrac-tion results from Barnes et al. (2018) and \u00d8vrelid et al. (2020)."
  },
  {
    "id": 72,
    "name": "RACL",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "state-of-the-art baselines for extracting targets and expressions and predicting the polarity"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/NLPWM-WHU/RACL",
    "section_title": "4 Modeling 4.3 Baselines",
    "add_info": "7 https://github.com/NLPWM-WHU/RACL.",
    "text": "We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN , RACL [Cite_Footnote_7] , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models. Note, however, that our model does not update the mBERT representa-tions, putting it at a disadvantage to RACL-BERT. We also compare with previously reported extrac-tion results from Barnes et al. (2018) and \u00d8vrelid et al. (2020)."
  },
  {
    "id": 73,
    "name": "word2vec skip-gram embeddings",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "skip-gram embeddings",
      "300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58)."
    ],
    "citationtag": [
      "Fares et al., 2017"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://vectors.nlpl.eu/repository/",
    "section_title": "6 Experiments",
    "add_info": "8 Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58).",
    "text": "All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [Cite_Footnote_8] (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set (Targeted F 1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Ap-pendix) and run all of our models five times with different random seeds and report the mean (stan-dard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference be-tween the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3 shows the results for all datasets."
  },
  {
    "id": 74,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Model implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/jerbarnes/sentiment_graphs/src",
    "section_title": "8 Conclusion",
    "add_info": null,
    "text": "The computations were performed on resources scheme (y-axis) on the evaluation metrics (x-axis) for MultiB EU . percentage points for MultiB CA . GPU Infrastructure NVIDIA P100, 16 GiB RAM CPU Infrastructure Intel Xeon-Gold 6138 2.0 GHz Training duration 00:31:43 (MultiB EU ) \u2013 07:40:54 (NoReC Fine ) Model implementation  https://github.com/jerbarnes/sentiment_graphs/src Hyperparameter Best assignment embedding Word2Vec SkipGram 100D contexualized embedding mBERT embeddings trainable False number of epochs 100 batch size 50 beta1 0 beta2 0.95 l2 3e-09 hidden lstm 200 hidden char lstm 100 layers lstm 3 dim mlp 200 dim embedding 100 dim char embedding 80 early stopping 0 pos style xpos attention bilinear model interpolation 0.5 loss interpolation 0.025 lstm implementation drop connect char implementation convolved emb dropout type replace bridge dpa+ dropout embedding 0.2 dropout edge 0.2 dropout label 0.3 dropout main recurrent 0.2 dropout recurrent char 0.3 dropout main ff 0.4 dropout char ff 0.3 dropout char linear 0.3"
  },
  {
    "id": 75,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "rele-vance scale",
      "all relevance grades"
    ],
    "description": [
      "a five-level graded rele-vance scale (perfect, excellent, good, fair, bad)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/nickvosk/acl2015-",
    "section_title": "5 Experimental setup 5.1 Dataset",
    "add_info": "5 https://github.com/nickvosk/acl2015- dataset-learning-to-explain-entity- relationships",
    "text": "Five human annotators provided relevance judg-ments, manually judging sentences based on how well they describe the relationship for an entity pair, for which we use a five-level graded rele-vance scale (perfect, excellent, good, fair, bad). [Cite_Footnote_5] Of all relevance grades 8.1% is perfect, 15.69% excellent, 19.98% good, 8.05% fair, and 48.15% bad. Out of 1 476 entity pairs, 1 093 have at least one sentence annotated as fair. As is common in information retrieval evaluation, we discard entity pairs that have only \u201cbad\u201d sentences. We examine the difficulty of the task for human annotators by measuring inter-annotator agreement on a subset of 105 sentences that are judged by 3 annotators. Fleiss\u2019 kappa is k = 0.449, which is considered to be moderate agreement."
  },
  {
    "id": 76,
    "name": "Dynet code",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "The Dynet code for differentiable dynamic programming"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/FilippoC/diffdp",
    "section_title": "1 Introduction",
    "add_info": "1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.",
    "text": "We study properties of our approach on a syn-thetic structure induction task and experiment on sentiment classification (Socher et al., 2013) and natural language inference (Bowman et al., 2015). Our experiments confirm that the structural bias encoded in our approach is beneficial. For ex-ample, our approach achieves a 4.9% improve-ment on multi-genre natural language inference (MultiNLI) over a structure-agnostic baseline. We show that stochastisticity and higher-order statis-tics given by the global inference are both impor-tant. In ablation experiments, we also observe that forcing the structures to be projective dependency trees rather than permitting any general graphs yields substantial improvements without sacrific-ing execution time. This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [Cite_Footnote_1] Our main contributions can be summarized as follows:"
  },
  {
    "id": 77,
    "name": "cQA pipeline",
    "fullname": "N/A",
    "genericmention": [
      "The pipeline"
    ],
    "description": [
      "The pipeline is able to process natural language texts and metadata information associated with them and offers three main func-tionalities"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/QAML/S3QACoreFramework",
    "section_title": "5 Software Package",
    "add_info": "1 https://github.com/QAML/S3QACoreFramework",
    "text": "Our cQA pipeline is available for download [Cite_Footnote_1] and is distributed under the terms of the Apache 2.0 Li-cense. By taking advantage of the Apache Maven project management tool, most dependencies are automatically handled. The only exception is the UIMA framework toolkit. Still, its installation is straightforward. The pipeline is able to process natural language texts and metadata information associated with them and offers three main func-tionalities:"
  },
  {
    "id": 78,
    "name": "KeLP",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "learning algorithms on vectorial or structured data",
      "KeLP allows to apply a growing number of kernel-based algo-rithms and kernel functions to perform unsuper-vised, online and batch supervised kernel meth-ods."
    ],
    "citationtag": [
      "Filice et al., 2018"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.kelp-ml.org",
    "section_title": "5 Software Package",
    "add_info": "2 http://www.kelp-ml.org",
    "text": "Learning and classification allow to apply a variety of learning algorithms on vectorial or structured data. Currently KeLP (Filice et al., 2018) [Cite_Footnote_2] is integrated in the pipeline. KeLP allows to apply a growing number of kernel-based algo-rithms and kernel functions to perform unsuper-vised, online and batch supervised kernel meth-ods. We opt for integrating KeLP because the kernel-based cQA systems relying on it perform at state-of-the-art level (see Section 2). Our pipeline is able to reproduce the state-of-the-art models for SemEval cQA tasks 3-A and 3-B."
  },
  {
    "id": 79,
    "name": "MDN-VQG model",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://badripatro.github.io/MDN-VQG/",
    "section_title": "4 Method 4.3 Cost function",
    "add_info": "1 The project page for MDN-VQG Model is https://badripatro.github.io/MDN-VQG/",
    "text": "Our objective is to minimize the total loss, that is the sum of cross entropy loss and triplet loss over all training examples. The total loss is: where M is the total number of samples,\u03b3 is a con-stant, which controls both the loss. L triplet is the triplet loss function 5. L cross is the cross entropy loss between the predicted and ground truth ques-tions and is given by: where, N is the total number of question tokens, y t is the ground truth label. The code for MDN-VQG model is provided [Cite_Footnote_1] ."
  },
  {
    "id": 80,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a regularly trained Chinese parser",
      "the English and Chinese parsers"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Petrov and Klein (2007)"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://nlp.cs.berkeley.edu",
    "section_title": "6 Statistical Parsing Experiments",
    "add_info": "5 Available at http://nlp.cs.berkeley.edu.",
    "text": "We used the English and Chinese parsers in Petrov and Klein (2007) [Cite_Footnote_5] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270). This modified grammar was used to generate the k-best lists that we trained our model on. However, as we tested on the same set of articles used for monolingual Chi-nese parser evaluation, there was no need to use a modified grammar to generate k-best lists at test time, and so we used a regularly trained Chinese parser for this purpose."
  },
  {
    "id": 81,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the word aligner"
    ],
    "description": [
      "the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [Cite_Footnote_6] , trained on approxi-mately 1.7 million sentence pairs"
    ],
    "citationtag": [
      "Liang et al. (2006) and DeNero and Klein (2007)"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://nlp.cs.berkeley.edu",
    "section_title": "6 Statistical Parsing Experiments",
    "add_info": "6 Available at http://nlp.cs.berkeley.edu.",
    "text": "Posterior word alignment probabilities were ob-tained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [Cite_Footnote_6] , trained on approxi-mately 1.7 million sentence pairs. For our alignment model we used an HMM in each direction, trained to agree (Liang et al., 2006), and we combined the pos-teriors using DeNero and Klein\u2019s (2007) soft union method."
  },
  {
    "id": 82,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the [Cite_Footnote_2] million pre-processed translation pairs"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.phontron.com/kytea/",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "2 http://www.phontron.com/kytea/",
    "text": "Preprocessings For Japanese sentences, we per-formed tokenization using KyTea 0.4.7 2 (Neu-big et al., 2011). Then we performed bunsetsu-chunking with J.DepP 2015.10.05 3 (Yoshinaga and Kitsuregawa, 2009, 2010, 2014). Special end-of-chunk tokens were inserted at the end of the chunks. Our word-level decoders described in \u00a7 will stop generating words after each end-of-chunk token. For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the [Cite_Footnote_2] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data."
  },
  {
    "id": 83,
    "name": "MGIZA++ 0.7.0 [Cite_Footnote_5] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "word alignment tool"
    ],
    "citationtag": [
      "Och and Ney, 2003; Gao and Vogel, 2008"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mgiza",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "5 https://github.com/moses-smt/mgiza",
    "text": "Postprocessing To perform unknown word re-placement (Luong et al., 2015a), we built a bilin-gual English-Japanese dictionary from all of the three million translation pairs. The dictionary was extracted with the MGIZA++ 0.7.0 [Cite_Footnote_5] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words."
  },
  {
    "id": 84,
    "name": "Moses 2.1.1",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Koehn et al., 2007"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.statmt.org/moses/",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "6 http://www.statmt.org/moses/",
    "text": "Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 [Cite_Footnote_6] (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 (Isozaki et al., 2010). Follow-ing Cho et al. (2014a), we performed beam search with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set dur-ing training and used them to evaluate the systems with the test set."
  },
  {
    "id": 85,
    "name": "RIBES.py 1.03.1",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Isozaki et al., 2010"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "7 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html",
    "text": "Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [Cite_Footnote_7] (Isozaki et al., 2010). Follow-ing Cho et al. (2014a), we performed beam search with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set dur-ing training and used them to evaluate the systems with the test set."
  },
  {
    "id": 86,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the exam-ple configuration for a transformer model from Marian",
      "This configuration"
    ],
    "description": [
      "This configuration utilises a six-layer deep encoder and decoder, learning rate warm-up and tied em-beddings for source, target and output layer."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/marian-nmt/marian-examples/blob/master/transformer",
    "section_title": "4 Experiment setup 4.3 Training setup",
    "add_info": "2 https://github.com/marian-nmt/marian-examples/blob/master/transformer",
    "text": "Transformer We used the same training, dev and test sets as in the big RNN model, and the exam-ple configuration for a transformer model from Marian [Cite_Footnote_2] adapted to our needs as shown in Table 1. This configuration utilises a six-layer deep encoder and decoder, learning rate warm-up and tied em-beddings for source, target and output layer. As suggested by Karita et al. (2019), we increased the minibatch size for the transformer model from 5,000 to 10,000."
  },
  {
    "id": 87,
    "name": "MeCab",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "We used MeCab [Cite_Footnote_10] to tokenize the feedback comments."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://taku910.github.io/mecab/",
    "section_title": "6 Evaluation 6.1 Conditions and Procedures",
    "add_info": "10 http://taku910.github.io/mecab/",
    "text": "We implemented and trained the baseline meth-ods with the created dataset. We first obtain word embeddings for learner sentences from the cor-pora as shown in Appendix A. We also used the word embeddings for English words in the LSTMLMs to encode feedback comments. For the rest (i.e., Japanese words), we initialized them using random-valued vectors. We used MeCab [Cite_Footnote_10] to tokenize the feedback comments. With these word embeddings, we trained the networks on the training set of the respective subsets (PART-TIME JOB and SMOKING). We implemented the case frame-based method with the following cor-pora: British National Corpus (BNC) (Burnard, 1995), the EDR corpus (Japan electronic dictio-nary research institute Ltd, 1993) as a native cor-pus and the training and development set of the corresponding dataset as a learner corpus. As a result, we obtained two versions of each method. We determined the hyperparameters by using the corresponding development set 12 . We tested the resulting models on the corresponding test set."
  },
  {
    "id": 88,
    "name": "CEEEJUS",
    "fullname": "Corpus of English Essays Written by Japanese University Students",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://language.sakura.ne.jp/s/doc/projects/CEEAUS.pdf",
    "section_title": "References",
    "add_info": "15 http://language.sakura.ne.jp/s/doc/projects/CEEAUS.pdf",
    "text": "Learner corpora: Corpus of English Essays Written by Japanese University Students (CEEEJUS) [Cite_Footnote_15] , ETS Corpus of non-native written English (Daniel Blanchard et al., 2014), The International Corpus of Learner English (ICLE) (Granger, 1993), Cambridge Learner Corpus (CLC) First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and Nagoya Interlanguage Corpus of English (NICE) (Sug-iura et al., 2007). Native corpus: English Web Treebank (EWT) (Bies, Ann, et al., 2012)."
  },
  {
    "id": 89,
    "name": "Project Gutenberg (English)",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.gutenberg.org",
    "section_title": "2 A Parallel Corpus of Literary Texts 2.1 Data Selection",
    "add_info": "1 http://www.gutenberg.org and http://gutenberg.spiegel.de/",
    "text": "We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1] Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align."
  },
  {
    "id": 90,
    "name": "Project Gutenberg-DE (German)",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://gutenberg.spiegel.de/",
    "section_title": "2 A Parallel Corpus of Literary Texts 2.1 Data Selection",
    "add_info": "1 http://www.gutenberg.org and http://gutenberg.spiegel.de/",
    "text": "We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1] Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align."
  },
  {
    "id": 91,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The corpus"
    ],
    "description": [
      "Choice of English units to label.",
      "On the German side, we assign the T/V labels to pronouns, and the most straightforward way of setting up annotation projection would be to label their word-aligned En-glish pronouns as T/V.",
      "we decided to treat complete sentences as either T or V",
      "English sentences can receive conflicting labels, if a German sentence con-tains both a T and a V label",
      "of the 76K German sentences with T or V pronouns, only 515, or less than 1%, contain both",
      "A Parallel Corpus of Literary Texts",
      "Our projection on the English side results in 53K V and 35K T sentences"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.nlpado.de/~sebastian/data.shtml",
    "section_title": "2 A Parallel Corpus of Literary Texts 2.3 T/V Gold Labels for English Utterances",
    "add_info": "4 The corpus can be downloaded for research purposes from http://www.nlpado.de/~sebastian/data.shtml.",
    "text": "Choice of English units to label. On the German side, we assign the T/V labels to pronouns, and the most straightforward way of setting up annotation projection would be to label their word-aligned En-glish pronouns as T/V. However, pronouns are not necessarily translated into pronouns; additionally, we found word alignment accuracy for pronouns, as a function of word class, to be far from perfect. For these reasons, we decided to treat complete sentences as either T or V. This means that sentence alignment is sufficient for projection, but English sentences can receive conflicting labels, if a German sentence con-tains both a T and a V label. However, this occurs very rarely: of the 76K German sentences with T or V pronouns, only 515, or less than 1%, contain both. Our projection on the English side results in 53K V and 35K T sentences, of which 731 are labeled as both T and V. Finally, from the English labeled sentences we ex-tracted a training set with 72 novels (63K sentences) and a test set with 21 novels (15K sentences). [Cite_Footnote_4]"
  },
  {
    "id": 92,
    "name": "C2AE",
    "fullname": "N/A",
    "genericmention": [
      "its",
      "it",
      "its"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/dhruvramani/C2AE-Multilabel-Classification",
    "section_title": "2 The Proposed Method (Rank-AE) 2.3 L h and L ae Loss Functions",
    "add_info": "1 https://github.com/dhruvramani/C2AE-Multilabel-Classification",
    "text": "Reconstructing Output (L ae ). Unlike L h with small space, L ae loss usually involves a large num-ber of labels. Moreover, L ae also directly affects the classification performance significantly since different loss functions lead to their own proper-ties (Hajiabadi et al., 2017). Accordingly, solving such problems with large scale and desirable prop-erties presents open challenges in three aspects: 1) how to improve time efficiency, 2) how to produce comparable labels scores and 3) how to deal with noise labels. Unfortunately, most of the related deep learning methods only target one or two as-pects. C2AE attempts to minimize the number of misclassified pairs between relevant and irrelevant labels, as a result its computational complexity is quadratic with number of labels in the worst case; also it fails to scale well on large number of in-put features or labels due to its inefficient imple-mentation [Cite_Footnote_1] . XML-CNN (Liu et al., 2017) achieves computational efficiency by training a deep neural network with hidden layers much smaller than the output layer with binary cross-entropy loss (BCE), which has linear complexity in number of labels. Despite this, BCE loss could neither capture la-bel dependencies nor produce directly compara-ble label scores, since each label is treated inde-pendently. Moreover, BCE loss tends to be sensi-tive to label noise, which is frequently observed in XML data (Reed et al., 2014; Ghosh et al., 2017)."
  },
  {
    "id": 93,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "online movie database"
    ],
    "description": [
      "online movie database"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.imdb.com/",
    "section_title": "3 Experiments & Analysis 3.1 Dataset & Experiment Setup",
    "add_info": "2 https://www.imdb.com/",
    "text": "Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database [Cite_Footnote_2] and the rest five datasets are downloaded from the extreme classification repository . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set. For all datasets, we reserve another 20% of train-ing data as validation for tuning hyper-parameters. After tuning, all models are trained on the entire training set."
  },
  {
    "id": 94,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the extreme classification repository",
      "the repository"
    ],
    "description": [
      "the extreme classification repository"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://manikvarma.org/downloads/XC/XMLRepository.html",
    "section_title": "3 Experiments & Analysis 3.1 Dataset & Experiment Setup",
    "add_info": "3 http://manikvarma.org/downloads/XC/XMLRepository.html",
    "text": "Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [Cite_Footnote_3] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set. For all datasets, we reserve another 20% of train-ing data as validation for tuning hyper-parameters. After tuning, all models are trained on the entire training set."
  },
  {
    "id": 95,
    "name": "Glove",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a pre-trained word embed-dings of 100 dimensions"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Experiments & Analysis 3.1 Dataset & Experiment Setup",
    "add_info": "4 https://nlp.stanford.edu/projects/glove/",
    "text": "Hyper-parameters. In Rank-AE, we use the fixed neural network architecture, with two fully con-nected layers in both Encoder and Decoder, and one fully connected layer following Embedding & Atten network in Feature Embedding. We also fix most of the hyper-parameters, including hidden dimension h (100 for small number of la-bels data and 200 for large ones), word embed-ding size C = 100, and reduction ratio r = 4. The remaining hyper-parameters, such as balance \u03bb between L h and L ae , margin m in L ae , and oth-ers (decay, learning rate) in the optimization algo-rithms, are tuned on validation set. In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [Cite_Footnote_4] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed."
  },
  {
    "id": 96,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The visualization tool",
      "the visualization tool",
      "the visualiza-tion tool"
    ],
    "description": [
      "to highlight important words based on the attention output"
    ],
    "citationtag": [
      "Lin et al., 2017"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/kaushalshetty/Structured-Self-Attention",
    "section_title": "3 Experiments & Analysis 3.4 More Analysis in Rank-AE",
    "add_info": "5 The visualization tool is provided by https://github.com/kaushalshetty/Structured-Self-Attention",
    "text": ". employ the visualization tool (Lin et al., 2017) to highlight important words based on the attention output. Specifically, we run our method on IMDb dataset, wherein each instance is a movie story as-sociated with relevant genres as labels. Instead of extracting V 0 matrix using the proposed spatial-wise attention, we obtain a fixed size embeddings from a bidirectional LSTM on variable length of sentence, fed to our channel-attention network. Through the channel-attention network, we can observe the attention matrix A for each input doc-ument. By summing up the attention weights of each word embedding vector, we can visualize the overall attention for that word with the visualiza-tion tool [Cite_Footnote_5] . We randomly select three movies from IMDb testing set (See Figure 5). By looking at the highlighted regions, we can see that the pro-posed channel-attention is able to focus more on the words that are highly related to the topics."
  },
  {
    "id": 97,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Source code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/facebookresearch/Zero-Shot-DST",
    "section_title": "3 Methodology 3.1 T5DST",
    "add_info": "1 Source code is available in https://github.com/facebookresearch/Zero-Shot-DST",
    "text": "The design of our model follows the basis of gen-erative question answering models. As illustrated in Figure 1, given a dialogue history which con-sists of an alternating set of utterances from two speakers, denoted as C t = {U 1 , R 1 , . . . , R t\u22121 , U t }, we add the \"user:\" and \"system:\" prefixes to the user and system utterance respectively. Then all the utterances and slot names s i are concatenated into a single sequence, i.e., user:U [Cite_Footnote_1] . . .system:R t\u22121 user:U t [sep] s i . The sequence is used as the in-put to the encoder, and the decoder generates the corresponding slot value v i :"
  },
  {
    "id": 98,
    "name": "Mallet Toolkits",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu/",
    "section_title": "3 The Proposed Approach 3.1 Text Representation",
    "add_info": "3 http://mallet.cs.umass.edu/",
    "text": "Classification algorithm: The maximum en-tropy (ME) classifier is implemented with the public tool, Mallet Toolkits [Cite_Footnote_3] ."
  },
  {
    "id": 99,
    "name": "Common Crawl Web scrape",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://webdatacommons.org/webtables",
    "section_title": "A Pretraining Details A.1 Training Data",
    "add_info": "10 http://webdatacommons.org/webtables",
    "text": "WDC WebTable Corpus (Lehmberg et al., 2016) is a large collection of Web tables extracted from the Common Crawl Web scrape [Cite_Footnote_10] . We use its 2015 English-language relational subset, which consists of 50.8 million relational tables and their surrounding NL contexts."
  },
  {
    "id": 100,
    "name": "MSN News",
    "fullname": "N/A",
    "genericmention": [
      "The logs in the last week",
      "the rest"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.msn.com/en-us/news",
    "section_title": "3 Experiments 3.1 Datasets and Experimental Settings",
    "add_info": "3 https://www.msn.com/en-us/news",
    "text": "We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_3] logs in one month (Dec. 13, 2018 to Jan. 12, 2019). The detailed statistics are shown in Ta-ble 1. The logs in the last week were used for test, and the rest were used for training. We randomly sampled 10% of training data for validation."
  },
  {
    "id": 101,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Makwen1995/LDGNMLTC",
    "section_title": "3 Experiment 3.1 Experimental Setup",
    "add_info": "1 https://github.com/Makwen1995/LDGNMLTC",
    "text": "The word embeddings in the proposed network are initialized with the 300-dimensional word vec-tors, which are trained on the datasets by Skip-gram (Mikolov et al., 2013) algorithm. The hid-den sizes of Bi-LSTM and GCNs are set to 300 and 512, respectively. We use the Adam optimiza-tion method (Kingma and Ba, 2014) to minimize the cross-entropy loss, the learning rate is initial-ized to 1e-3 and gradually decreased during the process of training. We select the best parameter configuration based on performance on the valida-tion set and evaluate the configuration on the test set. Our code is available on GitHub [Cite_Footnote_1] ."
  },
  {
    "id": 102,
    "name": "CamRest dataset",
    "fullname": "N/A",
    "genericmention": [
      "The dataset",
      "the datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wen et al., 2017b"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/yizhen20133868/Retriever-Dialogue",
    "section_title": "4 Training the KB-Retriever 4.3 Experimental Settings",
    "add_info": "3 The dataset can be available at: https://github.com/yizhen20133868/Retriever-Dialogue",
    "text": "We choose the InCar Assistant dataset (Eric et al., 2017) including three distinct domains: naviga-tion, weather and calendar domain. For weather domain, we follow Wen et al. (2018) to separate the highest temperature, lowest temperature and weather attribute into three different columns. For calendar domain, there are some dialogues with-out a KB or incomplete KB. In this case, we padding a special token \u201c-\u201d in these incomplete KBs. Our framework is trained separately in these three domains, using the same train/validation/test split sets as Eric et al. (2017). To justify the gen-eralization of the proposed model, we also use an-other public CamRest dataset (Wen et al., 2017b) and partition the datasets into training, validation and testing set in the ratio 3:1:1. [Cite_Footnote_3] Especially, we hired some human experts to format the CamRest dataset by equipping the corresponding KB to ev-ery dialogues."
  },
  {
    "id": 103,
    "name": "Apple Dis-cussion forums",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://discussions.apple.com",
    "section_title": "5 Experimental Evaluation",
    "add_info": "10 http://discussions.apple.com",
    "text": "We use a crawl of 140k threads from Apple Dis-cussion forums [Cite_Footnote_10] . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement of 0.71. On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure 12 for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set \u03bb = 0.5 and \u03c4 = 0.4 unless other-wise mentioned. Since we have only 300 labeled threads, accuracy measures are reported on those (like in (Catherine et al., 2013)). We pre-process the post data by stemming words (Porter, 1980)."
  },
  {
    "id": 104,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the F-measure",
      "an F-measure of 53%"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://en.wikipedia.org/wiki/F1score",
    "section_title": "5 Experimental Evaluation",
    "add_info": "12 http://en.wikipedia.org/wiki/F1score",
    "text": "We use a crawl of 140k threads from Apple Dis-cussion forums 10 . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement 11 of 0.71. On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure [Cite_Footnote_12] for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set \u03bb = 0.5 and \u03c4 = 0.4 unless other-wise mentioned. Since we have only 300 labeled threads, accuracy measures are reported on those (like in (Catherine et al., 2013)). We pre-process the post data by stemming words (Porter, 1980)."
  },
  {
    "id": 105,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code of this paper"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/mrlyk423/relation_extraction",
    "section_title": "References",
    "add_info": null,
    "text": "Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns be-tween entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learn-ing, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allo-cation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as com-pared with baselines, our model achieves significant and consistent improvements on knowledge base completion and re-lation extraction from text. The source code of this paper can be obtained from  https://github.com/mrlyk423/relation_extraction."
  },
  {
    "id": 106,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an interface provided by a third party machine translation service"
    ],
    "description": [
      "to train a cus-tom MT engine for English to French translations"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://hub.microsofttranslator.com",
    "section_title": "1 Introduction",
    "add_info": "2 http://hub.microsofttranslator.com",
    "text": "To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [Cite_Footnote_2] to train a cus-tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida-tion process, first using crowd-sourced evaluations with Amazon\u2019s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior."
  },
  {
    "id": 107,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ElliottYan/Multi",
    "section_title": "References",
    "add_info": "1 Code is available at https://github.com/ElliottYan/Multi Unit Transformer",
    "text": "Transformer models (Vaswani et al., 2017) achieve remarkable success in Neural Ma-chine Translation. Many efforts have been de-voted to deepening the Transformer by stack-ing several units (i.e., a combination of Multi-head Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we pro-pose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use sev-eral parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit set-ting, we design biased module and sequen-tial dependency that guide and encourage com-plementariness among different units. Exper-imental results on three machine translation tasks, the NIST Chinese-to-English, WMT\u201914 English-to-German and WMT\u201918 Chinese-to- English, show that the MUTE models signif-icantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its ef-ficiency in both the inference process and pa-rameter usage. [Cite_Footnote_1]"
  },
  {
    "id": 108,
    "name": "multi-bleu.perl",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl",
    "section_title": "4 Experimental Settings",
    "add_info": "4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl",
    "text": "Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [Cite_Footnote_4] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention."
  },
  {
    "id": 109,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the statistical significance test"
    ],
    "description": [
      "the statistical significance test"
    ],
    "citationtag": [
      "Koehn, 2004"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/boots",
    "section_title": "4 Experimental Settings",
    "add_info": "5 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance. pl",
    "text": "Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [Cite_Footnote_5] for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention."
  },
  {
    "id": 110,
    "name": "Moses mteval-v13a.pl script",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl",
    "section_title": "4 Experimental Settings",
    "add_info": "6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl",
    "text": "Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [Cite_Footnote_6] . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention."
  },
  {
    "id": 111,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://spacy.io",
    "section_title": "A Appendices",
    "add_info": "9 Using https://spacy.io",
    "text": "\u2022 The number of occurrences of part-of-speech (PoS) tags from Penn-treebank [Cite_Footnote_9] ."
  },
  {
    "id": 112,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an 11.5 million words of user contributed comments",
      "This corpus"
    ],
    "description": [
      "an 11.5 million words of user contributed comments."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.cs.jhu.edu/~ozaidan/AOC",
    "section_title": "3.2. Experiments and Results",
    "add_info": "1 Available from http://www.cs.jhu.edu/~ozaidan/AOC",
    "text": "We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. [Cite_Footnote_1] This corpus is highly dialectal"
  },
  {
    "id": 113,
    "name": "ELMo",
    "fullname": "N/A",
    "genericmention": [
      "the released model"
    ],
    "description": [
      "effec-tive deep contextualized word representations"
    ],
    "citationtag": [
      "Peters et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md",
    "section_title": "4 Experiment 4.1 Settings",
    "add_info": "3 We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md",
    "text": "For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [Cite_Footnote_3] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations ."
  },
  {
    "id": 114,
    "name": "BERT",
    "fullname": "N/A",
    "genericmention": [
      "the released model",
      "The model"
    ],
    "description": [
      "effec-tive deep contextualized word representations",
      "The model uses character-based tokenization for Chinese"
    ],
    "citationtag": [
      "Devlin et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "4 Experiment 4.1 Settings",
    "add_info": "4 We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the first word piece as the whole word representation.",
    "text": "For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [Cite_Footnote_4] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations ."
  },
  {
    "id": 115,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/chenyangh/Seq2Emo",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/chenyangh/Seq2Emo",
    "text": "Multi-label emotion classification is an impor-tant task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval\u201918 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) ap-proaches in a fair setting. [Cite_Footnote_1]"
  },
  {
    "id": 116,
    "name": "SGM",
    "fullname": "N/A",
    "genericmention": [
      "its"
    ],
    "description": [
      "SGM (Yang et al., 2018), however, is a CC-based model for multi-label classification."
    ],
    "citationtag": [
      "Yang et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/lancopku/SGM",
    "section_title": "4 Experimental Setup",
    "add_info": "2 https://github.com/lancopku/SGM",
    "text": "Baselines. On SemEval\u201918, we compare our system with the top submissions from the SemEval-2018 competition and recent development. NTUA-SLP (Baziotis et al., 2018) uses large amount of external emotion-related data to pretrain an LSTM-based model. TCS Research\u2019s system (Meish-eri and Dey, 2018) uses the support vector ma-chine with mannually engineered features: out-put from LSTM models, emotion lexicons (Mo-hammad and Kiritchenko, 2015), and SentiNeural (Radford et al., 2017). PlusEmo2Vec (Park et al., 2018) combines neural network models, which are pretrained by using emojis as labels (Felbo et al., 2017). Apart from the competition, Yu et al. (2018) propose DATN, which introduces sentiment information through dual-attention. These afore-mentioned systems are based on the BR approach. SGM (Yang et al., 2018), however, is a CC-based model for multi-label classification. We include it as a baseline by using its publicly released code. [Cite_Footnote_2]"
  },
  {
    "id": 117,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Software for R TE R and M T +R TE R"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/mteval.shtml",
    "section_title": "4 Experimental Evaluation 4.4 Combination Metrics",
    "add_info": "3 Software for R TE R and M T +R TE R is available from http://nlp.stanford.edu/software/mteval.shtml.",
    "text": "M T +R TE R uses all M T R and R TE R features, combining matching and entailment evidence. [Cite_Footnote_3]"
  },
  {
    "id": 118,
    "name": "NIST OpenMT 2008 corpus",
    "fullname": "N/A",
    "genericmention": [
      "The corpus"
    ],
    "description": [
      "The corpus contains trans-lations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur)).",
      "Each language consists of 1500\u20132800 sen-tence pairs produced by 7\u201315 MT systems."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.nist.gov",
    "section_title": "5 Expt. 1: Predicting Absolute Scores",
    "add_info": "4 Available from http://www.nist.gov.",
    "text": "Data. Our first experiment evaluates the models we have proposed on a corpus with traditional an-notation on a seven-point scale, namely the NIST OpenMT 2008 corpus. [Cite_Footnote_4] The corpus contains trans-lations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur)). Each language consists of 1500\u20132800 sen-tence pairs produced by 7\u201315 MT systems."
  },
  {
    "id": 119,
    "name": "WMT 2006 and 2007",
    "fullname": "2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT)",
    "genericmention": [
      "It"
    ],
    "description": [
      "It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian)."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.statmt.org/",
    "section_title": "6 Expt. 2: Predicting Pairwise Preferences",
    "add_info": "7 Available from http://www.statmt.org/.",
    "text": "In this experiment, we predict human pairwise pref-erence judgments (cf. Section 4). We reuse the linear regression framework from Section 2 and predict pairwise preferences by predicting two ab-solute scores (as before) and comparing them. Data. This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [Cite_Footnote_7] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems). The test set is formed by the WMT 2008 relative rank annotation task. As in Experi-ment 1, we set \u03b5 so that the incidence of ties in the training and test set is equal (60%)."
  },
  {
    "id": 120,
    "name": "CatVar",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://clipdemos.umiacs.umd.edu/catvar/",
    "section_title": "3 Building the CatVar",
    "add_info": null,
    "text": "The CatVar is web-browseable at  http://clipdemos.umiacs.umd.edu/catvar/. Figure 2 shows the CatVar web-based interface with the hunger cluster as an example. The interface allows searching clusters using regular expressions as well as cluster length restrictions. The database is also available for researchers in perl/C and lisp searchable formats."
  },
  {
    "id": 121,
    "name": "N/A",
    "fullname": "Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases",
    "genericmention": [
      "each of these sources"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Dorr, 2001",
      "Bonnie J. Dorr. 2001. LCS Verb Database. Technical Report Online Software Database, University of Mary-land, College Park, MD."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.umiacs.umd.edu/\u02dcbonnie/LCS",
    "section_title": "3 Building the CatVar",
    "add_info": "Bonnie J. Dorr. 2001. LCS Verb Database. Technical Report Online Software Database, University of Mary-land, College Park, MD. http://www.umiacs.umd.edu/\u02dcbonnie/LCS Database Docmentation.html.",
    "text": "The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001)  , the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998), and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4"
  },
  {
    "id": 122,
    "name": "WordNet 1.6",
    "fullname": "N/A",
    "genericmention": [
      "each of these sources"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Fellbaum, 1998",
      "Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cogsci.princeton.edu/\u02dcwn",
    "section_title": "3 Building the CatVar",
    "add_info": "Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/\u02dcwn [2000, Septem-ber 7].",
    "text": "The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001), the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998)  , and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4"
  },
  {
    "id": 123,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/movie$-data",
    "section_title": "5 Conclusion",
    "add_info": null,
    "text": "We conclude that text features from pre-release re-views can substitute for and improve over a strong metadata-based first-weekend movie revenue pre-diction. The dataset used in this paper has been made available for research at  http://www.ark.cs.cmu.edu/movie$-data."
  },
  {
    "id": 124,
    "name": "TextBlob library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the TextBlob library [Cite_Footnote_1] , which provides a sim-ple API for common natural language processing tasks"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://pypi.org/project/textblob/",
    "section_title": "3 Concept and Implementation 3.1.1 Pre-Analysis",
    "add_info": "1 https://pypi.org/project/textblob/",
    "text": "We use PoS (Part of Speech) information as a way of capturing error types of current word-level QE systems. First, the MT output from the training set of the WMT 2019 QE shared task is PoS tagged using the TextBlob library [Cite_Footnote_1] , which provides a sim-ple API for common natural language processing tasks . The PoS-tagged MT is then fed into the \u201cQEBrain\u201d model (Wang et al., 2018) to generate quality predictions. This QE model was chosen be-cause it was the best performing system according to the assessment carried out by Shterionov et al. (2019). Next, we check how often each PoS is in-correctly labelled by comparing the QE output to the reference annotations. The probability of each PoS and the corresponding conditional error prob-ability (given as (P (P oS), P (error|P oS))) are: nouns are most often wrong (32%, 48%), followed by prepositions (10.9%, 15.6%), pronouns (8.69%, 14.8%), determiners (13.04%, 14.3%), conjunc-tions (7%, 13.9%), interjections (4.5%, 12.9%), verbs (28%, 9.6%), adjectives (4.34%, 7.9%) and adverbs (5%, 2.9%)."
  },
  {
    "id": 125,
    "name": "MMPE CAT tool",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Herbig et al., 2019, 2020a,b,c; Jamara et al., 2021"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/NicoHerbig/MMPE",
    "section_title": "3 Concept and Implementation 3.3 QE Integration into CAT Environment",
    "add_info": "4 https://github.com/NicoHerbig/MMPE",
    "text": "A Computer-Aided Translation (CAT) tool or PE environment allows the capture and correction of mistakes, as well as the selection, manipulation, adaptation and recombination of good segments (Herbig et al., 2020c). Our implementation was done within the MMPE CAT tool [Cite_Footnote_4] (Herbig et al., 2019, 2020a,b,c; Jamara et al., 2021), as it is open source and easily extendable."
  },
  {
    "id": 126,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The complete resource",
      "our dataset",
      "the dataset"
    ],
    "description": [
      "a multilingual dataset of nominal com-pounds containing human judgments about composi-tionality",
      "It contains 180 compounds for each of the 3 target languages: English, French and Portuguese.",
      "The resulting resource can be used for applications and tasks involving some degree of semantic processing, such as lexical substitution and text simplification.",
      "For the cases where the numerical judgments alone are not enough for a given task, our dataset also provides sets of paraphrases, which serve as a symbolic counterpart to those scores."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/?page=downloads/compounds",
    "section_title": "5 Conclusions and Future Work",
    "add_info": "3 http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/?page=downloads/compounds",
    "text": "We presented a multilingual dataset of nominal com-pounds containing human judgments about composi-tionality. It contains 180 compounds for each of the 3 target languages: English, French and Portuguese. An-notations are collected through crowdsourcing. Since the task is performed by native speakers who may not have a background in linguistics, it needs to be appro-priately constrained not to require expert knowledge. The resulting resource can be used for applications and tasks involving some degree of semantic processing, such as lexical substitution and text simplification. For the cases where the numerical judgments alone are not enough for a given task, our dataset also provides sets of paraphrases, which serve as a symbolic counterpart to those scores. The complete resource will be made freely available. [Cite_Footnote_3] As future work, we plan to validate these scores through compositionality prediction (Yaz-dani et al., 2015; Salehi et al., 2015) and by incorporat-ing the scores and paraphrases into a machine transla-tion system. We also envisage extending the dataset for each of the languages and for additional languages."
  },
  {
    "id": 127,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the 23 entail-ment graphs published by Berant et al."
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.cs.tau.ac.il/~jonatha6/homepage_files/resources/HealthcareGraphs.rar",
    "section_title": "5 Application to the Health-care Domain",
    "add_info": "2 http://www.cs.tau.ac.il/~jonatha6/homepage_files/resources/HealthcareGraphs.rar",
    "text": "For the entailment graph we used the 23 entail-ment graphs published by Berant et al. [Cite_Footnote_2] . For the ar-gument taxonomy we employed UMLS \u2013 a database that maps natural language phrases to over one mil-lion unique concept identifiers (CUIs) in the health-care domain. The CUIs are also mapped in UMLS to a concept taxonomy for the health-care domain."
  },
  {
    "id": 128,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The data",
      "our error analysis data",
      "they"
    ],
    "description": [
      "our error analysis data"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://bit.ly/mt-para",
    "section_title": "6 Conclusions",
    "add_info": "4 The data is available at http://bit.ly/mt-para.",
    "text": "We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [Cite_Footnote_4]"
  },
  {
    "id": 129,
    "name": "English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task",
    "fullname": "N/A",
    "genericmention": [
      "Both sides",
      "English text"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.statmt.org/wmt16/translation-task.html",
    "section_title": "3 Experiments 3.1 Data and preprocessing",
    "add_info": "1 http://www.statmt.org/wmt16/translation-task.html",
    "text": "For the syntax-based NMT, we take syntac-tic trees of source texts as inputs. We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task [Cite_Footnote_1] . Both sides are tokenized and split into subwords using BPE with 8000 merge operations. English text is parsed using SyntaxNet (Alberti et al., 2017). Then we transform the labeled dependency tree into the extended Levi graph as described in Section 2.2. Unlike AMR-to-text generation, in NMT task the input sentence contains significant sequential in-formation. This information is lost when treating the sentence as a graph. Guo et al. (2019) consider this information by adding sequential connections between each word node. In our model, we also add forward and backward edges in the extended Levi graph. Thus, the edge types vocabulary for the extended Levi graph of the dependency tree is T ={default, reverse, self, forward, backward}. So the set of subgraphs for NMT is G sub = {fully-connected, connected, default, reverse, forward, backward}. Note that we do not change the model architecture in the NMT tasks. However, we still get good results, which indicates the effectiveness of our model on Graph2Seq tasks. Except for in-troducing BPE into Levi graph, the above prepro-cessing steps are following Bastings et al. (2017). We refer to them for further information on the preprocessing steps."
  },
  {
    "id": 130,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/QAQ-v/HetGT",
    "section_title": "3 Experiments 3.2 Parameter Settings",
    "add_info": null,
    "text": "Both our encoder and decoder have 6 layers with 512-dimensional word embeddings and hidden states. We employ 8 heads and dropout with a rate of 0.3. For optimization, we use Adam opti-mizer with \u03b2 2 = 0.998 and set batch size to 4096 tokens. Meanwhile, we increase learning rate lin-early for the first warmup steps, and decrease it thereafter proportionally to the inverse square root of the step number. We set warmup steps to 8000. The similar learning rate schedule is adopted in (Vaswani et al., 2017). Our implementa-tion uses the openNMT library (Klein et al., 2017). We train the models for 250K steps on a single GeForce GTX 1080 Ti GPU. Our code is available at  https://github.com/QAQ-v/HetGT."
  },
  {
    "id": 131,
    "name": "OpenNMT FAQ",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model",
    "section_title": "3 Experiments 3.3 Metrics and Baselines",
    "add_info": "2 Parameters were chosen following the OpenNMT FAQ: http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model",
    "text": "Our baseline is the original Transformer [Cite_Footnote_2] . For AMR-to-text generation, Transformer takes lin-earized graphs as inputs. For syntax-based NMT, Transformer is trained on the preprocessed trans-lation dataset without syntactic information. We also compare the performance of HetGT with pre-vious single/ensenmble approaches which can be grouped into three categories: (1) Recurrent neu-ral network (RNN) based methods (GGNN2Seq, GraphLSTM); (2) Graph neural network (GNN) based methods (GCNSEQ, DGCN, G2S-GGNN); (3) The Transformer based methods (Structural Transformer, GTransformer). The ensemble mod-els are denoted by subscripts in Table 2 and Table 3."
  },
  {
    "id": 132,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset",
      "The final dataset"
    ],
    "description": [
      "Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., hameliorate, RESULT , ameliorationi.",
      "We intentionally avoid zero-derivations.",
      "We also exclude overly orthographically distant pairs",
      "The final dataset includes 6,029 derivational samples"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://github.com/ryancotterell/derviational-paradigms",
    "section_title": "3 Task and Models 3.1 Data",
    "add_info": "5 The dataset is available at http://github.com/ryancotterell/derviational-paradigms.",
    "text": "We experiment on English derivational triples ex-tracted from NomBank (Meyers et al., 2004). 4 Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., hameliorate, RESULT , ameliorationi. Note that in this task we do not predict whether a slot ex-ists, merely what form it would take given the base and the slot. In terms of current study, we consider the following derivational types: verb nominaliza-tion such as RESULT , AGENT and PATIENT , ad-verbalization and adjective-noun transformations. We intentionally avoid zero-derivations. We also exclude overly orthographically distant pairs by fil-tering out those for which the Levenshtein distance exceeds half the sum of their lengths, which ap-pear to be misannotations in NomBank. The final dataset includes 6,029 derivational samples, which we split into train (70%), development (15%), and test (15%). [Cite_Footnote_5] We also note that NomBank annota-tions are often semantically more coarse-grained."
  },
  {
    "id": 133,
    "name": "Nematus toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sennrich et al., 2017"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/rsennrich/nematus/",
    "section_title": "3 Task and Models 3.4 RNN Encoder-Decoder",
    "add_info": "6 https://github.com/rsennrich/nematus/",
    "text": "Training. We use the Nematus toolkit (Sennrich et al., 2017). [Cite_Footnote_6] We exactly follow the recipe in Kann and Schu\u0308tze (2016), the winning submission on the 2016 SIGMORPHON shared task for inflectional morphology. Accordingly, we use a character em-bedding size of 300, 100 hidden units in both the encoder and decoder, Adadelta (Zeiler, 2012) with a minibatch size of 20, and a beam size of 12. We train for 300 epochs and select the test model based on the performance on the development set."
  },
  {
    "id": 134,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the scripts",
      "Scripts"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/abisee/cnn-dailymail",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "3 Scripts publicly available at https://github.com/abisee/cnn-dailymail",
    "text": "We conducted our summarization experiments on the non-anonymous version CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; See et al., 2017), and the New York Times dataset (Durrett et al., 2016; Xu and Durrett, 2019). For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017) [Cite_Footnote_3] . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we cre-ated the NYT50 dataset by removing the docu-ments whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Dur-rett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive sum-marization, we used a strategy similar to Nallapati et al. (2017). We label the subset of sentences in a document that maximizes R OUGE (Lin, 2004) (against the human summary) as True and all other sentences as False."
  },
  {
    "id": 135,
    "name": "En-glish Gigaword [Cite_Footnote_4] dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2012T21",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "4 https://catalog.ldc.upenn.edu/LDC2012T21",
    "text": "To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [Cite_Footnote_4] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al., 2014). To re-duce the vocabulary size, we applied byte pair en-coding (BPE; Sennrich et al. 2016) to all of our datasets. To limit the memory consumption dur-ing training, we limit the length of each sentence to be 50 words (51th word and onwards are re-moved) and split documents with more than 30 sentences into smaller documents with each con-taining at most 30 sentences."
  },
  {
    "id": 136,
    "name": "pre-trained BERT",
    "fullname": "N/A",
    "genericmention": [
      "this imple-mentation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Devlin et al., 2018"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "4 Experiments 4.4 Results",
    "add_info": "7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "Our main results on the CNNDM dataset are shown in Table 1, with abstractive models in the top block and extractive models in the bot-tom block. Pointer+Coverage (See et al., 2017), Abstract-ML+RL (Paulus et al., 2017) and DCA (Celikyilmaz et al., 2018) are all sequence to se-quence learning based models with copy and cov-erage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite (Hsu et al., 2018) and InconsisLoss (Chen and Bansal, 2018) all try to decompose the word by word summary generation into sentence selection from document and \u201csentence\u201d level summariza-tion (or compression). Bottom-Up (Gehrmann et al., 2018) generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; Nalla-pati et al. 2017 and NeuSum; Cheng and Lapata 2016). They have been extended with reinforce-ment learning (Refresh; Narayan et al. 2018 and BanditSum; Dong et al. 2018), Maximal Marginal Relevance (NeuSum-MMR; Zhou et al. 2018), la-tent variable modeling (LatentSum; Zhang et al. 2018) and syntactic compression (JECS; Xu and Durrett 2019). Lead3 is a baseline which sim-ply selects the first three sentences. Our model H IBERT S (in-domain), which only use one pre-training stage on the in-domain CNNDM training set, outperforms all of them and differences be-tween them are all significant with a 0.95 confi-dence interval (estimated with the ROUGE script). Note that pre-training H IBERT S (in-domain) is very fast and it only takes around 30 minutes for one epoch on the CNNDM training set. Our models with two pre-training stages (H IBERT S ) or larger size (H IBERT M ) perform even better and H IBERT M outperforms BERT by 0.5 ROUGE . We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in 3.3) without pre-training. Note the setting for HeriTransfomer is (L = 4,H = 300 and A = 4) . We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [Cite_Footnote_7] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1. The posi-tional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences 8 ). We feed each block (the BOS and EOS tokens of each sentence are replaced with [CLS] and [SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model H IBERT S outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters (H IBERT S 54.6M v.s. BERT 110M)."
  },
  {
    "id": 137,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code for our system"
    ],
    "description": [
      "a novel approach that simultaneously extracts events and entities within a document context",
      "a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/bishanyang/EventEntityExtractor",
    "section_title": "1 Introduction",
    "add_info": "1 The code for our system is available at https://github.com/bishanyang/EventEntityExtractor.",
    "text": "In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context. [Cite_Footnote_1] We first decompose the learn-ing problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the co-occurrence relations between events across the doc-ument, and (3) learning for entity extraction. Then we combine the learned models for these subprob-lems into a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document. In summary, our main contribu-tions are:"
  },
  {
    "id": 138,
    "name": "ACE2005 corpus",
    "fullname": "N/A",
    "genericmention": [
      "It"
    ],
    "description": [
      "It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.itl.nist.gov/iad/mig/tests/ace/2005/",
    "section_title": "4 Experiments",
    "add_info": "6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/",
    "text": "We conduct experiments on the ACE2005 corpus. [Cite_Footnote_6] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al. (2013). Table 2 shows the data statistics."
  },
  {
    "id": 139,
    "name": "Wiki article",
    "fullname": "Wikipedia (Wiki) [Cite_Footnote_1] articles",
    "genericmention": [
      "the orig-inal articles"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://en.wikipedia.org",
    "section_title": "2 The Document Grounded Dataset 2.1 Document Set Creation",
    "add_info": "1 https://en.wikipedia.org",
    "text": "We choose Wikipedia (Wiki) [Cite_Footnote_1] articles to cre-ate a set of documents D = {d 1 ,...,d 30 } for grounding of conversations. We randomly select 30 movies, covering various genres like thriller, super-hero, animation, romantic, biopic etc. We extract the key information provided in the Wiki article and divide it into four separate sections. This was done to reduce the load of the users to read, absorb and discuss the information in the document. Hence, each movie document d i con-sists of four sections {s 1 , s 2 , s 3 , s 4 } correspond-ing to basic information and three key scenes of the movie. The basic information section s 1 con-tains data from the Wikipedia article in a stan-dard form such as year, genre, director. It also includes a short introduction about the movie, rat-ings from major review websites, and some crit-ical responses. Each of the key scene sections {s 2 , s 3 , s 4 } contains one short paragraph from the plot of the movie. Each paragraph contains on an average 7 sentences and 143 words. These para-graphs were extracted automatically from the orig-inal articles, and were then lightly edited by hand to make them of consistent size and detail. An ex-ample of the document is attached in Appendix."
  },
  {
    "id": 140,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a predefined set of documents",
      "the document"
    ],
    "description": [
      "a crowd-sourced con-versations dataset "
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.github.com/festvox/datasets/CMUDoG",
    "section_title": "References",
    "add_info": "4 https://www.github.com/festvox/datasets/CMUDoG",
    "text": "In this paper we introduce a crowd-sourced con-versations dataset that is grounded in a predefined set of documents which is available for download [Cite_Footnote_4] . We perform multiple automatic and human judgment based analysis to understand the value the information from the document provides to the generation of responses. The SEQS model which uses the information from the section to generate responses outperforms the SEQ model in the eval-uation tasks of engagement, fluency and perplex-ity. as a part of The Conversational Intelligence Chal-lenge (ConvAI, NIPS 2017) and we would like to thank the ConvAI team. We are also grateful to the anonymous reviewers for their constructive feedback and to Carolyn Penstein Rose, Shivani Poddar, Sreecharan Sankaranarayanan, Samridhi Shree Choudhary and Zhou Yu for valuable discussions at earlier stages of this work."
  },
  {
    "id": 141,
    "name": "COMLEX 3.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Grishman et al., 1998",
      "Ralph Grishman, Catherine Macleod, and Adam Myers, 1998. COMLEX Syntax Reference Manual. Proteus Project, NYU."
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/comlex/refman.ps",
    "section_title": "2 Preliminaries 2.2 Gold standard data",
    "add_info": "Ralph Grishman, Catherine Macleod, and Adam Myers, 1998. COMLEX Syntax Reference Manual. Proteus Project, NYU. (http://nlp.cs.nyu.edu/comlex/refman.ps).",
    "text": "Information about noun countability was obtained from two sources: COMLEX 3.0 (Grishman et al., 1998)  and the common noun part of ALT-J/E \u2019s Japanese-to-English semantic transfer dictio-nary (Ikehara et al., 1991). Of the approximately 22,000 noun entries in COMLEX , 13,622 are marked as countable , 710 as uncountable and the remainder are unmarked for countability. ALT-J/E has 56,245 English noun types with distinct countability."
  },
  {
    "id": 142,
    "name": "ROUGE",
    "fullname": "N/A",
    "genericmention": [
      "It"
    ],
    "description": [
      "the new automatic sum-mary evaluation metric",
      "ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.isi.edu/\u02dccyl/ROUGE",
    "section_title": "4 Experiments on DUC 2004 data 4.1 DUC 2004 data and ROUGE",
    "add_info": "1 http://www.isi.edu/\u02dccyl/ROUGE",
    "text": "For evaluation, we used the new automatic sum-mary evaluation metric, ROUGE [Cite_Footnote_1] , which was used for the first time in DUC 2004. ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence. It reports separate scores for 1, 2, 3, and 4-gram, and also for longest common subsequence co-occurences. Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgements most (Lin and Hovy, 2003). We show three of the ROUGE metrics in our experiment results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on longest common subsequence weighted by the length)."
  },
  {
    "id": 143,
    "name": "MEAD",
    "fullname": "N/A",
    "genericmention": [
      "it",
      "its"
    ],
    "description": [
      "MEAD [Cite_Footnote_2] is a publicly available toolkit for extractive multi-document summarization."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.summarization.com",
    "section_title": "4 Experiments on DUC 2004 data 4.2 MEAD summarization toolkit",
    "add_info": "2 http://www.summarization.com",
    "text": "MEAD [Cite_Footnote_2] is a publicly available toolkit for extractive multi-document summarization. Although it comes as a centroid-based summarization system by de-fault, its feature set can be extended to implement other methods."
  },
  {
    "id": 144,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the officially released evaluation scorer"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/sheffieldnlp/fever-scorer",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "3 https://github.com/sheffieldnlp/fever-scorer",
    "text": "This task has three evaluations: (i) N O S CORE E V \u2013 accuracy of claim verifica-tion, neglecting the validity of evidence; (ii) S CORE E V \u2013 accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for S UPPORTED and R E - FUTED ; (iii) F 1 \u2013 between the predicted evidence sentences and the ones chosen by annotators. We use the officially released evaluation scorer [Cite_Footnote_3] ."
  },
  {
    "id": 145,
    "name": "Pushshift API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pushshift/api",
    "section_title": "4 Data 4.1 Data collection",
    "add_info": "4 https://github.com/pushshift/api model with all features. The numerical results are given in the appendix (Table 6.)",
    "text": "Data was scraped from the two subreddits by querying the Pushshift API. [Cite_Footnote_4] 3 years\u2019 worth of posts, ranging from 1 January 2017 to 31 Decem-ber 2019, were collected for each subreddit. To ensure each post had sufficient linguistic content, I excluded any posts containing less than 101 char-acters."
  },
  {
    "id": 146,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/KodairaTomonori/EvaluationDataset",
    "section_title": "1 Introduction",
    "add_info": "2 https://github.com/KodairaTomonori/EvaluationDataset",
    "text": "\u2022 The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators. Our dataset is available at GitHub [Cite_Footnote_2] ."
  },
  {
    "id": 147,
    "name": "Lancers",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "to perform substitute extraction, substitute evalua-tion, and substitute ranking"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.lancers.jp/",
    "section_title": "4 Balanced dataset for evaluation of Japanese lexical simplification",
    "add_info": "3 http://www.lancers.jp/",
    "text": "We use a crowdsourcing application, Lancers, [Cite_Footnote_3] to perform substitute extraction, substitute evalua-tion, and substitute ranking. In each task, we re-quested the annotators to complete at least 95% of their previous assignments correctly. They were native Japanese speakers."
  },
  {
    "id": 148,
    "name": "Lexicon for Japanese Language Edu-cation",
    "fullname": "N/A",
    "genericmention": [
      "the lex-icon"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sunakawa et al., 2012"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://jhlee.sakura.ne.jp/JEV.html",
    "section_title": "4 Balanced dataset for evaluation of Japanese lexical simplification 4.1 Extracting sentences",
    "add_info": "4 http://jhlee.sakura.ne.jp/JEV.html",
    "text": "Our work defines complex words as \u201cHigh Level\u201d words in the Lexicon for Japanese Language Edu-cation (Sunakawa et al., 2012). [Cite_Footnote_4] The word level is calculated by five teachers of Japanese, based on their experience and intuition. There were 7,940 high-level words out of 17,921 words in the lex-icon. In addition, target words of this work com-prised content words (nouns, verbs, adjectives, ad-verbs, adjectival nouns, sahen nouns, and sahen verbs )."
  },
  {
    "id": 149,
    "name": "ERNIE",
    "fullname": "Enhanced Represen-tation through Knowledge Integration",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sun et al., 2019"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/PaddlePaddle/ERNIE",
    "section_title": "5 Experiments 5.1 Methods",
    "add_info": "3 https://github.com/PaddlePaddle/ERNIE",
    "text": "\u2022 ERNIE (115M) [Cite_Footnote_3] , a.k.a Enhanced Represen-tation through Knowledge Integration (Sun et al., 2019), which is trained with not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc."
  },
  {
    "id": 150,
    "name": "ROBERTa-wwm-est-large",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/ymcui/Chinese-BERT-wwm",
    "section_title": "5 Experiments 5.1 Methods",
    "add_info": "4 https://github.com/ymcui/Chinese-BERT-wwm",
    "text": "\u2022 ROBERTa, a robust BERT (Liu et al., 2019). We used ROBERTa-wwm-est-large. [Cite_Footnote_4]"
  },
  {
    "id": 151,
    "name": "Huggingface",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "6 Transfer Learning via TED-CDB 6.2 Same-Domain Cross-Language Learning",
    "add_info": "5 https://github.com/huggingface/transformers",
    "text": "TED-MDB (Zeyrek et al., 2018) corpus annotation follows the PDTB 3.0 framework. It contains man-ual annotation of 6 TED talks in seven languages (English, Turkish, European Potuguese, Polish, German, Russian, and Lithuanian). The sub-corpus for each language is quite small, with about 200 im-plicit discourse relations each, compared with the \u223c7.0 K implicit relations in the TED-CDB. There-fore, we can see whether the TED-CDB can help them. For this experiment, the multilingual BERT was used, which is as large as BERT-wwm but the training data is expanded to cover 104 languages. We used the multilingual BERT implementation from Huggingface. [Cite_Footnote_5] The design for these exper-iments is making a comparison between a cross validation within the TED-MDB and a zero-shot transfer learning from TED-CDB to TED-MDB. Due to the unbalanced distribution of senses in TED-MDB, using the method of Easy Ensemble (Liu, 2009), we divided the Expansion data of every language in the TED-MDB into 4 parts and then each part was added into the data of other types to become the training set. Finally, we integrated these training sets from 6 language into one train-ing set, and the left data for one language would be the test set. Therefore, what we used here is 4-fold cross validation where each fold is used as the test set exactly once. The average test set accuracy is then reported."
  },
  {
    "id": 152,
    "name": "Acc Recall F1 SeqAttack",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/WalterSimoncini/SeqAttack",
    "section_title": "5 Results and discussion 5.2 Adversarial training",
    "add_info": "1 https://github.com/WalterSimoncini/SeqAttack",
    "text": "Figure 4: KDE plots for the labels score and modifica-tion rate distributions for NER small and its robust coun-terpart, when attacked with DeepWordBug-I 5 . To be Examples Acc Recall F1 SeqAttack [Cite_Footnote_1] , a Python library for conducting ad- 500 examples 91% 1000 examples 90% 1500 examples 90% 2000 examples 90% NER small 91% 90% versarial attacks against token classification models.90% 89% The library is accompanied by a web application 90% 90% to inspect the generated adversarial examples and90%"
  },
  {
    "id": 153,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Application",
      "a web application"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://ner-attack.ashita.nl/",
    "section_title": "5 Results and discussion 5.2 Adversarial training",
    "add_info": "2 Application available at https://ner-attack.ashita.nl/",
    "text": "Figure 4: KDE plots for the labels score and modifica-tion rate distributions for NER small and its robust coun-terpart, when attacked with DeepWordBug-I 5 . To be Examples Acc Recall F1 SeqAttack , a Python library for conducting ad- 500 examples 91% 1000 examples 90% 1500 examples 90% 2000 examples 90% NER small 91% 90% versarial attacks against token classification models.90% 89% The library is accompanied by a web application [Cite_Footnote_2] 90% 90% to inspect the generated adversarial examples and90%"
  },
  {
    "id": 154,
    "name": "seqeval",
    "fullname": "N/A",
    "genericmention": [
      "Software"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Nakayama, 2018",
      "Hiroki Nakayama. 2018. seqeval: A python framework for sequence labeling evaluation."
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/chakki-works/seqeval",
    "section_title": "4 Experiments 4.2 Adversarial training",
    "add_info": "Hiroki Nakayama. 2018. seqeval: A python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval.",
    "text": "The metrics were calculated using seqeval (Nakayama, 2018)  . \u2191 (\u2193) indicate whether the higher (or lower) the better from the attack perspective."
  },
  {
    "id": 155,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the source code of DIG"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/INK-USC/DIG",
    "section_title": "References",
    "add_info": "1 https://github.com/INK-USC/DIG",
    "text": "As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation ax-ioms and the ease of gradient computation. It measures feature importance by averaging the model\u2019s output gradient interpolated along a straight-line path in the input data space. How-ever, such straight-line interpolated points are not representative of text data due to the inher-ent discreteness of the word embedding space. This questions the faithfulness of the gradi-ents computed at the interpolated points and consequently, the quality of the generated ex-planations. Here we propose Discretized In-tegrated Gradients (DIG), which allows effec-tive attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yield-ing more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encour-age reproducible research [Cite_Footnote_1] ."
  },
  {
    "id": 156,
    "name": "Riemann summation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://en.wikipedia.org/wiki/Riemann_sum",
    "section_title": "2 Method 2.1 Discretized integrated gradients",
    "add_info": "2 https://en.wikipedia.org/wiki/Riemann_sum",
    "text": "Here m is the total number of steps for interpola-tion. This constraint is essential because it allows approximating the integral in Eq. 1 using Riemann summation [Cite_Footnote_2] which requires monotonic paths. We note that the interpolation points used by IG nat-urally satisfy this constraint since they lie along a straight line joining x and x 0 . The key distinction of our formulation from IG is that DIG is agnostic of any fixed step size parameter \u03b1 and thus allows non-linear interpolation paths in the embedding space. The integral approximation of DIG is de-fined as follows:"
  },
  {
    "id": 157,
    "name": "HuggingFace Dataset library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2020b"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/huggingface/datasets",
    "section_title": "3 Experimental Setup",
    "add_info": "3 https://github.com/huggingface/datasets",
    "text": "In this section, we describe the datasets and models used for evaluating our proposed algorithm. Datasets. The SST2 (Socher et al., 2013) dataset has 6920/872/1821 example sentences in the train/dev/test sets. The task is binary classifica-tion into positive/negative sentiment. The IMDB (Maas et al., 2011) dataset has 25000/25000 exam-ple reviews in the train/test sets with similar binary labels for positive and negative sentiment. Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library [Cite_Footnote_3] (Wolf et al., 2020b)."
  },
  {
    "id": 158,
    "name": "HuggingFace Dataset library 3",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2020b"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/huggingface/datasets",
    "section_title": "3 Experimental Setup",
    "add_info": "Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, Fran\u00e7ois Lagunas, Lysandre Debut, Morgan Funtow-icz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Mu\u0161tar, Jeff Boudier, and Anna Tordjmann. 2020b. Datasets. GitHub. Note: https://github.com/huggingface/datasets.",
    "text": "In this section, we describe the datasets and models used for evaluating our proposed algorithm. Datasets. The SST2 (Socher et al., 2013) dataset has 6920/872/1821 example sentences in the train/dev/test sets. The task is binary classifica-tion into positive/negative sentiment. The IMDB (Maas et al., 2011) dataset has 25000/25000 exam-ple reviews in the train/test sets with similar binary labels for positive and negative sentiment. Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library 3 (Wolf et al., 2020b)  ."
  },
  {
    "id": 159,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Codes for the experiments"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/parag1604/A2L",
    "section_title": "4 Experiments 4.1 Active Learning Strategies for Sequence Tagging",
    "add_info": "1 Codes for the experiments are available at the following github link: https://github.com/parag1604/A2L.",
    "text": "Margin-based strategy: Let s(y) = P \u03b8 (Y = y|X = x) be the score assigned by a model M with parameters \u03b8 to output y for a given example x. Margin is defined as the difference in scores ob-tained by the best scoring output y and the second best scoring output y 0 , i.e.: where, y max = arg max y s(y). The strategy se-lects examples for which M margin \u2264 \u03c4 [Cite_Footnote_1] , where \u03c4 1 is a hyper-parameter. We use Viterbi\u2019s algorithm (Ryan and Nudd, 1993) to compute the scores s(y)."
  },
  {
    "id": 160,
    "name": "Simple PPDB",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "section_title": "1 Motivation",
    "add_info": "1 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "text": "Recent research in data-driven paraphrasing has produced enormous resources containing millions of meaning-equivalent phrases (Ganitkevitch et al., 2013). Such resources capture a wide range of language variation, including the types of lexical and phrasal simplifications just described. In this work, we apply state-of-the-art machine learned models for lexical simplification in order to iden-tify phrase pairs from the Paraphrase Database (PPDB) applicable to the task of text simplifica-tion. We introduce Simple PPDB, [Cite_Footnote_1] a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules. The large scale of Sim-ple PPDB will support research into increasingly advanced methods for text simplification."
  },
  {
    "id": 161,
    "name": "PPDB-TLDR [Cite_Footnote_2] dataset",
    "fullname": "N/A",
    "genericmention": [
      "the data"
    ],
    "description": [
      "the PPDB-TLDR [Cite_Footnote_2] dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://paraphrase.org/#/download",
    "section_title": "2 Identifying Simplification Rules 2.1 Paraphrase Rules",
    "add_info": "2 http://paraphrase.org/#/download",
    "text": "The Paraphrase Database (PPDB) is currently the largest available collection of paraphrases. Each paraphrase rule in the database has an automatically-assigned quality score between 1 and 5 (Pavlick et al., 2015). In this work, we use the PPDB-TLDR [Cite_Footnote_2] dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases, and is intended to give a generally good tradeoff between precision and recall. To preprocess the data, we lemmatize all of the phrases, and remove rules which differ only by morphology, punctu-ation, or stop words, or which involve phrases longer than 3 words. The resulting list contains 7.5 million paraphrase rules covering 625K unique lemmatized words and phrases."
  },
  {
    "id": 162,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a multi-class logistic regression model"
    ],
    "description": [
      "a multi-class logistic regression model"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://scikit-learn.org/",
    "section_title": "2 Identifying Simplification Rules 2.2 Lexical Simplification Model",
    "add_info": "4 http://scikit-learn.org/",
    "text": "We train a multi-class logistic regression model [Cite_Footnote_4] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output."
  },
  {
    "id": 163,
    "name": "Simple PPDB",
    "fullname": "N/A",
    "genericmention": [
      "its"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "section_title": "References",
    "add_info": "6 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "text": "We have described Simple PPDB, a subset of the Paraphrase Database adapted for the task of text simplification. Simple PPDB is built by apply-ing state-of-the-art machine learned models for lexical simplification to the largest available re-source of lexical and phrasal paraphrases, result-ing in a web-scale resource capable of supporting research in data-driven methods for text simplifi-cation. We have shown that Simple PPDB offers substantially increased coverage of both words and multiword phrases, while maintaining high quality compared to existing methods for lexical simplification. Simple PPDB, along with the hu-man judgements collected as part of its creation, is freely available with the publication of this paper. [Cite_Footnote_6] rial is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this pub-lication are those of the authors and should not be interpreted as representing official policies or en-dorsements of DARPA and the U.S. Government."
  },
  {
    "id": 164,
    "name": "multilingual BERT",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/google-research/bert/blob/master/multilingual.md",
    "section_title": "6 Data",
    "add_info": "10 Information about multilingual BERT can be found in: https://github.com/google-research/bert/blob/master/multilingual.md",
    "text": "We used Wikipedia as the main data source for all our experiments. Multilingual BERT [Cite_Footnote_10] was trained on the 104 languages with the largest Wikipedias \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba."
  },
  {
    "id": 165,
    "name": "Wikipedias",
    "fullname": "N/A",
    "genericmention": [
      "these"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://meta.wikimedia.org/wiki/List_of_Wikipedias",
    "section_title": "6 Data",
    "add_info": "11 List of Wikipedias can be found in https://meta.wikimedia.org/wiki/List_of_Wikipedias",
    "text": "We used Wikipedia as the main data source for all our experiments. Multilingual BERT was trained on the 104 languages with the largest Wikipedias [Cite_Footnote_11] \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba."
  },
  {
    "id": 166,
    "name": "CLUTO",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "A Clustering Toolkit"
    ],
    "citationtag": [
      "Tech Report 02-017, Dept. of Computer Science, University of Minnesota.",
      "S. Ploux and H. Ji. 2003. A Model for Matching Semantic Maps Between Languages (French/English, English/French). Computational Linguistics, 29(2):155- 178."
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.cs.umn.edu\u02dccluto",
    "section_title": "References",
    "add_info": null,
    "text": "Finally, evaluation of induced sense taxonomies is always problematic. First of all, there is no agreed \u201ccorrect\u201d way to G. Karypis. 2002. CLUTO: A Clustering Toolkit. Tech Report 02-017, Dept. of Computer Science, University of Minnesota. Available at  http://www.cs.umn.edu\u02dcclutoS. Ploux and H. Ji. 2003. A Model for Matching Semantic Maps Between Languages (French/English, English/French). Computational Linguistics, 29(2):155- 178."
  },
  {
    "id": 167,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our re-analysis code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/qingsongma/percentage-refBias",
    "section_title": "2 Background 2.1 Measures of Central Tendency",
    "add_info": "7 Our re-analysis code is available at https://github.com/qingsongma/percentage-refBias",
    "text": "Table 2 shows proportions of all judge pairs with significant differences in Kappa point esti-mates (non-overlapping confidence intervals) for each combination of settings (Revelle, 2014). [Cite_Footnote_7] The number of significant differences in Kappa point estimates for pairs of judges in SAME and DIFF is only 13%, or, in other words, 87% of judge pairs across SAME and DIFF have no significant difference in agreement levels. Table 2 also in-cludes proportions of significant differences for Kappa point estimates resulting from judges be-longing to a single setting (significance testing all Kappa of SAME with respect to all other Kappa be-longing to SAME , for example), revealing that the proportion of significant differences within SAME (12%) to be very similar to that of SAME \u00d7 DIFF (13%), and similarly for DIFF (12%), with only a single percentage point difference in both cases in proportions of significant differences. Sub-sequently, even after correcting the measure of central tendency error in Fomicheva and Specia (2016), evidence of reference bias can still not be concluded."
  },
  {
    "id": 168,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/sweetalyssum/Seq2Seq-DU",
    "section_title": "1 Introduction",
    "add_info": "1 The code is available at https://github.com/sweetalyssum/Seq2Seq-DU.",
    "text": "Experimental results on benchmark datasets show that Seq2Seq-DU [Cite_Footnote_1] performs much better than the baselines on SGD, MultiWOZ2.2, and Multi-WOZ2.1 in multi-turn dialogue with schema de-scriptions, is superior to BERT-DST on WOZ2.0, DSTC2, and M2M, in multi-turn dialogue with-out schema descriptions, and works equally well as Joint BERT on ATIS and SNIPS in single turn dialogue (in fact, it degenerates to Joint BERT)."
  },
  {
    "id": 169,
    "name": "Mantidae toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Cohn et al., 2016"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/duyvuleo/Mantidae",
    "section_title": "5 Experiments 5.1 Setup",
    "add_info": "4 https://github.com/duyvuleo/Mantidae",
    "text": "NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [Cite_Footnote_4] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words. For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs."
  },
  {
    "id": 170,
    "name": "dynet deep learning library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Neubig et al., 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/clab/dynet",
    "section_title": "5 Experiments 5.1 Setup",
    "add_info": "5 https://github.com/clab/dynet",
    "text": "NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [Cite_Footnote_5] (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words. For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs."
  },
  {
    "id": 171,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/McGill-NLP/contextual-nmn",
    "section_title": "References",
    "add_info": null,
    "text": "Neural module networks (NMN) are a pop-ular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit associ-ations between similar textual contexts (e.g. \u201cdark cube on the left\u201d vs. \u201cblack cube on the left\u201d). In this work, we address these limitations and evaluate the impact of contex-tual clues in improving the performance of NMN models. First, we address the prob-lem of fixed textual inputs by parameteriz-ing the module arguments. This substan-tially reduce the number of modules in NMN by up to 75% without any loss in perfor-mance. Next we propose a method to con-textualize our parameterized model to enhance the module\u2019s capacity in exploiting the vi-siolinguistic associations. Our model out-performs the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1% improve-ment in accuracy on the single-referent test set and +4.3% on the full test set. Addi-tionally, we demonstrate that contextualization provides +11.2% and +1.7% improvements in accuracy over prior NMN models on CLO-SURE and NLVR2. We further evaluate the impact of our contextualization by construct-ing a contrast set for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the baselines by as much as +10.4% absolute ac-curacy on CC-Ref+, illustrating the generaliza-tion skills of our approach. Our dataset is pub-licly available at  https://github.com/McGill-NLP/contextual-nmn."
  },
  {
    "id": 172,
    "name": "IEP-Ref implementation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a NMN solution based on IEP"
    ],
    "citationtag": [
      "Liu et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ruotianluo/iep-ref",
    "section_title": "3 Module Parameterization in NMN",
    "add_info": "1 We used the IEP-Ref implementation provided at the link https://github.com/ruotianluo/iep-ref",
    "text": "We propose parametrization as the first step to en-able weight sharing and exploiting associations be-tween similar textual contexts. Specifically, we evaluate the effectiveness of parameterizing mod-ule textual inputs using IEP-Ref (Liu et al., 2019) as the baseline NMN implementation. IEP-Ref, a NMN solution based on IEP (Johnson et al., 2017b), is the current state-of-the-art model on CLEVR-Ref+ dataset. [Cite_Footnote_1] As shown Figure 2(a), the neural modules in IEP-Ref are represented using a stan-dard Residual Convolution Block (RCB). Formally, each RCB module (f n ) of arity n receives n feature maps (F i ) of shape 128 \u00d7 20 \u00d7 20 and outputs a same-sized tensor f o = f n (F 1 , F 2 , ..., F n )."
  },
  {
    "id": 173,
    "name": "IEP-Ref implementation",
    "fullname": "N/A",
    "genericmention": [
      "the model"
    ],
    "description": [
      "IEP-Ref (Liu et al., 2019), the current state-of-the-art neural module network (NMN) model for the CLEVR-Ref+ dataset, uses a generic design of neural module architecture adapted from IEP"
    ],
    "citationtag": [
      "Liu et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ruotianluo/iep-ref",
    "section_title": "A Appendix A.2 Neural Modules in Parameterized IEP-Ref",
    "add_info": "4 We used the IEP-Ref implementation provided at the link https://github.com/ruotianluo/iep-ref",
    "text": "IEP-Ref (Liu et al., 2019), the current state-of-the-art neural module network (NMN) model for the CLEVR-Ref+ dataset, uses a generic design of neural module architecture adapted from IEP (John-son et al., 2017b), which was designed for VQA task. [Cite_Footnote_4] The modules take either two visual inputs (bi-nary modules) or one visual input (unary modules). There are total 60 distinct modules in IEP-Ref. Af-ter parameterization (see Figure 8b), the distinct number of modules drop to 15 without any drop in the model performance (section 2 of main paper). That is, the number of a distinct set of modules (and the total number of parameters) used in the parameterized model reduces by 75%. Moreover, although the network parameters of each parameter-ized module slightly increase due to the additional LSTM unit, since each module in IEP-Ref can have multiple instantiations for the same textual input, we have fewer parameters than IEF-Ref in total. Table 11 presents the list of all the 15 modules in our parameterized NMN model. We compare the parameters per module of all baseline NMN mod-els and our proposed models (section 3 of main paper) in Table 10."
  },
  {
    "id": 174,
    "name": "monolingual WMT news crawl datasets",
    "fullname": "N/A",
    "genericmention": [
      "all available monolingual news crawl training data"
    ],
    "description": [
      "the monolingual WMT news crawl datasets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://data.statmt.org/news-crawl/",
    "section_title": "5 Experiments 5.1 Datasets",
    "add_info": "3 http://data.statmt.org/news-crawl/",
    "text": "Estonian (Et)\u2013En translation tasks. The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [Cite_Footnote_3] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences. For the low-resource languages Ro and Et, we used all available monolingual news crawl training data. To make our experiments comparable with previous work (Lample and Conneau, 2019), we report the results on newstest2014 for Fr\u2013En, newstest2016 for Ro\u2013En, and newstest2018 for Et\u2013En."
  },
  {
    "id": 175,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/diegma/neural-dep-srl",
    "section_title": "1 Introduction",
    "add_info": "1 The code is available at https://github.com/diegma/neural-dep-srl.",
    "text": "One layer GCN encodes only information about immediate neighbors and K layers are needed to encode K-order neighborhoods (i.e., informa-tion about nodes at most K hops aways). This contrasts with recurrent and recursive neural net-works (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence. However, as we will further discuss in Section 3.3, this is not a serious limitation when GCNs are used in combination with encoders based on recurrent networks (LSTMs). When we stack GCNs on top of LSTM layers, we obtain a substantial improve-ment over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for En-glish and Chinese. [Cite_Footnote_1]"
  },
  {
    "id": 176,
    "name": "Gender Genie",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "an online gender-detector"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://bookblog.net/gender/genie.php",
    "section_title": "7 Gender Classification Results",
    "add_info": "7 http://bookblog.net/gender/genie.php",
    "text": "Table 4 combines the results of the experiments re-ported in the previous sections, assessed on both the Fisher and Switchboard corpora for gender classification. The evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. Baseline performance (always guessing female) yields 57.47% and 51.6% on Fisher and Switchboard respectively. As noted before, the standard reference algorithm is Boulis and Osten-dorf (2005), and all cited relative error reductions are based on this established standard, as imple-mented in this paper. Also, as a second reference, performance is also cited for the popular \u201cGender Genie\u201d, an online gender-detector [Cite_Footnote_7] , based on the manually weighted word-level sociolinguistic fea-tures discussed in Argamon et al. (2003). The ad-ditional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the Boulis and Ostendorf (2005) standard."
  },
  {
    "id": 177,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All data",
      "This data",
      "it",
      "it"
    ],
    "description": [
      "a newly gathered corpus with dense document-level sentiment la-bels in news articles",
      "This data includes compre-hensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "http://homes.cs.washington.edu/\u02dceunsol/project_page/acl16",
    "section_title": "1 Introduction",
    "add_info": "1 All data will be made publicly available. You can browse it at http://homes.cs.washington.edu/\u02dceunsol/project_page/acl16, and download it from the author\u2019s webpage.",
    "text": "We evaluate the approach on a newly gathered corpus with dense document-level sentiment la-bels in news articles. [Cite_Footnote_1] This data includes compre-hensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demon-strate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ab-lation and error analysis, showing cases where the global constraints contribute and pointing towards important areas for future work."
  },
  {
    "id": 178,
    "name": "CPLEX4",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://tinyurl.com/joccfqy",
    "section_title": "5 Experimental Setup",
    "add_info": "11 http://tinyurl.com/joccfqy",
    "text": "We also report a proxy for doing similar ag-gregation over a state-of-the-art entity-entity sen-timent classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annota-tions, we can simply predict the union of the orig-inal gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 [Cite_Footnote_11] to solve the ILP described in Sec. 2. For compu-tational efficiency and to avoid erroneous propa-gation, soft constraints associated with reciprocity and balance theory are introduced only on pairs for which a high-precision classifier assigned po-larity. For the pairwise classifier, we use a class-weighted linear SVM. We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset. We made two versions of pair-wise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search. One is tuned for high precision to be used as a base classifier for ILP (ILP base), and the other is tuned for the best F1 (Pairwise)."
  },
  {
    "id": 179,
    "name": "class-weighted linear SVM",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the pairwise classifier"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://scikit-learn.org/",
    "section_title": "5 Experimental Setup",
    "add_info": "12 http://scikit-learn.org/",
    "text": "We also report a proxy for doing similar ag-gregation over a state-of-the-art entity-entity sen-timent classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annota-tions, we can simply predict the union of the orig-inal gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 to solve the ILP described in Sec. 2. For compu-tational efficiency and to avoid erroneous propa-gation, soft constraints associated with reciprocity and balance theory are introduced only on pairs for which a high-precision classifier assigned po-larity. For the pairwise classifier, we use a class-weighted linear SVM. [Cite_Footnote_12] We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset. We made two versions of pair-wise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search. One is tuned for high precision to be used as a base classifier for ILP (ILP base), and the other is tuned for the best F1 (Pairwise)."
  },
  {
    "id": 180,
    "name": "acoustic classifier h",
    "fullname": "N/A",
    "genericmention": [
      "These two classifiers",
      "two distinct classifiers"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/",
    "section_title": "3 Prosodic Model 3.3 Prosodic Model Training",
    "add_info": "1 LIBSVM \u2013 A Library for Support Vector Machines, loca-tion: http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/",
    "text": "In our experiments, we investigate two kinds of training methods for prosodic modeling. The first one is a supervised method where models are trained using all the labeled data. The second is a semi-supervised method using co-training algo-rithm (Blum and Mitchell, 1998), described in Algo-rithm 1. Given a set L of labeled data and a set U of unlabeled data with two views, it then iterates in the following procedure. The algorithm first creates a smaller pool U \u2032 containing unlabeled data from U. It uses L i (i = 1, 2) to train two distinct classifiers: the acoustic classifier h [Cite_Footnote_1] , and the lexical classifier h 2 . We use function V i (i = 1, 2) to represent that only a single view is used for training h 1 or h 2 . These two classifiers are used to make predictions for the unla-beled set U \u2032 , and only when they agree on the predic-tion for a sample, their predicted class is used as the label for this sample. Then among these self-labeled samples, the most confident ones by one classifier are added to the data set L i for training the other classifier. This iteration continues until reaching the defined number of iterations. In our experiment, the size of the pool U\u00b4 is 5 times of the size of training data L i , and the size of the added self-labeled ex-ample set, D h i , is 5% of L i . For the newly selected D h i , the distribution of the positive and negative ex-amples is the same as that of the training data L i ."
  },
  {
    "id": 181,
    "name": "CMU Sphinx",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Speech Recognition Toolkit"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.speech.cs.cmu.edu/sphinx/tutorial.html",
    "section_title": "5 Data and Baseline Systems",
    "add_info": "2 CMU Sphinx - Speech Recognition Toolkit, location: http://www.speech.cs.cmu.edu/sphinx/tutorial.html",
    "text": "The first data set is the Boston University Radio News Corpus (BU) (Ostendorf et al., 1995), which consists of broadcast news style read speech. The BU corpus has about 3 hours of read speech from 7 speakers (3 female, 4 male). Part of the data has been labeled with ToBI-style prosodic annotations. In fact, the reason that we use this corpus, instead of other corpora typically used for ASR experiments, is because of its prosodic labels. We divided the entire data corpus into a training set and a test set. There was no speaker overlap between training and test sets. The training set has 2 female speakers (f2 and f3) and 3 male ones (m2, m3, m4). The test set is from the other two speakers (f1 and m1). We use 200 utterances for the recognition experiments. Each ut-terance in BU corpus consists of more than one sen-tences, so we segmented each utterance based on pause, resulting in a total number of 713 segments for testing. We divided the test set roughly equally into two sets, and used one for parameter tuning and the other for rescoring test. The recognizer used for this data set was based on Sphinx-3 [Cite_Footnote_2] . The context-dependent triphone acoustic models with 32 Gaus-sian mixtures were trained using the training par-tition of the BU corpus described above, together with the broadcast new data. A standard back-off tri-gram language model with Kneser-Ney smoothing was trained using the combined text from the train-ing partition of the BU, Wall Street Journal data, and part of Gigaword corpus. The vocabulary size was about 10K words and the out-of-vocabulary (OOV) rate on the test set was 2.1%."
  },
  {
    "id": 182,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an unofficial score conversion table [Cite_Footnote_2] between the tests"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/",
    "section_title": "2 Experimental Setup 2.3 Standardized English Tests",
    "add_info": "2 http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/ Although both TOEFL and TOEIC are administered by the same company (ETS), to the best of our knowledge there is no publicly available official conversion table between the two tests.",
    "text": "TOEFL Berzak et al. (2017) also collected self-reported scores on the most recently taken offi-cial English proficiency test, which we use here as a secondary evaluation benchmark. We focus on the most commonly reported test, the TOEFL-iBT whose scores range from 0 to 120. We take into ac-count only test results obtained less than four years prior to the experiment, yielding 33 participants. We sum the scores of the reading and listening sec-tions of test, with a total possible score range of 0 to 60. In cases where participants reported only the overall score, we divided that score by two. We further augment this data with 20 participants who took the TOEIC Listening and Reading test within the same four years range, resulting in a total of 53 external proficiency scores. The TOEIC scores were converted to the TOEFL scale by fitting a third degree polynomial on an unofficial score conversion table [Cite_Footnote_2] between the tests. The converted scores were then divided by two. Henceforth we refer to both TOEFL-iBT and TOEIC scores con-verted to TOEFL-iBT scale as TOEFL scores. The mean TOEFL score is 47.6 (std 9.55). The Pear-son\u2019s r correlation between the TOEFL and MET scores in the dataset is 0.74."
  },
  {
    "id": 183,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/kentonl/e2e-coref",
    "section_title": "7 Experiments 7.1 Hyperparameters",
    "add_info": "3 https://github.com/kentonl/e2e-coref",
    "text": "All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [Cite_Footnote_3]"
  },
  {
    "id": 184,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our embeddings and datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/yogarshi/bisparse-dep/",
    "section_title": "References",
    "add_info": "1 https://github.com/yogarshi/bisparse-dep/",
    "text": "Cross-lingual Hypernymy Detection involves determining if a word in one language (\u201cfruit\u201d) is a hypernym of a word in another language (\u201cpomme\u201d i.e. apple in French). The abil-ity to detect hypernymy cross-lingually can aid in solving cross-lingual versions of tasks such as textual entailment and event coreference. We propose B I S PARSE -D EP , a family of un-supervised approaches for cross-lingual hyper-nymy detection, which learns sparse, bilingual word embeddings based on dependency con-texts. We show that B I S PARSE -D EP can sig-nificantly improve performance on this task, compared to approaches based only on lexical context. Our approach is also robust, show-ing promise for low-resource settings: our dependency-based embeddings can be learned using a parser trained on related languages, with negligible loss in performance. We also crowd-source a challenging dataset for this task on four languages \u2013 Russian, French, Arabic, and Chinese. Our embeddings and datasets are publicly available. [Cite_Footnote_1]"
  },
  {
    "id": 185,
    "name": "Crowd-Flower",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://crowdflower.com",
    "section_title": "4 Crowd-Sourcing Annotations",
    "add_info": "4 http://crowdflower.com",
    "text": "There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [Cite_Footnote_4] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English."
  },
  {
    "id": 186,
    "name": "OPUS",
    "fullname": "N/A",
    "genericmention": [
      "this paral-lel data"
    ],
    "description": [
      "open-source repository of parallel corpora"
    ],
    "citationtag": [
      "Tiedemann, 2012"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://opus.nlpl.eu/",
    "section_title": "2 A FRO MT benchmark 2.2 Data Sources",
    "add_info": "3 http://opus.nlpl.eu/",
    "text": "For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [Cite_Footnote_3] (Tiedemann, 2012) and (2) ParaCrawl (Espl\u00e0 et al., 2019). From OPUS, we use the JW300 corpus (Agic\u0301 and Vulic\u0301, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014). Despite the existence of this paral-lel data, these text datasets were often collected from large, relatively unclean multilingual corpora, e.g. JW300 which was extracted from Jehovah\u2019s Witnesses text, or QED which was extracted from transcribed educational videos. This leads to many sentences with high lexical overlap, inconsistent tokenization, and other undesirable properties for a clean, reproducible benchmark."
  },
  {
    "id": 187,
    "name": "Moses (Koehn et al., 2007) toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Koehn et al., 2007"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/",
    "section_title": "2 A FRO MT benchmark 2.3 Data Preparation",
    "add_info": "5 https://github.com/moses-smt/mosesdecoder/",
    "text": "Tokenization normalization We perform detok-enization on all corpora using the detokenization script provided in the Moses (Koehn et al., 2007) toolkit [Cite_Footnote_5] . Given that we collect data from various sources, this step is important to allow for consis-tent tokenization across corpora."
  },
  {
    "id": 188,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Levenshtein-based fuzzy string matching"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/maxbachmann/RapidFuzz",
    "section_title": "2 A FRO MT benchmark 2.3 Data Preparation",
    "add_info": "6 https://github.com/maxbachmann/RapidFuzz pseudo monolingual data and dictionaries",
    "text": "Removal of sentences with high text overlap To prevent data leakage, we remove sentences with high text overlap. To do this, we use Levenshtein-based fuzzy string matching [Cite_Footnote_6] and remove sentences that have a similarity score of over 60. Given that measuring this score against all sentences in a corpus grows quadratically with respect to cor-pus length, we use the following two heuristics to remove sentences with high overlap in an effi-cient manner: (1) scoring similarity between the 50 alphabetically-sorted previous sentences, (2): ex-tracting the top 100K four-grams and performing the similarity score within each group of sentences containing at least one instance of a certain four-gram."
  },
  {
    "id": 189,
    "name": "eflomal",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a statistical word aligner"
    ],
    "citationtag": [
      "\u00d6stling and Tiedemann, 2016"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/robertostling/eflomal/",
    "section_title": "3 AfroBART 3.2 Dictionary Augmentation",
    "add_info": "7 https://github.com/robertostling/eflomal/",
    "text": "Dictionary Extraction As our data augmenta-tion technique requires a dictionary, we propose to extract the dictionary from parallel corpora using a statistical word aligner, eflomal [Cite_Footnote_7] (\u00d6stling and Tiedemann, 2016). Once we produce word align-ments between tokens in our parallel corpora, we simply take word alignments that appear over 20 times to produce our bilingual dictionary."
  },
  {
    "id": 190,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "pseudo-monolingual data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://data.statmt.org/cc-100/",
    "section_title": "4 Experimental Setup 4.1 Pretraining",
    "add_info": "10 http://data.statmt.org/cc-100/",
    "text": "Hyperparameters We use the following setup to train our AfroBART models, utilizing the mBART implementation in the fairseq library (Ott et al., 2019). We tokenize data using Sentence-Piece (Kudo and Richardson, 2018), using a 80K subword vocabulary. We use the Transformer-base architecture of a hidden dimension of 512, feed-forward size of 2048, and 6 layers for both the encoder and decoder. We set the maximum se-quence length to be 512, using a batch size of 1024 for 100K iterations with 32 NVIDIA V100 GPUs for one day. When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 \u00d7 [Cite_Footnote_10] \u22125 and warm up over 5K iterations and train for 35K iterations."
  },
  {
    "id": 191,
    "name": "mBART implementation",
    "fullname": "N/A",
    "genericmention": [
      "data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ott et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/pytorch/fairseq",
    "section_title": "4 Experimental Setup 4.1 Pretraining",
    "add_info": "12 https://github.com/pytorch/fairseq",
    "text": "Hyperparameters We use the following setup to train our AfroBART models, utilizing the mBART implementation in the fairseq [Cite_Footnote_12] library (Ott et al., 2019). We tokenize data using Sentence-Piece (Kudo and Richardson, 2018), using a 80K subword vocabulary. We use the Transformer-base architecture of a hidden dimension of 512, feed-forward size of 2048, and 6 layers for both the encoder and decoder. We set the maximum se-quence length to be 512, using a batch size of 1024 for 100K iterations with 32 NVIDIA V100 GPUs for one day. When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 \u00d7 \u22125 and warm up over 5K iterations and train for 35K iterations."
  },
  {
    "id": 192,
    "name": "SacreBLEU library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Post, 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/mjpost/sacrebleu",
    "section_title": "4 Experimental Setup 4.2 Finetuning",
    "add_info": "13 https://github.com/mjpost/sacrebleu",
    "text": "Evaluation We evaluate our system outputs us-ing two automatic evaluation metrics: detokenized BLEU (Papineni et al., 2002; Post, 2018) and chrF (Popovic\u0301, 2015). Although BLEU is a standard metric for machine translation, being cognizant of the morphological richness of the languages in the A FRO MT benchmark, we use chrF to measure per-formance at a character level. Both metrics are measured using the SacreBLEU library [Cite_Footnote_13] (Post, 2018)."
  },
  {
    "id": 193,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an exist-ing English POS tagger in the spaCy [Cite_Footnote_14] library"
    ],
    "description": [
      "English POS tagger"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "5 Results and Discussion 5.3 Fine-grained Language Analysis",
    "add_info": "14 https://spacy.io/",
    "text": "We further provide a suite of fine-grained analysis tools to compare the baseline systems. In partic-ular, we are interested in evaluating the transla-tion accuracy of noun classes in the considered African languages in the Niger-Congo family, as these languages are morphologically rich and of-ten have more than 10 classes based on the prefix of the word. For example, kitabu and vitabu in Swahili refer to book and books in English, respec-tively. Based on this language characteristic, our fine-grained analysis tool calculates the translation accuracy of the nouns with the top 10 most fre-quent prefixes in the test data. To do so, one of the challenges is to identify nouns in a sentence written in the target African language. However, there is no available part-of-speech (POS) tagger for these languages. To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy [Cite_Footnote_14] library to annotate the English source sentences. We then use the fast_align tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language. We then measure the transla-tion accuracy of the nouns in the African language by checking whether the correct nouns are included in the translated sentences by systems in compar-ison. Notably, our analysis tool can also measure the translation accuracy of the words in the other POS tags, (e.g. verbs, adjectives) which are often adjusted with different noun classes."
  },
  {
    "id": 194,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the fast_align [Cite_Footnote_15] tool"
    ],
    "description": [
      "fast_align [Cite_Footnote_15] tool"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/clab/fast_align",
    "section_title": "5 Results and Discussion 5.3 Fine-grained Language Analysis",
    "add_info": "15 https://github.com/clab/fast_align",
    "text": "We further provide a suite of fine-grained analysis tools to compare the baseline systems. In partic-ular, we are interested in evaluating the transla-tion accuracy of noun classes in the considered African languages in the Niger-Congo family, as these languages are morphologically rich and of-ten have more than 10 classes based on the prefix of the word. For example, kitabu and vitabu in Swahili refer to book and books in English, respec-tively. Based on this language characteristic, our fine-grained analysis tool calculates the translation accuracy of the nouns with the top 10 most fre-quent prefixes in the test data. To do so, one of the challenges is to identify nouns in a sentence written in the target African language. However, there is no available part-of-speech (POS) tagger for these languages. To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [Cite_Footnote_15] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language. We then measure the transla-tion accuracy of the nouns in the African language by checking whether the correct nouns are included in the translated sentences by systems in compar-ison. Notably, our analysis tool can also measure the translation accuracy of the words in the other POS tags, (e.g. verbs, adjectives) which are often adjusted with different noun classes."
  },
  {
    "id": 195,
    "name": "DBpedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://wiki.dbpedia.org/Downloads2015-04",
    "section_title": "4 Evaluation 4.3 End-to-End Evaluation",
    "add_info": "4 We used DBpedia long abstract: http://wiki.dbpedia.org/Downloads2015-04.",
    "text": "\u2022 TF*IDF weighting: This simple heuristic was introduced by Luhn (1958). Each sen-tence receives a score from the TF*IDF of its terms. We trained IDFs (Inverse Document Frequencies) on a background corpus [Cite_Footnote_4] to im-prove the original algorithm."
  },
  {
    "id": 196,
    "name": "sumy package",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/miso-belica/sumy",
    "section_title": "4 Evaluation 4.3 End-to-End Evaluation",
    "add_info": "5 https://github.com/miso-belica/sumy",
    "text": "the cosine similarity between them is above a given threshold. Sentences are scored ac-cording to their PageRank score in G. For our experiments, we use the implementation available in the sumy package. [Cite_Footnote_5]"
  },
  {
    "id": 197,
    "name": "SFOUR",
    "fullname": "N/A",
    "genericmention": [
      "the publicly available imple-mentation"
    ],
    "description": [
      "SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.cs.cornell.edu/\u02dcrs/sfour/",
    "section_title": "4 Evaluation 4.3 End-to-End Evaluation",
    "add_info": "6 http://www.cs.cornell.edu/\u02dcrs/sfour/",
    "text": "\u2022 SFOUR: SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE (Sipos et al., 2012). We use the publicly available imple-mentation. [Cite_Footnote_6]"
  },
  {
    "id": 198,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a real-world news recommendation dataset",
      "this dataset",
      "The news data in the last week",
      "the rest"
    ],
    "description": [
      "a real-world news recommendation dataset [Cite_Footnote_3] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/wuch15/NRHUB",
    "section_title": "4 Experiments 4.1 Datasets and Experimental Settings",
    "add_info": "3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.",
    "text": "We conducted experiments on a real-world news recommendation dataset [Cite_Footnote_3] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine. The detailed statistics of this dataset are summarized in Table 1. The news data in the last week is used for test, and the rest is used for model training. In addition, we randomly sampled 10% of the training data for validation."
  },
  {
    "id": 199,
    "name": "MSN News [Cite_Footnote_4] logs",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "MSN News [Cite_Footnote_4] logs during Dec. 13, 2018 and Jan. 12, 2019"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.msn.com/en-us/news",
    "section_title": "4 Experiments 4.1 Datasets and Experimental Settings",
    "add_info": "4 https://www.msn.com/en-us/news",
    "text": "We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_4] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine. The detailed statistics of this dataset are summarized in Table 1. The news data in the last week is used for test, and the rest is used for model training. In addition, we randomly sampled 10% of the training data for validation."
  },
  {
    "id": 200,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/syxu828/CSRL_dataset",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "In addition, we introduce a multi-task learning method with two new objectives. Experimental re-sults on benchmark datasets show that our model substantially outperforms existing baselines. Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at  https://github.com/syxu828/CSRL_dataset."
  },
  {
    "id": 201,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the dialogue act transition ta-ble"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html",
    "section_title": "4 Training on Speech Acts",
    "add_info": "1 Due to space constraints, the dialogue act transition ta-ble has been omitted from this paper and is made available at http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html",
    "text": "The use of P(d) in Equation 3 assumes that dia-logue acts are independent of one another. However, we intuitively know that if someone asks a Y ES -N O - Q UESTION then the response is more likely to be a Y ES -A NSWER rather than, say, C ONVENTIONAL - C LOSING . This intuition is reflected in the bigram transition probabilities obtained from our corpus. [Cite_Footnote_1]"
  },
  {
    "id": 202,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our dataset",
      "our dataset",
      "Our dataset"
    ],
    "description": [
      "the first benchmark for direct linguis-tic sense making and explanation"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/wangcunxiang/Sen-Making-and-Explanation",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Note that there has been dataset which fo-cus on non-linguistic world knowledge plausibil-ity (Wang et al., 2018) or only limited attributes or actions of physical knowledge like verbphysics (Forbes and Choi, 2017). They are related to our dataset but serve robotic research mainly. Our dataset is the first benchmark for direct linguis-tic sense making and explanation. We hope this benchmark can promote commonsense reason-ing by the NLP community, and further applied on other applications such as machine transla-tion and dialogue. Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at  https://github.com/wangcunxiang/Sen-Making-and-Explanation."
  },
  {
    "id": 203,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code of CLINE"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/kandorm/CLINE",
    "section_title": "1 Introduction",
    "add_info": "1 The source code of CLINE will be publicly available at https://github.com/kandorm/CLINE",
    "text": "To train a robust semantic-aware PLM, we pro-pose Contrastive Learning with semantIc Negative Examples (CLINE). CLINE is a simple and effec-tive method to generate adversarial and contrastive examples and contrastively learn from both of them. The contrastive manner has shown effectiveness in learning sentence representations (Luo et al., 2020; Wu et al., 2020; Gao et al., 2021), yet these studies neglect the generation of negative instances. In CLINE, we use external semantic knowledge, i.e., WordNet (Miller, 1995), to generate adversarial and contrastive examples by unsupervised replac-ing few specific representative tokens. Equipped by replaced token detection and contrastive objec-tives, our method gathers similar sentences with semblable semantics and disperse ones with differ-ent even opposite semantics, simultaneously im-proving the robustness and semantic sensitivity of PLMs. We conduct extensive experiments on sev-eral widely used text classification benchmarks to verify the effectiveness of CLINE. To be more spe-cific, our model achieves +1.6% absolute improve-ment on 4 contrastive test sets and +0.5% absolute improvement on 4 adversarial test sets compared to RoBERTa model (Liu et al., 2019). That is, with the training on the proposed objectives, CLINE si-multaneously gains the robustness of adversarial attacks and sensitivity of semantic changes [Cite_Footnote_1] ."
  },
  {
    "id": 204,
    "name": "spaCy",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "to conduct segmentation and POS for the original sentences, extracting verbs, nouns, adjectives, and adverbs"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/explosion/spaCy",
    "section_title": "3 Method 3.1 Generation of Examples",
    "add_info": "2 https://github.com/explosion/spaCy",
    "text": "We generate two sentences from the original in-put sequence x ori , which express substantially dif-ferent semantics but have few different words. One of the sentences is semantically close to x ori (de-noted as x syn ), while the other is far from or even opposite to x ori (denoted as x ant ). In specific, we utilize spaCy [Cite_Footnote_2] to conduct segmentation and POS for the original sentences, extracting verbs, nouns, adjectives, and adverbs. x syn is generated by re-placing the extracted words with synonyms, hy-pernyms and morphological changes, and x ant is generated by replacing them with antonyms and random words. For x syn , about 40% tokens are replaced. For x ant , about 20% tokens are replaced."
  },
  {
    "id": 205,
    "name": "Contrast Sets",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/allenai/contrast-sets",
    "section_title": "4 Experiments 4.3 Experiments on Contrastive Sets",
    "add_info": "3 https://github.com/allenai/contrast-sets",
    "text": "We evaluate our model on four contrastive sets: IMDB, PERSPECTRUM, BoolQ and SNLI, which were provided by Contrast Sets [Cite_Footnote_3] (Gardner et al., 2020). We compare our approach with BERT and"
  },
  {
    "id": 206,
    "name": "phrase2vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/artetxem/phrase2vec",
    "section_title": "3 Principled unsupervised SMT 3.1 Initial phrase-table",
    "add_info": "1 https://github.com/artetxem/phrase2vec",
    "text": "More concretely, we train our n-gram embed-dings using phrase2vec [Cite_Footnote_1] , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature \u03c4 is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction. In addition to the phrase translation probabilities in both direc-tions, the forward and reverse lexical weightings are also estimated by aligning each word in the tar-get phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. The reader is referred to Artetxe et al. (2018b) for more details."
  },
  {
    "id": 207,
    "name": "VecMap",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/artetxem/vecmap",
    "section_title": "3 Principled unsupervised SMT 3.1 Initial phrase-table",
    "add_info": "3 https://github.com/artetxem/vecmap",
    "text": "More concretely, we train our n-gram embed-dings using phrase2vec , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap [Cite_Footnote_3] with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature \u03c4 is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction. In addition to the phrase translation probabilities in both direc-tions, the forward and reverse lexical weightings are also estimated by aligning each word in the tar-get phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. The reader is referred to Artetxe et al. (2018b) for more details."
  },
  {
    "id": 208,
    "name": "Moses",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "http://www.statmt.org/moses/",
    "section_title": "5 Experiments and results",
    "add_info": "10 http://www.statmt.org/moses/",
    "text": "Our SMT implementation is based on Moses [Cite_Footnote_10] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018)."
  },
  {
    "id": 209,
    "name": "fairseq",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq",
    "section_title": "5 Experiments and results",
    "add_info": "11 https://github.com/pytorch/fairseq",
    "text": "Our SMT implementation is based on Moses , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq [Cite_Footnote_11] for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018)."
  },
  {
    "id": 210,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/artetxem/monoses",
    "section_title": "6 Conclusions and future work",
    "add_info": null,
    "text": "In this paper, we identify several deficiencies in previous unsupervised SMT systems, and pro-pose a more principled approach that addresses them by incorporating subword information, us-ing a theoretically well founded unsupervised tun-ing method, and developing a joint refinement pro-cedure. In addition to that, we use our improved SMT approach to initialize a dual NMT model that is further improved through on-the-fly back-translation. Our experiments show the effective-ness of our approach, as we improve the previous state-of-the-art in unsupervised machine transla-tion by 5-7 BLEU points in French-English and German-English WMT 2014 and 2016. Our code is available as an open source project at  https://github.com/artetxem/monoses."
  },
  {
    "id": 211,
    "name": "JUMAN",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html",
    "section_title": "2 Test Set for Evaluating Machine Translation Quality Translation Quality",
    "add_info": "4 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html",
    "text": "The term S i indicates a similarity between a trans-lated sentence and its reference translation, and \u03bb S i is a weight for the similarity. Many methods for cal-culating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gimen\u0301ez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (ex-act) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S\u2217, SU\u2217, W-1.2), were used to cal-culate each similarity S i . Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN [Cite_Footnote_4] in our experiments."
  },
  {
    "id": 212,
    "name": "Mecab",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://sourceforge.net/projects/mecab/files/",
    "section_title": "5 Experiments 5.1 Setting",
    "add_info": "4 http://sourceforge.net/projects/mecab/files/",
    "text": "We evaluated the effectiveness of the proposed ap-proach for Chinese-to-English (CE), Japanese-to- English (JE) and French-to-English (FE) transla-tion tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [Cite_Footnote_4] for Japanese. For the FE language pair, we used stan-dard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively."
  },
  {
    "id": 213,
    "name": "IRSTLM Toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://hlt.fbk.eu/en/irstlm",
    "section_title": "5 Experiments 5.1 Setting",
    "add_info": "5 http://hlt.fbk.eu/en/irstlm",
    "text": "For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [Cite_Footnote_5] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003)."
  },
  {
    "id": 214,
    "name": "Toronto Book Corpus",
    "fullname": "N/A",
    "genericmention": [
      "The corpus",
      "This corpus"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.toronto.edu/\u02dcmbweb/",
    "section_title": "3 Experimental Setup 3.1 Data",
    "add_info": "1 The corpus can be downloaded from http://www.cs.toronto.edu/\u02dcmbweb/;cf. (Zhu et al., 2015).",
    "text": "We use the Toronto Book Corpus [Cite_Footnote_1] to train word embeddings. This corpus contains 74,004,228 already pre-processed sentences in total, which are made up of 1,057,070,918 tokens, originating from 7,087 unique books. In our experiments, we consider tokens appearing 5 times or more, which leads to a vocabulary of 315,643 words."
  },
  {
    "id": 215,
    "name": "word2vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "3 Experimental Setup 3.2 Baselines",
    "add_info": "2 The code is available from https://code.google.com/archive/p/word2vec/.",
    "text": "We employ two baselines for producing sentence embeddings in our experiments. We obtain simi-larity scores between sentence pairs from the base-lines in the same way as the ones produced by Siamese CBOW, i.e., we calculate the cosine sim-ilarity between the sentence embeddings they pro-duce. Word2vec We average word embeddings trained with word2vec. [Cite_Footnote_2] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples."
  },
  {
    "id": 216,
    "name": "skip-thought architecture",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a recently proposed method that learns sentence representations in a different way from ours, by using recurrent neural networks",
      "This al-lows it to take word order into account."
    ],
    "citationtag": [
      "Kiros et al., 2015"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ryankiros/skip-thoughts/",
    "section_title": "3 Experimental Setup 3.2 Baselines",
    "add_info": "3 The code and the trained models can be down-loaded from https://github.com/ryankiros/skip-thoughts/.",
    "text": "Skip-thought As a second baseline we use the sentence representations produced by the skip-thought architecture (Kiros et al., 2015). [Cite_Footnote_3] Skip-thought is a recently proposed method that learns sentence representations in a different way from ours, by using recurrent neural networks. This al-lows it to take word order into account. As it trains sentence embeddings from unlabeled data, like we do, it is a natural baseline to consider."
  },
  {
    "id": 217,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code for Siamese CBOW"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://bitbucket.org/TomKenter/siamese-cbow",
    "section_title": "3 Experimental Setup 3.4 Network",
    "add_info": "4 The code for Siamese CBOW is available under an open-source license at https://bitbucket.org/TomKenter/siamese-cbow.",
    "text": "We use Theano (Theano Development Team, 2016) to implement our network. [Cite_Footnote_4] We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al., 2016)."
  },
  {
    "id": 218,
    "name": "Twit-ter streaming API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://dev.twitter.com/streaming/",
    "section_title": "1 Introduction",
    "add_info": "1 https://dev.twitter.com/streaming/overview. Last accessed on 10-01-2018.",
    "text": "In this paper, we first develop a temporal-orientation classifier to classify tweets into past, present, and future and then group over the users to create user-level assessments. We use a Bidi-rectional Long Short Term Memory (Bi-LSTM) network for tweet temporal classification where tweet vectors are fed to generate the classifica-tion model. We propose a hash tag-based mini-mally supervised method with the two-pass filter-ing to create the past, present and future-oriented tweets for the training of the Bi-LSTM network. We manually examined trending hashtags in Twit-ter for a specific period of time and selected hash-tags which represent past, present/ongoing, or fu-ture events. The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [Cite_Footnote_1] The tweet temporal orienta-tion classifier is validated on a manually annotated test set. Finally, we use this classifier to automat-ically classify a large dataset consisting of \u224810 million tweets from 5,191 users mapped to their user-level features."
  },
  {
    "id": 219,
    "name": "Glove vectors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pennington et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Methodology 3.1 Temporal Orientation Classification",
    "add_info": "2 https://nlp.stanford.edu/projects/glove/",
    "text": "Our experiment uses Bi-LSTM with 200 neu-rons at the input layer. The loss function we used is categorical cross-entropy and the opti-mizer used is Root Mean Square Propagation (rm-sprop). We repeat the training for 100 number of epochs with batch size set to 128. We also employ dropout (Srivastava et al., 2014) for reg-ularization with a dropout rate of 0.2 to prevent over-fitting. All of these attributes are finalized by parameter tuning with the performance obtained on 10-fold cross-validation using the grid search method. Tweet vectors are generated by existing Glove vectors (Pennington et al., 2014) for tweets [Cite_Footnote_2] of 200 dimensions which are trained on 27 billion tweets. We also validate our model on the valida-tion set which was 10% of the training set."
  },
  {
    "id": 220,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All the developed resources",
      "the datasets",
      "Train-ing set",
      "the test set",
      "The user-level tweets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.iitp.ac.in/\u02dcai-nlp-ml/resources.html",
    "section_title": "4 Data Sets",
    "add_info": "4 All the developed resources are available at http://www.iitp.ac.in/\u02dcai-nlp-ml/resources.html",
    "text": "For experiments we categorize the datasets into three kinds: training, test and user-level. Train-ing set consists of 27k tweets, whereas the test set is manually annotated with 741 tweets. [Cite_Footnote_4] The user-level tweets consist of \u224810 million tweets from 5,191 users mapped to their user-level features."
  },
  {
    "id": 221,
    "name": "Twitter streaming API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://developer.twitter.com/en/docs",
    "section_title": "4 Data Sets 4.1 Training Set",
    "add_info": "5 https://developer.twitter.com/en/docs",
    "text": "Training tweets are collected using the Twitter streaming API. [Cite_Footnote_5] The tweets are collected for the duration of September 2017 and October 2017. We consider day-wise trending topics during this period. We only consider those hashtags which signify a temporal event. Finally, we chose world-wide trending events and collected the tweets based on the hashtags."
  },
  {
    "id": 222,
    "name": "GoogleSets",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "the North American Chapter of the ACL, pages 290\u2013298,"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://labs.google.com/sets",
    "section_title": "1 Introduction",
    "add_info": "1 http://labs.google.com/sets the North American Chapter of the ACL, pages 290\u2013298,",
    "text": "Even for state of the art methods, expansion er-rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art systems such as GoogleSets [Cite_Footnote_1] , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed instances for the target set Roman Gods:"
  },
  {
    "id": 223,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the distributional thesaurus"
    ],
    "description": [
      "a distributional similarity thesaurus"
    ],
    "citationtag": [
      "Lin 1998"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://demo.patrickpantel.com/",
    "section_title": "1 Introduction",
    "add_info": "2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus.",
    "text": "The inherent semantic similarity between the errors can be leveraged to quickly clean up the expan-sion. For example, given a manually tagged error \u201casteroid\u201d, a distributional similarity thesaurus such as (Lin 1998) [Cite_Footnote_2] can identify comet as similar to asteroid and therefore potentially also as an error. This method has its limitations since a manually tagged error such as Earth would correctly remove Moon and Sun, but it would also incorrectly re-move Mars, Venus and Jupiter since they are also similar to Earth ."
  },
  {
    "id": 224,
    "name": "PKPB",
    "fullname": "Penn Korean PropBank",
    "genericmention": [
      "the two Korean corpora"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "http://catalog.ldc.upenn.edu/LDC2006T03",
    "section_title": "2 A Semantically Annotated Korean Corpus",
    "add_info": "2 http://catalog.ldc.upenn.edu/LDC2006T03",
    "text": "We view our work as building on the efforts of the Penn Korean PropBank (PKPB). [Cite_Footnote_2] Our corpus is roughly similar in size to the PKPB, and taken together, the two Korean corpora now total about half the size of the Penn English PropBank. One advantage of our corpus is that it is built on top of the ETRI Korean corpus, which uses a richer Ko-rean morphological tagging scheme than the Penn Korean Treebank. Our experiments will show that these finer-grained tags are crucial for achieving high SRL accuracy."
  },
  {
    "id": 225,
    "name": "Donga news article corpus",
    "fullname": "N/A",
    "genericmention": [
      "this corpus"
    ],
    "description": [
      "The Donga cor-pus contains 366,636 sentences with 25.09 words on average.",
      "The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.donga.com",
    "section_title": "6 Experiments and Results",
    "add_info": "3 http://www.donga.com",
    "text": "For latent morpheme representations, we used the Donga news article corpus. [Cite_Footnote_3] The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions."
  },
  {
    "id": 226,
    "name": "Kokoma Korean morpheme analyzer",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Korean morpheme analyzer"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://kkma.snu.ac.kr/",
    "section_title": "6 Experiments and Results",
    "add_info": "4 http://kkma.snu.ac.kr/",
    "text": "For latent morpheme representations, we used the Donga news article corpus. The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer [Cite_Footnote_4] on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions."
  },
  {
    "id": 227,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "he col-lection",
      "this dataset"
    ],
    "description": [
      "Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set.",
      "We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/cxsoto/article-regions",
    "section_title": "4 Novel Labeled Dataset",
    "add_info": null,
    "text": "Therefore, a novel dataset was created. We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015) to manually annotate regions. Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set. The col-lection will be available at  https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats. The default format is PASCAL VOC. Nine labeled region classes are included in the annotations:"
  },
  {
    "id": 228,
    "name": "YOLOv3",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Erik Linder-Noren. 2018. A mini-mal pytorch implementation of yolov3.",
      "Linder-Noren, 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/eriklindernoren/PyTorch-YOLOv3",
    "section_title": "5 Implementation and Experiments",
    "add_info": "Erik Linder-Noren. 2018. A mini-mal pytorch implementation of yolov3. https://github.com/eriklindernoren/PyTorch-YOLOv3.",
    "text": "Figure 4 shows per-class performance over 30 training epochs, as well as comparative per-formance against the baseline Faster R-CNN model and reference model implementations of YOLOv3 (Linder-Noren, 2018)  and RetinaNet (Henon, 2018). Most models plateaued early on this small dataset, except YOLOv3 which peak-ing at 68.9% after 49 epochs (beyond the figure bounds, but still below our model\u2019s results). Pro-cessing time for our model averaged 0.65 seconds per article. By contrast, CERMINE averaged 9.4 seconds per article on the same set of articles, on the same machine."
  },
  {
    "id": 229,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an open source utility"
    ],
    "description": [
      "an open source utility (Tzutalin, 2015) [Cite_Ref] to manually annotate regions"
    ],
    "citationtag": [
      "Tzutalin. 2015.",
      "Tzutalin, 2015"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/tzutalin/labelImg",
    "section_title": "4 Novel Labeled Dataset",
    "add_info": "Tzutalin. 2015. Labelimg. https://github.com/tzutalin/labelImg.",
    "text": "Therefore, a novel dataset was created. We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015)  to manually annotate regions. Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set. The col-lection will be available at https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats. The default format is PASCAL VOC. Nine labeled region classes are included in the annotations:"
  },
  {
    "id": 230,
    "name": "Faster R-CNN implementation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh. 2017. A faster pytorch implementation of faster r-cnn.",
      "Yang et al., 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/jwyang/faster-rcnn.pytorch",
    "section_title": "5 Implementation and Experiments",
    "add_info": "Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh. 2017. A faster pytorch implementation of faster r-cnn. https://github.com/jwyang/faster-rcnn.pytorch.",
    "text": "Using the novel labeled dataset described in Sec-tion 4, a baseline model was trained using a stan-dard Faster R-CNN implementation (Yang et al., 2017)  . The model was trained using a single NVIDIA P100 GPU for 30 epochs on 600 images, and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pre-trained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0.0001, with decay of 0.1 every 5 epochs. Stan-dard anchor scales of [8, 16, 32] and anchor ratios of [0.5, 1.0, 2.0] were used. At a intersection-over-union (IOU) threshold of 0.5, the model achieved a mean average precision (mAP) of 46.38% on all nine region labels, with peak class performance on \u2018body\u2019 regions (87.49%) and lowest performance on \u2018authors\u2019 (1.22%)."
  },
  {
    "id": 231,
    "name": "NeuronBlocks",
    "fullname": "N/A",
    "genericmention": [
      "This toolkit"
    ],
    "description": [
      "a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture",
      "This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/Microsoft/NeuronBlocks",
    "section_title": "References",
    "add_info": "1 Code: https://github.com/Microsoft/NeuronBlocks",
    "text": "Deep Neural Networks (DNN) have been widely employed in industry to address vari-ous Natural Language Processing (NLP) tasks. However, many engineers find it a big over-head when they have to choose from multi-ple frameworks, compare different types of models, and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [Cite_Footnote_1] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effective-ness of NeuronBlocks."
  },
  {
    "id": 232,
    "name": "NeuronBlocks",
    "fullname": "N/A",
    "genericmention": [
      "This toolkit"
    ],
    "description": [
      " toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture",
      "This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://youtu.be/x6cOpVSZcdo",
    "section_title": "References",
    "add_info": "2 Demo: https://youtu.be/x6cOpVSZcdo",
    "text": "Deep Neural Networks (DNN) have been widely employed in industry to address vari-ous Natural Language Processing (NLP) tasks. However, many engineers find it a big over-head when they have to choose from multi-ple frameworks, compare different types of models, and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [Cite_Footnote_2] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effective-ness of NeuronBlocks."
  },
  {
    "id": 233,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Yupei-Du/bias-in-wat",
    "section_title": "References",
    "add_info": "1 Our code is publicly available at https://github.com/Yupei-Du/bias-in-wat.",
    "text": "Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really re-flect true bias levels? For a small set of words (e.g. occupations), we can rely on hu-man annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which con-tains rich types of word connections anno-tated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, compar-ing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words. [Cite_Footnote_1]"
  },
  {
    "id": 234,
    "name": "TDT document clusters",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "TDT document clusters for 2 instances of airplane crashes, [Cite_Footnote_3] instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://news.bbc.co.uk/shared/bsp/search2/advanced/news_ifs.stm",
    "section_title": "4 Data Description 4.2 Test Data",
    "add_info": "3 http://news.bbc.co.uk/shared/bsp/search2/advanced/news_ifs.stm",
    "text": "To test our system, we used document clusters from the Topic Detection and Tracking (TDT) cor-pus (Fiscus et al., 1999). Each TDT topic has a topic label, such as Accidents or Natural Disas-ters. 4 These categories are broader than our do-mains. Thus, we manually filtered the TDT topics relevant to our four training domains (e.g., Acci-dents matching Airplane Crashes). In this way, we obtained TDT document clusters for 2 instances of airplane crashes, [Cite_Footnote_3] instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrorist attacks). This variation in the number of documents per topic is typical for the TDT corpus. Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents."
  },
  {
    "id": 235,
    "name": "FREQuent Tree miner",
    "fullname": "N/A",
    "genericmention": [
      "This software"
    ],
    "description": [
      "This software is an implementation of the algorithm presented by (Abe et al., 2002; Zaki, 2002), which extracts frequent ordered subtrees from a set of ordered trees."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.org/\u02dctaku/software/freqt/",
    "section_title": "5 Creating Templates",
    "add_info": "5 http://chasen.org/\u02dctaku/software/freqt/",
    "text": "Step 3: Identify most frequent subtrees containing the top 50 verbs. A domain template should con-tain not only the most important actions for the do-main, but also the entities that are linked to these actions or to each other through these actions. The lexemes referring to such entities can potentially be used within the domain template slots. Thus, we analyze those portions of the syntactic trees which contain the verbs themselves plus other lex-emes used in the same subtrees as the verbs. To do this we use FREQuent Tree miner. [Cite_Footnote_5] This software is an implementation of the algorithm presented by (Abe et al., 2002; Zaki, 2002), which extracts frequent ordered subtrees from a set of ordered trees. Following (Sudo et al., 2003) we are inter-ested only in the lexemes which are near neighbors of the most frequent verbs. Thus, we look only for those subtrees which contain the verbs themselves and from four to ten tree nodes, where a node is either a syntactic tag or a lexeme with its tag. We analyze not only NPs which correspond to the sub-ject or object of the verb, but other syntactic con-stituents as well. For example, PPs can potentially link the verb to locations or dates, and we want to include this information into the template. Table 1 contains a sample of subtrees for the terrorist at-tack domain mined from the sentences containing the verb killed. The first column of Table 1 shows how many nodes are in the subtree."
  },
  {
    "id": 236,
    "name": "subreddits WRITINGPROMPTS",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.reddit.com/r/WritingPrompts/",
    "section_title": "2 SCOPE: Style Transfer through COmmonsense PropErty 2.1 Automatic Parallel Corpus Creation",
    "add_info": "5 https://www.reddit.com/r/WritingPrompts/",
    "text": "Simile Dataset Collection. One of the possible ways to collect similes would be to train a super-vised model using existing data and methods for simile detection but most data sets are very small in size (in the order of a few hundreds). The only large-scale dataset is that of (Niculae and Danescu-Niculescu-Mizil, 2014), however their data is from a rather restricted domain of product reviews on Amazon, which might lack variety, diversity and creativity needed for this task. For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS [Cite_Footnote_5] and FUNNY from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, \u201cThe boy was as strong as an ox\u201d gives strong as the PROPERTY shared by the boy and ox. But most similes do not give an explicit PROP-ERTY such as the Open Simile (e.g., \u201cThe boy was like an ox\u201d) leaving the reader to infer that the boy is strong/large/fast (Qadir et al., 2016). Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io to mine comments. Through this process we collect 87,843 from Reddit. For each example, we show the top five commonsense properties associated with the vehicle obtained from COMET, and the best literal sentence constructed from these properties. The blue italic texts in the literal sentences represent the property inferred from the vehicle in the simile (denoted in black italic). self-labeled human written similes, from which we use 82,697 samples for training and 5,146 for validation."
  },
  {
    "id": 237,
    "name": "FUNNY",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.reddit.com/r/funny/",
    "section_title": "2 SCOPE: Style Transfer through COmmonsense PropErty 2.1 Automatic Parallel Corpus Creation",
    "add_info": "6 https://www.reddit.com/r/funny/",
    "text": "Simile Dataset Collection. One of the possible ways to collect similes would be to train a super-vised model using existing data and methods for simile detection but most data sets are very small in size (in the order of a few hundreds). The only large-scale dataset is that of (Niculae and Danescu-Niculescu-Mizil, 2014), however their data is from a rather restricted domain of product reviews on Amazon, which might lack variety, diversity and creativity needed for this task. For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS and FUNNY [Cite_Footnote_6] from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, \u201cThe boy was as strong as an ox\u201d gives strong as the PROPERTY shared by the boy and ox. But most similes do not give an explicit PROP-ERTY such as the Open Simile (e.g., \u201cThe boy was like an ox\u201d) leaving the reader to infer that the boy is strong/large/fast (Qadir et al., 2016). Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io to mine comments. Through this process we collect 87,843 from Reddit. For each example, we show the top five commonsense properties associated with the vehicle obtained from COMET, and the best literal sentence constructed from these properties. The blue italic texts in the literal sentences represent the property inferred from the vehicle in the simile (denoted in black italic). self-labeled human written similes, from which we use 82,697 samples for training and 5,146 for validation."
  },
  {
    "id": 238,
    "name": "COMET",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "COMET is an adaptation framework for constructing common-sense knowledge based on pre-trained language models."
    ],
    "citationtag": [
      "Bosselut et al., 2019"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://mosaickg.apps.allenai.org/comet_conceptnet",
    "section_title": "2 SCOPE: Style Transfer through COmmonsense PropErty 2.1 Automatic Parallel Corpus Creation",
    "add_info": "9 https://mosaickg.apps.allenai.org/comet_conceptnet",
    "text": "To generate the common sense PROPERTY that is implied by the VEHICLE in the simile, we take advantage of the simple syntactic structure of a simile. We extract the VEHICLE by extract-ing the phrase after like a and feed it as input to COMET (Bosselut et al., 2019). COMET is an adaptation framework for constructing common-sense knowledge based on pre-trained language models. Our work only leverages the HasProp-erty relation from COMET [Cite_Footnote_9] ."
  },
  {
    "id": 239,
    "name": "pre-trained COMET model",
    "fullname": "N/A",
    "genericmention": [
      "the model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/atcbosselut/comet-commonsense",
    "section_title": "References",
    "add_info": "11 https://github.com/atcbosselut/comet-commonsense",
    "text": "For retrieving commonsense properties of the vehi-cle, we use the pre-trained COMET model [Cite_Footnote_11] and retrieve top 5 candidates for each input. Vehicle and Overall Quality. WORKERS denote num-ber of workers employed for each task and \u03b1 denotes Krippendorff\u2019s alpha (\u03b1 ) , reliability coefficient used for our study scheme (Fan et al., 2018). At each timestep, the model generates the probability of each word in the vocabulary being the likely next word. We randomly sample from the k = 5 most likely candidates from this distribution. We also use a softmax temperature of 0.7."
  },
  {
    "id": 240,
    "name": "FAIRSEQ",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ott et al., 2019"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "section_title": "References",
    "add_info": "12 https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "text": "1. Number of Parameters: For BART we use the BART large checkpoint (400M parame-ters) and use the implementation by FAIRSEQ (Ott et al., 2019). [Cite_Footnote_12]"
  },
  {
    "id": 241,
    "name": "SentiWordNet",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "The set of semantic orientation scores of all WordNet synsets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://sentiwordnet.isti.cnr.it/",
    "section_title": "2 Related Work",
    "add_info": "2 http://sentiwordnet.isti.cnr.it/",
    "text": "Esuli and Sebastiani (2006) used a supervised algorithm to attach semantic orientation scores to WordNet glosses. They train a set of ternary clas-sifiers using different training data and learning methods. The set of semantic orientation scores of all WordNet synsets is released by the name SentiWordNet. [Cite_Footnote_2] An evaluation of SentiWordNet by comparing orientation scores of about 1,000 WordNet glosses to scores assigned by human an-notators is presented in Esuli (2008). Our ap-proach uses a Roget-like thesaurus, and it does not use any supervised classifiers."
  },
  {
    "id": 242,
    "name": "MPQA corpus",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "The MPQA corpus contains news articles man-ually annotated for opinions and private states.",
      "it also has polarity annotations (posi-tive/negative) at the phrase-level",
      "collection of positive and neg-ative phrases (1,726 positive and 4,485 negative)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cs.pitt.edu/mpqa",
    "section_title": "4 Evaluation 4.2 Extrinsic: Identifying phrase polarity",
    "add_info": "4 http://www.cs.pitt.edu/mpqa",
    "text": "The MPQA corpus contains news articles man-ually annotated for opinions and private states. [Cite_Footnote_4] Notably, it also has polarity annotations (posi-tive/negative) at the phrase-level. We conducted an extrinsic evaluation of the manually-generated and automatically-generated lexicons by using them to determine the polarity of phrases in the MPQA version 1.1 collection of positive and neg-ative phrases (1,726 positive and 4,485 negative)."
  },
  {
    "id": 243,
    "name": "NodeXL network analy-sis tool",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Smith et al., 2009"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.codeplex.com/NodeXL",
    "section_title": "6 Visualizing the semantic orientation of thesaurus categories",
    "add_info": "5 Available from http://www.codeplex.com/NodeXL",
    "text": "As discussed in Section 3.1.1, the affix seeds set connects the thesaurus words with opposite se-mantic orientation. Usually these pairs of words occur in different thesaurus categories, but this is not necessary. We can think of these connections as relationships of contrast in meaning and seman-tic orientation, not just between the two words but also between the two categories. To better aid our understanding of the automatically deter-mined category relationships we visualized this network using the Fruchterman-Reingold force-directed graph layout algorithm (Fruchterman and Reingold, 1991) and the NodeXL network analy-sis tool (Smith et al., 2009) [Cite_Footnote_5] ."
  },
  {
    "id": 244,
    "name": "L6 Yahoo! An-swers Comprehensive Questions and Answers cor-pus",
    "fullname": "N/A",
    "genericmention": [
      "This dataset",
      "this larger dataset",
      "the L6 dataset"
    ],
    "description": [
      "This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://webscope.sandbox.yahoo.com/",
    "section_title": "3 Experiments 3.1 Data",
    "add_info": "4 http://webscope.sandbox.yahoo.com/",
    "text": "Our approach requires unlabelled data for unsu-pervised pre-training of the word and paragraph vec-tors. For these purposes we use the L6 Yahoo! An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [Cite_Footnote_4] This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. The Yahoo! Answers man-ner question dataset prepared by Jansen et al. (2014) and described in the previous paragraph, was ini-tially sampled from this larger dataset. We want to emphasize that the L6 dataset is only used for unsu-pervised pretraining \u2013 no meta-information is used in our experiments."
  },
  {
    "id": 245,
    "name": "English Gigaword corpus,",
    "fullname": "N/A",
    "genericmention": [
      "this cor-pus"
    ],
    "description": [
      "the English Gigaword corpus, [Cite_Footnote_6] which contains data from several English newswire sources"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2003T05",
    "section_title": "3 Experiments 3.1 Data",
    "add_info": "6 https://catalog.ldc.upenn.edu/LDC2003T05",
    "text": "We also experiment with the English Gigaword corpus, [Cite_Footnote_6] which contains data from several English newswire sources. Jansen et al. (2014) used this cor-pus to train word embeddings, which were then in-cluded as features in their answer reranker."
  },
  {
    "id": 246,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The data",
      "the gensim [Cite_Footnote_7] implementation of the DBOW and DM paragraph vector models"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://radimrehurek.com/gensim/models/doc2vec.html",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "7 https://radimrehurek.com/gensim/models/doc2vec.html",
    "text": "We use the gensim [Cite_Footnote_7] implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. The data was tokenized with the Stan-ford tokenizer and then lowercased."
  },
  {
    "id": 247,
    "name": "word2vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "8 https://code.google.com/p/word2vec/",
    "text": "We use the gensim implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. [Cite_Footnote_8] The data was tokenized with the Stan-ford tokenizer and then lowercased."
  },
  {
    "id": 248,
    "name": "Stan-ford tokenizer",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/tokenizer.shtml",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "9 http://nlp.stanford.edu/software/tokenizer.shtml",
    "text": "We use the gensim implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. The data was tokenized with the Stan-ford tokenizer [Cite_Footnote_9] and then lowercased."
  },
  {
    "id": 249,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code of our model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Justin1904/Low-rank-Multimodal-Fusion",
    "section_title": "4 Experimental Methodology 4.3 Model Architecture",
    "add_info": "2 The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion",
    "text": "In order to compare our fusion method with previ-ous work, we adopt a simple and straightforward model architecture [Cite_Footnote_2] for extracting unimodal rep-resentations. Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as f a , f v , f l , to extract unimodal representations z a , z v , z l from uni-modal input features x a , x v , x l . For acoustic and visual modality, the sub-embedding network is a simple 2-layer feed-forward neural network, and for language modality, we used an LSTM (Hochre-iter and Schmidhuber, 1997) to extract represen-tations. The model architecture is illustrated in Figure 1."
  },
  {
    "id": 250,
    "name": "TextEssence",
    "fullname": "N/A",
    "genericmention": [
      "the sys-tem"
    ],
    "description": [
      "an interactive system designed to enable comparative analysis of cor-pora using embeddings",
      "TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://textessence.github.io",
    "section_title": "References",
    "add_info": null,
    "text": "Embeddings of words and concepts capture syntactic and semantic regularities of lan-guage; however, they have seen limited use as tools to study characteristics of different cor-pora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of cor-pora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for cor-pus analysis. A case study on COVID-19 sci-entific literature illustrates the utility of the sys-tem. TextEssence can be found at  https://textessence.github.io."
  },
  {
    "id": 251,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our implementation and experimental code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/drgriffis/",
    "section_title": "7 Conclusion",
    "add_info": null,
    "text": "TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at  https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io."
  },
  {
    "id": 252,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the database derived from our CORD-19 analysis"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://doi.org/10.5281/zenodo.4432958",
    "section_title": "7 Conclusion",
    "add_info": null,
    "text": "TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at  https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io."
  },
  {
    "id": 253,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset, along with associated scripts"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://infotabs.github.io/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "The dataset, along with associated scripts, are avail-able at  https://infotabs.github.io/."
  },
  {
    "id": 254,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code and the processed datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/mponza/SalIE",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/mponza/SalIE",
    "text": "The source code and the processed datasets are publicly available [Cite_Footnote_1] to encourage further develop-ments of the fact salience task."
  },
  {
    "id": 255,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/zdou0830/DAFE",
    "section_title": "References",
    "add_info": "1 Our code is publicly available at: https://github.com/zdou0830/DAFE.",
    "text": "The recent success of neural machine transla-tion models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised do-main adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these meth-ods use generic representations for text regard-less of domain shift, which makes it infeasible for translation models to control outputs con-ditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental set-tings. In addition, we show that combining our method with back translation can further improve the performance of the model. [Cite_Footnote_1]"
  },
  {
    "id": 256,
    "name": "TED",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Kevin Duh. 2018. The multitarget ted talks task."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/",
    "section_title": "3 Experiments 3.1 Setup",
    "add_info": "Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/.",
    "text": "Datasets. We validate our models in two differ-ent data settings. First, we train on the law, medi-cal and IT datasets of the German-English OPUS corpus (Tiedemann, 2012) and test our methods\u2019 ability to adapt from one domain to another. The dataset contain 2K development and test sentences in each domain, and about 715K, 1M and 337K training sentences respectively. These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018)  and law, medical OPUS datasets. For this setting, we con-sider two language pairs, namely Czech and Ger-man to English. The Czech-English and German-English datasets consist of 1M and 4.5M sentences and the development and test sets contain about 2K sentences."
  },
  {
    "id": 257,
    "name": "ProofWiki",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "ProofWiki is an online compendium of mathemati-cal proofs, with a goal to collect and classify math-ematical proofs.",
      "ProofWiki contains links between theorems, definitions and axioms in the context of a mathematical proof, determining which dependen-cies are present."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://proofwiki.org/",
    "section_title": "3 The Natural Language Premise Selection task",
    "add_info": "1 http://proofwiki.org/",
    "text": "In order to evaluate the premise selection, we used a corpus extracted from ProofWiki [Cite_Footnote_1] . ProofWiki is an online compendium of mathemati-cal proofs, with a goal to collect and classify math-ematical proofs. ProofWiki contains links between theorems, definitions and axioms in the context of a mathematical proof, determining which dependen-cies are present. Definitions and axioms are state-ments accepted without formal proof, while theo-rems, lemmas and corollaries require one (Solow, 2002). All entries are composed by a statement written in a combination of natural language and mathematical latex notation. The extracted cor-pus, which is named PS-ProofWiki, contains more than 18, 000 entries. We also computed how many times each statement is used as a premise, and we observed that most of the statements are used as dependencies for only a small subset of premises. A total of 6, 866 statements has between one and three dependants. On average, statements contain a total length of 289 symbols (characters and math-ematical symbols). The specific number of tokens will depend on the type of tokenisation used for the mathematical symbols. A complete analysis of this corpus is made available in (Ferreira and Freitas, 2020)."
  },
  {
    "id": 258,
    "name": "Ubuntu dialogue corpus",
    "fullname": "N/A",
    "genericmention": [
      "The original training data",
      "The validation data",
      "the test data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator",
    "section_title": "5 Experiments 5.1 Experimental Settings 5.1.1 Datasets",
    "add_info": "1 https://github.com/rkadlec/ubuntu-ranking-dataset-creator",
    "text": "We use two public datasets in our experiments. For the specific-requirement scenario, we use the Ubuntu dialogue corpus [Cite_Footnote_1] extracted from Ubuntu question-answering forum, named Ubuntu (Lowe et al., 2015). The original training data consists of 7 million conversational post-responses pairs from 2014 to April 27,2012. The validation data are conversational pairs from April 27,2014 to Au-gust 7,2012, and the test data are from August 7,2012 to December 1,2012. We set the number of positive examples as 4,000,000 in the Github to directly sample data from the whole corpus. Then we construct post and response pairs based on the period from both context and utterance. We also conduct some data pro-processing. For ex-ample, we use the official script to tokenize, stem and lemmatize, and the duplicates and sentences with length less than 5 or longer than 50 are re-moved. Finally, we obtain 3,200,000, 100,000 and 100,000 for training, validation and testing, re-spectively."
  },
  {
    "id": 259,
    "name": "STC",
    "fullname": "N/A",
    "genericmention": [
      "It"
    ],
    "description": [
      "It consists of 3,788,571 post-response pairs extracted from the Chinese Weibo website and cleaned by the data publishers.",
      "We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. "
    ],
    "citationtag": [
      "Shang et al., 2015"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/zhanghainan/TailoredSeq2Seq2DifferentConversationScenarios",
    "section_title": "5 Experiments 5.1 Experimental Settings 5.1.1 Datasets",
    "add_info": "2 https://github.com/zhanghainan/TailoredSeq2Seq2DifferentConversationScenarios",
    "text": "For the diverse-requirement scenario, we use the Chinese Weibo dataset, named STC (Shang et al., 2015). It consists of 3,788,571 post-response pairs extracted from the Chinese Weibo website and cleaned by the data publishers. We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. [Cite_Footnote_2]"
  },
  {
    "id": 260,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code",
      "Our approach",
      "our method",
      "our method",
      "our method"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/yangkevin2/emnlp2020-stream-beam-mt",
    "section_title": "1 Introduction",
    "add_info": "1 Code available at https://github.com/yangkevin2/emnlp2020-stream-beam-mt.",
    "text": "We apply our method to variable-width beam search. For variable-output-length decoding even in batched settings, variable-width beam search often modestly decreases accuracy in exchange for substantial speedups over fixed-width beam search (Freitag and Al-Onaizan, 2017; Wu et al., 2016). When decoding with Fairseq\u2019s state-of-the-art WMT\u201919 model (Ng et al., 2019), our method further improves over the speed of base-line variable-width beam search: up to 16.5% on a 32GB V100 GPU, without changing BLEU (Pa-pineni et al., 2002). Our approach also improves decoding efficiency in lightweight models for se-mantic and syntactic parsing. [Cite_Footnote_1] In principle, our method can be applied to any task which sequen-tially processes variable-length data."
  },
  {
    "id": 261,
    "name": "newstest2018 and new-stest2017",
    "fullname": "N/A",
    "genericmention": [
      "datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.statmt.org/wmt19/translation-task.html",
    "section_title": "- Hyperparameters",
    "add_info": null,
    "text": "In Tables 6 and 7, we present results from applying our method to the JOBS and GEO datasets. We use the same hyperparameters and heuristics as for ATIS, and operate under the same candidate-expansion constraint. V AR -S TREAM is substan-tially faster than Fixed and V AR -B ATCH under this setting. A.5 Dataset Details A.5.1 Machine Translation Evaluation datasets (newstest2018 and new-stest2017) are available at  http://www.statmt.org/wmt19/translation-task.html. new-stest2018 contains 2998 and 3000 examples for De-En and Ru-En respectively, while newstest2017 contains 3004 and 3001."
  },
  {
    "id": 262,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/Alex-Fabbri/",
    "section_title": "- A.5.2 Semantic Parsing",
    "add_info": null,
    "text": "Datasets can be obtained by running the data scripts at  https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch. We use Dong and Lapata (2016)\u2019s training, development (for ATIS), and test sets. ATIS, JOBS, and GEO contain 5410, 640, and 880 examples respectively."
  },
  {
    "id": 263,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our model",
      "Implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/neulab/NL2code",
    "section_title": "1 Introduction",
    "add_info": "1 Implementation available at https://github.com/neulab/NL2code",
    "text": "Experiments (\u00a7 5) on two Python code gener-ation tasks show 11.7% and 9.3% absolute im-provements in accuracy against the state-of-the-art system (Ling et al., 2016). Our model also gives competitive performance on a standard semantic parsing benchmark [Cite_Footnote_1] ."
  },
  {
    "id": 264,
    "name": "lamtram",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a strong neural machine translation ( NMT ) system",
      "A toolkit for lan-guage and translation modeling using neural net-works"
    ],
    "citationtag": [
      "Graham Neubig. 2015. lamtram: A toolkit for lan-guage and translation modeling using neural net-works.",
      "Neubig, 2015"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://www.github.com/neubig/lamtram",
    "section_title": "5 Experimental Evaluation 5.3 Results",
    "add_info": "Graham Neubig. 2015. lamtram: A toolkit for lan-guage and translation modeling using neural net-works. http://www.github.com/neubig/lamtram.",
    "text": "Evaluation results for Python code generation tasks are listed in Tab. 3. Numbers for our sys-tems are averaged over three runs. We compare primarily with two approaches: (1) Latent Pre-dictor Network ( LPN ), a state-of-the-art sequence-to-sequence code generation model (Ling et al., 2016), and (2) S EQ 2T REE , a neural semantic pars-ing model (Dong and Lapata, 2016). S EQ 2T REE generates trees one node at a time, and the tar-get grammar is not explicitly modeled a priori, but implicitly learned from data. We test both the original S EQ 2T REE model released by the au-thors and our revised one (S EQ 2T REE \u2013UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015). For completeness, we also compare with a strong neural machine translation ( NMT ) system (Neubig, 2015)  using a standard encoder-decoder architecture with atten-tion and unknown word replacement , and include numbers from other baselines used in Ling et al. (2016). On the HS dataset, which has relatively large ASTs, we use unary closure for our model and S EQ 2T REE , and for D JANGO we do not."
  },
  {
    "id": 265,
    "name": "MeSH",
    "fullname": "(Medical Subject Head-ings) lexical hierarchy",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "In MeSH, each concept is assigned one or more alphanumeric descriptor codes corresponding to particular positions in the hierarchy.",
      "For example, A (Anatomy), A01 (Body Regions), A01.456 (Head), A01.456.505 (Face), A01.456.505.420 (Eye)."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.nlm.nih.gov/mesh",
    "section_title": "2 MeSH and Medline",
    "add_info": "1 http://www.nlm.nih.gov/mesh",
    "text": "In this paper we use the MeSH (Medical Subject Head-ings) lexical hierarchy [Cite_Footnote_1] , but the approach should be equally applicable to other domains using other thesauri and ontologies. In MeSH, each concept is assigned one or more alphanumeric descriptor codes corresponding to particular positions in the hierarchy. For example, A (Anatomy), A01 (Body Regions), A01.456 (Head), A01.456.505 (Face), A01.456.505.420 (Eye). Eye is ambiguous according to MeSH and has a second code: A09.371 (A09 represents Sense Organs)."
  },
  {
    "id": 266,
    "name": "Hugging-Face Transformers (Wolf et al., 2020) [Cite_Footnote_2] library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2020"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "4 Experiment Settings",
    "add_info": "2 https://github.com/huggingface/transformers",
    "text": "Supervised Metrics For all supervised model-based approaches, we experiment with fine-tuning two multilingual pre-trained language models: 1. multilingual BERT , dubbed m BERT (Devlin et al., 2019)\u2014a transformer-based model pre-trained with a masked language model objective on the concatenation of monolingual Wikipedia corpora from the 104 languages with the largest Wikipedias. 2. XLM - R (Conneau et al., 2020)\u2014a transformer-based masked language model trained on 100 languages using monolingual Common-Crawl data. All models are based on the Hugging-Face Transformers (Wolf et al., 2020) [Cite_Footnote_2] library. We fine-tune with the Adam optimizer (Kingma and Ba, 2015), a batch size of 32, and a learning rate of 5e\u22125 for 3 and 5 epochs for classification and regression tasks, respectively. We perform a grid search on held-out validation sets over learning rate with values: 2e\u22123, 2e\u22124, 2e\u22125, and 5e\u22125 and over number of epochs with values: 3, 5, and 8."
  },
  {
    "id": 267,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the training data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/mjpost/sacrebleu",
    "section_title": "4 Experiment Settings",
    "add_info": "3 https://github.com/mjpost/sacrebleu",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the [Cite_Footnote_3] ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 268,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the implementation of Salazar et al."
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "2020"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/awslabs/mlm-scoring",
    "section_title": "4 Experiment Settings",
    "add_info": "8 https://github.com/awslabs/mlm-scoring",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. [Cite_Footnote_8] PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 269,
    "name": "5-gram Ken LM model",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Heafield, 2011"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/kpu/kenlm",
    "section_title": "4 Experiment Settings",
    "add_info": "9 https://github.com/kpu/kenlm",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). [Cite_Footnote_9] Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 270,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "machine translated re-sources for STS",
      "The former"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/PhilipMay/stsb-multi-mt",
    "section_title": "4 Experiment Settings",
    "add_info": "10 https://github.com/PhilipMay/stsb-multi-mt",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS [Cite_Footnote_10] and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 271,
    "name": "Open-IE4",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://knowitall.github.io/openie",
    "section_title": "1 Introduction",
    "add_info": "1 http://knowitall.github.io/openie",
    "text": "Further, Stanovsky et al. (2015) compared the performance of several off-the-shelf parsers in dif-ferent semantic tasks. Most relevant to this work is the comparison between Open-IE and SRL. Specifically, they suggest that SRL\u2019s longer argu-ments introduce noise which hurts performance for downstream tasks. This is sustained empiri-cally by showing that extractions from Open-IE4 [Cite_Footnote_1] significantly outperform ClearNLP\u2019s SRL (Choi, 2012) in textual similarity, analogies, and reading comprehension tasks."
  },
  {
    "id": 272,
    "name": "Theano",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://deeplearning.net/software/theano/",
    "section_title": "5 Experiments and Results 5.2 Experiments on Veracity-based Datasets",
    "add_info": "3 http://deeplearning.net/software/theano/",
    "text": "We implement our models and DeClarE with Theano [Cite_Footnote_3] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation."
  },
  {
    "id": 273,
    "name": "Google Translate",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://translate.google.com",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "1 https://translate.google.com",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [Cite_Footnote_1] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 274,
    "name": "Microsoft Translator",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.bing.com/translator",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "2 https://www.bing.com/translator",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [Cite_Footnote_2] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 275,
    "name": "Amazon Translate",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://aws.amazon.com/translate",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "3 https://aws.amazon.com/translate",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [Cite_Footnote_3] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 276,
    "name": "SYSTRAN",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.systransoft.com",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "4 http://www.systransoft.com",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, [Cite_Footnote_4] (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 277,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the pre-trained models provided by the Fairseq toolkit"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "5 https://github.com/pytorch/fairseq",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit. [Cite_Footnote_5]"
  },
  {
    "id": 278,
    "name": "RMN code",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/miyyer/rmn",
    "section_title": "6 Evaluation",
    "add_info": "8 Our implementation builds on the available RMN code https://github.com/miyyer/rmn.",
    "text": "Parameter settings Across all experiments and corpus-specific models, we set \u03b2=0.99 for MV-Plot, and for both MVPlot and RMN we set \u03b1=0.5, \u03bb=10 \u22125 , k=50. We train both RMN and MVPlot for 15 epochs using SGD and ADAM (Kingma and Ba, 2014). [Cite_Footnote_8]"
  },
  {
    "id": 279,
    "name": "mkcls",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "statis-tical machine translation systems"
    ],
    "citationtag": [
      "Och, 1999"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://fjoch.com/mkcls.html",
    "section_title": "2 Background",
    "add_info": "1 Available from http://fjoch.com/mkcls.html.",
    "text": "Past research in unsupervised PoS induction has largely been driven by two different motivations: a task based perspective which has focussed on induc-ing word classes to improve various applications, and a linguistic perspective where the aim is to induce classes which correspond closely to anno-tated part-of-speech corpora. Early work was firmly situtated in the task-based setting of improving gen-eralisation in language models. Brown et al. (1992) presented a simple first-order HMM which restricted word types to always be generated from the same class. Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the sig-nificant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language mod-elling and bilingual word alignment, and an imple-mentation with improved inference called mkcls (Och, 1999) [Cite_Footnote_1] has become a standard part of statis-tical machine translation systems."
  },
  {
    "id": 280,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code for the work presented in this paper",
      "this formulation",
      "this model architecture"
    ],
    "description": [
      "a sim-ple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://groups.csail.mit.edu/rbg/code/typetagging/",
    "section_title": "References",
    "add_info": "1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/.",
    "text": "Part-of-speech (POS) tag distributions are known to exhibit sparsity \u2014 a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incor-porating this sparsity constraint improves tag-ging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity. This paper proposes a sim-ple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parame-ters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial per-formance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems. [Cite_Footnote_1]"
  },
  {
    "id": 281,
    "name": "AQUAINT-2",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection",
      "Articles are in English and come from a variety of sources."
    ],
    "citationtag": [
      "AQUAINT-2, 2007",
      "2007"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents",
    "section_title": "4 Evaluation 4.2 Setup",
    "add_info": "AQUAINT-2 (2007). URL: http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents.",
    "text": "To estimate the timeliness and semantic cred-ibility indicators, we use AQUAINT-2, a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007)  . Articles are in English and come from a variety of sources."
  },
  {
    "id": 282,
    "name": "Splog blog dataset",
    "fullname": "N/A",
    "genericmention": [
      "For each classified blog d"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Kolari, P., Finin, T., Java, A., and Joshi, A. (2006).",
      "Kolari et al., 2006"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://ebiquity.umbc.edu/resource/html/id/212/Splog-Blog-Dataset",
    "section_title": "3 Modeling 3.3.2 Blog level credibility indicators",
    "add_info": "Kolari, P., Finin, T., Java, A., and Joshi, A. (2006). Splog blog dataset. URL: http://ebiquity.umbc.edu/resource/html/id/212/Splog-Blog-Dataset.",
    "text": "Spam filtering To estimate the spaminess of a blog, we take a simple approach. We train an SVM classifier on a labeled splog blog dataset (Kolari et al., 2006)  using the top 1500 words for both spam and non-spam blogs as features. For each classified blog d we have a confidence value s(d). If the clas-sifier cannot make a decision (s(d) = 0) we set p spam (d) to 0, otherwise we use the following to transform s(d) into a spam prior p spam (d): where n(r, d) is the number of comments on post d. Regularity To estimate the regularity prior we use where \u03c3 interval expresses the standard deviation of the temporal intervals between two successive posts. Topical consistency Here we use an approach similar to query clarity (Cronen-Townsend and Croft, 2002): based on the list of posts from the same blog we compare the topic distribution of blog B to the topic distribution in the collection C and assign a \u2018clarity\u2019 value to B; a score further away from zero indicates a higher topical consistency. We estimate the topical consistency prior as where clarity(d) is estimated by"
  },
  {
    "id": 283,
    "name": "Lycos Retriever",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a patent-pending information fusion engine",
      "Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia article.",
      "These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://www.lycos.com/retriever.html",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.lycos.com/retriever.html. Work on Retriever was done while author was employed at Lycos.",
    "text": "Lycos Retriever [Cite_Footnote_1] is something new on the Web: a patent-pending information fusion engine. That is, unlike a search engine, rather than returning ranked documents links in response to a query, Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia article. These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching."
  },
  {
    "id": 284,
    "name": "King Kong",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/king-kong-1933.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 285,
    "name": "Zoloft",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/zoloft.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 286,
    "name": "Public-Key Cryptography",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/public-key-cryptography.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 287,
    "name": "Lyme Disease",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/lyme-disease.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 288,
    "name": "Reggaeton",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/reggaeton.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 289,
    "name": "spidered DMOZ [Cite_Footnote_5] hierarchy",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.dmoz.com",
    "section_title": "4 Topic Selection",
    "add_info": "5 http://www.dmoz.com",
    "text": "After a topic was input to the system, the Retriever system assigned it a category using a na\u00efve Bayes classifier built on a spidered DMOZ [Cite_Footnote_5] hierarchy. Various heuristics were implemented to make the returned set of categories uniform in length and depth, up-to-date, and readable."
  },
  {
    "id": 290,
    "name": "CMU Link Parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.link.cs.cmu.edu/link/",
    "section_title": "7 Passage Extraction",
    "add_info": "6 http://www.link.cs.cmu.edu/link/",
    "text": "When a passage was identified as being potentially interesting, it was then fully parsed to see if an expression denoting the topic was the Discourse Topic of the passage. Discourse Topic is an under-theorized notion in linguistic theory: not all linguists agree that the notion of Discourse Topic is required in discourse analysis at all (cf. Asher, 2004). For our purposes, however, we for-mulated a set of patterns for identifying Discourse Topics on the basis of the output of the CMU Link Parser [Cite_Footnote_6] the system uses."
  },
  {
    "id": 291,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "an openly available blocklist"
    ],
    "citationtag": [
      "In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics."
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/LDNOOBW",
    "section_title": "6 Analysis of Safety and Gender Bias 6.3 Safety",
    "add_info": "3 https://github.com/LDNOOBWdataset. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics.",
    "text": "The MMB models may demonstrate offensiveness beyond gender bias for several reasons: (1) its gen-erative nature makes it rather difficult to define a limited set of utterances; (2) the model\u2019s training data contains real-world conversations from the Internet; and (3) the Image-Chat dataset has neg-ative styles to better capture the range of human styles. All of these factors could lead to an unsafe response given a multi-modal context. To mitigate this problem, we first measure our models\u2019 toxicity using an openly available blocklist [Cite_Footnote_3] and an offen-sive language classifier presented in Dinan et al. (2019b). We define the term \u201ctoxicity\" to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model. We evaluate our model on the Image-Chat validation set, with a fixed style trait to control the generation, presenting results for different choices of fixed trait. We first evaluate our model in the first round of the Image-Chat validation set. The results in Table 9 indicate that positive styles reduce the level of toxicity by a large margin for both metrics (classifier and blocklist). The results also align well with our previous experiments on degendering, as toxicity is reduced across all styles after applying the degendering process. After degendering, we can considerably improve our model\u2019s safety by en-forcing that it uses positive styles. We also evaluate our model in the second round of the conversation and collect the statistics based on the first round style, as shown in Table 23. This result suggests that even if the model is controlled with a positive style, it is less safe when responding to negative conversations."
  },
  {
    "id": 292,
    "name": "ResNeXt-IG-3.5B and Faster R-CNN image features",
    "fullname": "N/A",
    "genericmention": [
      "that model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron.",
      "Girshick et al., 2018"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/facebookresearch/detectron",
    "section_title": "2 Related Work 2.3 Comparison to Existing Models",
    "add_info": "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron.",
    "text": "2AMMC: a retrieval model in which multiple Transformers are attended over in order to make use of a combination of ResNeXt-IG-3.5B and Faster R-CNN image features (Girshick et al., 2018)  . We specifically use the 2AMMC model from Ju et al. (2019) because that model has the best test-set per-formance on Image-Chat in that work."
  },
  {
    "id": 293,
    "name": "Detectron framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron."
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/facebookresearch/detectron",
    "section_title": "References",
    "add_info": "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron.",
    "text": "Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018)  ; specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\"."
  },
  {
    "id": 294,
    "name": "Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Amanpreet Singh, Vedanuj Goswami, Vivek Natara-jan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020a. Mmf: A multimodal framework for vision and language research.",
      "Singh et al., 2020a"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/facebookresearch/mmf",
    "section_title": "References",
    "add_info": "Amanpreet Singh, Vedanuj Goswami, Vivek Natara-jan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020a. Mmf: A multimodal framework for vision and language research. https://github.com/facebookresearch/mmf.",
    "text": "Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a)  . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\"."
  },
  {
    "id": 295,
    "name": "ImageNet1K dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Rus-sakovsky et al., 2015"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/",
    "section_title": "References",
    "add_info": "4 https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/",
    "text": "ResNeXt WSL We first experiment with im-age representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion pub-lic images (Mahajan et al., 2018), with subse-quent fine-tuning on the ImageNet1K dataset (Rus-sakovsky et al., 2015) [Cite_Footnote_4] . The output of this model is a 2048-dimensional vector, and we refer to these representations as \u201cResNeXt WSL\" features."
  },
  {
    "id": 296,
    "name": "Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Singh et al., 2020a"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/facebookresearch/vilbert-multi-task",
    "section_title": "References",
    "add_info": "5 https://github.com/facebookresearch/vilbert-multi-task",
    "text": "Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [Cite_Footnote_5] . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\"."
  },
  {
    "id": 297,
    "name": "Glove vec-tors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pennington et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "5 Experiments 5.3 Hyperparameters and Training Details",
    "add_info": "5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.",
    "text": "We initialized our word representations using publicly available 300-dimensional Glove vec-tors [Cite_Footnote_5] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned."
  },
  {
    "id": 298,
    "name": "Tesseract OCR engine",
    "fullname": "N/A",
    "genericmention": [
      "OCR"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/tesseract-ocr/",
    "section_title": "3 Data Description 3.1 Parallel Data",
    "add_info": "6 https://github.com/tesseract-ocr/",
    "text": "Fifty-six percent of our parallel data is derived from the Cherokee New Testament. Other texts are novels, children\u2019s books, newspaper articles, etc. These texts vary widely in dates of publica-tion, the oldest being dated to 1860. Addition-ally, our data encompasses both existing dialects of Cherokee: the Overhill dialect, mostly spoken in Oklahoma (OK), and the Middle dialect, mostly used in North Carolina (NC). These two dialects are mainly phonologically different and only have a few lexical differences (Uchihara, 2016). In this work, we do not explicitly distinguish them during translation. The left pie chart of Figure 2 shows the parallel data distributions over text types and dialects, and the complete information is in Ta-ble 14 of Appendix A.1. Many of these texts were translations of English materials, which means that the Cherokee structures may not be 100% natural in terms of what a speaker might spontaneously produce. But each text was translated by people who speak Cherokee as the first language, which means there is a high probability of grammatical-ity. These data were originally available in PDF version. We apply the Optical Character Recog-nition (OCR) via Tesseract OCR engine [Cite_Footnote_6] to ex-tract the Cherokee and English text. Then our co-author, a proficient second-language speaker of Cherokee, manually aligned the sentences and fixed the errors introduced by OCR. This process is time-consuming and took several months."
  },
  {
    "id": 299,
    "name": "News Crawl 2017",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "English monolingual data"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://data.statmt.org/news-crawl/en/",
    "section_title": "5 Results 5.1 Experimental Details",
    "add_info": "10 http://data.statmt.org/news-crawl/en/",
    "text": "We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [Cite_Footnote_10] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 . We apply tokenizer and truecaser from Moses (Koehn et al., 2007). We also apply the BPE tokonization (Sennrich et al., 2016c), but instead of using it as default, we treat it as hyper-parameter. For systems with BERT, we apply the WordPiece tokenizer (Devlin et al., 2019). We compute detokenized and case-sensitive BLEU score (Papineni et al., 2002) using SacreBLEU (Post, 2018). 13"
  },
  {
    "id": 300,
    "name": "News Crawl 2017",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "English monolingual data"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://data.statmt.org/news-crawl/en/",
    "section_title": "B Experimental Details B.1 Data and Preprocessing",
    "add_info": "18 http://data.statmt.org/news-crawl/en/",
    "text": "For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. [Cite_Footnote_18] We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 19 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 20 , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary."
  },
  {
    "id": 301,
    "name": "News Commentary v13 of WMT2018",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.statmt.org/wmt18/index.html",
    "section_title": "B Experimental Details B.1 Data and Preprocessing",
    "add_info": "19 http://www.statmt.org/wmt18/index.html",
    "text": "For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. 18 We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 [Cite_Footnote_19] or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary."
  },
  {
    "id": 302,
    "name": "OPUS",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://opus.nlpl.eu/bible-uedin.php",
    "section_title": "B Experimental Details B.1 Data and Preprocessing",
    "add_info": "20 http://opus.nlpl.eu/bible-uedin.php",
    "text": "For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. 18 We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS [Cite_Footnote_20] , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary."
  },
  {
    "id": 303,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code",
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/jiesutd/SubwordEncoding-CWS",
    "section_title": "1 Introduction",
    "add_info": "1 Our code is released at https://github.com/jiesutd/SubwordEncoding-CWS.",
    "text": "In this paper, we fill this gap by proposing a subword-based neural word segmentor, by inte-grating two strands of works: the byte pair en-coding (BPE) algorithm (Gage, 1994) and the lat-tice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In partic-ular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code [Cite_Footnote_1] is based on NCRF++ (Yang and Zhang, 2018)."
  },
  {
    "id": 304,
    "name": "Chinese Gigaword",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2011T13",
    "section_title": "4 Experiments 4.1 Experimental Settings",
    "add_info": "3 https://catalog.ldc.upenn.edu/LDC2011T13.",
    "text": "Embeddings. We take the same character un-igram and bigram embeddings as Zhang et al. (2016), who pretrain embeddings using word2vec (Mikolov et al., 2013) on Chinese Gigaword [Cite_Footnote_3] . The vocabulary of subword is constructed with 200000 merge operations and the subword embeddings are also trained using word2vec (Heinzerling and Strube, 2018). Trie (Fredkin, 1960) is used to ac-celerate lattice building. All the embeddings are fine-tuned during training."
  },
  {
    "id": 305,
    "name": "20NG",
    "fullname": "20 Newsgroup dataset",
    "genericmention": [
      "all data sets"
    ],
    "description": [
      "a collection of approximately 20,000 20-category documents"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://people.csail.mit.edu/~jrennie/20Newsgroups/",
    "section_title": "4 Experimental Studies 4.1 Experimental Setup",
    "add_info": "1 http://people.csail.mit.edu/~jrennie/20Newsgroups/",
    "text": "Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [Cite_Footnote_1] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories."
  },
  {
    "id": 306,
    "name": "Cornell movie-review dataset",
    "fullname": "N/A",
    "genericmention": [
      "Both of them",
      "all data sets"
    ],
    "description": [
      "Both of them are 2-category tasks and each consists of 2,000 reviews."
    ],
    "citationtag": [
      "Pang and Lee, 2004"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.cornell.edu/People/pabo/movie-review-data/",
    "section_title": "4 Experimental Studies 4.1 Experimental Setup",
    "add_info": "2 http://www.cs.cornell.edu/People/pabo/movie-review-data/",
    "text": "Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset [Cite_Footnote_2] (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories."
  },
  {
    "id": 307,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "dataset from product reviews of domain DVD",
      "Both of them",
      "all data sets"
    ],
    "description": [
      "Both of them are 2-category tasks and each consists of 2,000 reviews."
    ],
    "citationtag": [
      "Blitzer et al., 2007"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.seas.upenn.edu/~mdredze/datasets/sentiment/",
    "section_title": "4 Experimental Studies 4.1 Experimental Setup",
    "add_info": "3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/",
    "text": "Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD [Cite_Footnote_3] (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories."
  },
  {
    "id": 308,
    "name": "Kyoto University Corpus [Cite_Footnote_2] 2.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html",
    "section_title": "1 Introduction",
    "add_info": "2 http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html",
    "text": "It takes a long time to construct high-quality an-notated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki\u2019s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus [Cite_Footnote_2] 2.0. These data are divided into two groups: gen-eral and editorial. General contains 30 general news articles, and editorial contains 30 editorial articles. According to his experiments, editorial is harder than general. Perhaps this is caused by the differ-ence in rhetorical styles and the lengths of articles. The average number of sentences in an editorial ar-ticle is 28.7, while that in a general article is 13.9."
  },
  {
    "id": 309,
    "name": "ChaSen 2.2.9",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.aist-nara.ac.jp/",
    "section_title": "1 Introduction",
    "add_info": "3 http://chasen.aist-nara.ac.jp/",
    "text": "In addition, we decided to use the output of ChaSen 2.2.9 [Cite_Footnote_3] and CaboCha 0.34 instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus."
  },
  {
    "id": 310,
    "name": "CaboCha 0.34",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cl.aist-nara.ac.jp/\u02dctaku-ku/software/cabocha/",
    "section_title": "1 Introduction",
    "add_info": "4 http://cl.aist-nara.ac.jp/\u02dctaku-ku/software/cabocha/",
    "text": "In addition, we decided to use the output of ChaSen 2.2.9 and CaboCha 0.34 [Cite_Footnote_4] instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus."
  },
  {
    "id": 311,
    "name": "Py-Torch",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Paszke et al., 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://pytorch.org/",
    "section_title": "- A.5 Implementation Details",
    "add_info": "15 https://pytorch.org/",
    "text": "Our models are implemented based on Py-Torch (Paszke et al., 2017). [Cite_Footnote_15] To speed up training, we use Nvidia Apex 16 for mixed precision train-ing. Gradient accumulation (Ott et al., 2018) is applied to reduce multi-GPU communication over-heads. All pre-training experiments are run on Nvidia V100 GPUs (32GB VRAM; NVLink con-nection). We use AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 3e\u22125 and weight decay of 0.01 to pre-train our model. The best pre-trained model is trained on 16 V100 GPUs for about 3 weeks. Finetuning experiments are implemented on the same hardware or Titan RTX GPUs (24GB VRAM) with AdamW optimizer but different learning rates."
  },
  {
    "id": 312,
    "name": "E CHO project",
    "fullname": "European CHronicles On-line",
    "genericmention": [
      "The project"
    ],
    "description": [
      "The E CHO project ( [Cite] http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives.",
      "The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://pc-erato2.iei.pi.cnr.it/echo",
    "section_title": "8 Recent Research Projects",
    "add_info": null,
    "text": "Two other related FP5 IST projects are: C ORE - TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line. C ORETEX (http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology. In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions. The E CHO project (  http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives. The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation."
  },
  {
    "id": 313,
    "name": "ukWaC corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://wacky.sslmit.unibo.it/",
    "section_title": "3 Method 3.1 Semantic space 3.1.1 Source corpus",
    "add_info": "2 http://wacky.sslmit.unibo.it/",
    "text": "Our source corpus is the concatenation of the ukWaC corpus [Cite_Footnote_2] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information."
  },
  {
    "id": 314,
    "name": "English Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://en.wikipedia.org",
    "section_title": "3 Method 3.1 Semantic space 3.1.1 Source corpus",
    "add_info": "3 http://en.wikipedia.org",
    "text": "Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [Cite_Footnote_3] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information."
  },
  {
    "id": 315,
    "name": "British National Corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.natcorp.ox.ac.uk/",
    "section_title": "3 Method 3.1 Semantic space 3.1.1 Source corpus",
    "add_info": "4 http://www.natcorp.ox.ac.uk/",
    "text": "Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia and the British National Corpus [Cite_Footnote_4] . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information."
  },
  {
    "id": 316,
    "name": "Color terms",
    "fullname": "N/A",
    "genericmention": [
      "two datasets of adjective-noun phrases",
      "one with color terms and one with intensional adjectives",
      "This dataset",
      "The dataset"
    ],
    "description": [
      "This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above.",
      "From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low)",
      "From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety).",
      "The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole).",
      "They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN.",
      "The dataset as used here consists of 239 intersective and 130 subsective ANs."
    ],
    "citationtag": [
      "Bruni et al. (to appear) for an analysis of the color term dataset from a multi-modal perspective."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip",
    "section_title": "p = Bv (5) 3.3 Datasets",
    "add_info": "6 Available at http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip. See Bruni et al. (to appear) for an analysis of the color term dataset from a multi-modal perspective.",
    "text": "We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. [Cite_Footnote_6] Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above. From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low), since the remaining (grey, orange, pink, and purple) are not in the 700 most frequent set of ad-jectives in the corpora used. From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety). The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole). They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN. In this paper, only the first label (most frequent interpretation, accord-ing to the judges) has been used. The \u03ba coefficient of the annotation on the three categories (first interpre-tation only) was 0.87 (conf. int. 0.82-0.92, according to Fleiss et al. (1969)), observed agreement 0.96. 7 There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. 8 The dataset as used here consists of 239 intersective and 130 subsective ANs."
  },
  {
    "id": 317,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code for the computation of inter-annotator agreement by Stefan Evert"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.collocations.de/temp/kappa_example.zip",
    "section_title": "p = Bv (5) 3.3 Datasets",
    "add_info": "7 Code for the computation of inter-annotator agreement by Stefan Evert, available at http://www.collocations.de/temp/kappa_example.zip.",
    "text": "We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. 6 Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above. From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low), since the remaining (grey, orange, pink, and purple) are not in the 700 most frequent set of ad-jectives in the corpora used. From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety). The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole). They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN. In this paper, only the first label (most frequent interpretation, accord-ing to the judges) has been used. The \u03ba coefficient of the annotation on the three categories (first interpre-tation only) was 0.87 (conf. int. 0.82-0.92, according to Fleiss et al. (1969)), observed agreement 0.96. [Cite_Footnote_7] There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. The dataset as used here consists of 239 intersective and 130 subsective ANs."
  },
  {
    "id": 318,
    "name": "ClueWeb",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Evgeniy Gabrilovich, Michael Ringgaard, , and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora.",
      "Gabrilovich et al., 2013"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://lemurproject.org/clueweb09/FACC1/",
    "section_title": "2 Related Work",
    "add_info": "Evgeniy Gabrilovich, Michael Ringgaard, , and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora. http://lemurproject.org/clueweb09/FACC1/.",
    "text": "All of the above work resorted to using the Free-base annotation of ClueWeb (Gabrilovich et al., 2013)  to gain extra advantage of paraphrasing QA pairs or dealing with data sparsity problem. How-ever, ClueWeb is proprietary data and costs hun-dreds of dollars to purchase. Moreover, even though the implementation systems from (Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014) are open-source, they all take considerable disk space (in tens of gigabytes) and training time (in days). In this paper we present a system that can be easily implemented in 300 lines of Python code with no compromise in accuracy and speed."
  },
  {
    "id": 319,
    "name": "Wikidata",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.wikidata.org/",
    "section_title": "2 Retrieving Facts from LMs",
    "add_info": "1 https://www.wikidata.org/",
    "text": "In this paper we follow the protocol of Petroni et al. (2019)\u2019s English-language LAMA bench-mark, which targets factual knowledge expressed in the form of subject-relation-object triples from Wikidata [Cite_Footnote_1] curated in the T-REx dataset (ElSahar et al., 2018). The cloze-style prompts used therein are manually created and consist of a sequence of tokens, where [X] and [Y] are placeholders for sub-jects and objects (e.g. \u201c[X] is a [Y] by profession.\u201d). To assess the existence of a certain fact, [X] is re-placed with the actual subject (e.g. \u201cObama is a hmaski by profession.\u201d) and the model predicts the object in the blank y\u0302 i = argmax y i p(y i |s i:i ), where s i:i is the sentence with the i-th token masked out. Finally, the predicted fact is compared to the ground truth. In the next section, we extend this setting to more languages and predict multiple tokens instead of a single one."
  },
  {
    "id": 320,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the appropriately inflected surface form of the bracketed words"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/antonisa/unimorph_inflect",
    "section_title": "3 Multilingual Multi-token Factual Retrieval Benchmark 3.3 Prompts",
    "add_info": "3 https://github.com/antonisa/unimorph_inflect",
    "text": "Once all the morphological features have been specified as detailed above, we use the unimorph_inflect package (Anastasopoulos and Neubig, 2019) to generate the appropriately inflected surface form of the bracketed words. [Cite_Footnote_3] We note that the target entity ([Y]) might also need to be inflected, as in the above Russian example, in which case we require the model\u2019s predictions to match the inflected target forms."
  },
  {
    "id": 321,
    "name": "HurtLex lexicon",
    "fullname": "N/A",
    "genericmention": [
      "This lexi-con"
    ],
    "description": [
      "HurtLex is a multilingual lexicon of hate words, originally built from 1,082 Italian hate words compiled in a manual fashion by the linguist Tullio De Mauro",
      "This lexi-con is semi-automatically extended and translated into 53 languages by using BabelNet (Navigli and Ponzetto, 2012), and the lexical items are divided into 17 categories such as homophobic slurs, eth-nic slurs, genitalia, cognitive and physical disabil-ities, animals and more"
    ],
    "citationtag": [
      "Bassig-nana et al., 2018"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://hatespeech.di.unito.it/resources.html",
    "section_title": "4 Cross-domain Classification",
    "add_info": "1 http://hatespeech.di.unito.it/resources.html",
    "text": "In this experiment, we investigate the performance of machine learning classifiers which are trained on a particular dataset and tested on different datasets ones. We focus on investigating the in-fluence of captured phenomena coverage between datasets. We hypothesize that a classifier which is trained on a broader coverage dataset and tested on narrower coverage dataset will give better perfor-mance than the opposite. Furthermore, we analyse the impact of using the HurtLex lexicon (Bassig-nana et al., 2018) to transfer knowledge between domains. HurtLex is a multilingual lexicon of hate words, originally built from 1,082 Italian hate words compiled in a manual fashion by the linguist Tullio De Mauro (De Mauro, 2016). This lexi-con is semi-automatically extended and translated into 53 languages by using BabelNet (Navigli and Ponzetto, 2012), and the lexical items are divided into 17 categories such as homophobic slurs, eth-nic slurs, genitalia, cognitive and physical disabil-ities, animals and more [Cite_Footnote_1] ."
  },
  {
    "id": 322,
    "name": "Google Translate",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "machine translation"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://translate.google.com/",
    "section_title": "5 Cross-lingual Classification",
    "add_info": "5 http://translate.google.com/",
    "text": "monolingual word embedding. We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate [Cite_Footnote_5] ) to translate training data from source to target language. In this model, we use pre-trained word embedding from FastText ."
  },
  {
    "id": 323,
    "name": "FastText",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://fasttext.cc/",
    "section_title": "5 Cross-lingual Classification",
    "add_info": "6 https://fasttext.cc/",
    "text": "monolingual word embedding. We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate ) to translate training data from source to target language. In this model, we use pre-trained word embedding from FastText [Cite_Footnote_6] ."
  },
  {
    "id": 324,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "mul-tilingual word embeddings"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/facebookresearch/MUSE",
    "section_title": "5 Cross-lingual Classification",
    "add_info": "7 https://github.com/facebookresearch/MUSE",
    "text": "(b). JL + ME. We also propose a joint-learning model with multilingual word embedding. We take advantage of the availability of mul-tilingual word embeddings [Cite_Footnote_7] to build a joint-learning model. Figure 1 summarize how the data is transformed and learned in this model. We create bilingual training data automatically by using Google Translate to translate the data in both directions (training from source to target language and testing from target to source language), then using it as training data for the two LSTM-based ar-chitectures (similar architecture of the model in cross-domain experiment). We concate-nate these two architectures before the output layer, which produces the final prediction. In the, we expect to reduce some of the noise from the translation while keeping the origi-nal structure of the training set."
  },
  {
    "id": 325,
    "name": "MS COCO image cap-tioning dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://cocodataset.org/",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "8 http://cocodataset.org/",
    "text": "We train our contextual token-image matching model (in Sec. 3.1) on MS COCO image cap-tioning dataset [Cite_Footnote_8] for 20 epochs. The concatena-tion of the last 4 layers of BERT outputs (fol-lowing Devlin et al. (2019)) and mean pooling of"
  },
  {
    "id": 326,
    "name": "English Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/withattardi/wikiextractor",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "9 Downloaded https://github.com/withattardi/wikiextractor",
    "text": "When pre-training the model on pure language corpus, we unify the training process to avoid pos-sible side effects from different training protocols. We follow previous work to conduct two simplifi-cations: 1. Removing the next-sentence-prediction task (Liu et al., 2019) 2. Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia [Cite_Footnote_9] for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison. It results in around 10% more training steps in pure MLM training. All models take batch sizes of 256 and a learning rate of 2e-4."
  },
  {
    "id": 327,
    "name": "Wiki103",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/",
    "text": "When pre-training the model on pure language corpus, we unify the training process to avoid pos-sible side effects from different training protocols. We follow previous work to conduct two simplifi-cations: 1. Removing the next-sentence-prediction task (Liu et al., 2019) 2. Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [Cite_Footnote_10] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison. It results in around 10% more training steps in pure MLM training. All models take batch sizes of 256 and a learning rate of 2e-4."
  },
  {
    "id": 328,
    "name": "PyTorch Trans-formers",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "11 https://github.com/huggingface/transformers",
    "text": "The whole framework is built on Py-Torch (Paszke et al., 2019). The implementations of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are borrowed from PyTorch Trans-formers (Wolf et al., 2019) [Cite_Footnote_11] . All evaluation code is from the PyTorch Transformers as well."
  },
  {
    "id": 329,
    "name": "CheckList",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a new eval-uation methodology and accompanying tool [Cite_Footnote_1] for comprehensive behavioral testing of NLP models",
      "CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks.",
      "To break down potential ca-pability failures into specific behaviors, CheckList introduces different test types, such as prediction invariance in the presence of certain perturbations, or performance on a set of \u201csanity checks.\u201d",
      "our implementation of CheckList includes multiple abstractions that help users generate large numbers of test cases easily, such as templates, lexi-cons, general-purpose perturbations, visualizations, and context-aware suggestions."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/marcotcr/checklist",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/marcotcr/checklist",
    "text": "In this work, we propose CheckList, a new eval-uation methodology and accompanying tool [Cite_Footnote_1] for comprehensive behavioral testing of NLP models. CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks. To break down potential ca-pability failures into specific behaviors, CheckList introduces different test types, such as prediction invariance in the presence of certain perturbations, or performance on a set of \u201csanity checks.\u201d Fi-nally, our implementation of CheckList includes multiple abstractions that help users generate large numbers of test cases easily, such as templates, lexi-cons, general-purpose perturbations, visualizations, and context-aware suggestions. (examples of each type in A, B and C)."
  },
  {
    "id": 330,
    "name": "CheckList",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "it contains var-ious visualizations, abstractions for writing test expectations (e.g. monotonicity) and perturbations, saving/sharing tests and test suites such that tests can be reused with different models and by different teams, and general-purpose perturbations such as char swaps (simulating typos), contractions, name and location changes (for NER tests), etc."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/marcotcr/",
    "section_title": "2 CheckList 2.3 Generating Test Cases at Scale",
    "add_info": null,
    "text": "Open source We release an implementation of CheckList at  https://github.com/marcotcr/checklist. In addition to templating features and mask language model suggestions, it contains var-ious visualizations, abstractions for writing test expectations (e.g. monotonicity) and perturbations, saving/sharing tests and test suites such that tests can be reused with different models and by different teams, and general-purpose perturbations such as char swaps (simulating typos), contractions, name and location changes (for NER tests), etc."
  },
  {
    "id": 331,
    "name": "CheckList",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/marcotcr/checklist",
    "section_title": "6 Conclusion",
    "add_info": null,
    "text": "Our user studies indicate that CheckList is easy to learn and use, and helpful both for expert users who have tested their models at length as well as for practitioners with little experience in a task. The tests presented in this paper are part of Check-List\u2019s open source release, and can easily be in-corporated into existing benchmarks. More impor-tantly, the abstractions and tools in CheckList can be used to collectively create more exhaustive test suites for a variety of tasks. Since many tests can be applied across tasks as is (e.g. typos) or with minor variations (e.g. changing names), we ex-pect that collaborative test creation will result in evaluation of NLP models that is much more ro-bust and detailed, beyond just accuracy on held-out data. CheckList is open source, and available at  https://github.com/marcotcr/checklist."
  },
  {
    "id": 332,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a dictionary"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.askoxford.com",
    "section_title": "4 Evaluating the GMM Approach 4.1 Data",
    "add_info": "2 We used http://www.askoxford.com.",
    "text": "To determine how well our model deals with dif-ferent types of figurative usage, we distinguish four phenomena: Phrase-level figurative means that the whole phrase is used figuratively. We further divide this class into expressions which are potentially am-biguous between literal and figurative usage (nsa), e.g., spill the beans, and those that are unambigu-ously figurative irrespective of the context (nsu), e.g., trip the light fantastic. The latter can, theoreti-cally, be detected by dictionary look-up, the former cannot. The label token-level figurative (nw) is used when part of the phrase is used figuratively (e.g., sparrow in (2)). Often it is difficult to determine whether a word is still used in a \u2019literal\u2019 sense or whether it is already used figuratively. Since we are interested in improving the performance of NLP ap-plications such as MT, we take a pragmatic approach and classify usages as \u2019figurative\u2019 if they are not lex-icalized, i.e., if the specific sense is not listed in a dictionary. [Cite_Footnote_2] For example, we would classify summit in the \u2019meeting\u2019 sense as \u2019literal\u2019 (l). In our data set, 7.3% of the instances were annotated as \u2019nsa\u2019, 1.9% as \u2019nsu\u2019, 9.2% as \u2019nw\u2019 and 81.5% as \u2019l\u2019. A randomly selected sample (100 instances) was annotated inde-pendently by a second annotator. The kappa score (Cohen, 1960) is 0.84, which suggest that the anno-tations are reliable."
  },
  {
    "id": 333,
    "name": "CoNLL-2000 dataset",
    "fullname": "N/A",
    "genericmention": [
      "the origi-nal training set",
      "the test set"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cnts.ua.ac.be/conll2000/chunking/",
    "section_title": "4 Experiments",
    "add_info": "4 http://www.cnts.ua.ac.be/conll2000/chunking/",
    "text": "To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task on the CoNLL-2000 dataset [Cite_Footnote_4] . We split the origi-nal training set into a dev set (top 1,000 sent.) and used the rest as train set (7,936 sent.); the test set was kept intact (2,012 sent.). For an input sentence x, each CRF node x i carries an observable word and its part-of-speech tag, and has to be assigned a chunk tag c i out of 3 labels: Beginning, Inside, or Outside (of a noun phrase). Chunk labels are not nested. As in Sha and Pereira (2003), we use second order Markov dependencies (bigram chunk tags), such that for sentence position i, the state is y i = c i\u22121 c i , increasing the label set size from 3 to 9. Out of the full list of Sha and Pereira (2003)\u2019s features we implemented all except two feature templates, y i = y and c(y i ) = c, to simplify im-plementation. Impossible bigrams (OI) and label transitions of the pattern ?O \u2192 I? were prohib-ited by setting the respective potentials to \u2212\u221e. As the active feature count in the train set was just un-der 2M, we hashed all features and weights into a sparse array of 2M entries. Despite the reduced train size and feature set, and hashing, our full in-formation baseline trained with log-likelihood at-tained the test F1-score of 0.935, which is compa-rable to the original result of 0.9438."
  },
  {
    "id": 334,
    "name": "Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.wikipedia.org",
    "section_title": "5 Experiments 5.1 Data",
    "add_info": "1 http://www.wikipedia.org",
    "text": "We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia [Cite_Footnote_1] and labeled a to-tal of 4701 relation instances. In addition to a large set of person-to-person relations, we also included links between people and organizations, as well as biographical facts such as birthday and jobTitle . In all, there are 53 labels in the training data (Table 1)."
  },
  {
    "id": 335,
    "name": "MALLET CRF implementation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Mc-Callum, 2002",
      "Andrew McCallum. 2002. Mallet: A machine learning for language toolkit."
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "5 Experiments 5.1 Data",
    "add_info": "Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.",
    "text": "We use the MALLET CRF implementation (Mc-Callum, 2002)  with the default regularization pa-rameters."
  },
  {
    "id": 336,
    "name": "Reddit submissions and comments",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://files.pushshift.io/reddit/",
    "section_title": "4 Dataset 4.1 Users\u2019 utterances",
    "add_info": "4 https://files.pushshift.io/reddit/",
    "text": "We consider publicly-available Reddit submissions and comments [Cite_Footnote_4] from 2006 to 2018 as users\u2019 ut-terances. Given a Reddit user having a set of ut-terances U = u 0 ..u N , we aim to label the user with a set of profession and hobby values, based on explicit personal assertions (e.g., \u201cI work as a doctor\u201d) found in the user\u2019s posts. To label the candidate users with attribute values we utilized the Snorkel framework (Ratner et al., 2017). We provide details on our data labeling using Snorkel in Appendix A.1."
  },
  {
    "id": 337,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/Anna146/CHARM",
    "section_title": "4 Dataset 4.2 Document collection",
    "add_info": "5 Available at https://github.com/Anna146/CHARM",
    "text": "The scope of possible attribute values may be open-ended in nature, and thus, calls for an automatic method for collecting Web documents. In this work, we consider three different Web document collec-tions; summary statistics on the number of docu-ments per attribute value are provided in Table 1. Each document may be associated with multiple attribute values. To provide more diversity and comprehensiveness we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms. [Cite_Footnote_5]"
  },
  {
    "id": 338,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All datasets used in the experiments"
    ],
    "description": [
      "We pro-vide IDs and texts of the posts used as training and test data for CHARM.",
      "we provide the posts containing explicit personal assertions, which have been used for ground truth labeling with the Snorkel framework"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/Anna146/CHARM",
    "section_title": "A Data",
    "add_info": null,
    "text": "All datasets used in the experiments are available at  https://github.com/Anna146/CHARM. We pro-vide IDs and texts of the posts used as training and test data for CHARM. All users are anonymized by replacing usernames with IDs. Additionally, we provide the posts containing explicit personal assertions, which have been used for ground truth labeling with the Snorkel framework."
  },
  {
    "id": 339,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www-nlp.stanford.edu/software/sempre/",
    "section_title": "4 Experiments 4.1 Data & evaluation metric",
    "add_info": "6 We used the official evaluation script from http://www-nlp.stanford.edu/software/sempre/.",
    "text": "We use the W EB Q UESTIONS dataset (Berant et al., 2013), which consists of 5,810 ques-tion/answer pairs. These questions were collected using Google Suggest API and the answers were obtained from Freebase with the help of Amazon MTurk. The questions are split into training and testing sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively. This dataset has several unique properties that make it appealing and was used in several recent papers on semantic parsing and question answering. For instance, although the questions are not directly sampled from search query logs, the selection pro-cess was still biased to commonly asked questions on a search engine. The distribution of this ques-tion set is thus closer to the \u201creal\u201d information need of search users than that of a small number of human editors. The system performance is ba-sically measured by the ratio of questions that are answered correctly. Because there can be more than one answer to a question, precision, recall and F 1 are computed based on the system output for each individual question. The average F 1 score is reported as the main evaluation metric [Cite_Footnote_6] ."
  },
  {
    "id": 340,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The discourse parser",
      "The discourse parser",
      "the parser"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://alt.qcri.org/tools/",
    "section_title": "3 Our Discourse-Based Measures 3.1 Generating Discourse Trees",
    "add_info": "2 The discourse parser is freely available from http://alt.qcri.org/tools/",
    "text": "The discourse parser uses a dynamic Condi-tional Random Field (Sutton et al., 2007) as a pars-ing model in order to infer the probability of all possible discourse tree constituents. The inferred (posterior) probabilities are then used in a proba-bilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F 1 -score of 79.8%, which is very close to the human agree-ment of 83%. These high scores allowed us to de-velop successful discourse similarity metrics. [Cite_Footnote_2]"
  },
  {
    "id": 341,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "This",
      "the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English"
    ],
    "description": [
      "the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.statmt.org/wmt{11,12}/results.html",
    "section_title": "4 Experimental Setup",
    "add_info": "3 http://www.statmt.org/wmt{11,12}/results.html",
    "text": "In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [Cite_Footnote_3] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references."
  },
  {
    "id": 342,
    "name": "A SIYA toolkit",
    "fullname": "N/A",
    "genericmention": [
      "the toolkit"
    ],
    "description": [
      "A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation."
    ],
    "citationtag": [
      "Gime\u0301nez and Ma\u0300rquez, 2010a"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.lsi.upc.edu/asiya/",
    "section_title": "4 Experimental Setup 4.1 MT Evaluation Metrics",
    "add_info": "4 http://nlp.lsi.upc.edu/asiya/",
    "text": "Metrics from A SIYA . We used the freely avail-able version of the A SIYA toolkit [Cite_Footnote_4] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation. For reproducibility, below we explain the individual metrics with the exact names required by the toolkit to calculate them."
  },
  {
    "id": 343,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code",
      "Our implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/pytorch/fairseq/pull/1095",
    "section_title": "References",
    "add_info": "1 Code can be found at https://github.com/pytorch/fairseq/pull/1095",
    "text": "The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, tradi-tional statistical word alignment models of-ten remain the go-to solution. In this pa-per, we present an approach to train a Trans-former model to produce both accurate trans-lations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine trans-lation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces com-petitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accu-racies compared to GIZA++ on three publicly available data sets. Our implementation has been open-sourced [Cite_Footnote_1] ."
  },
  {
    "id": 344,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the decoder",
      "the decoder"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/lilt/alignment-scripts",
    "section_title": "4 Proposed Method 4.3 Providing Full Target Context",
    "add_info": "2 https://github.com/lilt/alignment-scripts",
    "text": "The Transformer decoder computes the probabil-ity of the next target token conditioned on the past target tokens and all source tokens. This is imple-mented by masking the self attention probabilities, i.e. while computing the representation for the i th target token, the decoder can only self-attend to the representations of {1, [Cite_Footnote_2] . . . i \u2212 1} tokens from the previous layer. This auto-regressive behavior of the decoder is crucial for the model to repre-sent a valid probability distribution over the target sentence. However, conditioning on just the past target tokens is limiting for the alignment task. As described in Section 4.2, the alignment head is trained to model the alignment distribution for the i th target token given only the past target to-kens and all source tokens. Since the alignment head does not know the identity of the next tar-get token, it becomes difficult for it to learn this token\u2019s alignment to the source tokens. Previous work has also identified this problem and alleviate it by feeding the target token to be aligned as an input to the module computing the alignment (Pe-ter et al., 2017), or forcing the module to predict the target token (Zenkel et al., 2019) or its prop-erties, e.g. POS tags (Li et al., 2018). Feeding the next target token assumes that we know it in ad-vance and thus calls for separate translation and alignment models. Forcing the alignment module to predict target token\u2019s properties helps but still passes the information of the target token in an in-direct manner. We overcome these limitations by conditioning the two components of our loss func-tion on different amounts of context. The NLL loss L t is conditioned on the past target tokens to preserve the auto-regressive property:"
  },
  {
    "id": 345,
    "name": "NAACL\u201903 Building and Using Parallel Texts word align-ment",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://web.eecs.umich.edu/\u02dcmihalcea/wpt/index.html#resources",
    "section_title": "5 Experiments 5.1 Setup 5.1.1 Alignment Task",
    "add_info": "3 http://web.eecs.umich.edu/\u02dcmihalcea/wpt/index.html#resources",
    "text": "The purpose of the this task is to fairly com-pare with state-of-the-art results in terms of alignment quality and perform a hyperparame-ter search. We use the same experimental setup as described in (Zenkel et al., 2019). The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian\u2192English, English\u2192French and German\u2192English. Train-ing data and test data for Romanian\u2192English and English\u2192French are provided by the NAACL\u201903 Building and Using Parallel Texts word align-ment shared task [Cite_Footnote_3] (Mihalcea and Pedersen, 2003). The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German\u2192English we use the Europarl v7 cor-pus as training data and the gold alignments pro-vided by Vilar et al. (2006). The reference align-ments were created by randomly selecting a subset of the Europarl v7 corpus and manually annotating them following the guidelines suggested in (Och and Ney, 2003). Data statistics are shown in Ta-ble 1."
  },
  {
    "id": 346,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the gold alignments"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Vilar et al. (2006)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www-i6.informatik.rwth-aachen.de/goldAlignment/",
    "section_title": "5 Experiments 5.1 Setup 5.1.1 Alignment Task",
    "add_info": "4 https://www-i6.informatik.rwth-aachen.de/goldAlignment/",
    "text": "The purpose of the this task is to fairly com-pare with state-of-the-art results in terms of alignment quality and perform a hyperparame-ter search. We use the same experimental setup as described in (Zenkel et al., 2019). The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian\u2192English, English\u2192French and German\u2192English. Train-ing data and test data for Romanian\u2192English and English\u2192French are provided by the NAACL\u201903 Building and Using Parallel Texts word align-ment shared task (Mihalcea and Pedersen, 2003). The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German\u2192English we use the Europarl v7 cor-pus as training data and the gold alignments [Cite_Footnote_4] pro-vided by Vilar et al. (2006). The reference align-ments were created by randomly selecting a subset of the Europarl v7 corpus and manually annotating them following the guidelines suggested in (Och and Ney, 2003). Data statistics are shown in Ta-ble 1."
  },
  {
    "id": 347,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/moses-smt/mgiza/",
    "section_title": "5 Experiments 5.2 Statistical Baseline",
    "add_info": "5 https://github.com/moses-smt/mgiza/",
    "text": "For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit 5 implemented by Gao and Vo-gel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vo-gel et al., 1996), respectively. In particular, we perform [Cite_Footnote_5] iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and sym-metrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use the resulting word alignments to supervise the alignment loss for the method described in Section 4.4."
  },
  {
    "id": 348,
    "name": "CIA factsheet",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.cia.gov/cia/publications/factbook",
    "section_title": "2 References to people 2.3 Automatic semantic tagging",
    "add_info": "1 http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian.",
    "text": "As the task definition above suggests, our approach is to identify particular semantic attributes for a per-son, and generate a reference formally from this se-mantic input. Our analysis of human summaries tells us that the semantic attributes we need to identify are role, organization, country, state, location and temporal modifier. In addi-tion, we also need to identify the person name. We used BBN\u2019s I DENTI F INDER (Bikel et al., 1999) to mark up person names, organizations and lo-cations. We marked up countries and (American) states using a list obtained from the CIA factsheet [Cite_Footnote_1] . To mark up roles, we used a list derived from Word-Net (Miller et al., 1993) hyponyms of the person synset. Our list has 2371 entries including multi-word expressions such as chancellor of the exche-quer, brother in law, senior vice president etc. The list is quite comprehensive and includes roles from the fields of sports, politics, religion, military, busi-ness and many others. We also used WordNet to ob-tain a list of 58 temporal adjectives. WordNet classi-fies these as pre- (eg. occasional, former, incoming etc.) or post-nominal (eg. elect, designate, emeritus etc.). This information is used during generation. Further, we identified elementary noun phrases us-ing the LT TTT noun chunker (Grover et al., 2000), and combined NP of NP sequences into one com-plex noun phrase. An example of the output of our semantic tagging module on a portion of machine translated text follows:"
  },
  {
    "id": 349,
    "name": "BLEU (Papineni et al., 2002) and NIST [Cite_Footnote_2] MT metrics",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Papineni et al., 2002"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.nist.gov/speech/tests/mt/resources/scoring.htm",
    "section_title": "2 References to people 2.6 Evaluation",
    "add_info": "2 http://www.nist.gov/speech/tests/mt/resources/scoring.htm",
    "text": "We used 6 document sets from DUC\u201904 for devel-opment purposes and present the average P, R and F for the remaining 18 sets in Table 1. There were 210 generated references in the 18 testing sets. The table also shows the popular BLEU (Papineni et al., 2002) and NIST [Cite_Footnote_2] MT metrics. We also provide two base-lines - most frequent initial reference to the person in the input (Base1) and a randomly selected initial reference to the person (Base2). As Table 1 shows, Base1 performs better than random selection. This is intuitive as it also uses redundancy to correct er-rors, at the level of phrases rather than words. The generation module outperforms both baselines, par-ticularly on precision - which for unigrams gives an indication of the correctness of lexical choice, and for higher ngrams gives an indication of grammati-cality. The unigram recall of x\u2021}\u2030X\u0152C\u008d indicates that we are not losing too much information at the noise fil-tering stage. Note that we expect a low \u00a5\u00a7\u00a65\u00a8 for our approach, as we only generate particular attributes that are important for a summary. The important measure is , on which we do well. This is also reflected in the high scores on BLEU and NIST."
  },
  {
    "id": 350,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a public spell checker"
    ],
    "description": [
      "spell checker"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/",
    "section_title": "5 Experiments 5.2 Experimental setting",
    "add_info": "4 https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/",
    "text": "We resolve spelling errors with a public spell checker [Cite_Footnote_4] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do."
  },
  {
    "id": 351,
    "name": "NIST MT 06 dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.itl.nist.gov/iad/mig/tests/mt/",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "5 http://www.itl.nist.gov/iad/mig/tests/mt/",
    "text": "Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC cor-pora, with 27.9M Chinese words and 34.5M En-glish words respectively. We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. [Cite_Footnote_5] To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task."
  },
  {
    "id": 352,
    "name": "Berkeley Parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/slavpetrov/berkeleyparser",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "6 https://github.com/slavpetrov/berkeleyparser",
    "text": "Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC cor-pora, with 27.9M Chinese words and 34.5M En-glish words respectively. We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser [Cite_Footnote_6] (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task."
  },
  {
    "id": 353,
    "name": "cdecerarchical",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "an opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data."
    ],
    "citationtag": [
      "Dyerphrase-basedet al., 2010"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/redpony/cdec",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "7 https://github.com/redpony/cdec",
    "text": "\u2022 cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [Cite_Footnote_7]"
  },
  {
    "id": 354,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the usual evalb program"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Satoshi Sekine and Michael Collins. 2008. Evalb.",
      "Sekine and Collins, 2008"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://nlp.cs.nyu.edu/evalb/",
    "section_title": "2 Overview and Example 2.1 Creation of Dependency Representation",
    "add_info": "Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.",
    "text": "The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008)  , which ignores function tags and empty cate-gories. We return to this issue in the conclusion."
  },
  {
    "id": 355,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the standard evalb scoring code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Satoshi Sekine and Michael Collins. 2008. Evalb.",
      "Sekine and Collins, 2008"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "http://nlp.cs.nyu.edu/evalb/",
    "section_title": "4 Results of Dependency to Phrase Structure Conversion",
    "add_info": "Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.",
    "text": "To evaluate the correctness of conversion from de-pendency to phrase structure, we follow the same strategy as Xia and Palmer (2001) and Xia et al. (2009). We convert the phrase structure trees in the PTB to dependency structure and convert the depen-dency back to phrase structure. We then compare the original PTB trees with the newly-created phrase structure trees, using the standard evalb scoring code (Sekine and Collins, 2008)  . Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projec-tion chains, and where to attach dependent subtrees. They reported results for their system for Section 00 of the PTB, and we include in Table 2 only their highest scoring algorithm. The system of Xia et al. (2009) uses conversion rules learned from Section 19, and then tested on Sections 00 and Section 22."
  },
  {
    "id": 356,
    "name": "TensorFlow constrained optimization",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Cotter et al., 2019a,b"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/tensorflow_constrained_optimization",
    "section_title": "3 Proposed Approaches 3.2 Bias-constrained Model",
    "add_info": "3 https://github.com/google-research/tensorflow_constrained_optimization",
    "text": "Each group-wise constraint is denoted as \u03c8 g . The constraints involve a linear combination of indicator variables, which is not differentiable wrt \u03b8. A common approach to handle this constrained optimization problems is using the Lagrangian, which is minimized over \u03b8 and maximized over \u03bb. Similar formulations have been used for learn-ing fair models with structured data (Cotter et al., 2019b; Yang et al., 2020; Zafar et al., 2019). In this work, we apply this method to NLP tasks and use the two-player zero-sum game approach for optimization, where the first player chooses \u03b8 to minimize L(\u03b8, \u03bb), and the second player enforces fairness constraints by maximizing \u03bb (Kearns et al., 2018; Cotter et al., 2019b; Yang et al., 2020). Specifically we use the implementations available in TensorFlow constrained optimization (Cotter et al., 2019a,b). [Cite_Footnote_3]"
  },
  {
    "id": 357,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "this data",
      "their sil-ver data annotated with the high quality supertags"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://github.com/uwnlp/taggerflow",
    "section_title": "5 Tri-training",
    "add_info": "5 https://github.com/uwnlp/taggerflow",
    "text": "We simply combine the two previous ap-proaches. Lewis et al. (2016) obtain their sil-ver data annotated with the high quality supertags. Since they make this data publicly available [Cite_Footnote_5] , we obtain our silver data by assigning dependency"
  },
  {
    "id": 358,
    "name": "GloVe",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pen-nington et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "6 Experiments 6.1 English Experimental Settings",
    "add_info": "8 http://nlp.stanford.edu/projects/glove/",
    "text": "We use as word representation the concatena-tion of word vectors initialized to GloVe [Cite_Footnote_8] (Pen-nington et al., 2014), and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d."
  },
  {
    "id": 359,
    "name": "Jigg",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "shift-reduce CCG parser"
    ],
    "citationtag": [
      "Noji and Miyao, 2016"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/mynlp/jigg",
    "section_title": "6 Experiments 6.2 Japanese Experimental Settings",
    "add_info": "9 https://github.com/mynlp/jigg",
    "text": "We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [Cite_Footnote_9] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs."
  },
  {
    "id": 360,
    "name": "Japanese Wikipedia Entity Vector",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.cl.ecei.tohoku.ac.jp/\u02dcm-suzuki/jawiki_vector/",
    "section_title": "6 Experiments 6.2 Japanese Experimental Settings",
    "add_info": "10 http://www.cl.ecei.tohoku.ac.jp/\u02dcm-suzuki/jawiki_vector/",
    "text": "For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector [Cite_Footnote_10] , and 100-dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014). We do not use affix vectors as affixes are less informative in Japanese. All characters ap-pearing less than two times are mapped to \u201cUNK\u201d. We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization."
  },
  {
    "id": 361,
    "name": "CaboCha",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://taku910.github.io/cabocha/",
    "section_title": "6 Experiments 6.2 Japanese Experimental Settings",
    "add_info": "11 http://taku910.github.io/cabocha/",
    "text": "One issue in Japanese experiments is evalua-tion. The Japanese CCGbank is encoded in a dif-ferent format than the English bank, and no stan-dalone script for extracting semantic dependen-cies is available yet. For this reason, we evaluate the parser outputs by converting them to bunsetsu dependencies, the syntactic representation ordi-nary used in Japanese NLP (Kudo and Matsumoto, 2002). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha [Cite_Footnote_11] and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b. For ex-ample, the sentence in Figure 4e is segmented as \u201cBoku wa | eigo wo | hanashi tai\u201d, from which we extract two dependencies (Boku wa) \u2190 (hanashi tai) and (eigo wo) \u2190 (hanashi tai). We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy. Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser."
  },
  {
    "id": 362,
    "name": "C++ Tensor-Flow",
    "fullname": "N/A",
    "genericmention": [
      "Software"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Abadi et al., 2015"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://tensorflow.org/",
    "section_title": "6 Experiments 6.3 English Parsing Results",
    "add_info": "Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane\u0301, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Vie\u0301gas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Sys-tems. Software available from tensorflow.org. http://tensorflow.org/.",
    "text": "Efficiency Comparison We compare the ef-ficiency of our parser with neuralccg and EasySRL reimpl. The results are shown in Table 4. For the overall speed (the third row), our parser is faster than neuralccg al-though lags behind EasySRL reimpl. Inspect-ing the details, our supertagger runs slower than those of neuralccg and EasySRL reimpl, while in A* search our parser processes over 7 times more sentences than neuralccg. The delay in supertagging can be attributed to sev-eral factors, in particular the differences in net-work architectures including the number of bi- LSTM layers (4 vs. 2) and the use of bilin-ear transformation instead of linear one. There are also many implementation differences in our parser (C++ A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ Tensor-Flow (Abadi et al., 2015)  supertagger and recur-sive neural model in C++ DyNet (Neubig et al., 2017))."
  },
  {
    "id": 363,
    "name": "GloVe",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pen-nington et al., 2014",
      "Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543."
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.aclweb.org/anthology/D14-1162",
    "section_title": "6 Experiments 6.1 English Experimental Settings",
    "add_info": "Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.",
    "text": "We use as word representation the concatena-tion of word vectors initialized to GloVe (Pen-nington et al., 2014)  , and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d."
  },
  {
    "id": 364,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pascanu et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/fxsjy/jieba",
    "section_title": "5 Experiments 5.2 Neural Hidden Markov Model",
    "add_info": "1 https://github.com/fxsjy/jieba",
    "text": "Apart from the basic projection layer, we also applied LSTM layers for the source and target words embedding. The embedding layers have 350 nodes and the size of the projection layer is 800 (400 + 200 + 200, Figure 1). We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [Cite_Footnote_1] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance."
  },
  {
    "id": 365,
    "name": "RNNLM toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models.",
      "Mikolov, 2012a"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://rnnlm.org",
    "section_title": "3 Experimental Setup 3.1 Unlabeled tweets",
    "add_info": "Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.",
    "text": "In order to train our SRN language model we col-lected a set of tweets using the Twitter sampling API. We use the raw sample directly without fil-tering it in any way, relying on the SRN to learn the structure of the data. The sample consists of 414 million bytes of UTF-8 encoded in a variety of languages and scripts text. We trained a 400-hidden-unit SRN, to predict the next byte in the sequence using backpropagation through time. In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a)  to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer. We did try to filter tweets by language and create specific em-beddings for English but this had negligible effect on tweet normalization performance."
  },
  {
    "id": 366,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the system",
      "The system"
    ],
    "description": [
      "a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models",
      "the first system for word sense induction and disambigua-tion, which is unsupervised, knowledge-free, and interpretable at the same time",
      "The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledge-free framework."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/uhh-lt/wsd",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/uhh-lt/wsd",
    "text": "We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambigua-tion, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledge-free framework. Implementation of the system is open source. [Cite_Footnote_1] A live demo featuring several dis-ambiguation models is available online."
  },
  {
    "id": 367,
    "name": "Apache Spark framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://spark.apache.org",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.1 Induction of the WSD Models",
    "add_info": "4 http://spark.apache.org",
    "text": "Figure 1 presents architecture of the WSD sys-tem. As one may observe, no human labor is used to learn interpretable sense representa-tions and the corresponding disambiguation mod-els. Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework [Cite_Footnote_4] , enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are la-beled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) pat-terns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction con-fidence. Top sentences are used as sense usage ex-amples. For more details about the model induc-tion process refer to (Panchenko et al., 2017). Cur-rently, the following WSD models induced from a text corpus are available:"
  },
  {
    "id": 368,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a rela-tional database",
      "the database",
      "the database"
    ],
    "description": [
      "each word sense is represented by its hypernyms, related words, and usage examples",
      "for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.postgresql.org",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.2 WSD API",
    "add_info": "5 https://www.postgresql.org",
    "text": "To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. [Cite_Footnote_5] In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework."
  },
  {
    "id": 369,
    "name": "Breeze",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/scalanlp/breeze",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.2 WSD API",
    "add_info": "6 https://github.com/scalanlp/breeze",
    "text": "To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. [Cite_Footnote_6] Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework."
  },
  {
    "id": 370,
    "name": "Play framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.playframework.com",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.2 WSD API",
    "add_info": "7 https://www.playframework.com",
    "text": "To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework. [Cite_Footnote_7]"
  },
  {
    "id": 371,
    "name": "React framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://facebook.github.io/react",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.3 User Interface for Interpretable WSD",
    "add_info": "8 https://facebook.github.io/react",
    "text": "The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [Cite_Footnote_8] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:"
  },
  {
    "id": 372,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an image search API"
    ],
    "description": [
      "an image search API [Cite_Footnote_9] us-ing a query composed of the ambiguous word and its hypernym, e.g. \u201cjaguar animal\u201d"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://azure.microsoft.com/en-us/services/cognitive-services/search",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.3 User Interface for Interpretable WSD",
    "add_info": "9 https://azure.microsoft.com/en-us/services/cognitive-services/search",
    "text": "Single word disambiguation mode is illus-trated in Figure 2. In this mode, a user specifies an ambiguous word and its context. The output of the system is a ranked list of all word senses of the ambiguous word ordered by relevance to the input context. By default, only the best matching sense is displayed. The user can quickly understand the meaning of each induced sense by looking at the hypernym and the image representing the sense. Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API [Cite_Footnote_9] us-ing a query composed of the ambiguous word and its hypernym, e.g. \u201cjaguar animal\u201d. The first hit of this query is selected to represent the induced word sense. Interpretability of each sense is fur-ther ensured by providing to the user the list of related senses, the list of the most salient context clues, and the sense usage examples (cf. Figure 2). Note that all these elements are obtained without manual intervention."
  },
  {
    "id": 373,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a part-of-speech and a named entity taggers"
    ],
    "description": [
      "a part-of-speech and a named entity taggers"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.scalanlp.org",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.3 User Interface for Interpretable WSD",
    "add_info": "10 http://www.scalanlp.org",
    "text": "All words disambiguation mode is illustrated in Figure 3. In this mode, the system performs dis-ambiguation of all nouns and entities in the input text. First, the text is processed with a part-of-speech and a named entity taggers. [Cite_Footnote_10] Next, each detected noun or entity is disambiguated in the same way as in the single word disambiguation mode described above, yet the disambiguation re-sults are represented as annotations of a running text. The best matching sense is represented by a hypernym and an image as depicted in Figure 3. This mode performs \u201csemantification\u201d of a text, which can, for instance, assist language learners with the understanding of a text in a foreign lan-guage: Meaning of unknown to the learner words can be deduced from hypernyms and images."
  },
  {
    "id": 374,
    "name": "Docker containers",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.docker.com",
    "section_title": "5 Conclusion",
    "add_info": "12 https://www.docker.com",
    "text": "We present the first openly available word sense disambiguation system that is unsupervised, knowledge-free, and interpretable at the same time. The system performs extraction of word and super sense inventories from a text corpus. The disambiguation models are learned in an unsuper-vised way for all words in the corpus on the ba-sis on the induced inventories. The user inter-face of the system provides efficient access to the produced WSD models via a RESTful API or via an interactive Web-based graphical user interface. The system is available online and can be directly used from external applications. The code and the WSD models are open source. Besides, in-house deployments of the system are made easy due to the use of the Docker containers. [Cite_Footnote_12] A prominent direction for future work is supporting more lan-guages and establishing cross-lingual sense links."
  },
  {
    "id": 375,
    "name": "IMDB",
    "fullname": "IMDB Movie Review Corpus",
    "genericmention": [
      "This cor-pus"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Maas et al. (2011)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://ai.stanford.edu/\u02dcamaas/data/sentiment/",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "1 http://ai.stanford.edu/\u02dcamaas/data/sentiment/",
    "text": "We use the IMDB Movie Review Corpus (IMDB) prepared by Maas et al. (2011). [Cite_Footnote_1] This cor-pus has 75k training reviews and 25k test reviews."
  },
  {
    "id": 376,
    "name": "BBC corpus",
    "fullname": "N/A",
    "genericmention": [
      "this corpus"
    ],
    "description": [
      "this corpus contains news articles which are almost always written in a formal style"
    ],
    "citationtag": [
      "Greene and Cunningham (2006)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mlg.ucd.ie/datasets/bbc.html",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "2 http://mlg.ucd.ie/datasets/bbc.html",
    "text": "BBC Similarly to movie reviews, each new ar-ticle tends to convey a single theme. We use the BBC corpus prepared by Greene and Cunningham (2006). [Cite_Footnote_2] Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro-posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages. We use the 10k most frequent words in the training corpus for recurrent language models."
  },
  {
    "id": 377,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl",
    "text": "Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization. [Cite_Footnote_3] Penn Treebank We evaluate a normal recurrent language model, count-based n-gram language model as well as the proposed RLM-BoW-EF-n and RLM-BoW-LF-n with varying n = 1, 2, 4, 8 on the Penn Treebank Corpus. We preprocess the corpus according to (Mikolov et al., 2011) and use a vocabulary of 10k words from the training cor-pus."
  },
  {
    "id": 378,
    "name": "Fil9",
    "fullname": "N/A",
    "genericmention": [
      "the corpus"
    ],
    "description": [
      "a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mattmahoney.net/dc/textdata",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "4 http://mattmahoney.net/dc/textdata",
    "text": "Fil9 Fil9 is a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens, and is pro-vided on Matthew Mahoney\u2019s website. [Cite_Footnote_4] We tok-enized the corpus and used the 44k most frequent words in the training corpus for recurrent language models."
  },
  {
    "id": 379,
    "name": "Stanford POS Tagger",
    "fullname": "Stanford log-linear part-of-speech tagge",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Toutanova et al., 2003"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/tagger.shtml",
    "section_title": "6 Results and Analysis",
    "add_info": "5 http://nlp.stanford.edu/software/tagger.shtml",
    "text": "We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [Cite_Footnote_5] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb."
  },
  {
    "id": 380,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://rule.alibaba.com/rule/detail/2034.htm",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 381,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.apple.com/legal/privacy/en-ww/",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 382,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.cbsinteractive.com/legal/cbsi/privacy-policy",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 383,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://help.bet365.com/en/privacy-policy",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 384,
    "name": "jsoup",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Jonathan Hedley. 2017. jsoup (1.11.3).",
      "Hedley, 2017"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://jsoup.org/",
    "section_title": "3 Approach 3.1 Domain-Independent Approach (DI)",
    "add_info": "Jonathan Hedley. 2017. jsoup (1.11.3). https://jsoup.org/.",
    "text": "Text Collection: Using jsoup (Hedley, 2017)  , we parse the HTML file and for each non-empty tag encountered we extract a tuple consisting of the text and its XPath."
  },
  {
    "id": 385,
    "name": "GVDB",
    "fullname": "Gun Violence Database",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://gun-violence.org/",
    "section_title": "3 The Gun Violence Database",
    "add_info": "2 http://gun-violence.org/",
    "text": "In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [Cite_Footnote_2] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence. The GVDB is the result of a large crowdsourced annotation effort. This an-notation is ongoing, and the GVDB will be regularly updated with new data and new layers of annotation, making it an interesting and challenging data set on which to evaluate state-of-the-art NLP tools."
  },
  {
    "id": 386,
    "name": "GVDB",
    "fullname": "Gun Violence Database",
    "genericmention": [
      "the database",
      "the database"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://gun-violence.org/",
    "section_title": "3 The Gun Violence Database",
    "add_info": null,
    "text": "At the time of writing, the GVDB contains 7,366 fully annotated articles (Table 1) coming from 1,512 US cities, and the database is continuing to grow. The latest version of the database will be main-tained and available for download at  http://gun-violence.org/."
  },
  {
    "id": 387,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/zhaozj89/TensorEmbeddingNLP",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at  https://github.com/zhaozj89/TensorEmbeddingNLP."
  },
  {
    "id": 388,
    "name": "Yelp dataset",
    "fullname": "N/A",
    "genericmention": [
      "The dataset"
    ],
    "description": [
      "The dataset consists of product reviews aligned with sentiment ratings from 1 to 5."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.yelp.com/dataset",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "2 https://www.yelp.com/dataset",
    "text": "We use one of the classic NLP tasks that commonly suffer profanity issues - text style transfer, to eval-uate the effectiveness of the proposed framework. Particularly, we conduct experiments on a subset of the widely used Yelp dataset [Cite_Footnote_2] . The dataset consists of product reviews aligned with sentiment ratings from 1 to 5. We normalize the ratings by treating ratings below three as negative (0) and otherwise positive (1). After data cleaning, we use the method presented in (Li et al., 2018b) to construct pseudo sentence pairs, which is commonly used in the style transfer field. Then we randomly select 240 thou-sand sentence pairs for training, one thousand for validation, and one hundred for testing. Our task is to transfer the sentence from positive opinion to negative. Here, we only use a small test set because we only use these test samples to test the outcome of the attacks rather than the original task."
  },
  {
    "id": 389,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/rivercold/BERT-unsupervised-OOD",
    "section_title": "References",
    "add_info": "1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.",
    "text": "Deployed real-world machine learning appli-cations are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety is-sues. Prior studies on out-of-domain detec-tion require in-domain task labels and are lim-ited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent represen-tations of pre-trained transformers and pro-pose a simple yet effective method to trans-form features across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further pro-posed to boost detection accuracy. Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [Cite_Footnote_1]"
  },
  {
    "id": 390,
    "name": "TensorFlow code",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Zhong-Yi Li. 2017.",
      "Li, 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow",
    "section_title": "4 Case Study: Transformer on Different Tasks",
    "add_info": "Zhong-Yi Li. 2017. https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow.",
    "text": "The first task we explored is the variational autoencoder (VAE) language modeling (Bowman et al., 2015). We test two models, one with an LSTM RNN decoder which is traditionally used in the task, and the other with a Transformer de-coder. All other model configurations including parameter size are the same across the two mod-els. Table 1, top panel, shows the Transformer VAE consistently improves over the LSTM VAE. With Texar, changing the decoder from an LSTM to a Transformer is easily achieved by modifying only 3 lines of code. It is also worth noting that, building the VAE language model (including data reading, model construction, and optimization) on Texar uses only 70 lines of code (with the length of each line < 80 chars). As a (rough) reference, a popular public TensorFlow code (Li, 2017)  of the same model has used around 400 lines of code for the same part (without line length limit)."
  },
  {
    "id": 391,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The framework and the DTs for Google Books, News-paper and Wikipedia"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://sf.net/projects/jobimtext/",
    "section_title": "6 Conclusion",
    "add_info": "3 https://sf.net/projects/jobimtext/",
    "text": "We have introduced a highly scalable approach to DT computation and showed its adequacy for very large corpora. Evaluating against thesauri and WordNet, we demonstrated that our similarity mea-sure yields better-quality DTs and scales to corpora of billions of sentences, even on comparably small compute clusters. We achieve this by a number of pruning operations, and distributed processing. The framework and the DTs for Google Books, News-paper and Wikipedia are available online [Cite_Footnote_3] under the ASL 2.0 licence."
  },
  {
    "id": 392,
    "name": "Morfessor tool",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology.",
      "Creutz and Lagus, 2005"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cis.hut.fi/projects/morpho/",
    "section_title": "4 Speech recognition experiments 4.4 Turkish",
    "add_info": "Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.",
    "text": "Turkish is another a highly-inflected and agglutina-tive language with relatively free word order. The same Morfessor tool (Creutz and Lagus, 2005)  as in Finnish and Estonian was applied to Turkish texts as well. Using the 360k most common words from the training corpus, 34k morph units were obtained. The training corpus consists of approximately 27M words taken from literature, law, politics, social sciences, popular science, information technology, medicine, newspapers, magazines and sports news. N-gram language models for different orders with interpolated Kneser-Ney smoothing as well as en-tropy based pruning were built for this morph lexi-con using the SRILM toolkit (Stolcke, 2002). The number of n-grams for the highest order we tried (6-grams without entropy-based pruning) are reported in Table 4. In average, there are 2.37 morphs per word including the word break symbol."
  },
  {
    "id": 393,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a joint corpus",
      "the same text corpus"
    ],
    "description": [
      "a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words"
    ],
    "citationtag": [
      "CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other.",
      "CSC, 2001"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.csc.fi/kielipankki/",
    "section_title": "4 Speech recognition experiments 4.2 Finnish",
    "add_info": "CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other. http://www.csc.fi/kielipankki/.",
    "text": "Finnish is a highly inflected language, in which words are formed mainly by agglutination and com-pounding. Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001)  . We obtained a lexicon of 25k morphs by feeding the learning algorithm with the word list containing the 160k most common words. For language model training we used the same text corpus and the re-cently developed growing n-gram training algorithm (Siivola and Pellom, 2005). The amount of resulted n-grams are listed in Table 4. The average length of a morph is such that a word corresponds to 2.52 morphs including a word break symbol."
  },
  {
    "id": 394,
    "name": "ACE parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://sweaglesw.org/linguistics/ace/",
    "section_title": "3 System Description",
    "add_info": "3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.",
    "text": "The new system described here is what we call the MRS Crawler. This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below). We used the grammar together with one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures. Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an \u2018off the shelf\u2019 setting. We combine our system with the outputs from the best-performing 2012 submission, the system of Read et al. (2012), firstly by relying on the latter for system negation cue detection, 4 and secondly as a fall-back in sys-tem combination as described in \u00a7 3.4 below."
  },
  {
    "id": 395,
    "name": "DELPH-IN resources",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.delph-in.net",
    "section_title": "3 System Description",
    "add_info": "3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.",
    "text": "The new system described here is what we call the MRS Crawler. This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below). We used the grammar together with one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures. Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an \u2018off the shelf\u2019 setting. We combine our system with the outputs from the best-performing 2012 submission, the system of Read et al. (2012), firstly by relying on the latter for system negation cue detection, 4 and secondly as a fall-back in sys-tem combination as described in \u00a7 3.4 below."
  },
  {
    "id": 396,
    "name": "N/A",
    "fullname": "NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "1 http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip",
    "text": "The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset [Cite_Footnote_1] . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC. 94,651 unlabeled Chinese products re-views from corresponding categories are used as the development set, denoted as Dev_CHN. In the testing set, there are 12,000 Chinese product reviews (shown in Table.1). This dataset is de-signed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN : where c is either DVD, BOOK or MUSIC."
  },
  {
    "id": 397,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "2 http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf",
    "text": "The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC. 94,651 unlabeled Chinese products re-views from corresponding categories are used as the development set, denoted as Dev_CHN. In the testing set, there are 12,000 Chinese product reviews (shown in Table.1). This dataset is de-signed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN [Cite_Footnote_2] : where c is either DVD, BOOK or MUSIC."
  },
  {
    "id": 398,
    "name": "Google Translator",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://translate.google.com",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "3 https://translate.google.com",
    "text": "In the experiment, the basic transfer learning algorithm is co-training. The Chinese word seg-mentation tool is ICTCLAS (Zhang et al, 2003) and Google Translator [Cite_Footnote_3] is the MT for the source language. The monolingual opinion classifier is SVM light4 , word unigram/bigram features are em-ployed."
  },
  {
    "id": 399,
    "name": "Rus-tomata",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a framework for weighted automata with storage written in the programming language Rust"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/tud-fop/",
    "section_title": "5 Evaluation and Conclusion",
    "add_info": "5 available on https://github.com/tud-fop/rustomata. We used commit 867a451 for evaluation.",
    "text": "We implemented the parser with the modifica-tions sketched in sec. 4 for \u03b5-free and simple wMCFGs, but no problems should arise gener-alising this implementation to arbitrary wMCFGs. The implementation is available as a part of Rus-tomata, [Cite_Footnote_5] a framework for weighted automata with storage written in the programming language Rust. We used the NeGra corpus (German newspaper articles, 20,602 sentences, 355,096 tokens; Skut et al., 1998) to compare our parser to Grammat-ical Framework (Angelov and Ljunglo\u0308f, 2014), rparse (Kallmeyer and Maier, 2013), and disco-dop (van Cranenburgh et al., 2016) with respect to parse time and accuracy. Our experiments were conducted on defoliated trees, i.e. we removed the leaves from each tree in the corpus. Parsing was performed on gold part-of-speech tags."
  },
  {
    "id": 400,
    "name": "InferWiki",
    "fullname": "N/A",
    "genericmention": [
      "Our datasets"
    ],
    "description": [
      "a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, as-sumptions, and patterns",
      "providing manually annotated nega-tive and unknown triples",
      "we include various inference patterns (e.g., reasoning path length and types) for comprehensive evalua-tion"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/TaoMiner/inferwiki",
    "section_title": "References",
    "add_info": null,
    "text": "We present InferWiki, a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, as-sumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. To ensure it, we propose to utilize rule-guided train/test generation, in-stead of conventional random split. Second, InferWiki initiates the evaluation following the open-world assumption and improves the in-ferential difficulty of the closed-world assump-tion, by providing manually annotated nega-tive and unknown triples. Third, we include various inference patterns (e.g., reasoning path length and types) for comprehensive evalua-tion. In experiments, we curate two settings of InferWiki varying in sizes and structures, and apply the construction process on CoDEx as comparative datasets. The results and em-pirical analyses demonstrate the necessity and high-quality of InferWiki. Nevertheless, the performance gap among various inferential as-sumptions and patterns presents the difficulty and inspires future research direction. Our datasets can be found in  https://github.com/TaoMiner/inferwiki."
  },
  {
    "id": 401,
    "name": "Wikidata",
    "fullname": "N/A",
    "genericmention": [
      "the September 2019 En-glish dump"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://www.wikidata.org/",
    "section_title": "3 Dataset Design 3.1 Data Preprocessing",
    "add_info": "1 https://www.wikidata.org/",
    "text": "More and more studies utilize Wikidata [Cite_Footnote_1] as a knowledge resource due to its high quality and large quantity. We utilize the September 2019 En-glish dump in experiments. Data preprocessing aims to define relation vocabulary and extract two sets of triples from Wikidata: a large one for rule mining T r and a relatively small one for dataset generation T d . The reason for using two sets is to avoid the leakage of rules. In other words, some frequent rules on the large set may be very few on the small set. The different distributions shall avoid that rule mining methods will easily achieve high performance. Besides, more triples can improve the quality of mined rules. In contrast, the relatively small set is enough for efficient KGC training and evaluation."
  },
  {
    "id": 402,
    "name": "AnyBURL",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Meilicke et al., 2019"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://web.informatik.uni-mannheim.de/AnyBURL/",
    "section_title": "3 Dataset Design 3.1 Data Preprocessing",
    "add_info": "2 http://web.informatik.uni-mannheim.de/AnyBURL/",
    "text": "3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Gala\u0301rraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL [Cite_Footnote_2] in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets \u03bb p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation \u2014 most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop 8 , and AnyBURL 9 . Because we utilize various"
  },
  {
    "id": 403,
    "name": "OpenKE",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/thunlp/OpenKE",
    "section_title": "F Experiment Setup",
    "add_info": "5 https://github.com/thunlp/OpenKE",
    "text": "Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [Cite_Footnote_5] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data. We then search the embedding size in the range of {256, 512}, number of negative samples in the range of {15, 25} and margin in the range of {4, 8}. The optimal parameters of each model on all of three datasets are listed in Table 10. The thresholds in triples classification are listed in Ta-ble 11"
  },
  {
    "id": 404,
    "name": "ConvE",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/TimDettmers/ConvE",
    "section_title": "F Experiment Setup",
    "add_info": "6 https://github.com/TimDettmers/ConvE",
    "text": "Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE [Cite_Footnote_6] , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data. We then search the embedding size in the range of {256, 512}, number of negative samples in the range of {15, 25} and margin in the range of {4, 8}. The optimal parameters of each model on all of three datasets are listed in Table 10. The thresholds in triples classification are listed in Ta-ble 11"
  },
  {
    "id": 405,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/salesforce/MultiHopKG",
    "section_title": "F Experiment Setup",
    "add_info": "8 https://github.com/salesforce/MultiHopKG",
    "text": "3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Gala\u0301rraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL 2 in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets \u03bb p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation \u2014 most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop [Cite_Footnote_8] , and AnyBURL . Because we utilize various"
  },
  {
    "id": 406,
    "name": "AnyBURL",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Meilicke et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://web.informatik.uni-mannheim.de/AnyBURL/",
    "section_title": "F Experiment Setup",
    "add_info": "9 http://web.informatik.uni-mannheim.de/AnyBURL/",
    "text": "3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Gala\u0301rraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL 2 in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets \u03bb p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation \u2014 most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop , and AnyBURL [Cite_Footnote_9] . Because we utilize various"
  },
  {
    "id": 407,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "this data set "
    ],
    "description": [
      "dialogue sessions",
      "For each session i, annotators create two correct responses and an arbitrary number(M i ) of incorrect responses based on the instruction described above.",
      "We set up one test case to consist of context, one correct response, and one incorrect response."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/kakaoenterprise/KorAdvMRSTestData",
    "section_title": "2 Adversarial Test Dataset",
    "add_info": null,
    "text": "Five annotators generate a total of 200 dialogue sessions. For each session i, annotators create two correct responses and an arbitrary number(M i ) of incorrect responses based on the instruction described above. All sessions and responses are reviewed and filtered by experts. We set up one test case to consist of context, one correct response, and one incorrect response. Therefore, 2 \u2217 M i test cases were extracted for each session, and a total of 2,220 test cases are constructed. It evaluates whether the model gives the correct answer a higher score than the incorrect one for a given context. Statistics and examples are described in Table 1. We release this data set at  https://github.com/kakaoenterprise/KorAdvMRSTestData."
  },
  {
    "id": 408,
    "name": "Ko-rean dialogue corpus",
    "fullname": "N/A",
    "genericmention": [
      "these corpora",
      "each",
      "each dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://corpus.korean.go.kr",
    "section_title": "4 Experiments and Results 4.1 Experiment Setup",
    "add_info": "1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294.",
    "text": "We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [Cite_Footnote_1] . We split these corpora into three, and each is for training, validation, and test. Statistics of each dataset are described in Table 2. #pairs denote the number of context-response pairs, #cands denotes the number of candidates per context, pos:neg denotes the ratio of positive and negative responses in candidates, and #turns de-note the average turns per context. Details on the construction are as follows."
  },
  {
    "id": 409,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our implementation",
      "our implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://groups.csail.mit.edu/nlp/dpo3/",
    "section_title": "3. A free distribution of our implementation. 2",
    "add_info": "2 http://groups.csail.mit.edu/nlp/dpo3/",
    "text": "3. A free distribution of our implementation. [Cite_Footnote_2]"
  },
  {
    "id": 410,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code to reproduce our results"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/adhigunasurya/distillation_parser.git",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/adhigunasurya/distillation_parser.git",
    "text": "The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the con-ventional Hamming cost function, (ii) recently pub-lished strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graph-based dependency parsing for English, Chinese, and German. The code to reproduce our results is pub-licly available. [Cite_Footnote_1]"
  },
  {
    "id": 411,
    "name": "stack LSTM parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Dyer et al., 2015",
      "Dyer et al. (2015)"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/clab/lstm-parser",
    "section_title": "3 Consensus and Minimum Bayes Risk",
    "add_info": "3 We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.",
    "text": "Next, note that if we let s(h, m, x) = votes(h, m)/N, this has no effect on the parser (we have only scaled by a constant factor). We can there-fore view s as a posterior marginal, and the ensemble parser as an MBR parser (Eq. 2). Experiment. We consider this approach on the Stanford dependencies version 3.3.0 (De Marneffe and Manning, 2008) Penn Treebank task. As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [Cite_Footnote_3]"
  },
  {
    "id": 412,
    "name": "CNN neural network library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/clab/cnn.git",
    "section_title": "6 Experiments",
    "add_info": "8 https://github.com/clab/cnn.git",
    "text": "Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library. [Cite_Footnote_8] Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncer-tainty under the ensemble. To obtain the ensemble uncertainty on each language, we use 21 base mod-els for English (see footnote 4), 17 for Chinese, and 11 for German."
  },
  {
    "id": 413,
    "name": "lex4all",
    "fullname": "N/A",
    "genericmention": [
      "its"
    ],
    "description": [
      "an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://lex4all.github.io/lex4all/",
    "section_title": "1 Introduction",
    "add_info": "1 http://lex4all.github.io/lex4all/",
    "text": "This is the motivation behind lex4all, [Cite_Footnote_1] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language."
  },
  {
    "id": 414,
    "name": "CMUSphinx",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.cmusphinx.org",
    "section_title": "2 Background and related work",
    "add_info": "3 http://www.cmusphinx.org",
    "text": "If, however, the target language is one of the many thousands of LRLs for which high-quality recognition engines have not yet been devel-oped, alternative strategies for developing speech-recognition interfaces must be employed. Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx [Cite_Footnote_3] ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit 4 (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low-or zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users."
  },
  {
    "id": 415,
    "name": "RLAT",
    "fullname": "Rapid Language Adaptation Toolkit",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Vu et al., 2010"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://i19pc5.ira.uka.de/rlat-dev",
    "section_title": "2 Background and related work",
    "add_info": "4 http://i19pc5.ira.uka.de/rlat-dev",
    "text": "If, however, the target language is one of the many thousands of LRLs for which high-quality recognition engines have not yet been devel-oped, alternative strategies for developing speech-recognition interfaces must be employed. Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx 3 ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit [Cite_Footnote_4] (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low-or zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users."
  },
  {
    "id": 416,
    "name": "NAudio",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "http://naudio.codeplex.com/",
    "section_title": "5 User interface 5.1 Audio input and recording",
    "add_info": "8 http://naudio.codeplex.com/",
    "text": "The recorder, built using the open-source library NAudio, [Cite_Footnote_8] takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1)."
  },
  {
    "id": 417,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/thu-coai/earl",
    "section_title": "4 Experiments 4.3 Implementation Details",
    "add_info": null,
    "text": "We used the stochastic gradient descent (SGD) algorithm with mini-batch. The batch size and learning rate are set to 100 and 0.5, respectively. The model was run at most 20 epochs, and the training stage of each model took about one day on a GPU machine. We selected the model performing best in the validation set to evaluate in test sets. Our code is available at:  https://github.com/thu-coai/earl."
  },
  {
    "id": 418,
    "name": "EQG-RACE dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Jia et al. (2020)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/jemmryx/EQG-RACE",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "1 https://github.com/jemmryx/EQG-RACE",
    "text": "We carry out the training and inference on EQG-RACE dataset [Cite_Footnote_1] proposed by Jia et al. (2020). The passage numbers of training set, validation set and test set are respectively 11457, 642, 609."
  },
  {
    "id": 419,
    "name": "JUMAN",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Japanese morpholog-ical analyzer"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html",
    "section_title": "2 Japanese Morphology",
    "add_info": "1 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html",
    "text": "In order to understand the task of lexicon acquisi-tion, we briefly describe the Japanese morpholog-ical analyzer JUMAN. [Cite_Footnote_1] We explain Japanese mor-phemes in Section 2.1, morphological constraints in Section 2.2, and unknown morpheme processing in Section 2.3."
  },
  {
    "id": 420,
    "name": "KNP",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "dependency parser",
      "KNP is used to form a phrasal unit called bunsetsu by chunking morphemes."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html",
    "section_title": "3 Lexicon Acquisition 3.2 System Architecture",
    "add_info": "2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html",
    "text": "Figure 1 shows the system architecture. Each sen-tence in texts is processed by the morphological an-alyzer JUMAN and the dependency parser KNP. [Cite_Footnote_2] JUMAN consults a hand-crafted dictionary and an automatically constructed dictionary. KNP is used to form a phrasal unit called bunsetsu by chunking morphemes."
  },
  {
    "id": 421,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the open-domain word embeddings"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/mmihaltz/word2vec-GoogleNews-vectors",
    "section_title": "6 Experiments 6.3 Training Details",
    "add_info": "1 https://github.com/mmihaltz/word2vec-GoogleNews-vectors",
    "text": "We use the open-domain word embeddings [Cite_Footnote_1] for the initialization of word vectors. We initialize other model parameters from a uniform distribu-tion U(-0.05, 0.05). The dimension of the word embedding and the size of the hidden layers are 300. The learning rate is set to 0.01 and the dropout rate is set to 0.1. Stochastic gradient de-scent is used as our optimizer. The position encod-ing is also used (Tang et al., 2016). We also com-pare the memory networks in their multiple com-putational layers version (i.e., multiple hops) and the number of hops is set to 3 as used in the men-tioned previous studies. We implemented all mod-els in the TensorFlow environment using same in-put, embedding size, dropout rate, optimizer, etc. so as to test our hypotheses, i.e., to make sure the achieved improvements do not come from else-where. Meanwhile, we can also report all evalua-tion measures discussed above 2 . 10% of the train-ing data is used as the development set. We report the best results for all models based on their F-1 Macro scores."
  },
  {
    "id": 422,
    "name": "Prote\u0301ge\u0301 4.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the freely available ontol-ogy editor"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://protege.stanford.edu/",
    "section_title": "3 Knowledge Representation and Semantic Inference",
    "add_info": "2 http://protege.stanford.edu/, as accessed 27 Oct 2009",
    "text": "The concepts and individuals of the particular domain are structured and organized in domain-specific ontologies. These ontologies are mod-elled in the Web Ontology Language (OWL). OWL allows us to define concept hierarchies, re-lations between concepts, domains and ranges of these relations, as well as specific relation in-stances between instances of a concept. Our on-tologies are defined by the freely available ontol-ogy editor Prote\u0301ge\u0301 4.0 [Cite_Footnote_2] . The advantage of using an ontology for structuring the domain knowledge is the modular non-redundant encoding. When com-bined with a reasoner, only a few statements about an individual have to be asserted explicitely, while the rest can be inferred from the ontology. We em-ploy several ontologies, among which the follow-ing are relevant for modelling the specific domains of our NPCs:"
  },
  {
    "id": 423,
    "name": "SwiftOwlim",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "It provides a forward-chaining inference engine which evaluates the domain definitions when loading the knowledge repository, and makes implicit knowledge explicit by asserting triples that must also hold true accord-ing to the ontology.",
      "SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.ontotext.com/owlim/",
    "section_title": "3 Knowledge Representation and Semantic Inference",
    "add_info": "3 http://www.ontotext.com/owlim/",
    "text": "We use SwiftOwlim [Cite_Footnote_3] for storing and querying the data. SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data. It provides a forward-chaining inference engine which evaluates the domain definitions when loading the knowledge repository, and makes implicit knowledge explicit by asserting triples that must also hold true accord-ing to the ontology. Once the reasoner is finished, the triple store can be queried directly using the RDF query language SPARQL."
  },
  {
    "id": 424,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/tingc9/LinearTemplatesSRW",
    "section_title": "9 Results",
    "add_info": "2 The code used for this research will be made available on https://github.com/tingc9/LinearTemplatesSRW",
    "text": "Computationally, the search time increases with increasing sentence lengths. On a reasonably modern machine, our implementation generated the above sentences in about 150 seconds while using 2.2 GB of memory. [Cite_Footnote_2]"
  },
  {
    "id": 425,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a publicly available preprocessing code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/najoungkim/pdtb3",
    "section_title": "3 Proposed Evaluation Protocol",
    "add_info": "4 https://github.com/najoungkim/pdtb3",
    "text": "While Xue et al. (2015) lay out one possible pro-tocol, it does not fully address the issues we have raised in Section 2. Another limitation is the un-availability of the preprocessing code as of the date of this submission. We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [Cite_Footnote_4] In addition to accounting for the variation previously discussed, we take Shi and Demberg (2017)\u2019s concerns into consideration."
  },
  {
    "id": 426,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a publicly available codebase"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface/pytorch-transformers",
    "section_title": "3 Proposed Evaluation Protocol 3.1 Baseline results",
    "add_info": "5 https://github.com/huggingface/pytorch-transformers",
    "text": "Following our proposed protocol, we report base-line results from two strong sentence encoder mod-els: BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), using a publicly available codebase. [Cite_Footnote_5] See Appendix C for training details. We present L2 results on PDTB 2.0 in Table 1 and results on PDTB 3.0 in Table 2 (see Appendix D for L1 re-sults). To maintain backwards compatibility to the literature, we also report PDTB 2.0 results on Ji, Lin and P&K splits (see Section 2.1). Ji & Lin are the most common splits, and P&K is the split used by Nie et al. (2019) who claim the current state-of-the-art for L2. For PDTB 2.0 (Table 1), our baselines showed strong performance on all splits. XLNet-large was the single best model, signifi-cantly outperforming every best reported result."
  },
  {
    "id": 427,
    "name": "WMT-15 corpora",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.statmt.org/wmt15/",
    "section_title": "4 Experimental Study 4.1 Experimental Setting",
    "add_info": "1 http://www.statmt.org/wmt15/",
    "text": "In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochas-tic gradient descent with Adam (Kingma and Ba, 2015). Chung et al. (2016) and Sennrich et al. (2016) demonstrated that bpe boosts NMT, so sim-ilar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora [Cite_Footnote_1] to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English\u2013Turkish (En\u2013Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016). The training side of the English\u2013German (En\u2013De), English\u2013Russian (En\u2013 Ru), and En\u2013Tr corpora include 4.5, 2.1, and 4 million parallel sentences, respectively. We ran-domly select 3K sentences for each of the develop-ment and test sets for En\u2013Tr. For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character."
  },
  {
    "id": 428,
    "name": "TIPSTER [Cite_Footnote_3] corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93T3A",
    "section_title": "4 Experiments 4.1 Data",
    "add_info": "3 Description at http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93T3A.",
    "text": "We take a subset of the TIPSTER [Cite_Footnote_3] corpus \u2013 all Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words) \u2013 and automatically anno-tate them with sentence boundaries, part of speech tags and dependency relations using the Stanford parser (Klein & Manning, 2003). We reserve a small subset of about 600 articles (340,000 words) for testing and use the rest to build a trigram LM with the CMU toolkit (Clarkson & Rosenfeld, 1997, with Good-Turing smoothing and vocabulary size of 30,000). To train the maximum entropy classifiers we use about 41,000 sentences."
  },
  {
    "id": 429,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Japanese short answer scoring dataset",
      "The dataset",
      "the dataset"
    ],
    "description": [
      "The dataset consists of six prompts.",
      "Each prompt has its rubric, student responses, and scores.",
      "Each response was manually scored using the multiple analytic criteria for the prompt, and the subscore for each criterion was rated individually on the basis of the correspond-ing rubric."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://aip-nlu.gitlab.io/resources/sas-japanese",
    "section_title": "5 Experiments 5.1 Dataset",
    "add_info": "1 https://aip-nlu.gitlab.io/resources/sas-japanese",
    "text": "We use the Japanese short answer scoring dataset [Cite_Footnote_1] introduced by Mizumoto et al. (2019). The dataset consists of six prompts. Each prompt has its rubric, student responses, and scores. The prompts, rubrics, and student responses in the dataset were collected from the examinations conducted by a Japanese education company, Takamiya Gakuen Yoyogi Seminar. Each response was manually scored using the multiple analytic criteria for the prompt, and the subscore for each criterion was rated individually on the basis of the correspond-ing rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response."
  },
  {
    "id": 430,
    "name": "pretrained character-based BERT",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Devlin et al., 2019"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/cl-tohoku/bert-japanese",
    "section_title": "5 Experiments 5.2 Settings",
    "add_info": "3 We adopted pretrained character-based BERT which is known to be suitable for processing Japanese texts. This is available at https://github.com/cl-tohoku/bert-japanese. for Low-Resource NLP, pages 175\u2013182.",
    "text": "We split the dataset into training data (1, 600), val-idation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [Cite_Footnote_3] We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set. We trained five models with different random seeds and reported the aver-age of the results."
  },
  {
    "id": 431,
    "name": "Akamon system",
    "fullname": "N/A",
    "genericmention": [
      "this system"
    ],
    "description": [
      "The Akamon system [Cite_Footnote_2] , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding"
    ],
    "citationtag": [
      "Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a",
      "Liu et al., 2006; Mi et al., 2008"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://sites.google.com/site/xianchaowu2012",
    "section_title": "1 Introduction",
    "add_info": "2 Code available at https://sites.google.com/site/xianchaowu2012",
    "text": "However, few tree/forest-to-string systems have been made open source and this makes it diffi-cult and time-consuming to testify and follow exist-ing proposals involved in recently published papers. The Akamon system [Cite_Footnote_2] , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding (Liu et al., 2006; Mi et al., 2008). We hope this system will help re-lated researchers to catch up with the achievements of tree/forest-based translations in the past several years without re-implementing the systems or gen-eral algorithms from scratch."
  },
  {
    "id": 432,
    "name": "RIBES",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://www.kecl.ntt.co.jp/icl/lirg/ribes",
    "section_title": "2 Akamon Toolkit Features",
    "add_info": "3 Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes",
    "text": "Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score [Cite_Footnote_3] , , i.e., the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes."
  },
  {
    "id": 433,
    "name": "Google online transla-tion system",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "online transla-tion system"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://translate.google.com/",
    "section_title": "2 Akamon Toolkit Features",
    "add_info": "4 http://translate.google.com/",
    "text": "Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT sys-tem. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system [Cite_Footnote_4] are also listed in this table."
  },
  {
    "id": 434,
    "name": "SRILM",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Stolcke, 2002"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.speech.sri.com/projects/srilm/",
    "section_title": "2 Akamon Toolkit Features",
    "add_info": "5 http://www.speech.sri.com/projects/srilm/",
    "text": "\u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM [Cite_Footnote_5] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003)."
  }
]
[
  {
    "id": 0,
    "name": "UUParser",
    "fullname": "N/A",
    "genericmention": [
      "the parser"
    ],
    "description": [
      "a near-SOTA model",
      "a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle"
    ],
    "citationtag": [
      "de Lhoneux et al. (2017b)"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/mdelhoneux/uuparser-composition",
    "section_title": "4 Composition in a K&G Parser",
    "add_info": "4 The code can be found at https://github.com/mdelhoneux/uuparser-composition",
    "text": "Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [Cite_Footnote_4] . The S WAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarse-grained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below the best system, which uses an ensemble system as well as ELMo embed-dings, as introduced by Peters et al. (2018). Note, however, that we use a slightly impoverished ver-sion of the model used for the shared task which is described in Smith et al. (2018a): we use a less ac-curate POS tagger (UDPipe) and we do not make use of multi-treebank models. In addition, Smith et al. (2018a) use the three top items of the stack as well as the first item of the buffer to represent the configuration, while we only use the two top items of the stack and the first item of the buffer. Smith et al. (2018a) also use an extended feature set as introduced by Kiperwasser and Goldberg (2016b) where they also use the rightmost and left-most children of the items of the stack and buffer that they consider. We do not use that extended feature set. This is to keep the parser settings as simple as possible and avoid adding confounding factors. It is still a near-SOTA model. We evaluate parsing models on the development sets and report the average of the 5 best results in 30 epochs and 5 runs with different random seeds."
  },
  {
    "id": 1,
    "name": "Universal Depen-dencies 2.0 treebanks",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Straka and Strakov, 2017",
      "Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88\u201399."
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://hdl.handle.net/11234/1-2364",
    "section_title": "5 What Correlates with Difficulty?",
    "add_info": "Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88\u201399. Documented models at http://hdl.handle.net/11234/1-2364.",
    "text": "Head-POS Entropy Dehouck and Denis (2018) propose an alternative measure of morphosyntactic complexity. Given a corpus of dependency graphs, they estimate the conditional entropy of the POS tag of a random token\u2019s parent, conditioned on the token\u2019s type. In a language where this HPE-mean metric is low, most tokens can predict the POS of their parent even without context. We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017)  ."
  },
  {
    "id": 2,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the re-versible language-agnostic tokenizer"
    ],
    "description": [
      "the re-versible language-agnostic tokenizer"
    ],
    "citationtag": [
      "Mielke and Eisner (2018)"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://sjmielke.com/papers/tokenize/",
    "section_title": "D Data selection: Europarl",
    "add_info": "31 http://sjmielke.com/papers/tokenize/",
    "text": "Finally, it should be said that the text in CoStEP itself contains some markup, marking reports, el-lipses, etc., but we strip this additional markup to obtain the raw text. We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [Cite_Footnote_31] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set. This way we ensure uniform division over sessions of the parliament and sizes of 2 / 3 , 1 / 6 , and 1 / 6 , respectively."
  },
  {
    "id": 3,
    "name": "Twitter API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://developer.twitter.com/en/docs.html",
    "section_title": "2 Problem Formulation 2.2 Data",
    "add_info": null,
    "text": "\u2022 Negative examples: We have col-lected 1% of tweets from Twitter\u2019s daily feed using the Twitter API (  https://developer.twitter.com/en/docs.html) to use as negative examples."
  },
  {
    "id": 4,
    "name": "MTurk",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Amazon, 2005",
      "Amazon. 2005"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.mturk.com/",
    "section_title": "5 User study",
    "add_info": "Amazon. 2005. MTurk. (https://www.mturk.com/).",
    "text": "To verify whether human evaluators are in agree-ment with our characterization model, we con-ducted a user study using MTurk (Amazon, 2005)  ."
  },
  {
    "id": 5,
    "name": "Fast-Text",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Facebook-Research, 2016",
      "Facebook-Research. 2016"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://research.fb.com/fasttext/",
    "section_title": "4 Approaches to authorship verification 4.4 Approach 4: Document embeddings",
    "add_info": "Facebook-Research. 2016. FastText. (https://research.fb.com/fasttext/).",
    "text": "1. We obtain representations of tweets as doc-ument embeddings. We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016)  (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768)."
  },
  {
    "id": 6,
    "name": "Google-n-Gram",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://books.google.com/ngrams",
    "section_title": "3 The Proposed Method 3.1 Automatic Seed Generation",
    "add_info": "1 Google-n-Gram (http://books.google.com/ngrams) is used as the background corpus.",
    "text": "The seed set consists of positive labeled examples (i.e. product features) and negative labeled exam-ples (i.e. noise terms). Intuitively, popular product features are frequently mentioned in reviews, so they can be extracted by simply mining frequently occurring nouns (Hu and Liu, 2004). However, this strategy will also find many noise terms (e.g., commonly used nouns like thing, one, etc.). To produce high quality seeds, we employ a Domain Relevance Measure (DRM) (Jiang and Tan, 2010), which combines term frequency with a domain-specific measuring metric called Likelihood Ratio Test (LRT) (Dunning, 1993). Let \u03bb(t) denotes the LRT score of a product feature candidate t, where k 1 and k 2 are the frequencies of t in the review corpus R and a background corpus [Cite_Footnote_1] B, n 1 and n 2 are the total number of terms in R and B, p = (k 1 + k 2 )/(n 1 + n 2 ), p 1 = k 1 /n 1 and p 2 = k 2 /n 2 . Then a modified DRM 2 is proposed, where tf(t) is the frequency of t in R and df(t) is the frequency of t in B."
  },
  {
    "id": 7,
    "name": "Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.wikipedia.org",
    "section_title": "3 The Proposed Method 3.2 Capturing Lexical Semantic Clue in a Semantic Similarity Graph 3.2.1 Learning Word Embedding for",
    "add_info": "3 Wikipedia(http://www.wikipedia.org) is used in practice.",
    "text": "To alleviate the data sparsity problem, EB is first trained on a very large corpus [Cite_Footnote_3] (denoted by C), and then fine-tuned on the target review cor-pus R. Particularly, for phrasal product features, a statistic-based method in (Zhu et al., 2009) is used to detect noun phrases in R. Then, an Unfold-ing Recursive Autoencoder (Socher et al., 2011) is trained on C to obtain embedding vectors for noun phrases. In this way, semantics of infrequent terms in R can be well captured. Finally, the phrase-based Skip-gram model in (Mikolov et al., 2013) is applied on R."
  },
  {
    "id": 8,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a benchmark dataset in Wang et al. (2011)",
      "The first one"
    ],
    "description": [
      "a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel)",
      "real world datasets"
    ],
    "citationtag": [
      "Wang et al. (2011)"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://timan.cs.uiuc.edu/downloads.html",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "5 http://timan.cs.uiuc.edu/downloads.html",
    "text": "Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) [Cite_Footnote_5] . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper."
  },
  {
    "id": 9,
    "name": "COAE 2008",
    "fullname": "Chinese Opinion Analysis Evalua-tion 2008",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "real world datasets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://ir-china.org.cn/coae2008.html",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "6 http://ir-china.org.cn/coae2008.html",
    "text": "Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) [Cite_Footnote_6] , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper."
  },
  {
    "id": 10,
    "name": "\u201cinterest\u201d, \u201cline\u201d",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://www.d.umn.edu/\u223ctpederse/data.html",
    "section_title": "4 Experiments and Results 4.1 Experiment Design",
    "add_info": "1 Available at http://www.d.umn.edu/\u223ctpederse/data.html",
    "text": "For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d [Cite_Footnote_1] and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3."
  },
  {
    "id": 11,
    "name": "the data in English lexical sample task of SENSEVAL-3",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://www.senseval.org/senseval3",
    "section_title": "4 Experiments and Results 4.1 Experiment Design",
    "add_info": "2 Available at http://www.senseval.org/senseval3",
    "text": "For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) [Cite_Footnote_2] . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3."
  },
  {
    "id": 12,
    "name": "SV M light",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://svmlight.joachims.org/",
    "section_title": "4 Experiments and Results 4.2 Experiment 1: LP vs. SVM",
    "add_info": "3 we SV M light ,used linear available at http://svmlight.joachims.org/.",
    "text": "Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [Cite_Footnote_3] systems in ELS task of SENSEVAL-3."
  },
  {
    "id": 13,
    "name": "Isomap",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://isomap.stanford.edu/",
    "section_title": "4 Experiments and Results 4.4 An Example: Word \u201cuse\u201d",
    "add_info": "5 We used Isomap to perform dimensionality reduction by computing two-dimensional, 39-nearest-neighbor-preserving embedding of 210-dimensional input. Isomap is available at http://isomap.stanford.edu/.",
    "text": "For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word \u201cuse\u201d in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set). For data visualization, we conducted unsupervised nonlinear dimensionality reduction [Cite_Footnote_5] on these 40 feature vec-tors with 210 dimensions. Figure 3 (a) shows the dimensionality reduced vectors in two-dimensional space. We randomly sampled only one labeled ex-ample for each sense of word \u201cuse\u201d as labeled data. The remaining data in training set and test set served as unlabeled data for bootstrapping and LP. All of these three algorithms are evaluated using accuracy on test set."
  },
  {
    "id": 14,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code and preprocessed datasets",
      "our proposed model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/GeneZC/ASGCN",
    "section_title": "References",
    "add_info": "1 Code and preprocessed datasets are available at https://github.com/GeneZC/ASGCN.",
    "text": "Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neu-ral Networks (CNNs) are widely applied for aspect-based sentiment classification. How-ever, these models lack a mechanism to ac-count for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we pro-pose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models [Cite_Footnote_1] , and further demon-strate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure."
  },
  {
    "id": 15,
    "name": "SVM light",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Joachims, 1999"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://svmlight.joachims.org",
    "section_title": "3 Problem Formulation",
    "add_info": "2 Available at http://svmlight.joachims.org.",
    "text": "The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Scho\u0308lkopf and Smola (2002). SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments. [Cite_Footnote_2]"
  },
  {
    "id": 16,
    "name": "N/A",
    "fullname": "Center for Research in Security Prices (CRSP) US Stocks Database",
    "genericmention": [
      "The text and volatility data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/10K",
    "section_title": "4 Dataset",
    "add_info": "4 The text and volatility data are publicly available at http://www.ark.cs.cmu.edu/10K.",
    "text": "In addition to the reports, we used the Center for Research in Security Prices (CRSP) US Stocks Database to obtain the price return series along with other firm characteristics. [Cite_Footnote_4] We proceeded to calcu-late two volatilities for each firm/report observation: the twelve months prior to the report (v (\u221212) ) and the twelve months after the report (v (+12) )."
  },
  {
    "id": 17,
    "name": "GMTG",
    "fullname": "Gener-alized multitext grammars",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Technical Report 04-003, NYU Proteus Project",
      "Dan Melamed, G. Satta, and B. Wellington. 2004"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/pubs/",
    "section_title": "1 Introduction",
    "add_info": "I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.",
    "text": "This paper begins with an informal description of GMTG. It continues with an investigation of this formalism\u2019s generative capacity. Next, we prove that in GMTG each component grammar retains its generative power, a requirement for synchronous formalisms that Rambow and Satta (1996) called the \u201cweak language preservation property.\u201d Lastly, we propose a synchronous generalization of Chom-sky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKY-style algorithm (Younger, 1967; Melamed, 2004  )."
  },
  {
    "id": 18,
    "name": "GMTG",
    "fullname": "Gener-alized multitext grammars",
    "genericmention": [
      "a GMTG"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Melamed, 2004",
      "Dan Melamed, G. Satta, and B. Wellington. 2004",
      "Technical Report 04-003, NYU Proteus Project"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/pubs/",
    "section_title": "6 Generalized Chomsky Normal Form 6.2 Step 4: Eliminate \u2019s",
    "add_info": "I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.",
    "text": "Grammars in GCNF cannot have \u2019s in their productions. Thus, GCNF is a more restrictive normal form than those used by Wu (1997) and Melamed (2003). The absence of \u2019s simplifies parsers for GMTG (Melamed, 2004)  . Given a GMTG u with in some productions, we give the construction of a weakly equivalent gram-mar u9O without any \u2019s. First, determine all nullable links and associated - strings in u . - A link * Z Z is nullable if < y is an ITV where at least one y\u2022bhg is . We say the link is nullable and the string at address in is nullable. For each nullable link, we create versions of the link, where is the number of nullable strings of that link. There is one version for each of the possible combinations of the nullable strings being present or absent. The version of the link with all strings present is its original version. Each non-original version of the link (except in the case of start links) gets a unique subscript, which is applied to all the nonterminals in the link, so that each link is unique in the grammar. We construct a new grammar u O whose set of productions w O is determined as follows: for each production, we identify the nullable links on the RHS and replace them with each combination of the non-original versions found earlier. If a string is left empty during this process, that string is removed from the RHS and the fan-out of the production component is reduced by one. The link on the LHS is replaced with its appropriate matching non-original link. There is one exception to the replacements. If a production consists of all nullable strings, do not include this case. Lastly, we remove all strings on the RHS of productions that have \u2019s, and reduce the fan-out of the productions accordingly. Once again, we replace the LHS link with the appropriate version. case and are nullable - so we create 54 a new version of both links: and . We then alter the productions. Pro-duction (31) gets replaced by (40). A new produc-tion based on (30) is Production (38). Lastly, Pro-duction (29) has two nullable strings on the RHS, so it gets altered to add three new productions, (34), (35) and (36). The altered set of productions are the following:"
  },
  {
    "id": 19,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our open-source code repository"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://hohocode.github.io/textSimilarityConvNet/",
    "section_title": "6 Experiments and Results",
    "add_info": "4 http://hohocode.github.io/textSimilarityConvNet/",
    "text": "Everything necessary to replicate our experimen-tal results can be found in our open-source code repository. [Cite_Footnote_4]"
  },
  {
    "id": 20,
    "name": "STAC corpus",
    "fullname": "N/A",
    "genericmention": [
      "the corpus",
      "the corpus",
      "the corpus"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.irit.fr/STAC/",
    "section_title": "1 Introduction",
    "add_info": "1 https://www.irit.fr/STAC/",
    "text": "In our study, we restrict the structure learning problem to predicting edges or attachments be-tween DU pairs in the dependency graph. After training a supervised deep learning algorithm to predict attachments on the STAC corpus [Cite_Footnote_1] , we then constructed a weakly supervised learning system in which we used 10% of the corpus as a develop-ment set. Experts on discourse structure wrote a set of attachment rules, Labeling Functions (LFs), with reference to this development set. Although the whole of the STAC corpus is annotated, we treated the remainder of the corpus as unseen/u-nannotated data in order to simulate the conditions in which the snorkel framework is meant to be used, i.e. where there is a large amount of unla-beled data but where it is only feasible to hand la-bel a relatively small portion of it. Accordingly, we applied the completed LFs to our \u201cunseen\u201d training set, 80% of the corpus, and used the final 10% as our test set."
  },
  {
    "id": 21,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a subset of the data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://tizirinagh.github.io/acl2019/",
    "section_title": "5 Results and Analysis",
    "add_info": "3 https://tizirinagh.github.io/acl2019/",
    "text": "We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [Cite_Footnote_3] . We then evaluated the generative model and the generative + discriminative model with the Snorkel architecture on the test set with the results in Table 2."
  },
  {
    "id": 22,
    "name": "ACE2005",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "https://catalog.ldc.upenn.edu/LDC2006T06",
    "section_title": "1 Introduction",
    "add_info": "1 https://catalog.ldc.upenn.edu/LDC2006T06",
    "text": "We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [Cite_Footnote_1] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community."
  },
  {
    "id": 23,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/liushulinle/event",
    "section_title": "1 Introduction",
    "add_info": "2 https://github.com/liushulinle/event detection without triggers",
    "text": "We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community. [Cite_Footnote_2]"
  },
  {
    "id": 24,
    "name": "Stanford CoreNLP tool-s",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Manning et al., 2014"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://stanfordnlp.github.io/CoreNLP",
    "section_title": "3 Methodology 3.1 Input Tokens",
    "add_info": "3 http://stanfordnlp.github.io/CoreNLP",
    "text": "Given a sentence, we use Stanford CoreNLP tool-s [Cite_Footnote_3] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b)."
  },
  {
    "id": 25,
    "name": "NYT corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2008T19",
    "section_title": "3 Methodology 3.2 Word/Entity Embeddings",
    "add_info": "4 https://catalog.ldc.upenn.edu/LDC2008T19",
    "text": "In this work, we use the Skip-gram mod-el(Mikolov et al., 2013) to learn word embeddings on the NYT corpus [Cite_Footnote_4] . Furthermore, we random-ly initialized an embedding table for each entity tags. All the input word tokens and entity tags will be transformed into low-dimensional vectors by looking up these embedding tables. In this work, we denote the dimension of word embeddings by d w , and that of entity embeddings by d e ."
  },
  {
    "id": 26,
    "name": "BIBTEX",
    "fullname": "N/A",
    "genericmention": [
      "The second dataset",
      "The dataset",
      "The dataset"
    ],
    "description": [
      "The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users.",
      "the BIBTEX dataset does not provide how many times each tag is annotated to a resource."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.kde.cs.uni-kassel.de/bibsonomy/dumps",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "2 The dataset can be obtained from http://www.kde.cs.uni-kassel.de/bibsonomy/dumps",
    "text": "The first dataset, denoted as BOOK, is obtained from a popular Chinese book review website www. douban.com, which contains the descriptions of books and the tags collaboratively annotated by users. The second dataset, denoted as BIBTEX, is obtained from an English online bibliography web-site www.bibsonomy.org [Cite_Footnote_2] . The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users. As shown in Table 2, the average length of descriptions in the BIBTEX dataset is much shorter than the BOOK dataset. Moreover, the BIBTEX dataset does not provide how many times each tag is annotated to a resource."
  },
  {
    "id": 27,
    "name": "Elasticsearch [Cite_Footnote_3] search engine",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Keyword search on tweets is powered by Elasticsearch which is cou-pled with querying the database to provide addi-tional filters."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.elastic.co/products/elasticsearch",
    "section_title": "2 System Architecture and Components 2.1 System Architecture",
    "add_info": "3 https://www.elastic.co/products/elasticsearch",
    "text": "ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [Cite_Footnote_3] search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers. The front-end presentation layer allows users to narrow down search results by applying multiple filters. Keyword search on tweets is powered by Elasticsearch which is cou-pled with querying the database to provide addi-tional filters. Additionally, it provides numerous visualized graphs. The back-end data collection and computation layer performs pre-processing of tweets, computing check-worthiness scores of tweets using the public ClaimBuster API (Hassan et al., 2017a), Elasticsearch batch insertion, de-tecting claim types of tweets, and finding similar fact-checked claims for each tweet, using Claim-Buster API. ClaimPortal stays up-to-date with current tweets by periodically calling the Twitter REST API."
  },
  {
    "id": 28,
    "name": "Twitter\u2019s card API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://developer.twitter.com/en/docs/tweets/optimize-with-cards",
    "section_title": "2 System Architecture and Components 2.2 Monitoring, Processing, and Storing Tweets",
    "add_info": "5 https://developer.twitter.com/en/docs/tweets/optimize-with-cards",
    "text": "ClaimPortal\u2019s back-end layer focuses on data processing and storage. The Twitter REST API provides us with the necessary data. However, the system does not require all of it. In fact, a lot of the API\u2019s response is discarded to keep our database small and yet sufficient enough to pro-vide all necessary information for the portal. This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters. Based on the keyword search and filters requested by a user, the API queries the database to find the resulting list of tweet IDs and returns the list as a JSON response. A tweet ID is a unique number assigned to a tweet by Twitter. By using Twitter\u2019s card API [Cite_Footnote_5] the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID."
  },
  {
    "id": 29,
    "name": "ClaimBuster API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a well-known fact-checking tool",
      "The Claim-Buster API returns a check-worthiness score for any given text.",
      "The score is on a scale from 0 to 1, ranging from least check-worthy to most check-worthy."
    ],
    "citationtag": [
      "Adair et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://idir.uta.edu/claimbuster/",
    "section_title": "2 System Architecture and Components 2.3 Claim Spotter",
    "add_info": "6 https://idir.uta.edu/claimbuster/",
    "text": "In ClaimPortal, each tweet is given a check-worthiness score which denotes whether the tweet has a factual claim of which the truthfulness is im-portant to the public. This score is obtained by probing the ClaimBuster API, [Cite_Footnote_6] a well-known fact-checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019). Claim-Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates. The Claim-Buster API returns a check-worthiness score for any given text. The score is on a scale from 0 to 1, ranging from least check-worthy to most check-worthy. The background task of probing Claim-Buster API for getting scores for tweets is another batch process, in parallel with the tweet collection and the Elasticsearch indexing processes."
  },
  {
    "id": 30,
    "name": "PolitiFact",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.politifact.com",
    "section_title": "2 System Architecture and Components 2.4 Detecting Claim Types 2.4.1 Frame detection",
    "add_info": "7 https://www.politifact.com",
    "text": "We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [Cite_Footnote_7] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it. Details of these purposely created new frames can be found in (Arslan et al., 2019). The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available."
  },
  {
    "id": 31,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The corpus of the newly-defined frames along with their annotated exemplary sentences"
    ],
    "description": [
      "the newly-defined frames along with their annotated exemplary sentences"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/idirlab/factframe",
    "section_title": "2 System Architecture and Components 2.4 Detecting Claim Types 2.4.1 Frame detection",
    "add_info": "8 https://github.com/idirlab/factframe",
    "text": "We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it. Details of these purposely created new frames can be found in (Arslan et al., 2019). The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available. [Cite_Footnote_8]"
  },
  {
    "id": 32,
    "name": "Share-the-facts [Cite_Footnote_10] fact checks",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.sharethefacts.org/",
    "section_title": "2 System Architecture and Components 2.5 Claim Matcher",
    "add_info": "10 http://www.sharethefacts.org/",
    "text": "ClaimPortal leverages the claim matching func-tion in the ClaimBuster API. The fact-check repository is composed of the Share-the-facts [Cite_Footnote_10] fact checks as well as fact checks collected from several fact-checking organizations like PolitiFact, Snopes, factcheck.org, Washington Post, etc. The system measures the similarity between a claim and a fact-check based on the similarity of their tokens. An Elasticsearch server is deployed for searching the repository based on token similarity."
  },
  {
    "id": 33,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our final parser",
      "Our implementation"
    ],
    "description": [
      "The default setting for our final parser is a 2-layer GNN model that uses hd \u25b7 h (Equation 8) aggregating function and \u201cH-first\u201d asynchronous update method (Equation 9)."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/AntNLP/gnn-dep-parsing",
    "section_title": "4 Experiments",
    "add_info": "6 Our implementation is publicly available at: https://github.com/AntNLP/gnn-dep-parsing",
    "text": "The default setting for our final parser is a 2-layer GNN model that uses hd \u25b7 h (Equation 8) aggregating function and \u201cH-first\u201d asynchronous update method (Equation 9). [Cite_Footnote_6]"
  },
  {
    "id": 34,
    "name": "FastText multilingual pretrained vectors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "multilingual pretrained vectors"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/facebookresearch/fastText",
    "section_title": "4 Experiments 4.1 Main Results",
    "add_info": "8 https://github.com/facebookresearch/fastText",
    "text": "Finally, we report the results of our model on partial UD treebanks on the CoNLL 2018 shared task (Table 5). Our model uses only word and XPOS tag (predict by UDPipe), without any cross lingual features. We use FastText multilingual pretrained vectors instead of Glove vectors. [Cite_Footnote_8] The results show that our GNN parser performs better on 10 UD 2.2 treebanks. For bg, our parser does not improve performance. For nl, our parser im-proves 0.22 UAS, although LAS is slightly lower than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser."
  },
  {
    "id": 35,
    "name": "UD 2.2",
    "fullname": "Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [Cite_Ref] tree-banks",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague",
      "Nivre et al., 2018"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://hdl.handle.net/11234/1-1983xxx",
    "section_title": "4 Experiments",
    "add_info": "Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.",
    "text": "We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018)  tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=\u00a72-21, dev=\u00a722, test=\u00a723), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy \u2248 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task\u2019s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016)."
  },
  {
    "id": 36,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/JiaweiSheng/FAAN",
    "section_title": "References",
    "add_info": null,
    "text": "Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a rela-tion given its few-shot reference entity pairs. Recent attempts solve this problem by learn-ing static representations of entities and refer-ences, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, en-tities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mecha-nism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acqui-sition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at  https://github.com/JiaweiSheng/FAAN."
  },
  {
    "id": 37,
    "name": "NELL and Wiki",
    "fullname": "N/A",
    "genericmention": [
      "both datasets",
      "both datasets"
    ],
    "description": [
      "re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks.",
      "There are 67 and 183 tasks in NELL and Wiki, re-spectively."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/xwhan/One-shot-Relational-Learning",
    "section_title": "5 Experiments 5.1 Datasets",
    "add_info": "1 https://github.com/xwhan/One-shot-Relational-Learning",
    "text": "We conduct experiments on two public benchmark datasets: NELL and Wiki [Cite_Footnote_1] . In both datasets, re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks. There are 67 and 183 tasks in NELL and Wiki, re-spectively. We use original 51/5/11 and 133/16/34 relations in NELL and Wiki, respectively, for train-ing/validation/testing as defined in Section 3. More-over, for each task relation, both datasets also pro-vide candidate entities, which are constructed based on the entity type constraint (Xiong et al., 2018). More details are shown in Table 1."
  },
  {
    "id": 38,
    "name": "OpenKE",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Han et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch",
    "section_title": "5 Experiments 5.3 Implementation Details",
    "add_info": "2 https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch",
    "text": "We perform 5-shot KG completion task for all the methods. Our implementation for KG embedding baselines is based on OpenKE [Cite_Footnote_2] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature. During training, all triples in back-ground KG G 0 and training set, as well as few-shot reference triples of validation and testing set are used to train models. For few-shot relational learn-ing baselines, we extend GMatching from original one-shot scenario to few-shot scenario by three set-tings: obtaining general reference representation by mean/max pooling (denoted as MeanP/MaxP) over references, or taking the reference that leads to the maximal similarity score to the query (denoted as Max). Because FSRL was reported in completely different experimental settings, we reimplement the model to make a fair comparison. We directly re-port the original results of MetaR with pre-trained embeddings to avoid re-implementation bias."
  },
  {
    "id": 39,
    "name": "Weibo",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Weibo [Cite_Footnote_2] , which contains massive multi-turn con-versation sessions and user identification informa-tion"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Produce",
    "url": "http://www.weibo.com/",
    "section_title": "4 Experiments 4.1 Dataset",
    "add_info": "2 http://www.weibo.com/",
    "text": "To evaluate the effectiveness of our proposed per-sonalized WAE model (PersonaWAE), we collect a dataset from an open online chatting forum, i.e., Weibo [Cite_Footnote_2] , which contains massive multi-turn con-versation sessions and user identification informa-tion. Overall, there are 31,128,520 utterances in the raw dataset with corresponded user identifica-tions. To construct the personalized conversation systems, we retrieve users with more than 14 utter-ances from the raw Weibo corpus. We also filtrate conversation sessions with less than 2 turns for training multi-turn conversation systems. We use a sliding window with a size of 3 to construct each dialogue session and there are 3 utterances in each dialogue session. By doing so, there are 336,342 conversation sessions in the cleaned corpus. We remove emojis in utterances and utilize NLTK for tokenization. Then, we randomly split the Weibo corpus into 335,342/5,000/5,000 sessions as train-ing/validation/testing sets. For each session, the last utterance is the target response for generation while other utterances are treated as context."
  },
  {
    "id": 40,
    "name": "word2vec vectors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/tmikolov/word2vec",
    "section_title": "4 Experiments 4.3 Settings",
    "add_info": "3 https://github.com/tmikolov/word2vec",
    "text": "The dimension of word embeddings is set to 200, which is initialized with pre-trained word2vec vectors [Cite_Footnote_3] . The vocabulary is comprised of the most frequent 31,000 words. The sentence encoder and the context encoder in our PersonaWAE model are two bi-directional RNN with the GRU cells, re-spectively. The decoder consists of a one-layer RNN with GRUs. The hidden state sizes of both GRU encoder and decoder are set to 256. Each user is allocated a user-level vector representation with dimension size 512. We set the mini-batch size to 100. The SGD optimizer is used to train the autoencoder module with the initial learning rate 1.0, and the learning rate decay strategy is employed. We use RMSprop optimizer (Hinton et al., 2012) to update the parameters of the gener-ator and the discriminator, where the initial learn-ing rates are set to 5e-5 and 1e-5, respectively. The gradient penalty is used for training discriminator (Gulrajani et al., 2017). The value of \u03c4 in Gumbel softmax is set to 0.1."
  },
  {
    "id": 41,
    "name": "smoothing 7",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "smoothing techniques"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.nltk.org/_modules/nltk/translate/bleu_score.html",
    "section_title": "4 Experiments 4.4 Evaluation Metrics",
    "add_info": "4 http://www.nltk.org/_modules/nltk/translate/bleu_score.html",
    "text": "Overlap-based Metric. We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated re-sponse. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [Cite_Footnote_4] . For each testing context, we sam-ple 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEU-Precision), n-gram recall (BLEU-Recall), and n-gram F1 (BLEU-F1)."
  },
  {
    "id": 42,
    "name": "Stan-ford Parser",
    "fullname": "N/A",
    "genericmention": [
      "the parser",
      "The parser"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/lex-parser.shtml",
    "section_title": "4 Experimentation 4.2 Experimental Settings",
    "add_info": "2 http://nlp.stanford.edu/software/lex-parser.shtml",
    "text": "In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [Cite_Footnote_2] . Specially, we train the parser on the GENIA Treebank 1.0 (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0."
  },
  {
    "id": 43,
    "name": "GENIA Treebank 1.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the GENIA Treebank 1.0 [Cite_Footnote_3] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus"
    ],
    "citationtag": [
      "Tateisi et al., 2005"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.geniaproject.org/genia-corpus/treebank",
    "section_title": "4 Experimentation 4.2 Experimental Settings",
    "add_info": "3 http://www.geniaproject.org/genia-corpus/treebank",
    "text": "In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser . Specially, we train the parser on the GENIA Treebank 1.0 [Cite_Footnote_3] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0."
  },
  {
    "id": 44,
    "name": "Word2Vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Mikolov et al., 2013"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "4 Experimentation 4.2 Experimental Settings",
    "add_info": "4 https://code.google.com/archive/p/word2vec/",
    "text": "For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, \u03bb=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [Cite_Footnote_4] (Mikolov et al., 2013)."
  },
  {
    "id": 45,
    "name": "CNN-based models",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu/",
    "section_title": "4 Experimentation 4.3 Experimental Results on Abstracts",
    "add_info": null,
    "text": "Table 2 illustrates that the performance of spec-ulation scope detection is higher than that of nega-tion (Best PCS: 85.75% vs 77.14%). It is mainly attributed to the shorter scopes of negation cues. Under the circumstances that the average length of negation sentences is almost as long as that of speculation ones (29.28 vs 29.77), shorter negation scopes mean that more tokens do not belong to the scopes, indicating more negative instances. The imbalance between positive and negative instances has negative effects on both the baseline and the 5  http://mallet.cs.umass.edu/ CNN-based models for negation scope detection."
  },
  {
    "id": 46,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code and trained models",
      "our model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/doug919/entity_based_narrative_graph",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/doug919/entity_based_narrative_graph",
    "text": "The evaluated downstream tasks include two challenging narrative analysis tasks, predicting characters\u2019 psychological states (Rashkin et al., 2018) and desire fulfilment (Rahimtoroghi et al., 2017). Results show that our model can outperform competitive transformer-based representations of the narrative text, suggesting that explicitly model-ing the relational structure of entities and events is beneficial. Our code and trained models are pub-licly available [Cite_Footnote_1] ."
  },
  {
    "id": 47,
    "name": "Twitter APIs",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://dev.twitter.com/",
    "section_title": "3 Experiments and Evaluations",
    "add_info": "3 https://dev.twitter.com/",
    "text": "The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [Cite_Footnote_3] . The statistics are shown in Table 1."
  },
  {
    "id": 48,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our source codes"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/luyaojie/text2event",
    "section_title": "1 Introduction",
    "add_info": "1 Our source codes are openly available at https://github.com/luyaojie/text2event",
    "text": "We conducted experiments [Cite_Footnote_1] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings. In summary, the contri-butions are as follows:"
  },
  {
    "id": 49,
    "name": "Wikipedia snapshot",
    "fullname": "N/A",
    "genericmention": [
      "The snapshot"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://archive.org/download/",
    "section_title": "3 Analysis Setup 3.1 Experiment Procedure",
    "add_info": "1 The snapshot is available at https://archive.org/download/enwiki-20181220. Wikipedia is licensed under CC BY-SA 3.0.",
    "text": "Our goal is to analyze the influence in downstream task performance brought by different masking policies g(.;\u03c6) during intermediate pre-training. Towards this goal, we ensure that the only vari-able is the masking policy, while all other aspects are controlled, so that the downstream performance reveal the influence we aim to study. We first initial-ize with a BART-base model (Lewis et al., 2020); then for each masking policy, we conduct experi-ments following a two-stage pipeline: Stage 1. Intermediate Pre-training. We per-form intermediate pre-training with a given mask-ing policy g(.; \u03c6). All intermediate pre-training is done with input sequence length of 128, batch size of 2048, learning rate of 0.0001, up to a total num-ber of 100, 000 updates, using Wikipedia snapshot from December 20, 2018 [Cite_Footnote_1] ."
  },
  {
    "id": 50,
    "name": "Cc-news",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sebastian Nagel. 2016",
      "Nagel, 2016"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://commoncrawl.org/2016/10/newsdatasetavailable",
    "section_title": "mixture of corpus 5 used to pre-train BART.",
    "add_info": "Sebastian Nagel. 2016. Cc-news. URL: http://web.archive. org/save/http://commoncrawl.org/2016/10/newsdatasetavailable.",
    "text": "5 Similar to RoBERTa, BART uses the combination of BookCorpus (Zhu et al., 2015), CC-News (Nagel, 2016)  , OpenWebText (Gokaslan and Cohen, 2019), and Stories (Trinh and Le, 2018) as pre-training corpus."
  },
  {
    "id": 51,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "closed-book QA datasets"
    ],
    "description": [
      "closed-book QA datasets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py",
    "section_title": "D Reproducibility D.1 Dataset Details",
    "add_info": null,
    "text": "We obtain closed-book QA datasets from  https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set."
  },
  {
    "id": 52,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "knowledge-intensive language tasks"
    ],
    "description": [
      "knowledge-intensive language tasks"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/facebookresearch/KILT/blob/master/scripts/donwload",
    "section_title": "D Reproducibility D.1 Dataset Details",
    "add_info": null,
    "text": "We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from  https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set."
  },
  {
    "id": 53,
    "name": "hug-gingface datasets",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://huggingface.co/datasets",
    "section_title": "D Reproducibility D.1 Dataset Details",
    "add_info": null,
    "text": "We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (  https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set."
  },
  {
    "id": 54,
    "name": "TextAttack",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a popular suite of NLP adversarial attacks"
    ],
    "citationtag": [
      "Morris et al., 2020"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py",
    "section_title": "3 Experiments",
    "add_info": "1 https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py",
    "text": "We present complete effectiveness graphs and details of human evaluation in Appendix B and C. BAE is implemented [Cite_Footnote_1] in TextAttack (Morris et al., 2020), a popular suite of NLP adversarial attacks."
  },
  {
    "id": 55,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "A dataset for classifying a sentence as objective or subjective"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences",
    "section_title": "A Experimental Reproducibility",
    "add_info": "2 https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences",
    "text": "\u2022 SUBJ: A dataset for classifying a sentence as objective or subjective. [Cite_Footnote_2]"
  },
  {
    "id": 56,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "A movie reviews dataset"
    ],
    "description": [
      "A movie reviews dataset based on sub-jective rating and sentiment polarity"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.cs.cornell.edu/people/pabo/movie-review-data/",
    "section_title": "A Experimental Reproducibility",
    "add_info": "3 https://www.cs.cornell.edu/people/pabo/movie-review-data/",
    "text": "\u2022 MR: A movie reviews dataset based on sub-jective rating and sentiment polarity [Cite_Footnote_3] ."
  },
  {
    "id": 57,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "An unbalanced dataset"
    ],
    "description": [
      "An unbalanced dataset for polarity detection of opinions"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mpqa.cs.pitt.edu/",
    "section_title": "A Experimental Reproducibility",
    "add_info": "4 http://mpqa.cs.pitt.edu/",
    "text": "\u2022 MPQA: An unbalanced dataset for polarity detection of opinions [Cite_Footnote_4] ."
  },
  {
    "id": 58,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "A dataset for classifying types of ques-tions with 6 classes"
    ],
    "description": [
      "A dataset for classifying types of ques-tions with 6 classes"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://cogcomp.org/Data/QA/QC/",
    "section_title": "A Experimental Reproducibility",
    "add_info": "5 http://cogcomp.org/Data/QA/QC/",
    "text": "\u2022 TREC: A dataset for classifying types of ques-tions with 6 classes [Cite_Footnote_5] ."
  },
  {
    "id": 59,
    "name": "SPRL",
    "fullname": "N/A",
    "genericmention": [
      "our model\u2019",
      "the proposed model",
      "Implementation"
    ],
    "description": [
      "an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/decomp-sem/neural-sprl",
    "section_title": "1 Introduction",
    "add_info": "2 Implementation available at https://github.com/decomp-sem/neural-sprl.",
    "text": "Figure 1 : BiLSTM sentence encoder with SPR de-coder. Semantic proto-role labeling is with respect to a specific predicate and argument within a sen-tence, so the decoder receives the two correspond-ing hidden states. achieves state-of-the-art performance for SPRL. [Cite_Footnote_2] As depicted in Figure 1, our model\u2019s architecture is an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training. We include a thorough quanti-tative analysis highlighting the contrasting errors between the proposed model and previous (non-neural) state-of-the-art."
  },
  {
    "id": 60,
    "name": "Fr-En corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Callison-Burch et al., 2009"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 \u201cNeural-Davidsonian\u201d Model",
    "add_info": "10 300-dimensional, uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/; 15,533 out-of-vocabulary words across all datasets were assigned a random embedding (uniformly from [\u2212 01 01]).. , . Embeddings remained fixed during training.",
    "text": "There are a few noteworthy differences between our neural model and the CRF of prior work. As an adapted BiLSTM, our model easily ex- 2017) trained on the [Cite_Footnote_10] Fr-En corpus (Callison-Burch et al., 2009) (Appendix A). ploits the benefits of large-scale pretraining, in the form of GloVe embeddings and MT pretrain-ing, both absent in the CRF. Ablation experiments (Appendix A) show the advantages conferred by these features. In contrast, the discrete-featured CRF model makes use of gold dependency labels, as well as joint modeling of SPR attribute pairs with explicit joint factors, both absent in our neu-ral model. Future SPRL work could explore the use of models like the LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016) to combine the advan-tages of both paradigms."
  },
  {
    "id": 61,
    "name": "SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/EducationalTestingService/skll",
    "section_title": "3 Models for Short Answer Scoring",
    "add_info": "2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.",
    "text": "Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Scho\u0308lkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [Cite_Footnote_2]"
  },
  {
    "id": 62,
    "name": "PropBank",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://verbs.colorado.edu/\u02dcmpalmer/projects/ace.html",
    "section_title": "3 Models for Short Answer Scoring 3.1 Response-based",
    "add_info": "3 http://verbs.colorado.edu/\u02dcmpalmer/projects/ace.html",
    "text": "\u2022 semantic roles in the form of PropBank [Cite_Footnote_3] style (e.g. say.01-A0-boy for \u201c(the) boy said\u201d)"
  },
  {
    "id": 63,
    "name": "ClearNLP parser",
    "fullname": "N/A",
    "genericmention": [
      "the parser"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.clearnlp.com,v2.0.2",
    "section_title": "3 Models for Short Answer Scoring 3.1 Response-based",
    "add_info": "4 http://www.clearnlp.com,v2.0.2",
    "text": "The syntactic and semantic features were extracted using the ClearNLP parser. [Cite_Footnote_4] We used the default models and options for the parser. We treat this model as a strong baseline to which we will add reference-based features."
  },
  {
    "id": 64,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset"
    ],
    "description": [
      "The dataset contains 2500 similarity judgements, provided by 25 participants"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt",
    "section_title": "5 Evaluation 5.1 Methodology",
    "add_info": "6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt",
    "text": "In order to evaluate the performance of our tensor-based factorization model of compositionality, we make use of the sentence similarity task for transi-tive sentences, defined in Grefenstette and Sadrzadeh (2011a). This is an extension of the similarity task for compositional models developed by Mitchell and Lapata (2008), and constructed according to the same guidelines. The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [Cite_Footnote_6]"
  },
  {
    "id": 65,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "official implementation [Cite_Footnote_3] of TDA"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/marziehf/DataAugmentationNMT",
    "section_title": "5 Experiments 5.5 Translation Result",
    "add_info": "3 https://github.com/marziehf/DataAugmentationNMT",
    "text": "We compare TCWR with six baselines, Word-Dropout, BPEDropout, SwitchOut, SCDA, TDA and DADA. For WordDropout and BPEDropout, we perform a range search on its dropout proba-bility from 0 to 1 and select the best one on de-velopment sets. Similarly, we choose the temper-ature with the highest score on development sets for SwitchOut. For SCDA, we search the replacing probability and set it to 0.15. We follow the official implementation [Cite_Footnote_3] of TDA. We reuse the hyperpa-rameters from Cheng et al. (2019) for DADA."
  },
  {
    "id": 66,
    "name": "VNTC",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/duyvuleo/VNTC",
    "section_title": "5 Experiments 5.6 Backtranslation Result",
    "add_info": "4 https://github.com/duyvuleo/VNTC",
    "text": "As backtranslation is a widely-used data augmenta-tion method by utilizing monolingual data to gener-ate new parallel pairs, we show how TCWR can be used with backtranslation. To perform backtransla-tion, we use the monolingual sequences from News Crawl 2017, News Crawl 2010 and VNTC [Cite_Footnote_4] for En-Tr, En-De and En-Vi, respectively. Then we per-form data augmentation on both training data and backtranslated data. As shown in Table 5, TCWR improves upon backtranslation, demonstrating that TCWR and backtranslation are not mutually exclu-sive, and TCWR can enhance the performance of backtranslation."
  },
  {
    "id": 67,
    "name": "Jieba",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "We use Jieba for segmentation",
      "a Chinese word segmentation system"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/fxsjy/jieba",
    "section_title": "3 Embeddings for Chinese Text",
    "add_info": "7 We use Jieba for segmentation: https://github.com/fxsjy/jieba",
    "text": "Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation system [Cite_Footnote_7] over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word. Since the NER system tags char-acters, we add the same word embedding features to each character in the word."
  },
  {
    "id": 68,
    "name": "CNN/Daily Mail dataset",
    "fullname": "N/A",
    "genericmention": [
      "the source documents and sum-mary sentences"
    ],
    "description": [
      "a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs"
    ],
    "citationtag": [
      "Her-mann et al., 2015"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/abisee/cnn-dailymail",
    "section_title": "4 Experiments 4.1 Dataset",
    "add_info": "1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail",
    "text": "We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [Cite_Footnote_1] and fairseq ."
  },
  {
    "id": 69,
    "name": "fairseq",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "section_title": "4 Experiments 4.1 Dataset",
    "add_info": "2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "text": "We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [Cite_Footnote_2] ."
  },
  {
    "id": 70,
    "name": "files2rouge",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/pltrdy/files2rouge",
    "section_title": "4 Experiments [Gold Summary]",
    "add_info": "3 files2rouge usage : https://github.com/pltrdy/files2rouge",
    "text": "About a dozen Native American actors walk off set of Adam Sandler comedy, says report . Actors say satirical Western\u2019s script is insulting to Native Americans and women . used files2rouge [Cite_Footnote_3] . Hie-BART was compared with LEAD-3 (Nallapati et al., 2017), PTGEN, PT-GEN+COV (See et al., 2017), B ERT S UM E XT A BS (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART with our environment, and BART with Lewis et al. (2020). The LEAD-3 method uses the first three sentences of the source document as a summary. PTGEN is a sequence-to-sequence model that incorporates a pointer generator net-work. PTGEN+COV introduces the coverage mechanism into PTGEN. B ERT S UM E XT A BS is a pre-training model that adapts BERT for sum-marization tasks. T5 is a generalized pre-training model for sequence-to-sequence tasks based on the Transformer model. The statistical signifi-cance test was performed by the Wilcoxon-Mann-Whitney test. In Table 1, * and ** indicate that the comparisons with BART (ours) are statistically significant at 5% significance level and 10% sig-nificance level, respectively."
  },
  {
    "id": 71,
    "name": "IMN",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "state-of-the-art baselines for extracting targets and expressions and predicting the polarity"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/ruidan/IMN-E2E-ABSA",
    "section_title": "4 Modeling 4.3 Baselines",
    "add_info": "6 IMN code available at https://github.com/ruidan/IMN-E2E-ABSA.",
    "text": "We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN [Cite_Footnote_6] , RACL , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models. Note, however, that our model does not update the mBERT representa-tions, putting it at a disadvantage to RACL-BERT. We also compare with previously reported extrac-tion results from Barnes et al. (2018) and \u00d8vrelid et al. (2020)."
  },
  {
    "id": 72,
    "name": "RACL",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "state-of-the-art baselines for extracting targets and expressions and predicting the polarity"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/NLPWM-WHU/RACL",
    "section_title": "4 Modeling 4.3 Baselines",
    "add_info": "7 https://github.com/NLPWM-WHU/RACL.",
    "text": "We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN , RACL [Cite_Footnote_7] , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models. Note, however, that our model does not update the mBERT representa-tions, putting it at a disadvantage to RACL-BERT. We also compare with previously reported extrac-tion results from Barnes et al. (2018) and \u00d8vrelid et al. (2020)."
  },
  {
    "id": 73,
    "name": "word2vec skip-gram embeddings",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "skip-gram embeddings",
      "300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58)."
    ],
    "citationtag": [
      "Fares et al., 2017"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://vectors.nlpl.eu/repository/",
    "section_title": "6 Experiments",
    "add_info": "8 Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58).",
    "text": "All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [Cite_Footnote_8] (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set (Targeted F 1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Ap-pendix) and run all of our models five times with different random seeds and report the mean (stan-dard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference be-tween the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3 shows the results for all datasets."
  },
  {
    "id": 74,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Model implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/jerbarnes/sentiment_graphs/src",
    "section_title": "8 Conclusion",
    "add_info": null,
    "text": "The computations were performed on resources scheme (y-axis) on the evaluation metrics (x-axis) for MultiB EU . percentage points for MultiB CA . GPU Infrastructure NVIDIA P100, 16 GiB RAM CPU Infrastructure Intel Xeon-Gold 6138 2.0 GHz Training duration 00:31:43 (MultiB EU ) \u2013 07:40:54 (NoReC Fine ) Model implementation  https://github.com/jerbarnes/sentiment_graphs/src Hyperparameter Best assignment embedding Word2Vec SkipGram 100D contexualized embedding mBERT embeddings trainable False number of epochs 100 batch size 50 beta1 0 beta2 0.95 l2 3e-09 hidden lstm 200 hidden char lstm 100 layers lstm 3 dim mlp 200 dim embedding 100 dim char embedding 80 early stopping 0 pos style xpos attention bilinear model interpolation 0.5 loss interpolation 0.025 lstm implementation drop connect char implementation convolved emb dropout type replace bridge dpa+ dropout embedding 0.2 dropout edge 0.2 dropout label 0.3 dropout main recurrent 0.2 dropout recurrent char 0.3 dropout main ff 0.4 dropout char ff 0.3 dropout char linear 0.3"
  },
  {
    "id": 75,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "rele-vance scale",
      "all relevance grades"
    ],
    "description": [
      "a five-level graded rele-vance scale (perfect, excellent, good, fair, bad)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/nickvosk/acl2015-",
    "section_title": "5 Experimental setup 5.1 Dataset",
    "add_info": "5 https://github.com/nickvosk/acl2015- dataset-learning-to-explain-entity- relationships",
    "text": "Five human annotators provided relevance judg-ments, manually judging sentences based on how well they describe the relationship for an entity pair, for which we use a five-level graded rele-vance scale (perfect, excellent, good, fair, bad). [Cite_Footnote_5] Of all relevance grades 8.1% is perfect, 15.69% excellent, 19.98% good, 8.05% fair, and 48.15% bad. Out of 1 476 entity pairs, 1 093 have at least one sentence annotated as fair. As is common in information retrieval evaluation, we discard entity pairs that have only \u201cbad\u201d sentences. We examine the difficulty of the task for human annotators by measuring inter-annotator agreement on a subset of 105 sentences that are judged by 3 annotators. Fleiss\u2019 kappa is k = 0.449, which is considered to be moderate agreement."
  },
  {
    "id": 76,
    "name": "Dynet code",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "The Dynet code for differentiable dynamic programming"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/FilippoC/diffdp",
    "section_title": "1 Introduction",
    "add_info": "1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.",
    "text": "We study properties of our approach on a syn-thetic structure induction task and experiment on sentiment classification (Socher et al., 2013) and natural language inference (Bowman et al., 2015). Our experiments confirm that the structural bias encoded in our approach is beneficial. For ex-ample, our approach achieves a 4.9% improve-ment on multi-genre natural language inference (MultiNLI) over a structure-agnostic baseline. We show that stochastisticity and higher-order statis-tics given by the global inference are both impor-tant. In ablation experiments, we also observe that forcing the structures to be projective dependency trees rather than permitting any general graphs yields substantial improvements without sacrific-ing execution time. This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [Cite_Footnote_1] Our main contributions can be summarized as follows:"
  },
  {
    "id": 77,
    "name": "cQA pipeline",
    "fullname": "N/A",
    "genericmention": [
      "The pipeline"
    ],
    "description": [
      "The pipeline is able to process natural language texts and metadata information associated with them and offers three main func-tionalities"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/QAML/S3QACoreFramework",
    "section_title": "5 Software Package",
    "add_info": "1 https://github.com/QAML/S3QACoreFramework",
    "text": "Our cQA pipeline is available for download [Cite_Footnote_1] and is distributed under the terms of the Apache 2.0 Li-cense. By taking advantage of the Apache Maven project management tool, most dependencies are automatically handled. The only exception is the UIMA framework toolkit. Still, its installation is straightforward. The pipeline is able to process natural language texts and metadata information associated with them and offers three main func-tionalities:"
  },
  {
    "id": 78,
    "name": "KeLP",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "learning algorithms on vectorial or structured data",
      "KeLP allows to apply a growing number of kernel-based algo-rithms and kernel functions to perform unsuper-vised, online and batch supervised kernel meth-ods."
    ],
    "citationtag": [
      "Filice et al., 2018"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.kelp-ml.org",
    "section_title": "5 Software Package",
    "add_info": "2 http://www.kelp-ml.org",
    "text": "Learning and classification allow to apply a variety of learning algorithms on vectorial or structured data. Currently KeLP (Filice et al., 2018) [Cite_Footnote_2] is integrated in the pipeline. KeLP allows to apply a growing number of kernel-based algo-rithms and kernel functions to perform unsuper-vised, online and batch supervised kernel meth-ods. We opt for integrating KeLP because the kernel-based cQA systems relying on it perform at state-of-the-art level (see Section 2). Our pipeline is able to reproduce the state-of-the-art models for SemEval cQA tasks 3-A and 3-B."
  },
  {
    "id": 79,
    "name": "MDN-VQG model",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://badripatro.github.io/MDN-VQG/",
    "section_title": "4 Method 4.3 Cost function",
    "add_info": "1 The project page for MDN-VQG Model is https://badripatro.github.io/MDN-VQG/",
    "text": "Our objective is to minimize the total loss, that is the sum of cross entropy loss and triplet loss over all training examples. The total loss is: where M is the total number of samples,\u03b3 is a con-stant, which controls both the loss. L triplet is the triplet loss function 5. L cross is the cross entropy loss between the predicted and ground truth ques-tions and is given by: where, N is the total number of question tokens, y t is the ground truth label. The code for MDN-VQG model is provided [Cite_Footnote_1] ."
  },
  {
    "id": 80,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a regularly trained Chinese parser",
      "the English and Chinese parsers"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Petrov and Klein (2007)"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://nlp.cs.berkeley.edu",
    "section_title": "6 Statistical Parsing Experiments",
    "add_info": "5 Available at http://nlp.cs.berkeley.edu.",
    "text": "We used the English and Chinese parsers in Petrov and Klein (2007) [Cite_Footnote_5] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270). This modified grammar was used to generate the k-best lists that we trained our model on. However, as we tested on the same set of articles used for monolingual Chi-nese parser evaluation, there was no need to use a modified grammar to generate k-best lists at test time, and so we used a regularly trained Chinese parser for this purpose."
  },
  {
    "id": 81,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the word aligner"
    ],
    "description": [
      "the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [Cite_Footnote_6] , trained on approxi-mately 1.7 million sentence pairs"
    ],
    "citationtag": [
      "Liang et al. (2006) and DeNero and Klein (2007)"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://nlp.cs.berkeley.edu",
    "section_title": "6 Statistical Parsing Experiments",
    "add_info": "6 Available at http://nlp.cs.berkeley.edu.",
    "text": "Posterior word alignment probabilities were ob-tained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [Cite_Footnote_6] , trained on approxi-mately 1.7 million sentence pairs. For our alignment model we used an HMM in each direction, trained to agree (Liang et al., 2006), and we combined the pos-teriors using DeNero and Klein\u2019s (2007) soft union method."
  },
  {
    "id": 82,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the [Cite_Footnote_2] million pre-processed translation pairs"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.phontron.com/kytea/",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "2 http://www.phontron.com/kytea/",
    "text": "Preprocessings For Japanese sentences, we per-formed tokenization using KyTea 0.4.7 2 (Neu-big et al., 2011). Then we performed bunsetsu-chunking with J.DepP 2015.10.05 3 (Yoshinaga and Kitsuregawa, 2009, 2010, 2014). Special end-of-chunk tokens were inserted at the end of the chunks. Our word-level decoders described in \u00a7 will stop generating words after each end-of-chunk token. For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the [Cite_Footnote_2] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data."
  },
  {
    "id": 83,
    "name": "MGIZA++ 0.7.0 [Cite_Footnote_5] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "word alignment tool"
    ],
    "citationtag": [
      "Och and Ney, 2003; Gao and Vogel, 2008"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mgiza",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "5 https://github.com/moses-smt/mgiza",
    "text": "Postprocessing To perform unknown word re-placement (Luong et al., 2015a), we built a bilin-gual English-Japanese dictionary from all of the three million translation pairs. The dictionary was extracted with the MGIZA++ 0.7.0 [Cite_Footnote_5] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words."
  },
  {
    "id": 84,
    "name": "Moses 2.1.1",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Koehn et al., 2007"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.statmt.org/moses/",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "6 http://www.statmt.org/moses/",
    "text": "Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 [Cite_Footnote_6] (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 (Isozaki et al., 2010). Follow-ing Cho et al. (2014a), we performed beam search with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set dur-ing training and used them to evaluate the systems with the test set."
  },
  {
    "id": 85,
    "name": "RIBES.py 1.03.1",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Isozaki et al., 2010"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "7 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html",
    "text": "Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [Cite_Footnote_7] (Isozaki et al., 2010). Follow-ing Cho et al. (2014a), we performed beam search with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set dur-ing training and used them to evaluate the systems with the test set."
  },
  {
    "id": 86,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the exam-ple configuration for a transformer model from Marian",
      "This configuration"
    ],
    "description": [
      "This configuration utilises a six-layer deep encoder and decoder, learning rate warm-up and tied em-beddings for source, target and output layer."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/marian-nmt/marian-examples/blob/master/transformer",
    "section_title": "4 Experiment setup 4.3 Training setup",
    "add_info": "2 https://github.com/marian-nmt/marian-examples/blob/master/transformer",
    "text": "Transformer We used the same training, dev and test sets as in the big RNN model, and the exam-ple configuration for a transformer model from Marian [Cite_Footnote_2] adapted to our needs as shown in Table 1. This configuration utilises a six-layer deep encoder and decoder, learning rate warm-up and tied em-beddings for source, target and output layer. As suggested by Karita et al. (2019), we increased the minibatch size for the transformer model from 5,000 to 10,000."
  },
  {
    "id": 87,
    "name": "MeCab",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "We used MeCab [Cite_Footnote_10] to tokenize the feedback comments."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://taku910.github.io/mecab/",
    "section_title": "6 Evaluation 6.1 Conditions and Procedures",
    "add_info": "10 http://taku910.github.io/mecab/",
    "text": "We implemented and trained the baseline meth-ods with the created dataset. We first obtain word embeddings for learner sentences from the cor-pora as shown in Appendix A. We also used the word embeddings for English words in the LSTMLMs to encode feedback comments. For the rest (i.e., Japanese words), we initialized them using random-valued vectors. We used MeCab [Cite_Footnote_10] to tokenize the feedback comments. With these word embeddings, we trained the networks on the training set of the respective subsets (PART-TIME JOB and SMOKING). We implemented the case frame-based method with the following cor-pora: British National Corpus (BNC) (Burnard, 1995), the EDR corpus (Japan electronic dictio-nary research institute Ltd, 1993) as a native cor-pus and the training and development set of the corresponding dataset as a learner corpus. As a result, we obtained two versions of each method. We determined the hyperparameters by using the corresponding development set 12 . We tested the resulting models on the corresponding test set."
  },
  {
    "id": 88,
    "name": "CEEEJUS",
    "fullname": "Corpus of English Essays Written by Japanese University Students",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://language.sakura.ne.jp/s/doc/projects/CEEAUS.pdf",
    "section_title": "References",
    "add_info": "15 http://language.sakura.ne.jp/s/doc/projects/CEEAUS.pdf",
    "text": "Learner corpora: Corpus of English Essays Written by Japanese University Students (CEEEJUS) [Cite_Footnote_15] , ETS Corpus of non-native written English (Daniel Blanchard et al., 2014), The International Corpus of Learner English (ICLE) (Granger, 1993), Cambridge Learner Corpus (CLC) First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and Nagoya Interlanguage Corpus of English (NICE) (Sug-iura et al., 2007). Native corpus: English Web Treebank (EWT) (Bies, Ann, et al., 2012)."
  },
  {
    "id": 89,
    "name": "Project Gutenberg (English)",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.gutenberg.org",
    "section_title": "2 A Parallel Corpus of Literary Texts 2.1 Data Selection",
    "add_info": "1 http://www.gutenberg.org and http://gutenberg.spiegel.de/",
    "text": "We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1] Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align."
  },
  {
    "id": 90,
    "name": "Project Gutenberg-DE (German)",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://gutenberg.spiegel.de/",
    "section_title": "2 A Parallel Corpus of Literary Texts 2.1 Data Selection",
    "add_info": "1 http://www.gutenberg.org and http://gutenberg.spiegel.de/",
    "text": "We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1] Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align."
  },
  {
    "id": 91,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The corpus"
    ],
    "description": [
      "Choice of English units to label.",
      "On the German side, we assign the T/V labels to pronouns, and the most straightforward way of setting up annotation projection would be to label their word-aligned En-glish pronouns as T/V.",
      "we decided to treat complete sentences as either T or V",
      "English sentences can receive conflicting labels, if a German sentence con-tains both a T and a V label",
      "of the 76K German sentences with T or V pronouns, only 515, or less than 1%, contain both",
      "A Parallel Corpus of Literary Texts",
      "Our projection on the English side results in 53K V and 35K T sentences"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.nlpado.de/~sebastian/data.shtml",
    "section_title": "2 A Parallel Corpus of Literary Texts 2.3 T/V Gold Labels for English Utterances",
    "add_info": "4 The corpus can be downloaded for research purposes from http://www.nlpado.de/~sebastian/data.shtml.",
    "text": "Choice of English units to label. On the German side, we assign the T/V labels to pronouns, and the most straightforward way of setting up annotation projection would be to label their word-aligned En-glish pronouns as T/V. However, pronouns are not necessarily translated into pronouns; additionally, we found word alignment accuracy for pronouns, as a function of word class, to be far from perfect. For these reasons, we decided to treat complete sentences as either T or V. This means that sentence alignment is sufficient for projection, but English sentences can receive conflicting labels, if a German sentence con-tains both a T and a V label. However, this occurs very rarely: of the 76K German sentences with T or V pronouns, only 515, or less than 1%, contain both. Our projection on the English side results in 53K V and 35K T sentences, of which 731 are labeled as both T and V. Finally, from the English labeled sentences we ex-tracted a training set with 72 novels (63K sentences) and a test set with 21 novels (15K sentences). [Cite_Footnote_4]"
  },
  {
    "id": 92,
    "name": "C2AE",
    "fullname": "N/A",
    "genericmention": [
      "its",
      "it",
      "its"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/dhruvramani/C2AE-Multilabel-Classification",
    "section_title": "2 The Proposed Method (Rank-AE) 2.3 L h and L ae Loss Functions",
    "add_info": "1 https://github.com/dhruvramani/C2AE-Multilabel-Classification",
    "text": "Reconstructing Output (L ae ). Unlike L h with small space, L ae loss usually involves a large num-ber of labels. Moreover, L ae also directly affects the classification performance significantly since different loss functions lead to their own proper-ties (Hajiabadi et al., 2017). Accordingly, solving such problems with large scale and desirable prop-erties presents open challenges in three aspects: 1) how to improve time efficiency, 2) how to produce comparable labels scores and 3) how to deal with noise labels. Unfortunately, most of the related deep learning methods only target one or two as-pects. C2AE attempts to minimize the number of misclassified pairs between relevant and irrelevant labels, as a result its computational complexity is quadratic with number of labels in the worst case; also it fails to scale well on large number of in-put features or labels due to its inefficient imple-mentation [Cite_Footnote_1] . XML-CNN (Liu et al., 2017) achieves computational efficiency by training a deep neural network with hidden layers much smaller than the output layer with binary cross-entropy loss (BCE), which has linear complexity in number of labels. Despite this, BCE loss could neither capture la-bel dependencies nor produce directly compara-ble label scores, since each label is treated inde-pendently. Moreover, BCE loss tends to be sensi-tive to label noise, which is frequently observed in XML data (Reed et al., 2014; Ghosh et al., 2017)."
  },
  {
    "id": 93,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "online movie database"
    ],
    "description": [
      "online movie database"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.imdb.com/",
    "section_title": "3 Experiments & Analysis 3.1 Dataset & Experiment Setup",
    "add_info": "2 https://www.imdb.com/",
    "text": "Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database [Cite_Footnote_2] and the rest five datasets are downloaded from the extreme classification repository . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set. For all datasets, we reserve another 20% of train-ing data as validation for tuning hyper-parameters. After tuning, all models are trained on the entire training set."
  },
  {
    "id": 94,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the extreme classification repository",
      "the repository"
    ],
    "description": [
      "the extreme classification repository"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://manikvarma.org/downloads/XC/XMLRepository.html",
    "section_title": "3 Experiments & Analysis 3.1 Dataset & Experiment Setup",
    "add_info": "3 http://manikvarma.org/downloads/XC/XMLRepository.html",
    "text": "Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [Cite_Footnote_3] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set. For all datasets, we reserve another 20% of train-ing data as validation for tuning hyper-parameters. After tuning, all models are trained on the entire training set."
  },
  {
    "id": 95,
    "name": "Glove",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a pre-trained word embed-dings of 100 dimensions"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Experiments & Analysis 3.1 Dataset & Experiment Setup",
    "add_info": "4 https://nlp.stanford.edu/projects/glove/",
    "text": "Hyper-parameters. In Rank-AE, we use the fixed neural network architecture, with two fully con-nected layers in both Encoder and Decoder, and one fully connected layer following Embedding & Atten network in Feature Embedding. We also fix most of the hyper-parameters, including hidden dimension h (100 for small number of la-bels data and 200 for large ones), word embed-ding size C = 100, and reduction ratio r = 4. The remaining hyper-parameters, such as balance \u03bb between L h and L ae , margin m in L ae , and oth-ers (decay, learning rate) in the optimization algo-rithms, are tuned on validation set. In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [Cite_Footnote_4] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed."
  },
  {
    "id": 96,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The visualization tool",
      "the visualization tool",
      "the visualiza-tion tool"
    ],
    "description": [
      "to highlight important words based on the attention output"
    ],
    "citationtag": [
      "Lin et al., 2017"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/kaushalshetty/Structured-Self-Attention",
    "section_title": "3 Experiments & Analysis 3.4 More Analysis in Rank-AE",
    "add_info": "5 The visualization tool is provided by https://github.com/kaushalshetty/Structured-Self-Attention",
    "text": ". employ the visualization tool (Lin et al., 2017) to highlight important words based on the attention output. Specifically, we run our method on IMDb dataset, wherein each instance is a movie story as-sociated with relevant genres as labels. Instead of extracting V 0 matrix using the proposed spatial-wise attention, we obtain a fixed size embeddings from a bidirectional LSTM on variable length of sentence, fed to our channel-attention network. Through the channel-attention network, we can observe the attention matrix A for each input doc-ument. By summing up the attention weights of each word embedding vector, we can visualize the overall attention for that word with the visualiza-tion tool [Cite_Footnote_5] . We randomly select three movies from IMDb testing set (See Figure 5). By looking at the highlighted regions, we can see that the pro-posed channel-attention is able to focus more on the words that are highly related to the topics."
  },
  {
    "id": 97,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Source code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/facebookresearch/Zero-Shot-DST",
    "section_title": "3 Methodology 3.1 T5DST",
    "add_info": "1 Source code is available in https://github.com/facebookresearch/Zero-Shot-DST",
    "text": "The design of our model follows the basis of gen-erative question answering models. As illustrated in Figure 1, given a dialogue history which con-sists of an alternating set of utterances from two speakers, denoted as C t = {U 1 , R 1 , . . . , R t\u22121 , U t }, we add the \"user:\" and \"system:\" prefixes to the user and system utterance respectively. Then all the utterances and slot names s i are concatenated into a single sequence, i.e., user:U [Cite_Footnote_1] . . .system:R t\u22121 user:U t [sep] s i . The sequence is used as the in-put to the encoder, and the decoder generates the corresponding slot value v i :"
  },
  {
    "id": 98,
    "name": "Mallet Toolkits",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu/",
    "section_title": "3 The Proposed Approach 3.1 Text Representation",
    "add_info": "3 http://mallet.cs.umass.edu/",
    "text": "Classification algorithm: The maximum en-tropy (ME) classifier is implemented with the public tool, Mallet Toolkits [Cite_Footnote_3] ."
  },
  {
    "id": 99,
    "name": "Common Crawl Web scrape",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://webdatacommons.org/webtables",
    "section_title": "A Pretraining Details A.1 Training Data",
    "add_info": "10 http://webdatacommons.org/webtables",
    "text": "WDC WebTable Corpus (Lehmberg et al., 2016) is a large collection of Web tables extracted from the Common Crawl Web scrape [Cite_Footnote_10] . We use its 2015 English-language relational subset, which consists of 50.8 million relational tables and their surrounding NL contexts."
  },
  {
    "id": 100,
    "name": "MSN News",
    "fullname": "N/A",
    "genericmention": [
      "The logs in the last week",
      "the rest"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.msn.com/en-us/news",
    "section_title": "3 Experiments 3.1 Datasets and Experimental Settings",
    "add_info": "3 https://www.msn.com/en-us/news",
    "text": "We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_3] logs in one month (Dec. 13, 2018 to Jan. 12, 2019). The detailed statistics are shown in Ta-ble 1. The logs in the last week were used for test, and the rest were used for training. We randomly sampled 10% of training data for validation."
  },
  {
    "id": 101,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Makwen1995/LDGNMLTC",
    "section_title": "3 Experiment 3.1 Experimental Setup",
    "add_info": "1 https://github.com/Makwen1995/LDGNMLTC",
    "text": "The word embeddings in the proposed network are initialized with the 300-dimensional word vec-tors, which are trained on the datasets by Skip-gram (Mikolov et al., 2013) algorithm. The hid-den sizes of Bi-LSTM and GCNs are set to 300 and 512, respectively. We use the Adam optimiza-tion method (Kingma and Ba, 2014) to minimize the cross-entropy loss, the learning rate is initial-ized to 1e-3 and gradually decreased during the process of training. We select the best parameter configuration based on performance on the valida-tion set and evaluate the configuration on the test set. Our code is available on GitHub [Cite_Footnote_1] ."
  },
  {
    "id": 102,
    "name": "CamRest dataset",
    "fullname": "N/A",
    "genericmention": [
      "The dataset",
      "the datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wen et al., 2017b"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/yizhen20133868/Retriever-Dialogue",
    "section_title": "4 Training the KB-Retriever 4.3 Experimental Settings",
    "add_info": "3 The dataset can be available at: https://github.com/yizhen20133868/Retriever-Dialogue",
    "text": "We choose the InCar Assistant dataset (Eric et al., 2017) including three distinct domains: naviga-tion, weather and calendar domain. For weather domain, we follow Wen et al. (2018) to separate the highest temperature, lowest temperature and weather attribute into three different columns. For calendar domain, there are some dialogues with-out a KB or incomplete KB. In this case, we padding a special token \u201c-\u201d in these incomplete KBs. Our framework is trained separately in these three domains, using the same train/validation/test split sets as Eric et al. (2017). To justify the gen-eralization of the proposed model, we also use an-other public CamRest dataset (Wen et al., 2017b) and partition the datasets into training, validation and testing set in the ratio 3:1:1. [Cite_Footnote_3] Especially, we hired some human experts to format the CamRest dataset by equipping the corresponding KB to ev-ery dialogues."
  },
  {
    "id": 103,
    "name": "Apple Dis-cussion forums",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://discussions.apple.com",
    "section_title": "5 Experimental Evaluation",
    "add_info": "10 http://discussions.apple.com",
    "text": "We use a crawl of 140k threads from Apple Dis-cussion forums [Cite_Footnote_10] . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement of 0.71. On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure 12 for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set \u03bb = 0.5 and \u03c4 = 0.4 unless other-wise mentioned. Since we have only 300 labeled threads, accuracy measures are reported on those (like in (Catherine et al., 2013)). We pre-process the post data by stemming words (Porter, 1980)."
  },
  {
    "id": 104,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the F-measure",
      "an F-measure of 53%"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://en.wikipedia.org/wiki/F1score",
    "section_title": "5 Experimental Evaluation",
    "add_info": "12 http://en.wikipedia.org/wiki/F1score",
    "text": "We use a crawl of 140k threads from Apple Dis-cussion forums 10 . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement 11 of 0.71. On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure [Cite_Footnote_12] for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set \u03bb = 0.5 and \u03c4 = 0.4 unless other-wise mentioned. Since we have only 300 labeled threads, accuracy measures are reported on those (like in (Catherine et al., 2013)). We pre-process the post data by stemming words (Porter, 1980)."
  },
  {
    "id": 105,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code of this paper"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/mrlyk423/relation_extraction",
    "section_title": "References",
    "add_info": null,
    "text": "Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns be-tween entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learn-ing, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allo-cation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as com-pared with baselines, our model achieves significant and consistent improvements on knowledge base completion and re-lation extraction from text. The source code of this paper can be obtained from  https://github.com/mrlyk423/relation_extraction."
  },
  {
    "id": 106,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an interface provided by a third party machine translation service"
    ],
    "description": [
      "to train a cus-tom MT engine for English to French translations"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://hub.microsofttranslator.com",
    "section_title": "1 Introduction",
    "add_info": "2 http://hub.microsofttranslator.com",
    "text": "To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [Cite_Footnote_2] to train a cus-tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida-tion process, first using crowd-sourced evaluations with Amazon\u2019s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior."
  },
  {
    "id": 107,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ElliottYan/Multi",
    "section_title": "References",
    "add_info": "1 Code is available at https://github.com/ElliottYan/Multi Unit Transformer",
    "text": "Transformer models (Vaswani et al., 2017) achieve remarkable success in Neural Ma-chine Translation. Many efforts have been de-voted to deepening the Transformer by stack-ing several units (i.e., a combination of Multi-head Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we pro-pose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use sev-eral parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit set-ting, we design biased module and sequen-tial dependency that guide and encourage com-plementariness among different units. Exper-imental results on three machine translation tasks, the NIST Chinese-to-English, WMT\u201914 English-to-German and WMT\u201918 Chinese-to- English, show that the MUTE models signif-icantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its ef-ficiency in both the inference process and pa-rameter usage. [Cite_Footnote_1]"
  },
  {
    "id": 108,
    "name": "multi-bleu.perl",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl",
    "section_title": "4 Experimental Settings",
    "add_info": "4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl",
    "text": "Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [Cite_Footnote_4] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention."
  },
  {
    "id": 109,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the statistical significance test"
    ],
    "description": [
      "the statistical significance test"
    ],
    "citationtag": [
      "Koehn, 2004"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/boots",
    "section_title": "4 Experimental Settings",
    "add_info": "5 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance. pl",
    "text": "Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [Cite_Footnote_5] for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention."
  },
  {
    "id": 110,
    "name": "Moses mteval-v13a.pl script",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl",
    "section_title": "4 Experimental Settings",
    "add_info": "6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl",
    "text": "Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [Cite_Footnote_6] . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention."
  },
  {
    "id": 111,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://spacy.io",
    "section_title": "A Appendices",
    "add_info": "9 Using https://spacy.io",
    "text": "\u2022 The number of occurrences of part-of-speech (PoS) tags from Penn-treebank [Cite_Footnote_9] ."
  },
  {
    "id": 112,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an 11.5 million words of user contributed comments",
      "This corpus"
    ],
    "description": [
      "an 11.5 million words of user contributed comments."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.cs.jhu.edu/~ozaidan/AOC",
    "section_title": "3.2. Experiments and Results",
    "add_info": "1 Available from http://www.cs.jhu.edu/~ozaidan/AOC",
    "text": "We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. [Cite_Footnote_1] This corpus is highly dialectal"
  },
  {
    "id": 113,
    "name": "ELMo",
    "fullname": "N/A",
    "genericmention": [
      "the released model"
    ],
    "description": [
      "effec-tive deep contextualized word representations"
    ],
    "citationtag": [
      "Peters et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md",
    "section_title": "4 Experiment 4.1 Settings",
    "add_info": "3 We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md",
    "text": "For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [Cite_Footnote_3] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations ."
  },
  {
    "id": 114,
    "name": "BERT",
    "fullname": "N/A",
    "genericmention": [
      "the released model",
      "The model"
    ],
    "description": [
      "effec-tive deep contextualized word representations",
      "The model uses character-based tokenization for Chinese"
    ],
    "citationtag": [
      "Devlin et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "4 Experiment 4.1 Settings",
    "add_info": "4 We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the first word piece as the whole word representation.",
    "text": "For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [Cite_Footnote_4] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations ."
  },
  {
    "id": 115,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/chenyangh/Seq2Emo",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/chenyangh/Seq2Emo",
    "text": "Multi-label emotion classification is an impor-tant task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval\u201918 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) ap-proaches in a fair setting. [Cite_Footnote_1]"
  },
  {
    "id": 116,
    "name": "SGM",
    "fullname": "N/A",
    "genericmention": [
      "its"
    ],
    "description": [
      "SGM (Yang et al., 2018), however, is a CC-based model for multi-label classification."
    ],
    "citationtag": [
      "Yang et al., 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/lancopku/SGM",
    "section_title": "4 Experimental Setup",
    "add_info": "2 https://github.com/lancopku/SGM",
    "text": "Baselines. On SemEval\u201918, we compare our system with the top submissions from the SemEval-2018 competition and recent development. NTUA-SLP (Baziotis et al., 2018) uses large amount of external emotion-related data to pretrain an LSTM-based model. TCS Research\u2019s system (Meish-eri and Dey, 2018) uses the support vector ma-chine with mannually engineered features: out-put from LSTM models, emotion lexicons (Mo-hammad and Kiritchenko, 2015), and SentiNeural (Radford et al., 2017). PlusEmo2Vec (Park et al., 2018) combines neural network models, which are pretrained by using emojis as labels (Felbo et al., 2017). Apart from the competition, Yu et al. (2018) propose DATN, which introduces sentiment information through dual-attention. These afore-mentioned systems are based on the BR approach. SGM (Yang et al., 2018), however, is a CC-based model for multi-label classification. We include it as a baseline by using its publicly released code. [Cite_Footnote_2]"
  },
  {
    "id": 117,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Software for R TE R and M T +R TE R"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/mteval.shtml",
    "section_title": "4 Experimental Evaluation 4.4 Combination Metrics",
    "add_info": "3 Software for R TE R and M T +R TE R is available from http://nlp.stanford.edu/software/mteval.shtml.",
    "text": "M T +R TE R uses all M T R and R TE R features, combining matching and entailment evidence. [Cite_Footnote_3]"
  },
  {
    "id": 118,
    "name": "NIST OpenMT 2008 corpus",
    "fullname": "N/A",
    "genericmention": [
      "The corpus"
    ],
    "description": [
      "The corpus contains trans-lations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur)).",
      "Each language consists of 1500\u20132800 sen-tence pairs produced by 7\u201315 MT systems."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.nist.gov",
    "section_title": "5 Expt. 1: Predicting Absolute Scores",
    "add_info": "4 Available from http://www.nist.gov.",
    "text": "Data. Our first experiment evaluates the models we have proposed on a corpus with traditional an-notation on a seven-point scale, namely the NIST OpenMT 2008 corpus. [Cite_Footnote_4] The corpus contains trans-lations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur)). Each language consists of 1500\u20132800 sen-tence pairs produced by 7\u201315 MT systems."
  },
  {
    "id": 119,
    "name": "WMT 2006 and 2007",
    "fullname": "2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT)",
    "genericmention": [
      "It"
    ],
    "description": [
      "It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian)."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.statmt.org/",
    "section_title": "6 Expt. 2: Predicting Pairwise Preferences",
    "add_info": "7 Available from http://www.statmt.org/.",
    "text": "In this experiment, we predict human pairwise pref-erence judgments (cf. Section 4). We reuse the linear regression framework from Section 2 and predict pairwise preferences by predicting two ab-solute scores (as before) and comparing them. Data. This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [Cite_Footnote_7] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems). The test set is formed by the WMT 2008 relative rank annotation task. As in Experi-ment 1, we set \u03b5 so that the incidence of ties in the training and test set is equal (60%)."
  },
  {
    "id": 120,
    "name": "CatVar",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://clipdemos.umiacs.umd.edu/catvar/",
    "section_title": "3 Building the CatVar",
    "add_info": null,
    "text": "The CatVar is web-browseable at  http://clipdemos.umiacs.umd.edu/catvar/. Figure 2 shows the CatVar web-based interface with the hunger cluster as an example. The interface allows searching clusters using regular expressions as well as cluster length restrictions. The database is also available for researchers in perl/C and lisp searchable formats."
  },
  {
    "id": 121,
    "name": "N/A",
    "fullname": "Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases",
    "genericmention": [
      "each of these sources"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Dorr, 2001",
      "Bonnie J. Dorr. 2001. LCS Verb Database. Technical Report Online Software Database, University of Mary-land, College Park, MD."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.umiacs.umd.edu/\u02dcbonnie/LCS",
    "section_title": "3 Building the CatVar",
    "add_info": "Bonnie J. Dorr. 2001. LCS Verb Database. Technical Report Online Software Database, University of Mary-land, College Park, MD. http://www.umiacs.umd.edu/\u02dcbonnie/LCS Database Docmentation.html.",
    "text": "The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001)  , the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998), and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4"
  },
  {
    "id": 122,
    "name": "WordNet 1.6",
    "fullname": "N/A",
    "genericmention": [
      "each of these sources"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Fellbaum, 1998",
      "Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cogsci.princeton.edu/\u02dcwn",
    "section_title": "3 Building the CatVar",
    "add_info": "Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/\u02dcwn [2000, Septem-ber 7].",
    "text": "The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001), the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998)  , and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4"
  },
  {
    "id": 123,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/movie$-data",
    "section_title": "5 Conclusion",
    "add_info": null,
    "text": "We conclude that text features from pre-release re-views can substitute for and improve over a strong metadata-based first-weekend movie revenue pre-diction. The dataset used in this paper has been made available for research at  http://www.ark.cs.cmu.edu/movie$-data."
  },
  {
    "id": 124,
    "name": "TextBlob library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the TextBlob library [Cite_Footnote_1] , which provides a sim-ple API for common natural language processing tasks"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://pypi.org/project/textblob/",
    "section_title": "3 Concept and Implementation 3.1.1 Pre-Analysis",
    "add_info": "1 https://pypi.org/project/textblob/",
    "text": "We use PoS (Part of Speech) information as a way of capturing error types of current word-level QE systems. First, the MT output from the training set of the WMT 2019 QE shared task is PoS tagged using the TextBlob library [Cite_Footnote_1] , which provides a sim-ple API for common natural language processing tasks . The PoS-tagged MT is then fed into the \u201cQEBrain\u201d model (Wang et al., 2018) to generate quality predictions. This QE model was chosen be-cause it was the best performing system according to the assessment carried out by Shterionov et al. (2019). Next, we check how often each PoS is in-correctly labelled by comparing the QE output to the reference annotations. The probability of each PoS and the corresponding conditional error prob-ability (given as (P (P oS), P (error|P oS))) are: nouns are most often wrong (32%, 48%), followed by prepositions (10.9%, 15.6%), pronouns (8.69%, 14.8%), determiners (13.04%, 14.3%), conjunc-tions (7%, 13.9%), interjections (4.5%, 12.9%), verbs (28%, 9.6%), adjectives (4.34%, 7.9%) and adverbs (5%, 2.9%)."
  },
  {
    "id": 125,
    "name": "MMPE CAT tool",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Herbig et al., 2019, 2020a,b,c; Jamara et al., 2021"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/NicoHerbig/MMPE",
    "section_title": "3 Concept and Implementation 3.3 QE Integration into CAT Environment",
    "add_info": "4 https://github.com/NicoHerbig/MMPE",
    "text": "A Computer-Aided Translation (CAT) tool or PE environment allows the capture and correction of mistakes, as well as the selection, manipulation, adaptation and recombination of good segments (Herbig et al., 2020c). Our implementation was done within the MMPE CAT tool [Cite_Footnote_4] (Herbig et al., 2019, 2020a,b,c; Jamara et al., 2021), as it is open source and easily extendable."
  },
  {
    "id": 126,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The complete resource",
      "our dataset",
      "the dataset"
    ],
    "description": [
      "a multilingual dataset of nominal com-pounds containing human judgments about composi-tionality",
      "It contains 180 compounds for each of the 3 target languages: English, French and Portuguese.",
      "The resulting resource can be used for applications and tasks involving some degree of semantic processing, such as lexical substitution and text simplification.",
      "For the cases where the numerical judgments alone are not enough for a given task, our dataset also provides sets of paraphrases, which serve as a symbolic counterpart to those scores."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/?page=downloads/compounds",
    "section_title": "5 Conclusions and Future Work",
    "add_info": "3 http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/?page=downloads/compounds",
    "text": "We presented a multilingual dataset of nominal com-pounds containing human judgments about composi-tionality. It contains 180 compounds for each of the 3 target languages: English, French and Portuguese. An-notations are collected through crowdsourcing. Since the task is performed by native speakers who may not have a background in linguistics, it needs to be appro-priately constrained not to require expert knowledge. The resulting resource can be used for applications and tasks involving some degree of semantic processing, such as lexical substitution and text simplification. For the cases where the numerical judgments alone are not enough for a given task, our dataset also provides sets of paraphrases, which serve as a symbolic counterpart to those scores. The complete resource will be made freely available. [Cite_Footnote_3] As future work, we plan to validate these scores through compositionality prediction (Yaz-dani et al., 2015; Salehi et al., 2015) and by incorporat-ing the scores and paraphrases into a machine transla-tion system. We also envisage extending the dataset for each of the languages and for additional languages."
  },
  {
    "id": 127,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the 23 entail-ment graphs published by Berant et al."
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.cs.tau.ac.il/~jonatha6/homepage_files/resources/HealthcareGraphs.rar",
    "section_title": "5 Application to the Health-care Domain",
    "add_info": "2 http://www.cs.tau.ac.il/~jonatha6/homepage_files/resources/HealthcareGraphs.rar",
    "text": "For the entailment graph we used the 23 entail-ment graphs published by Berant et al. [Cite_Footnote_2] . For the ar-gument taxonomy we employed UMLS \u2013 a database that maps natural language phrases to over one mil-lion unique concept identifiers (CUIs) in the health-care domain. The CUIs are also mapped in UMLS to a concept taxonomy for the health-care domain."
  },
  {
    "id": 128,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The data",
      "our error analysis data",
      "they"
    ],
    "description": [
      "our error analysis data"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://bit.ly/mt-para",
    "section_title": "6 Conclusions",
    "add_info": "4 The data is available at http://bit.ly/mt-para.",
    "text": "We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [Cite_Footnote_4]"
  },
  {
    "id": 129,
    "name": "English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task",
    "fullname": "N/A",
    "genericmention": [
      "Both sides",
      "English text"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.statmt.org/wmt16/translation-task.html",
    "section_title": "3 Experiments 3.1 Data and preprocessing",
    "add_info": "1 http://www.statmt.org/wmt16/translation-task.html",
    "text": "For the syntax-based NMT, we take syntac-tic trees of source texts as inputs. We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task [Cite_Footnote_1] . Both sides are tokenized and split into subwords using BPE with 8000 merge operations. English text is parsed using SyntaxNet (Alberti et al., 2017). Then we transform the labeled dependency tree into the extended Levi graph as described in Section 2.2. Unlike AMR-to-text generation, in NMT task the input sentence contains significant sequential in-formation. This information is lost when treating the sentence as a graph. Guo et al. (2019) consider this information by adding sequential connections between each word node. In our model, we also add forward and backward edges in the extended Levi graph. Thus, the edge types vocabulary for the extended Levi graph of the dependency tree is T ={default, reverse, self, forward, backward}. So the set of subgraphs for NMT is G sub = {fully-connected, connected, default, reverse, forward, backward}. Note that we do not change the model architecture in the NMT tasks. However, we still get good results, which indicates the effectiveness of our model on Graph2Seq tasks. Except for in-troducing BPE into Levi graph, the above prepro-cessing steps are following Bastings et al. (2017). We refer to them for further information on the preprocessing steps."
  },
  {
    "id": 130,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/QAQ-v/HetGT",
    "section_title": "3 Experiments 3.2 Parameter Settings",
    "add_info": null,
    "text": "Both our encoder and decoder have 6 layers with 512-dimensional word embeddings and hidden states. We employ 8 heads and dropout with a rate of 0.3. For optimization, we use Adam opti-mizer with \u03b2 2 = 0.998 and set batch size to 4096 tokens. Meanwhile, we increase learning rate lin-early for the first warmup steps, and decrease it thereafter proportionally to the inverse square root of the step number. We set warmup steps to 8000. The similar learning rate schedule is adopted in (Vaswani et al., 2017). Our implementa-tion uses the openNMT library (Klein et al., 2017). We train the models for 250K steps on a single GeForce GTX 1080 Ti GPU. Our code is available at  https://github.com/QAQ-v/HetGT."
  },
  {
    "id": 131,
    "name": "OpenNMT FAQ",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model",
    "section_title": "3 Experiments 3.3 Metrics and Baselines",
    "add_info": "2 Parameters were chosen following the OpenNMT FAQ: http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model",
    "text": "Our baseline is the original Transformer [Cite_Footnote_2] . For AMR-to-text generation, Transformer takes lin-earized graphs as inputs. For syntax-based NMT, Transformer is trained on the preprocessed trans-lation dataset without syntactic information. We also compare the performance of HetGT with pre-vious single/ensenmble approaches which can be grouped into three categories: (1) Recurrent neu-ral network (RNN) based methods (GGNN2Seq, GraphLSTM); (2) Graph neural network (GNN) based methods (GCNSEQ, DGCN, G2S-GGNN); (3) The Transformer based methods (Structural Transformer, GTransformer). The ensemble mod-els are denoted by subscripts in Table 2 and Table 3."
  },
  {
    "id": 132,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset",
      "The final dataset"
    ],
    "description": [
      "Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., hameliorate, RESULT , ameliorationi.",
      "We intentionally avoid zero-derivations.",
      "We also exclude overly orthographically distant pairs",
      "The final dataset includes 6,029 derivational samples"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://github.com/ryancotterell/derviational-paradigms",
    "section_title": "3 Task and Models 3.1 Data",
    "add_info": "5 The dataset is available at http://github.com/ryancotterell/derviational-paradigms.",
    "text": "We experiment on English derivational triples ex-tracted from NomBank (Meyers et al., 2004). 4 Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., hameliorate, RESULT , ameliorationi. Note that in this task we do not predict whether a slot ex-ists, merely what form it would take given the base and the slot. In terms of current study, we consider the following derivational types: verb nominaliza-tion such as RESULT , AGENT and PATIENT , ad-verbalization and adjective-noun transformations. We intentionally avoid zero-derivations. We also exclude overly orthographically distant pairs by fil-tering out those for which the Levenshtein distance exceeds half the sum of their lengths, which ap-pear to be misannotations in NomBank. The final dataset includes 6,029 derivational samples, which we split into train (70%), development (15%), and test (15%). [Cite_Footnote_5] We also note that NomBank annota-tions are often semantically more coarse-grained."
  },
  {
    "id": 133,
    "name": "Nematus toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sennrich et al., 2017"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/rsennrich/nematus/",
    "section_title": "3 Task and Models 3.4 RNN Encoder-Decoder",
    "add_info": "6 https://github.com/rsennrich/nematus/",
    "text": "Training. We use the Nematus toolkit (Sennrich et al., 2017). [Cite_Footnote_6] We exactly follow the recipe in Kann and Schu\u0308tze (2016), the winning submission on the 2016 SIGMORPHON shared task for inflectional morphology. Accordingly, we use a character em-bedding size of 300, 100 hidden units in both the encoder and decoder, Adadelta (Zeiler, 2012) with a minibatch size of 20, and a beam size of 12. We train for 300 epochs and select the test model based on the performance on the development set."
  },
  {
    "id": 134,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the scripts",
      "Scripts"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/abisee/cnn-dailymail",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "3 Scripts publicly available at https://github.com/abisee/cnn-dailymail",
    "text": "We conducted our summarization experiments on the non-anonymous version CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; See et al., 2017), and the New York Times dataset (Durrett et al., 2016; Xu and Durrett, 2019). For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017) [Cite_Footnote_3] . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we cre-ated the NYT50 dataset by removing the docu-ments whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Dur-rett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive sum-marization, we used a strategy similar to Nallapati et al. (2017). We label the subset of sentences in a document that maximizes R OUGE (Lin, 2004) (against the human summary) as True and all other sentences as False."
  },
  {
    "id": 135,
    "name": "En-glish Gigaword [Cite_Footnote_4] dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2012T21",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "4 https://catalog.ldc.upenn.edu/LDC2012T21",
    "text": "To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [Cite_Footnote_4] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al., 2014). To re-duce the vocabulary size, we applied byte pair en-coding (BPE; Sennrich et al. 2016) to all of our datasets. To limit the memory consumption dur-ing training, we limit the length of each sentence to be 50 words (51th word and onwards are re-moved) and split documents with more than 30 sentences into smaller documents with each con-taining at most 30 sentences."
  },
  {
    "id": 136,
    "name": "pre-trained BERT",
    "fullname": "N/A",
    "genericmention": [
      "this imple-mentation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Devlin et al., 2018"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "4 Experiments 4.4 Results",
    "add_info": "7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "Our main results on the CNNDM dataset are shown in Table 1, with abstractive models in the top block and extractive models in the bot-tom block. Pointer+Coverage (See et al., 2017), Abstract-ML+RL (Paulus et al., 2017) and DCA (Celikyilmaz et al., 2018) are all sequence to se-quence learning based models with copy and cov-erage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite (Hsu et al., 2018) and InconsisLoss (Chen and Bansal, 2018) all try to decompose the word by word summary generation into sentence selection from document and \u201csentence\u201d level summariza-tion (or compression). Bottom-Up (Gehrmann et al., 2018) generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; Nalla-pati et al. 2017 and NeuSum; Cheng and Lapata 2016). They have been extended with reinforce-ment learning (Refresh; Narayan et al. 2018 and BanditSum; Dong et al. 2018), Maximal Marginal Relevance (NeuSum-MMR; Zhou et al. 2018), la-tent variable modeling (LatentSum; Zhang et al. 2018) and syntactic compression (JECS; Xu and Durrett 2019). Lead3 is a baseline which sim-ply selects the first three sentences. Our model H IBERT S (in-domain), which only use one pre-training stage on the in-domain CNNDM training set, outperforms all of them and differences be-tween them are all significant with a 0.95 confi-dence interval (estimated with the ROUGE script). Note that pre-training H IBERT S (in-domain) is very fast and it only takes around 30 minutes for one epoch on the CNNDM training set. Our models with two pre-training stages (H IBERT S ) or larger size (H IBERT M ) perform even better and H IBERT M outperforms BERT by 0.5 ROUGE . We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in 3.3) without pre-training. Note the setting for HeriTransfomer is (L = 4,H = 300 and A = 4) . We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [Cite_Footnote_7] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1. The posi-tional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences 8 ). We feed each block (the BOS and EOS tokens of each sentence are replaced with [CLS] and [SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model H IBERT S outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters (H IBERT S 54.6M v.s. BERT 110M)."
  },
  {
    "id": 137,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code for our system"
    ],
    "description": [
      "a novel approach that simultaneously extracts events and entities within a document context",
      "a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/bishanyang/EventEntityExtractor",
    "section_title": "1 Introduction",
    "add_info": "1 The code for our system is available at https://github.com/bishanyang/EventEntityExtractor.",
    "text": "In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context. [Cite_Footnote_1] We first decompose the learn-ing problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the co-occurrence relations between events across the doc-ument, and (3) learning for entity extraction. Then we combine the learned models for these subprob-lems into a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document. In summary, our main contribu-tions are:"
  },
  {
    "id": 138,
    "name": "ACE2005 corpus",
    "fullname": "N/A",
    "genericmention": [
      "It"
    ],
    "description": [
      "It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.itl.nist.gov/iad/mig/tests/ace/2005/",
    "section_title": "4 Experiments",
    "add_info": "6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/",
    "text": "We conduct experiments on the ACE2005 corpus. [Cite_Footnote_6] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al. (2013). Table 2 shows the data statistics."
  },
  {
    "id": 139,
    "name": "Wiki article",
    "fullname": "Wikipedia (Wiki) [Cite_Footnote_1] articles",
    "genericmention": [
      "the orig-inal articles"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://en.wikipedia.org",
    "section_title": "2 The Document Grounded Dataset 2.1 Document Set Creation",
    "add_info": "1 https://en.wikipedia.org",
    "text": "We choose Wikipedia (Wiki) [Cite_Footnote_1] articles to cre-ate a set of documents D = {d 1 ,...,d 30 } for grounding of conversations. We randomly select 30 movies, covering various genres like thriller, super-hero, animation, romantic, biopic etc. We extract the key information provided in the Wiki article and divide it into four separate sections. This was done to reduce the load of the users to read, absorb and discuss the information in the document. Hence, each movie document d i con-sists of four sections {s 1 , s 2 , s 3 , s 4 } correspond-ing to basic information and three key scenes of the movie. The basic information section s 1 con-tains data from the Wikipedia article in a stan-dard form such as year, genre, director. It also includes a short introduction about the movie, rat-ings from major review websites, and some crit-ical responses. Each of the key scene sections {s 2 , s 3 , s 4 } contains one short paragraph from the plot of the movie. Each paragraph contains on an average 7 sentences and 143 words. These para-graphs were extracted automatically from the orig-inal articles, and were then lightly edited by hand to make them of consistent size and detail. An ex-ample of the document is attached in Appendix."
  },
  {
    "id": 140,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a predefined set of documents",
      "the document"
    ],
    "description": [
      "a crowd-sourced con-versations dataset "
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.github.com/festvox/datasets/CMUDoG",
    "section_title": "References",
    "add_info": "4 https://www.github.com/festvox/datasets/CMUDoG",
    "text": "In this paper we introduce a crowd-sourced con-versations dataset that is grounded in a predefined set of documents which is available for download [Cite_Footnote_4] . We perform multiple automatic and human judgment based analysis to understand the value the information from the document provides to the generation of responses. The SEQS model which uses the information from the section to generate responses outperforms the SEQ model in the eval-uation tasks of engagement, fluency and perplex-ity. as a part of The Conversational Intelligence Chal-lenge (ConvAI, NIPS 2017) and we would like to thank the ConvAI team. We are also grateful to the anonymous reviewers for their constructive feedback and to Carolyn Penstein Rose, Shivani Poddar, Sreecharan Sankaranarayanan, Samridhi Shree Choudhary and Zhou Yu for valuable discussions at earlier stages of this work."
  },
  {
    "id": 141,
    "name": "COMLEX 3.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Grishman et al., 1998",
      "Ralph Grishman, Catherine Macleod, and Adam Myers, 1998. COMLEX Syntax Reference Manual. Proteus Project, NYU."
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/comlex/refman.ps",
    "section_title": "2 Preliminaries 2.2 Gold standard data",
    "add_info": "Ralph Grishman, Catherine Macleod, and Adam Myers, 1998. COMLEX Syntax Reference Manual. Proteus Project, NYU. (http://nlp.cs.nyu.edu/comlex/refman.ps).",
    "text": "Information about noun countability was obtained from two sources: COMLEX 3.0 (Grishman et al., 1998)  and the common noun part of ALT-J/E \u2019s Japanese-to-English semantic transfer dictio-nary (Ikehara et al., 1991). Of the approximately 22,000 noun entries in COMLEX , 13,622 are marked as countable , 710 as uncountable and the remainder are unmarked for countability. ALT-J/E has 56,245 English noun types with distinct countability."
  },
  {
    "id": 142,
    "name": "ROUGE",
    "fullname": "N/A",
    "genericmention": [
      "It"
    ],
    "description": [
      "the new automatic sum-mary evaluation metric",
      "ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.isi.edu/\u02dccyl/ROUGE",
    "section_title": "4 Experiments on DUC 2004 data 4.1 DUC 2004 data and ROUGE",
    "add_info": "1 http://www.isi.edu/\u02dccyl/ROUGE",
    "text": "For evaluation, we used the new automatic sum-mary evaluation metric, ROUGE [Cite_Footnote_1] , which was used for the first time in DUC 2004. ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence. It reports separate scores for 1, 2, 3, and 4-gram, and also for longest common subsequence co-occurences. Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgements most (Lin and Hovy, 2003). We show three of the ROUGE metrics in our experiment results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on longest common subsequence weighted by the length)."
  },
  {
    "id": 143,
    "name": "MEAD",
    "fullname": "N/A",
    "genericmention": [
      "it",
      "its"
    ],
    "description": [
      "MEAD [Cite_Footnote_2] is a publicly available toolkit for extractive multi-document summarization."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.summarization.com",
    "section_title": "4 Experiments on DUC 2004 data 4.2 MEAD summarization toolkit",
    "add_info": "2 http://www.summarization.com",
    "text": "MEAD [Cite_Footnote_2] is a publicly available toolkit for extractive multi-document summarization. Although it comes as a centroid-based summarization system by de-fault, its feature set can be extended to implement other methods."
  },
  {
    "id": 144,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the officially released evaluation scorer"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/sheffieldnlp/fever-scorer",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "3 https://github.com/sheffieldnlp/fever-scorer",
    "text": "This task has three evaluations: (i) N O S CORE E V \u2013 accuracy of claim verifica-tion, neglecting the validity of evidence; (ii) S CORE E V \u2013 accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for S UPPORTED and R E - FUTED ; (iii) F 1 \u2013 between the predicted evidence sentences and the ones chosen by annotators. We use the officially released evaluation scorer [Cite_Footnote_3] ."
  },
  {
    "id": 145,
    "name": "Pushshift API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pushshift/api",
    "section_title": "4 Data 4.1 Data collection",
    "add_info": "4 https://github.com/pushshift/api model with all features. The numerical results are given in the appendix (Table 6.)",
    "text": "Data was scraped from the two subreddits by querying the Pushshift API. [Cite_Footnote_4] 3 years\u2019 worth of posts, ranging from 1 January 2017 to 31 Decem-ber 2019, were collected for each subreddit. To ensure each post had sufficient linguistic content, I excluded any posts containing less than 101 char-acters."
  },
  {
    "id": 146,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/KodairaTomonori/EvaluationDataset",
    "section_title": "1 Introduction",
    "add_info": "2 https://github.com/KodairaTomonori/EvaluationDataset",
    "text": "\u2022 The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators. Our dataset is available at GitHub [Cite_Footnote_2] ."
  },
  {
    "id": 147,
    "name": "Lancers",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "to perform substitute extraction, substitute evalua-tion, and substitute ranking"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.lancers.jp/",
    "section_title": "4 Balanced dataset for evaluation of Japanese lexical simplification",
    "add_info": "3 http://www.lancers.jp/",
    "text": "We use a crowdsourcing application, Lancers, [Cite_Footnote_3] to perform substitute extraction, substitute evalua-tion, and substitute ranking. In each task, we re-quested the annotators to complete at least 95% of their previous assignments correctly. They were native Japanese speakers."
  },
  {
    "id": 148,
    "name": "Lexicon for Japanese Language Edu-cation",
    "fullname": "N/A",
    "genericmention": [
      "the lex-icon"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sunakawa et al., 2012"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://jhlee.sakura.ne.jp/JEV.html",
    "section_title": "4 Balanced dataset for evaluation of Japanese lexical simplification 4.1 Extracting sentences",
    "add_info": "4 http://jhlee.sakura.ne.jp/JEV.html",
    "text": "Our work defines complex words as \u201cHigh Level\u201d words in the Lexicon for Japanese Language Edu-cation (Sunakawa et al., 2012). [Cite_Footnote_4] The word level is calculated by five teachers of Japanese, based on their experience and intuition. There were 7,940 high-level words out of 17,921 words in the lex-icon. In addition, target words of this work com-prised content words (nouns, verbs, adjectives, ad-verbs, adjectival nouns, sahen nouns, and sahen verbs )."
  },
  {
    "id": 149,
    "name": "ERNIE",
    "fullname": "Enhanced Represen-tation through Knowledge Integration",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Sun et al., 2019"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/PaddlePaddle/ERNIE",
    "section_title": "5 Experiments 5.1 Methods",
    "add_info": "3 https://github.com/PaddlePaddle/ERNIE",
    "text": "\u2022 ERNIE (115M) [Cite_Footnote_3] , a.k.a Enhanced Represen-tation through Knowledge Integration (Sun et al., 2019), which is trained with not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc."
  },
  {
    "id": 150,
    "name": "ROBERTa-wwm-est-large",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/ymcui/Chinese-BERT-wwm",
    "section_title": "5 Experiments 5.1 Methods",
    "add_info": "4 https://github.com/ymcui/Chinese-BERT-wwm",
    "text": "\u2022 ROBERTa, a robust BERT (Liu et al., 2019). We used ROBERTa-wwm-est-large. [Cite_Footnote_4]"
  },
  {
    "id": 151,
    "name": "Huggingface",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "6 Transfer Learning via TED-CDB 6.2 Same-Domain Cross-Language Learning",
    "add_info": "5 https://github.com/huggingface/transformers",
    "text": "TED-MDB (Zeyrek et al., 2018) corpus annotation follows the PDTB 3.0 framework. It contains man-ual annotation of 6 TED talks in seven languages (English, Turkish, European Potuguese, Polish, German, Russian, and Lithuanian). The sub-corpus for each language is quite small, with about 200 im-plicit discourse relations each, compared with the \u223c7.0 K implicit relations in the TED-CDB. There-fore, we can see whether the TED-CDB can help them. For this experiment, the multilingual BERT was used, which is as large as BERT-wwm but the training data is expanded to cover 104 languages. We used the multilingual BERT implementation from Huggingface. [Cite_Footnote_5] The design for these exper-iments is making a comparison between a cross validation within the TED-MDB and a zero-shot transfer learning from TED-CDB to TED-MDB. Due to the unbalanced distribution of senses in TED-MDB, using the method of Easy Ensemble (Liu, 2009), we divided the Expansion data of every language in the TED-MDB into 4 parts and then each part was added into the data of other types to become the training set. Finally, we integrated these training sets from 6 language into one train-ing set, and the left data for one language would be the test set. Therefore, what we used here is 4-fold cross validation where each fold is used as the test set exactly once. The average test set accuracy is then reported."
  },
  {
    "id": 152,
    "name": "Acc Recall F1 SeqAttack",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/WalterSimoncini/SeqAttack",
    "section_title": "5 Results and discussion 5.2 Adversarial training",
    "add_info": "1 https://github.com/WalterSimoncini/SeqAttack",
    "text": "Figure 4: KDE plots for the labels score and modifica-tion rate distributions for NER small and its robust coun-terpart, when attacked with DeepWordBug-I 5 . To be Examples Acc Recall F1 SeqAttack [Cite_Footnote_1] , a Python library for conducting ad- 500 examples 91% 1000 examples 90% 1500 examples 90% 2000 examples 90% NER small 91% 90% versarial attacks against token classification models.90% 89% The library is accompanied by a web application 90% 90% to inspect the generated adversarial examples and90%"
  },
  {
    "id": 153,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Application",
      "a web application"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://ner-attack.ashita.nl/",
    "section_title": "5 Results and discussion 5.2 Adversarial training",
    "add_info": "2 Application available at https://ner-attack.ashita.nl/",
    "text": "Figure 4: KDE plots for the labels score and modifica-tion rate distributions for NER small and its robust coun-terpart, when attacked with DeepWordBug-I 5 . To be Examples Acc Recall F1 SeqAttack , a Python library for conducting ad- 500 examples 91% 1000 examples 90% 1500 examples 90% 2000 examples 90% NER small 91% 90% versarial attacks against token classification models.90% 89% The library is accompanied by a web application [Cite_Footnote_2] 90% 90% to inspect the generated adversarial examples and90%"
  },
  {
    "id": 154,
    "name": "seqeval",
    "fullname": "N/A",
    "genericmention": [
      "Software"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Nakayama, 2018",
      "Hiroki Nakayama. 2018. seqeval: A python framework for sequence labeling evaluation."
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/chakki-works/seqeval",
    "section_title": "4 Experiments 4.2 Adversarial training",
    "add_info": "Hiroki Nakayama. 2018. seqeval: A python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval.",
    "text": "The metrics were calculated using seqeval (Nakayama, 2018)  . \u2191 (\u2193) indicate whether the higher (or lower) the better from the attack perspective."
  },
  {
    "id": 155,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the source code of DIG"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/INK-USC/DIG",
    "section_title": "References",
    "add_info": "1 https://github.com/INK-USC/DIG",
    "text": "As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation ax-ioms and the ease of gradient computation. It measures feature importance by averaging the model\u2019s output gradient interpolated along a straight-line path in the input data space. How-ever, such straight-line interpolated points are not representative of text data due to the inher-ent discreteness of the word embedding space. This questions the faithfulness of the gradi-ents computed at the interpolated points and consequently, the quality of the generated ex-planations. Here we propose Discretized In-tegrated Gradients (DIG), which allows effec-tive attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yield-ing more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encour-age reproducible research [Cite_Footnote_1] ."
  },
  {
    "id": 156,
    "name": "Riemann summation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://en.wikipedia.org/wiki/Riemann_sum",
    "section_title": "2 Method 2.1 Discretized integrated gradients",
    "add_info": "2 https://en.wikipedia.org/wiki/Riemann_sum",
    "text": "Here m is the total number of steps for interpola-tion. This constraint is essential because it allows approximating the integral in Eq. 1 using Riemann summation [Cite_Footnote_2] which requires monotonic paths. We note that the interpolation points used by IG nat-urally satisfy this constraint since they lie along a straight line joining x and x 0 . The key distinction of our formulation from IG is that DIG is agnostic of any fixed step size parameter \u03b1 and thus allows non-linear interpolation paths in the embedding space. The integral approximation of DIG is de-fined as follows:"
  },
  {
    "id": 157,
    "name": "HuggingFace Dataset library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2020b"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/huggingface/datasets",
    "section_title": "3 Experimental Setup",
    "add_info": "3 https://github.com/huggingface/datasets",
    "text": "In this section, we describe the datasets and models used for evaluating our proposed algorithm. Datasets. The SST2 (Socher et al., 2013) dataset has 6920/872/1821 example sentences in the train/dev/test sets. The task is binary classifica-tion into positive/negative sentiment. The IMDB (Maas et al., 2011) dataset has 25000/25000 exam-ple reviews in the train/test sets with similar binary labels for positive and negative sentiment. Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library [Cite_Footnote_3] (Wolf et al., 2020b)."
  },
  {
    "id": 158,
    "name": "HuggingFace Dataset library 3",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2020b"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/huggingface/datasets",
    "section_title": "3 Experimental Setup",
    "add_info": "Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, Fran\u00e7ois Lagunas, Lysandre Debut, Morgan Funtow-icz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Mu\u0161tar, Jeff Boudier, and Anna Tordjmann. 2020b. Datasets. GitHub. Note: https://github.com/huggingface/datasets.",
    "text": "In this section, we describe the datasets and models used for evaluating our proposed algorithm. Datasets. The SST2 (Socher et al., 2013) dataset has 6920/872/1821 example sentences in the train/dev/test sets. The task is binary classifica-tion into positive/negative sentiment. The IMDB (Maas et al., 2011) dataset has 25000/25000 exam-ple reviews in the train/test sets with similar binary labels for positive and negative sentiment. Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library 3 (Wolf et al., 2020b)  ."
  },
  {
    "id": 159,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Codes for the experiments"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/parag1604/A2L",
    "section_title": "4 Experiments 4.1 Active Learning Strategies for Sequence Tagging",
    "add_info": "1 Codes for the experiments are available at the following github link: https://github.com/parag1604/A2L.",
    "text": "Margin-based strategy: Let s(y) = P \u03b8 (Y = y|X = x) be the score assigned by a model M with parameters \u03b8 to output y for a given example x. Margin is defined as the difference in scores ob-tained by the best scoring output y and the second best scoring output y 0 , i.e.: where, y max = arg max y s(y). The strategy se-lects examples for which M margin \u2264 \u03c4 [Cite_Footnote_1] , where \u03c4 1 is a hyper-parameter. We use Viterbi\u2019s algorithm (Ryan and Nudd, 1993) to compute the scores s(y)."
  },
  {
    "id": 160,
    "name": "Simple PPDB",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "section_title": "1 Motivation",
    "add_info": "1 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "text": "Recent research in data-driven paraphrasing has produced enormous resources containing millions of meaning-equivalent phrases (Ganitkevitch et al., 2013). Such resources capture a wide range of language variation, including the types of lexical and phrasal simplifications just described. In this work, we apply state-of-the-art machine learned models for lexical simplification in order to iden-tify phrase pairs from the Paraphrase Database (PPDB) applicable to the task of text simplifica-tion. We introduce Simple PPDB, [Cite_Footnote_1] a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules. The large scale of Sim-ple PPDB will support research into increasingly advanced methods for text simplification."
  },
  {
    "id": 161,
    "name": "PPDB-TLDR [Cite_Footnote_2] dataset",
    "fullname": "N/A",
    "genericmention": [
      "the data"
    ],
    "description": [
      "the PPDB-TLDR [Cite_Footnote_2] dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://paraphrase.org/#/download",
    "section_title": "2 Identifying Simplification Rules 2.1 Paraphrase Rules",
    "add_info": "2 http://paraphrase.org/#/download",
    "text": "The Paraphrase Database (PPDB) is currently the largest available collection of paraphrases. Each paraphrase rule in the database has an automatically-assigned quality score between 1 and 5 (Pavlick et al., 2015). In this work, we use the PPDB-TLDR [Cite_Footnote_2] dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases, and is intended to give a generally good tradeoff between precision and recall. To preprocess the data, we lemmatize all of the phrases, and remove rules which differ only by morphology, punctu-ation, or stop words, or which involve phrases longer than 3 words. The resulting list contains 7.5 million paraphrase rules covering 625K unique lemmatized words and phrases."
  },
  {
    "id": 162,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a multi-class logistic regression model"
    ],
    "description": [
      "a multi-class logistic regression model"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://scikit-learn.org/",
    "section_title": "2 Identifying Simplification Rules 2.2 Lexical Simplification Model",
    "add_info": "4 http://scikit-learn.org/",
    "text": "We train a multi-class logistic regression model [Cite_Footnote_4] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output."
  },
  {
    "id": 163,
    "name": "Simple PPDB",
    "fullname": "N/A",
    "genericmention": [
      "its"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "section_title": "References",
    "add_info": "6 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz",
    "text": "We have described Simple PPDB, a subset of the Paraphrase Database adapted for the task of text simplification. Simple PPDB is built by apply-ing state-of-the-art machine learned models for lexical simplification to the largest available re-source of lexical and phrasal paraphrases, result-ing in a web-scale resource capable of supporting research in data-driven methods for text simplifi-cation. We have shown that Simple PPDB offers substantially increased coverage of both words and multiword phrases, while maintaining high quality compared to existing methods for lexical simplification. Simple PPDB, along with the hu-man judgements collected as part of its creation, is freely available with the publication of this paper. [Cite_Footnote_6] rial is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this pub-lication are those of the authors and should not be interpreted as representing official policies or en-dorsements of DARPA and the U.S. Government."
  },
  {
    "id": 164,
    "name": "multilingual BERT",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/google-research/bert/blob/master/multilingual.md",
    "section_title": "6 Data",
    "add_info": "10 Information about multilingual BERT can be found in: https://github.com/google-research/bert/blob/master/multilingual.md",
    "text": "We used Wikipedia as the main data source for all our experiments. Multilingual BERT [Cite_Footnote_10] was trained on the 104 languages with the largest Wikipedias \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba."
  },
  {
    "id": 165,
    "name": "Wikipedias",
    "fullname": "N/A",
    "genericmention": [
      "these"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://meta.wikimedia.org/wiki/List_of_Wikipedias",
    "section_title": "6 Data",
    "add_info": "11 List of Wikipedias can be found in https://meta.wikimedia.org/wiki/List_of_Wikipedias",
    "text": "We used Wikipedia as the main data source for all our experiments. Multilingual BERT was trained on the 104 languages with the largest Wikipedias [Cite_Footnote_11] \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba."
  },
  {
    "id": 166,
    "name": "CLUTO",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "A Clustering Toolkit"
    ],
    "citationtag": [
      "Tech Report 02-017, Dept. of Computer Science, University of Minnesota.",
      "S. Ploux and H. Ji. 2003. A Model for Matching Semantic Maps Between Languages (French/English, English/French). Computational Linguistics, 29(2):155- 178."
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.cs.umn.edu\u02dccluto",
    "section_title": "References",
    "add_info": null,
    "text": "Finally, evaluation of induced sense taxonomies is always problematic. First of all, there is no agreed \u201ccorrect\u201d way to G. Karypis. 2002. CLUTO: A Clustering Toolkit. Tech Report 02-017, Dept. of Computer Science, University of Minnesota. Available at  http://www.cs.umn.edu\u02dcclutoS. Ploux and H. Ji. 2003. A Model for Matching Semantic Maps Between Languages (French/English, English/French). Computational Linguistics, 29(2):155- 178."
  },
  {
    "id": 167,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our re-analysis code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/qingsongma/percentage-refBias",
    "section_title": "2 Background 2.1 Measures of Central Tendency",
    "add_info": "7 Our re-analysis code is available at https://github.com/qingsongma/percentage-refBias",
    "text": "Table 2 shows proportions of all judge pairs with significant differences in Kappa point esti-mates (non-overlapping confidence intervals) for each combination of settings (Revelle, 2014). [Cite_Footnote_7] The number of significant differences in Kappa point estimates for pairs of judges in SAME and DIFF is only 13%, or, in other words, 87% of judge pairs across SAME and DIFF have no significant difference in agreement levels. Table 2 also in-cludes proportions of significant differences for Kappa point estimates resulting from judges be-longing to a single setting (significance testing all Kappa of SAME with respect to all other Kappa be-longing to SAME , for example), revealing that the proportion of significant differences within SAME (12%) to be very similar to that of SAME \u00d7 DIFF (13%), and similarly for DIFF (12%), with only a single percentage point difference in both cases in proportions of significant differences. Sub-sequently, even after correcting the measure of central tendency error in Fomicheva and Specia (2016), evidence of reference bias can still not be concluded."
  },
  {
    "id": 168,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/sweetalyssum/Seq2Seq-DU",
    "section_title": "1 Introduction",
    "add_info": "1 The code is available at https://github.com/sweetalyssum/Seq2Seq-DU.",
    "text": "Experimental results on benchmark datasets show that Seq2Seq-DU [Cite_Footnote_1] performs much better than the baselines on SGD, MultiWOZ2.2, and Multi-WOZ2.1 in multi-turn dialogue with schema de-scriptions, is superior to BERT-DST on WOZ2.0, DSTC2, and M2M, in multi-turn dialogue with-out schema descriptions, and works equally well as Joint BERT on ATIS and SNIPS in single turn dialogue (in fact, it degenerates to Joint BERT)."
  },
  {
    "id": 169,
    "name": "Mantidae toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Cohn et al., 2016"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/duyvuleo/Mantidae",
    "section_title": "5 Experiments 5.1 Setup",
    "add_info": "4 https://github.com/duyvuleo/Mantidae",
    "text": "NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [Cite_Footnote_4] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words. For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs."
  },
  {
    "id": 170,
    "name": "dynet deep learning library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Neubig et al., 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/clab/dynet",
    "section_title": "5 Experiments 5.1 Setup",
    "add_info": "5 https://github.com/clab/dynet",
    "text": "NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [Cite_Footnote_5] (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words. For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs."
  },
  {
    "id": 171,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/McGill-NLP/contextual-nmn",
    "section_title": "References",
    "add_info": null,
    "text": "Neural module networks (NMN) are a pop-ular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit associ-ations between similar textual contexts (e.g. \u201cdark cube on the left\u201d vs. \u201cblack cube on the left\u201d). In this work, we address these limitations and evaluate the impact of contex-tual clues in improving the performance of NMN models. First, we address the prob-lem of fixed textual inputs by parameteriz-ing the module arguments. This substan-tially reduce the number of modules in NMN by up to 75% without any loss in perfor-mance. Next we propose a method to con-textualize our parameterized model to enhance the module\u2019s capacity in exploiting the vi-siolinguistic associations. Our model out-performs the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1% improve-ment in accuracy on the single-referent test set and +4.3% on the full test set. Addi-tionally, we demonstrate that contextualization provides +11.2% and +1.7% improvements in accuracy over prior NMN models on CLO-SURE and NLVR2. We further evaluate the impact of our contextualization by construct-ing a contrast set for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the baselines by as much as +10.4% absolute ac-curacy on CC-Ref+, illustrating the generaliza-tion skills of our approach. Our dataset is pub-licly available at  https://github.com/McGill-NLP/contextual-nmn."
  },
  {
    "id": 172,
    "name": "IEP-Ref implementation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a NMN solution based on IEP"
    ],
    "citationtag": [
      "Liu et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ruotianluo/iep-ref",
    "section_title": "3 Module Parameterization in NMN",
    "add_info": "1 We used the IEP-Ref implementation provided at the link https://github.com/ruotianluo/iep-ref",
    "text": "We propose parametrization as the first step to en-able weight sharing and exploiting associations be-tween similar textual contexts. Specifically, we evaluate the effectiveness of parameterizing mod-ule textual inputs using IEP-Ref (Liu et al., 2019) as the baseline NMN implementation. IEP-Ref, a NMN solution based on IEP (Johnson et al., 2017b), is the current state-of-the-art model on CLEVR-Ref+ dataset. [Cite_Footnote_1] As shown Figure 2(a), the neural modules in IEP-Ref are represented using a stan-dard Residual Convolution Block (RCB). Formally, each RCB module (f n ) of arity n receives n feature maps (F i ) of shape 128 \u00d7 20 \u00d7 20 and outputs a same-sized tensor f o = f n (F 1 , F 2 , ..., F n )."
  },
  {
    "id": 173,
    "name": "IEP-Ref implementation",
    "fullname": "N/A",
    "genericmention": [
      "the model"
    ],
    "description": [
      "IEP-Ref (Liu et al., 2019), the current state-of-the-art neural module network (NMN) model for the CLEVR-Ref+ dataset, uses a generic design of neural module architecture adapted from IEP"
    ],
    "citationtag": [
      "Liu et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ruotianluo/iep-ref",
    "section_title": "A Appendix A.2 Neural Modules in Parameterized IEP-Ref",
    "add_info": "4 We used the IEP-Ref implementation provided at the link https://github.com/ruotianluo/iep-ref",
    "text": "IEP-Ref (Liu et al., 2019), the current state-of-the-art neural module network (NMN) model for the CLEVR-Ref+ dataset, uses a generic design of neural module architecture adapted from IEP (John-son et al., 2017b), which was designed for VQA task. [Cite_Footnote_4] The modules take either two visual inputs (bi-nary modules) or one visual input (unary modules). There are total 60 distinct modules in IEP-Ref. Af-ter parameterization (see Figure 8b), the distinct number of modules drop to 15 without any drop in the model performance (section 2 of main paper). That is, the number of a distinct set of modules (and the total number of parameters) used in the parameterized model reduces by 75%. Moreover, although the network parameters of each parameter-ized module slightly increase due to the additional LSTM unit, since each module in IEP-Ref can have multiple instantiations for the same textual input, we have fewer parameters than IEF-Ref in total. Table 11 presents the list of all the 15 modules in our parameterized NMN model. We compare the parameters per module of all baseline NMN mod-els and our proposed models (section 3 of main paper) in Table 10."
  },
  {
    "id": 174,
    "name": "monolingual WMT news crawl datasets",
    "fullname": "N/A",
    "genericmention": [
      "all available monolingual news crawl training data"
    ],
    "description": [
      "the monolingual WMT news crawl datasets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://data.statmt.org/news-crawl/",
    "section_title": "5 Experiments 5.1 Datasets",
    "add_info": "3 http://data.statmt.org/news-crawl/",
    "text": "Estonian (Et)\u2013En translation tasks. The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [Cite_Footnote_3] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences. For the low-resource languages Ro and Et, we used all available monolingual news crawl training data. To make our experiments comparable with previous work (Lample and Conneau, 2019), we report the results on newstest2014 for Fr\u2013En, newstest2016 for Ro\u2013En, and newstest2018 for Et\u2013En."
  },
  {
    "id": 175,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/diegma/neural-dep-srl",
    "section_title": "1 Introduction",
    "add_info": "1 The code is available at https://github.com/diegma/neural-dep-srl.",
    "text": "One layer GCN encodes only information about immediate neighbors and K layers are needed to encode K-order neighborhoods (i.e., informa-tion about nodes at most K hops aways). This contrasts with recurrent and recursive neural net-works (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence. However, as we will further discuss in Section 3.3, this is not a serious limitation when GCNs are used in combination with encoders based on recurrent networks (LSTMs). When we stack GCNs on top of LSTM layers, we obtain a substantial improve-ment over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for En-glish and Chinese. [Cite_Footnote_1]"
  },
  {
    "id": 176,
    "name": "Gender Genie",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "an online gender-detector"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://bookblog.net/gender/genie.php",
    "section_title": "7 Gender Classification Results",
    "add_info": "7 http://bookblog.net/gender/genie.php",
    "text": "Table 4 combines the results of the experiments re-ported in the previous sections, assessed on both the Fisher and Switchboard corpora for gender classification. The evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. Baseline performance (always guessing female) yields 57.47% and 51.6% on Fisher and Switchboard respectively. As noted before, the standard reference algorithm is Boulis and Osten-dorf (2005), and all cited relative error reductions are based on this established standard, as imple-mented in this paper. Also, as a second reference, performance is also cited for the popular \u201cGender Genie\u201d, an online gender-detector [Cite_Footnote_7] , based on the manually weighted word-level sociolinguistic fea-tures discussed in Argamon et al. (2003). The ad-ditional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the Boulis and Ostendorf (2005) standard."
  },
  {
    "id": 177,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All data",
      "This data",
      "it",
      "it"
    ],
    "description": [
      "a newly gathered corpus with dense document-level sentiment la-bels in news articles",
      "This data includes compre-hensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "http://homes.cs.washington.edu/\u02dceunsol/project_page/acl16",
    "section_title": "1 Introduction",
    "add_info": "1 All data will be made publicly available. You can browse it at http://homes.cs.washington.edu/\u02dceunsol/project_page/acl16, and download it from the author\u2019s webpage.",
    "text": "We evaluate the approach on a newly gathered corpus with dense document-level sentiment la-bels in news articles. [Cite_Footnote_1] This data includes compre-hensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demon-strate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ab-lation and error analysis, showing cases where the global constraints contribute and pointing towards important areas for future work."
  },
  {
    "id": 178,
    "name": "CPLEX4",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://tinyurl.com/joccfqy",
    "section_title": "5 Experimental Setup",
    "add_info": "11 http://tinyurl.com/joccfqy",
    "text": "We also report a proxy for doing similar ag-gregation over a state-of-the-art entity-entity sen-timent classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annota-tions, we can simply predict the union of the orig-inal gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 [Cite_Footnote_11] to solve the ILP described in Sec. 2. For compu-tational efficiency and to avoid erroneous propa-gation, soft constraints associated with reciprocity and balance theory are introduced only on pairs for which a high-precision classifier assigned po-larity. For the pairwise classifier, we use a class-weighted linear SVM. We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset. We made two versions of pair-wise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search. One is tuned for high precision to be used as a base classifier for ILP (ILP base), and the other is tuned for the best F1 (Pairwise)."
  },
  {
    "id": 179,
    "name": "class-weighted linear SVM",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the pairwise classifier"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://scikit-learn.org/",
    "section_title": "5 Experimental Setup",
    "add_info": "12 http://scikit-learn.org/",
    "text": "We also report a proxy for doing similar ag-gregation over a state-of-the-art entity-entity sen-timent classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annota-tions, we can simply predict the union of the orig-inal gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 to solve the ILP described in Sec. 2. For compu-tational efficiency and to avoid erroneous propa-gation, soft constraints associated with reciprocity and balance theory are introduced only on pairs for which a high-precision classifier assigned po-larity. For the pairwise classifier, we use a class-weighted linear SVM. [Cite_Footnote_12] We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset. We made two versions of pair-wise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search. One is tuned for high precision to be used as a base classifier for ILP (ILP base), and the other is tuned for the best F1 (Pairwise)."
  },
  {
    "id": 180,
    "name": "acoustic classifier h",
    "fullname": "N/A",
    "genericmention": [
      "These two classifiers",
      "two distinct classifiers"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/",
    "section_title": "3 Prosodic Model 3.3 Prosodic Model Training",
    "add_info": "1 LIBSVM \u2013 A Library for Support Vector Machines, loca-tion: http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/",
    "text": "In our experiments, we investigate two kinds of training methods for prosodic modeling. The first one is a supervised method where models are trained using all the labeled data. The second is a semi-supervised method using co-training algo-rithm (Blum and Mitchell, 1998), described in Algo-rithm 1. Given a set L of labeled data and a set U of unlabeled data with two views, it then iterates in the following procedure. The algorithm first creates a smaller pool U \u2032 containing unlabeled data from U. It uses L i (i = 1, 2) to train two distinct classifiers: the acoustic classifier h [Cite_Footnote_1] , and the lexical classifier h 2 . We use function V i (i = 1, 2) to represent that only a single view is used for training h 1 or h 2 . These two classifiers are used to make predictions for the unla-beled set U \u2032 , and only when they agree on the predic-tion for a sample, their predicted class is used as the label for this sample. Then among these self-labeled samples, the most confident ones by one classifier are added to the data set L i for training the other classifier. This iteration continues until reaching the defined number of iterations. In our experiment, the size of the pool U\u00b4 is 5 times of the size of training data L i , and the size of the added self-labeled ex-ample set, D h i , is 5% of L i . For the newly selected D h i , the distribution of the positive and negative ex-amples is the same as that of the training data L i ."
  },
  {
    "id": 181,
    "name": "CMU Sphinx",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Speech Recognition Toolkit"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.speech.cs.cmu.edu/sphinx/tutorial.html",
    "section_title": "5 Data and Baseline Systems",
    "add_info": "2 CMU Sphinx - Speech Recognition Toolkit, location: http://www.speech.cs.cmu.edu/sphinx/tutorial.html",
    "text": "The first data set is the Boston University Radio News Corpus (BU) (Ostendorf et al., 1995), which consists of broadcast news style read speech. The BU corpus has about 3 hours of read speech from 7 speakers (3 female, 4 male). Part of the data has been labeled with ToBI-style prosodic annotations. In fact, the reason that we use this corpus, instead of other corpora typically used for ASR experiments, is because of its prosodic labels. We divided the entire data corpus into a training set and a test set. There was no speaker overlap between training and test sets. The training set has 2 female speakers (f2 and f3) and 3 male ones (m2, m3, m4). The test set is from the other two speakers (f1 and m1). We use 200 utterances for the recognition experiments. Each ut-terance in BU corpus consists of more than one sen-tences, so we segmented each utterance based on pause, resulting in a total number of 713 segments for testing. We divided the test set roughly equally into two sets, and used one for parameter tuning and the other for rescoring test. The recognizer used for this data set was based on Sphinx-3 [Cite_Footnote_2] . The context-dependent triphone acoustic models with 32 Gaus-sian mixtures were trained using the training par-tition of the BU corpus described above, together with the broadcast new data. A standard back-off tri-gram language model with Kneser-Ney smoothing was trained using the combined text from the train-ing partition of the BU, Wall Street Journal data, and part of Gigaword corpus. The vocabulary size was about 10K words and the out-of-vocabulary (OOV) rate on the test set was 2.1%."
  },
  {
    "id": 182,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an unofficial score conversion table [Cite_Footnote_2] between the tests"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/",
    "section_title": "2 Experimental Setup 2.3 Standardized English Tests",
    "add_info": "2 http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/ Although both TOEFL and TOEIC are administered by the same company (ETS), to the best of our knowledge there is no publicly available official conversion table between the two tests.",
    "text": "TOEFL Berzak et al. (2017) also collected self-reported scores on the most recently taken offi-cial English proficiency test, which we use here as a secondary evaluation benchmark. We focus on the most commonly reported test, the TOEFL-iBT whose scores range from 0 to 120. We take into ac-count only test results obtained less than four years prior to the experiment, yielding 33 participants. We sum the scores of the reading and listening sec-tions of test, with a total possible score range of 0 to 60. In cases where participants reported only the overall score, we divided that score by two. We further augment this data with 20 participants who took the TOEIC Listening and Reading test within the same four years range, resulting in a total of 53 external proficiency scores. The TOEIC scores were converted to the TOEFL scale by fitting a third degree polynomial on an unofficial score conversion table [Cite_Footnote_2] between the tests. The converted scores were then divided by two. Henceforth we refer to both TOEFL-iBT and TOEIC scores con-verted to TOEFL-iBT scale as TOEFL scores. The mean TOEFL score is 47.6 (std 9.55). The Pear-son\u2019s r correlation between the TOEFL and MET scores in the dataset is 0.74."
  },
  {
    "id": 183,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/kentonl/e2e-coref",
    "section_title": "7 Experiments 7.1 Hyperparameters",
    "add_info": "3 https://github.com/kentonl/e2e-coref",
    "text": "All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [Cite_Footnote_3]"
  },
  {
    "id": 184,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our embeddings and datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/yogarshi/bisparse-dep/",
    "section_title": "References",
    "add_info": "1 https://github.com/yogarshi/bisparse-dep/",
    "text": "Cross-lingual Hypernymy Detection involves determining if a word in one language (\u201cfruit\u201d) is a hypernym of a word in another language (\u201cpomme\u201d i.e. apple in French). The abil-ity to detect hypernymy cross-lingually can aid in solving cross-lingual versions of tasks such as textual entailment and event coreference. We propose B I S PARSE -D EP , a family of un-supervised approaches for cross-lingual hyper-nymy detection, which learns sparse, bilingual word embeddings based on dependency con-texts. We show that B I S PARSE -D EP can sig-nificantly improve performance on this task, compared to approaches based only on lexical context. Our approach is also robust, show-ing promise for low-resource settings: our dependency-based embeddings can be learned using a parser trained on related languages, with negligible loss in performance. We also crowd-source a challenging dataset for this task on four languages \u2013 Russian, French, Arabic, and Chinese. Our embeddings and datasets are publicly available. [Cite_Footnote_1]"
  },
  {
    "id": 185,
    "name": "Crowd-Flower",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://crowdflower.com",
    "section_title": "4 Crowd-Sourcing Annotations",
    "add_info": "4 http://crowdflower.com",
    "text": "There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [Cite_Footnote_4] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English."
  },
  {
    "id": 186,
    "name": "OPUS",
    "fullname": "N/A",
    "genericmention": [
      "this paral-lel data"
    ],
    "description": [
      "open-source repository of parallel corpora"
    ],
    "citationtag": [
      "Tiedemann, 2012"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://opus.nlpl.eu/",
    "section_title": "2 A FRO MT benchmark 2.2 Data Sources",
    "add_info": "3 http://opus.nlpl.eu/",
    "text": "For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [Cite_Footnote_3] (Tiedemann, 2012) and (2) ParaCrawl (Espl\u00e0 et al., 2019). From OPUS, we use the JW300 corpus (Agic\u0301 and Vulic\u0301, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014). Despite the existence of this paral-lel data, these text datasets were often collected from large, relatively unclean multilingual corpora, e.g. JW300 which was extracted from Jehovah\u2019s Witnesses text, or QED which was extracted from transcribed educational videos. This leads to many sentences with high lexical overlap, inconsistent tokenization, and other undesirable properties for a clean, reproducible benchmark."
  },
  {
    "id": 187,
    "name": "Moses (Koehn et al., 2007) toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Koehn et al., 2007"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/",
    "section_title": "2 A FRO MT benchmark 2.3 Data Preparation",
    "add_info": "5 https://github.com/moses-smt/mosesdecoder/",
    "text": "Tokenization normalization We perform detok-enization on all corpora using the detokenization script provided in the Moses (Koehn et al., 2007) toolkit [Cite_Footnote_5] . Given that we collect data from various sources, this step is important to allow for consis-tent tokenization across corpora."
  },
  {
    "id": 188,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Levenshtein-based fuzzy string matching"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/maxbachmann/RapidFuzz",
    "section_title": "2 A FRO MT benchmark 2.3 Data Preparation",
    "add_info": "6 https://github.com/maxbachmann/RapidFuzz pseudo monolingual data and dictionaries",
    "text": "Removal of sentences with high text overlap To prevent data leakage, we remove sentences with high text overlap. To do this, we use Levenshtein-based fuzzy string matching [Cite_Footnote_6] and remove sentences that have a similarity score of over 60. Given that measuring this score against all sentences in a corpus grows quadratically with respect to cor-pus length, we use the following two heuristics to remove sentences with high overlap in an effi-cient manner: (1) scoring similarity between the 50 alphabetically-sorted previous sentences, (2): ex-tracting the top 100K four-grams and performing the similarity score within each group of sentences containing at least one instance of a certain four-gram."
  },
  {
    "id": 189,
    "name": "eflomal",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a statistical word aligner"
    ],
    "citationtag": [
      "\u00d6stling and Tiedemann, 2016"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/robertostling/eflomal/",
    "section_title": "3 AfroBART 3.2 Dictionary Augmentation",
    "add_info": "7 https://github.com/robertostling/eflomal/",
    "text": "Dictionary Extraction As our data augmenta-tion technique requires a dictionary, we propose to extract the dictionary from parallel corpora using a statistical word aligner, eflomal [Cite_Footnote_7] (\u00d6stling and Tiedemann, 2016). Once we produce word align-ments between tokens in our parallel corpora, we simply take word alignments that appear over 20 times to produce our bilingual dictionary."
  },
  {
    "id": 190,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "pseudo-monolingual data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://data.statmt.org/cc-100/",
    "section_title": "4 Experimental Setup 4.1 Pretraining",
    "add_info": "10 http://data.statmt.org/cc-100/",
    "text": "Hyperparameters We use the following setup to train our AfroBART models, utilizing the mBART implementation in the fairseq library (Ott et al., 2019). We tokenize data using Sentence-Piece (Kudo and Richardson, 2018), using a 80K subword vocabulary. We use the Transformer-base architecture of a hidden dimension of 512, feed-forward size of 2048, and 6 layers for both the encoder and decoder. We set the maximum se-quence length to be 512, using a batch size of 1024 for 100K iterations with 32 NVIDIA V100 GPUs for one day. When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 \u00d7 [Cite_Footnote_10] \u22125 and warm up over 5K iterations and train for 35K iterations."
  },
  {
    "id": 191,
    "name": "mBART implementation",
    "fullname": "N/A",
    "genericmention": [
      "data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ott et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/pytorch/fairseq",
    "section_title": "4 Experimental Setup 4.1 Pretraining",
    "add_info": "12 https://github.com/pytorch/fairseq",
    "text": "Hyperparameters We use the following setup to train our AfroBART models, utilizing the mBART implementation in the fairseq [Cite_Footnote_12] library (Ott et al., 2019). We tokenize data using Sentence-Piece (Kudo and Richardson, 2018), using a 80K subword vocabulary. We use the Transformer-base architecture of a hidden dimension of 512, feed-forward size of 2048, and 6 layers for both the encoder and decoder. We set the maximum se-quence length to be 512, using a batch size of 1024 for 100K iterations with 32 NVIDIA V100 GPUs for one day. When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 \u00d7 \u22125 and warm up over 5K iterations and train for 35K iterations."
  },
  {
    "id": 192,
    "name": "SacreBLEU library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Post, 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/mjpost/sacrebleu",
    "section_title": "4 Experimental Setup 4.2 Finetuning",
    "add_info": "13 https://github.com/mjpost/sacrebleu",
    "text": "Evaluation We evaluate our system outputs us-ing two automatic evaluation metrics: detokenized BLEU (Papineni et al., 2002; Post, 2018) and chrF (Popovic\u0301, 2015). Although BLEU is a standard metric for machine translation, being cognizant of the morphological richness of the languages in the A FRO MT benchmark, we use chrF to measure per-formance at a character level. Both metrics are measured using the SacreBLEU library [Cite_Footnote_13] (Post, 2018)."
  },
  {
    "id": 193,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an exist-ing English POS tagger in the spaCy [Cite_Footnote_14] library"
    ],
    "description": [
      "English POS tagger"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "5 Results and Discussion 5.3 Fine-grained Language Analysis",
    "add_info": "14 https://spacy.io/",
    "text": "We further provide a suite of fine-grained analysis tools to compare the baseline systems. In partic-ular, we are interested in evaluating the transla-tion accuracy of noun classes in the considered African languages in the Niger-Congo family, as these languages are morphologically rich and of-ten have more than 10 classes based on the prefix of the word. For example, kitabu and vitabu in Swahili refer to book and books in English, respec-tively. Based on this language characteristic, our fine-grained analysis tool calculates the translation accuracy of the nouns with the top 10 most fre-quent prefixes in the test data. To do so, one of the challenges is to identify nouns in a sentence written in the target African language. However, there is no available part-of-speech (POS) tagger for these languages. To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy [Cite_Footnote_14] library to annotate the English source sentences. We then use the fast_align tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language. We then measure the transla-tion accuracy of the nouns in the African language by checking whether the correct nouns are included in the translated sentences by systems in compar-ison. Notably, our analysis tool can also measure the translation accuracy of the words in the other POS tags, (e.g. verbs, adjectives) which are often adjusted with different noun classes."
  },
  {
    "id": 194,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the fast_align [Cite_Footnote_15] tool"
    ],
    "description": [
      "fast_align [Cite_Footnote_15] tool"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/clab/fast_align",
    "section_title": "5 Results and Discussion 5.3 Fine-grained Language Analysis",
    "add_info": "15 https://github.com/clab/fast_align",
    "text": "We further provide a suite of fine-grained analysis tools to compare the baseline systems. In partic-ular, we are interested in evaluating the transla-tion accuracy of noun classes in the considered African languages in the Niger-Congo family, as these languages are morphologically rich and of-ten have more than 10 classes based on the prefix of the word. For example, kitabu and vitabu in Swahili refer to book and books in English, respec-tively. Based on this language characteristic, our fine-grained analysis tool calculates the translation accuracy of the nouns with the top 10 most fre-quent prefixes in the test data. To do so, one of the challenges is to identify nouns in a sentence written in the target African language. However, there is no available part-of-speech (POS) tagger for these languages. To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [Cite_Footnote_15] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language. We then measure the transla-tion accuracy of the nouns in the African language by checking whether the correct nouns are included in the translated sentences by systems in compar-ison. Notably, our analysis tool can also measure the translation accuracy of the words in the other POS tags, (e.g. verbs, adjectives) which are often adjusted with different noun classes."
  },
  {
    "id": 195,
    "name": "DBpedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://wiki.dbpedia.org/Downloads2015-04",
    "section_title": "4 Evaluation 4.3 End-to-End Evaluation",
    "add_info": "4 We used DBpedia long abstract: http://wiki.dbpedia.org/Downloads2015-04.",
    "text": "\u2022 TF*IDF weighting: This simple heuristic was introduced by Luhn (1958). Each sen-tence receives a score from the TF*IDF of its terms. We trained IDFs (Inverse Document Frequencies) on a background corpus [Cite_Footnote_4] to im-prove the original algorithm."
  },
  {
    "id": 196,
    "name": "sumy package",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/miso-belica/sumy",
    "section_title": "4 Evaluation 4.3 End-to-End Evaluation",
    "add_info": "5 https://github.com/miso-belica/sumy",
    "text": "the cosine similarity between them is above a given threshold. Sentences are scored ac-cording to their PageRank score in G. For our experiments, we use the implementation available in the sumy package. [Cite_Footnote_5]"
  },
  {
    "id": 197,
    "name": "SFOUR",
    "fullname": "N/A",
    "genericmention": [
      "the publicly available imple-mentation"
    ],
    "description": [
      "SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.cs.cornell.edu/\u02dcrs/sfour/",
    "section_title": "4 Evaluation 4.3 End-to-End Evaluation",
    "add_info": "6 http://www.cs.cornell.edu/\u02dcrs/sfour/",
    "text": "\u2022 SFOUR: SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE (Sipos et al., 2012). We use the publicly available imple-mentation. [Cite_Footnote_6]"
  },
  {
    "id": 198,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a real-world news recommendation dataset",
      "this dataset",
      "The news data in the last week",
      "the rest"
    ],
    "description": [
      "a real-world news recommendation dataset [Cite_Footnote_3] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/wuch15/NRHUB",
    "section_title": "4 Experiments 4.1 Datasets and Experimental Settings",
    "add_info": "3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.",
    "text": "We conducted experiments on a real-world news recommendation dataset [Cite_Footnote_3] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine. The detailed statistics of this dataset are summarized in Table 1. The news data in the last week is used for test, and the rest is used for model training. In addition, we randomly sampled 10% of the training data for validation."
  },
  {
    "id": 199,
    "name": "MSN News [Cite_Footnote_4] logs",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "MSN News [Cite_Footnote_4] logs during Dec. 13, 2018 and Jan. 12, 2019"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.msn.com/en-us/news",
    "section_title": "4 Experiments 4.1 Datasets and Experimental Settings",
    "add_info": "4 https://www.msn.com/en-us/news",
    "text": "We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_4] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine. The detailed statistics of this dataset are summarized in Table 1. The news data in the last week is used for test, and the rest is used for model training. In addition, we randomly sampled 10% of the training data for validation."
  },
  {
    "id": 200,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/syxu828/CSRL_dataset",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "In addition, we introduce a multi-task learning method with two new objectives. Experimental re-sults on benchmark datasets show that our model substantially outperforms existing baselines. Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at  https://github.com/syxu828/CSRL_dataset."
  },
  {
    "id": 201,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the dialogue act transition ta-ble"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html",
    "section_title": "4 Training on Speech Acts",
    "add_info": "1 Due to space constraints, the dialogue act transition ta-ble has been omitted from this paper and is made available at http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html",
    "text": "The use of P(d) in Equation 3 assumes that dia-logue acts are independent of one another. However, we intuitively know that if someone asks a Y ES -N O - Q UESTION then the response is more likely to be a Y ES -A NSWER rather than, say, C ONVENTIONAL - C LOSING . This intuition is reflected in the bigram transition probabilities obtained from our corpus. [Cite_Footnote_1]"
  },
  {
    "id": 202,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our dataset",
      "our dataset",
      "Our dataset"
    ],
    "description": [
      "the first benchmark for direct linguis-tic sense making and explanation"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/wangcunxiang/Sen-Making-and-Explanation",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Note that there has been dataset which fo-cus on non-linguistic world knowledge plausibil-ity (Wang et al., 2018) or only limited attributes or actions of physical knowledge like verbphysics (Forbes and Choi, 2017). They are related to our dataset but serve robotic research mainly. Our dataset is the first benchmark for direct linguis-tic sense making and explanation. We hope this benchmark can promote commonsense reason-ing by the NLP community, and further applied on other applications such as machine transla-tion and dialogue. Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at  https://github.com/wangcunxiang/Sen-Making-and-Explanation."
  },
  {
    "id": 203,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code of CLINE"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/kandorm/CLINE",
    "section_title": "1 Introduction",
    "add_info": "1 The source code of CLINE will be publicly available at https://github.com/kandorm/CLINE",
    "text": "To train a robust semantic-aware PLM, we pro-pose Contrastive Learning with semantIc Negative Examples (CLINE). CLINE is a simple and effec-tive method to generate adversarial and contrastive examples and contrastively learn from both of them. The contrastive manner has shown effectiveness in learning sentence representations (Luo et al., 2020; Wu et al., 2020; Gao et al., 2021), yet these studies neglect the generation of negative instances. In CLINE, we use external semantic knowledge, i.e., WordNet (Miller, 1995), to generate adversarial and contrastive examples by unsupervised replac-ing few specific representative tokens. Equipped by replaced token detection and contrastive objec-tives, our method gathers similar sentences with semblable semantics and disperse ones with differ-ent even opposite semantics, simultaneously im-proving the robustness and semantic sensitivity of PLMs. We conduct extensive experiments on sev-eral widely used text classification benchmarks to verify the effectiveness of CLINE. To be more spe-cific, our model achieves +1.6% absolute improve-ment on 4 contrastive test sets and +0.5% absolute improvement on 4 adversarial test sets compared to RoBERTa model (Liu et al., 2019). That is, with the training on the proposed objectives, CLINE si-multaneously gains the robustness of adversarial attacks and sensitivity of semantic changes [Cite_Footnote_1] ."
  },
  {
    "id": 204,
    "name": "spaCy",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "to conduct segmentation and POS for the original sentences, extracting verbs, nouns, adjectives, and adverbs"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/explosion/spaCy",
    "section_title": "3 Method 3.1 Generation of Examples",
    "add_info": "2 https://github.com/explosion/spaCy",
    "text": "We generate two sentences from the original in-put sequence x ori , which express substantially dif-ferent semantics but have few different words. One of the sentences is semantically close to x ori (de-noted as x syn ), while the other is far from or even opposite to x ori (denoted as x ant ). In specific, we utilize spaCy [Cite_Footnote_2] to conduct segmentation and POS for the original sentences, extracting verbs, nouns, adjectives, and adverbs. x syn is generated by re-placing the extracted words with synonyms, hy-pernyms and morphological changes, and x ant is generated by replacing them with antonyms and random words. For x syn , about 40% tokens are replaced. For x ant , about 20% tokens are replaced."
  },
  {
    "id": 205,
    "name": "Contrast Sets",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/allenai/contrast-sets",
    "section_title": "4 Experiments 4.3 Experiments on Contrastive Sets",
    "add_info": "3 https://github.com/allenai/contrast-sets",
    "text": "We evaluate our model on four contrastive sets: IMDB, PERSPECTRUM, BoolQ and SNLI, which were provided by Contrast Sets [Cite_Footnote_3] (Gardner et al., 2020). We compare our approach with BERT and"
  },
  {
    "id": 206,
    "name": "phrase2vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/artetxem/phrase2vec",
    "section_title": "3 Principled unsupervised SMT 3.1 Initial phrase-table",
    "add_info": "1 https://github.com/artetxem/phrase2vec",
    "text": "More concretely, we train our n-gram embed-dings using phrase2vec [Cite_Footnote_1] , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature \u03c4 is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction. In addition to the phrase translation probabilities in both direc-tions, the forward and reverse lexical weightings are also estimated by aligning each word in the tar-get phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. The reader is referred to Artetxe et al. (2018b) for more details."
  },
  {
    "id": 207,
    "name": "VecMap",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/artetxem/vecmap",
    "section_title": "3 Principled unsupervised SMT 3.1 Initial phrase-table",
    "add_info": "3 https://github.com/artetxem/vecmap",
    "text": "More concretely, we train our n-gram embed-dings using phrase2vec , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap [Cite_Footnote_3] with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature \u03c4 is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction. In addition to the phrase translation probabilities in both direc-tions, the forward and reverse lexical weightings are also estimated by aligning each word in the tar-get phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. The reader is referred to Artetxe et al. (2018b) for more details."
  },
  {
    "id": 208,
    "name": "Moses",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "http://www.statmt.org/moses/",
    "section_title": "5 Experiments and results",
    "add_info": "10 http://www.statmt.org/moses/",
    "text": "Our SMT implementation is based on Moses [Cite_Footnote_10] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018)."
  },
  {
    "id": 209,
    "name": "fairseq",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq",
    "section_title": "5 Experiments and results",
    "add_info": "11 https://github.com/pytorch/fairseq",
    "text": "Our SMT implementation is based on Moses , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq [Cite_Footnote_11] for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018)."
  },
  {
    "id": 210,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/artetxem/monoses",
    "section_title": "6 Conclusions and future work",
    "add_info": null,
    "text": "In this paper, we identify several deficiencies in previous unsupervised SMT systems, and pro-pose a more principled approach that addresses them by incorporating subword information, us-ing a theoretically well founded unsupervised tun-ing method, and developing a joint refinement pro-cedure. In addition to that, we use our improved SMT approach to initialize a dual NMT model that is further improved through on-the-fly back-translation. Our experiments show the effective-ness of our approach, as we improve the previous state-of-the-art in unsupervised machine transla-tion by 5-7 BLEU points in French-English and German-English WMT 2014 and 2016. Our code is available as an open source project at  https://github.com/artetxem/monoses."
  },
  {
    "id": 211,
    "name": "JUMAN",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html",
    "section_title": "2 Test Set for Evaluating Machine Translation Quality Translation Quality",
    "add_info": "4 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html",
    "text": "The term S i indicates a similarity between a trans-lated sentence and its reference translation, and \u03bb S i is a weight for the similarity. Many methods for cal-culating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gimen\u0301ez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (ex-act) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S\u2217, SU\u2217, W-1.2), were used to cal-culate each similarity S i . Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN [Cite_Footnote_4] in our experiments."
  },
  {
    "id": 212,
    "name": "Mecab",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://sourceforge.net/projects/mecab/files/",
    "section_title": "5 Experiments 5.1 Setting",
    "add_info": "4 http://sourceforge.net/projects/mecab/files/",
    "text": "We evaluated the effectiveness of the proposed ap-proach for Chinese-to-English (CE), Japanese-to- English (JE) and French-to-English (FE) transla-tion tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [Cite_Footnote_4] for Japanese. For the FE language pair, we used stan-dard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively."
  },
  {
    "id": 213,
    "name": "IRSTLM Toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://hlt.fbk.eu/en/irstlm",
    "section_title": "5 Experiments 5.1 Setting",
    "add_info": "5 http://hlt.fbk.eu/en/irstlm",
    "text": "For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [Cite_Footnote_5] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003)."
  },
  {
    "id": 214,
    "name": "Toronto Book Corpus",
    "fullname": "N/A",
    "genericmention": [
      "The corpus",
      "This corpus"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.toronto.edu/\u02dcmbweb/",
    "section_title": "3 Experimental Setup 3.1 Data",
    "add_info": "1 The corpus can be downloaded from http://www.cs.toronto.edu/\u02dcmbweb/;cf. (Zhu et al., 2015).",
    "text": "We use the Toronto Book Corpus [Cite_Footnote_1] to train word embeddings. This corpus contains 74,004,228 already pre-processed sentences in total, which are made up of 1,057,070,918 tokens, originating from 7,087 unique books. In our experiments, we consider tokens appearing 5 times or more, which leads to a vocabulary of 315,643 words."
  },
  {
    "id": 215,
    "name": "word2vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "3 Experimental Setup 3.2 Baselines",
    "add_info": "2 The code is available from https://code.google.com/archive/p/word2vec/.",
    "text": "We employ two baselines for producing sentence embeddings in our experiments. We obtain simi-larity scores between sentence pairs from the base-lines in the same way as the ones produced by Siamese CBOW, i.e., we calculate the cosine sim-ilarity between the sentence embeddings they pro-duce. Word2vec We average word embeddings trained with word2vec. [Cite_Footnote_2] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples."
  },
  {
    "id": 216,
    "name": "skip-thought architecture",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a recently proposed method that learns sentence representations in a different way from ours, by using recurrent neural networks",
      "This al-lows it to take word order into account."
    ],
    "citationtag": [
      "Kiros et al., 2015"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ryankiros/skip-thoughts/",
    "section_title": "3 Experimental Setup 3.2 Baselines",
    "add_info": "3 The code and the trained models can be down-loaded from https://github.com/ryankiros/skip-thoughts/.",
    "text": "Skip-thought As a second baseline we use the sentence representations produced by the skip-thought architecture (Kiros et al., 2015). [Cite_Footnote_3] Skip-thought is a recently proposed method that learns sentence representations in a different way from ours, by using recurrent neural networks. This al-lows it to take word order into account. As it trains sentence embeddings from unlabeled data, like we do, it is a natural baseline to consider."
  },
  {
    "id": 217,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code for Siamese CBOW"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://bitbucket.org/TomKenter/siamese-cbow",
    "section_title": "3 Experimental Setup 3.4 Network",
    "add_info": "4 The code for Siamese CBOW is available under an open-source license at https://bitbucket.org/TomKenter/siamese-cbow.",
    "text": "We use Theano (Theano Development Team, 2016) to implement our network. [Cite_Footnote_4] We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al., 2016)."
  },
  {
    "id": 218,
    "name": "Twit-ter streaming API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://dev.twitter.com/streaming/",
    "section_title": "1 Introduction",
    "add_info": "1 https://dev.twitter.com/streaming/overview. Last accessed on 10-01-2018.",
    "text": "In this paper, we first develop a temporal-orientation classifier to classify tweets into past, present, and future and then group over the users to create user-level assessments. We use a Bidi-rectional Long Short Term Memory (Bi-LSTM) network for tweet temporal classification where tweet vectors are fed to generate the classifica-tion model. We propose a hash tag-based mini-mally supervised method with the two-pass filter-ing to create the past, present and future-oriented tweets for the training of the Bi-LSTM network. We manually examined trending hashtags in Twit-ter for a specific period of time and selected hash-tags which represent past, present/ongoing, or fu-ture events. The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [Cite_Footnote_1] The tweet temporal orienta-tion classifier is validated on a manually annotated test set. Finally, we use this classifier to automat-ically classify a large dataset consisting of \u224810 million tweets from 5,191 users mapped to their user-level features."
  },
  {
    "id": 219,
    "name": "Glove vectors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pennington et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Methodology 3.1 Temporal Orientation Classification",
    "add_info": "2 https://nlp.stanford.edu/projects/glove/",
    "text": "Our experiment uses Bi-LSTM with 200 neu-rons at the input layer. The loss function we used is categorical cross-entropy and the opti-mizer used is Root Mean Square Propagation (rm-sprop). We repeat the training for 100 number of epochs with batch size set to 128. We also employ dropout (Srivastava et al., 2014) for reg-ularization with a dropout rate of 0.2 to prevent over-fitting. All of these attributes are finalized by parameter tuning with the performance obtained on 10-fold cross-validation using the grid search method. Tweet vectors are generated by existing Glove vectors (Pennington et al., 2014) for tweets [Cite_Footnote_2] of 200 dimensions which are trained on 27 billion tweets. We also validate our model on the valida-tion set which was 10% of the training set."
  },
  {
    "id": 220,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All the developed resources",
      "the datasets",
      "Train-ing set",
      "the test set",
      "The user-level tweets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.iitp.ac.in/\u02dcai-nlp-ml/resources.html",
    "section_title": "4 Data Sets",
    "add_info": "4 All the developed resources are available at http://www.iitp.ac.in/\u02dcai-nlp-ml/resources.html",
    "text": "For experiments we categorize the datasets into three kinds: training, test and user-level. Train-ing set consists of 27k tweets, whereas the test set is manually annotated with 741 tweets. [Cite_Footnote_4] The user-level tweets consist of \u224810 million tweets from 5,191 users mapped to their user-level features."
  },
  {
    "id": 221,
    "name": "Twitter streaming API",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://developer.twitter.com/en/docs",
    "section_title": "4 Data Sets 4.1 Training Set",
    "add_info": "5 https://developer.twitter.com/en/docs",
    "text": "Training tweets are collected using the Twitter streaming API. [Cite_Footnote_5] The tweets are collected for the duration of September 2017 and October 2017. We consider day-wise trending topics during this period. We only consider those hashtags which signify a temporal event. Finally, we chose world-wide trending events and collected the tweets based on the hashtags."
  },
  {
    "id": 222,
    "name": "GoogleSets",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "the North American Chapter of the ACL, pages 290\u2013298,"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://labs.google.com/sets",
    "section_title": "1 Introduction",
    "add_info": "1 http://labs.google.com/sets the North American Chapter of the ACL, pages 290\u2013298,",
    "text": "Even for state of the art methods, expansion er-rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art systems such as GoogleSets [Cite_Footnote_1] , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed instances for the target set Roman Gods:"
  },
  {
    "id": 223,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the distributional thesaurus"
    ],
    "description": [
      "a distributional similarity thesaurus"
    ],
    "citationtag": [
      "Lin 1998"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://demo.patrickpantel.com/",
    "section_title": "1 Introduction",
    "add_info": "2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus.",
    "text": "The inherent semantic similarity between the errors can be leveraged to quickly clean up the expan-sion. For example, given a manually tagged error \u201casteroid\u201d, a distributional similarity thesaurus such as (Lin 1998) [Cite_Footnote_2] can identify comet as similar to asteroid and therefore potentially also as an error. This method has its limitations since a manually tagged error such as Earth would correctly remove Moon and Sun, but it would also incorrectly re-move Mars, Venus and Jupiter since they are also similar to Earth ."
  },
  {
    "id": 224,
    "name": "PKPB",
    "fullname": "Penn Korean PropBank",
    "genericmention": [
      "the two Korean corpora"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "http://catalog.ldc.upenn.edu/LDC2006T03",
    "section_title": "2 A Semantically Annotated Korean Corpus",
    "add_info": "2 http://catalog.ldc.upenn.edu/LDC2006T03",
    "text": "We view our work as building on the efforts of the Penn Korean PropBank (PKPB). [Cite_Footnote_2] Our corpus is roughly similar in size to the PKPB, and taken together, the two Korean corpora now total about half the size of the Penn English PropBank. One advantage of our corpus is that it is built on top of the ETRI Korean corpus, which uses a richer Ko-rean morphological tagging scheme than the Penn Korean Treebank. Our experiments will show that these finer-grained tags are crucial for achieving high SRL accuracy."
  },
  {
    "id": 225,
    "name": "Donga news article corpus",
    "fullname": "N/A",
    "genericmention": [
      "this corpus"
    ],
    "description": [
      "The Donga cor-pus contains 366,636 sentences with 25.09 words on average.",
      "The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.donga.com",
    "section_title": "6 Experiments and Results",
    "add_info": "3 http://www.donga.com",
    "text": "For latent morpheme representations, we used the Donga news article corpus. [Cite_Footnote_3] The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions."
  },
  {
    "id": 226,
    "name": "Kokoma Korean morpheme analyzer",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Korean morpheme analyzer"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://kkma.snu.ac.kr/",
    "section_title": "6 Experiments and Results",
    "add_info": "4 http://kkma.snu.ac.kr/",
    "text": "For latent morpheme representations, we used the Donga news article corpus. The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer [Cite_Footnote_4] on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions."
  },
  {
    "id": 227,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "he col-lection",
      "this dataset"
    ],
    "description": [
      "Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set.",
      "We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/cxsoto/article-regions",
    "section_title": "4 Novel Labeled Dataset",
    "add_info": null,
    "text": "Therefore, a novel dataset was created. We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015) to manually annotate regions. Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set. The col-lection will be available at  https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats. The default format is PASCAL VOC. Nine labeled region classes are included in the annotations:"
  },
  {
    "id": 228,
    "name": "YOLOv3",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Erik Linder-Noren. 2018. A mini-mal pytorch implementation of yolov3.",
      "Linder-Noren, 2018"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/eriklindernoren/PyTorch-YOLOv3",
    "section_title": "5 Implementation and Experiments",
    "add_info": "Erik Linder-Noren. 2018. A mini-mal pytorch implementation of yolov3. https://github.com/eriklindernoren/PyTorch-YOLOv3.",
    "text": "Figure 4 shows per-class performance over 30 training epochs, as well as comparative per-formance against the baseline Faster R-CNN model and reference model implementations of YOLOv3 (Linder-Noren, 2018)  and RetinaNet (Henon, 2018). Most models plateaued early on this small dataset, except YOLOv3 which peak-ing at 68.9% after 49 epochs (beyond the figure bounds, but still below our model\u2019s results). Pro-cessing time for our model averaged 0.65 seconds per article. By contrast, CERMINE averaged 9.4 seconds per article on the same set of articles, on the same machine."
  },
  {
    "id": 229,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an open source utility"
    ],
    "description": [
      "an open source utility (Tzutalin, 2015) [Cite_Ref] to manually annotate regions"
    ],
    "citationtag": [
      "Tzutalin. 2015.",
      "Tzutalin, 2015"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/tzutalin/labelImg",
    "section_title": "4 Novel Labeled Dataset",
    "add_info": "Tzutalin. 2015. Labelimg. https://github.com/tzutalin/labelImg.",
    "text": "Therefore, a novel dataset was created. We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015)  to manually annotate regions. Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set. The col-lection will be available at https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats. The default format is PASCAL VOC. Nine labeled region classes are included in the annotations:"
  },
  {
    "id": 230,
    "name": "Faster R-CNN implementation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh. 2017. A faster pytorch implementation of faster r-cnn.",
      "Yang et al., 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/jwyang/faster-rcnn.pytorch",
    "section_title": "5 Implementation and Experiments",
    "add_info": "Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh. 2017. A faster pytorch implementation of faster r-cnn. https://github.com/jwyang/faster-rcnn.pytorch.",
    "text": "Using the novel labeled dataset described in Sec-tion 4, a baseline model was trained using a stan-dard Faster R-CNN implementation (Yang et al., 2017)  . The model was trained using a single NVIDIA P100 GPU for 30 epochs on 600 images, and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pre-trained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0.0001, with decay of 0.1 every 5 epochs. Stan-dard anchor scales of [8, 16, 32] and anchor ratios of [0.5, 1.0, 2.0] were used. At a intersection-over-union (IOU) threshold of 0.5, the model achieved a mean average precision (mAP) of 46.38% on all nine region labels, with peak class performance on \u2018body\u2019 regions (87.49%) and lowest performance on \u2018authors\u2019 (1.22%)."
  },
  {
    "id": 231,
    "name": "NeuronBlocks",
    "fullname": "N/A",
    "genericmention": [
      "This toolkit"
    ],
    "description": [
      "a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture",
      "This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/Microsoft/NeuronBlocks",
    "section_title": "References",
    "add_info": "1 Code: https://github.com/Microsoft/NeuronBlocks",
    "text": "Deep Neural Networks (DNN) have been widely employed in industry to address vari-ous Natural Language Processing (NLP) tasks. However, many engineers find it a big over-head when they have to choose from multi-ple frameworks, compare different types of models, and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [Cite_Footnote_1] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effective-ness of NeuronBlocks."
  },
  {
    "id": 232,
    "name": "NeuronBlocks",
    "fullname": "N/A",
    "genericmention": [
      "This toolkit"
    ],
    "description": [
      " toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture",
      "This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://youtu.be/x6cOpVSZcdo",
    "section_title": "References",
    "add_info": "2 Demo: https://youtu.be/x6cOpVSZcdo",
    "text": "Deep Neural Networks (DNN) have been widely employed in industry to address vari-ous Natural Language Processing (NLP) tasks. However, many engineers find it a big over-head when they have to choose from multi-ple frameworks, compare different types of models, and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [Cite_Footnote_2] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effective-ness of NeuronBlocks."
  },
  {
    "id": 233,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Yupei-Du/bias-in-wat",
    "section_title": "References",
    "add_info": "1 Our code is publicly available at https://github.com/Yupei-Du/bias-in-wat.",
    "text": "Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really re-flect true bias levels? For a small set of words (e.g. occupations), we can rely on hu-man annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which con-tains rich types of word connections anno-tated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, compar-ing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words. [Cite_Footnote_1]"
  },
  {
    "id": 234,
    "name": "TDT document clusters",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "TDT document clusters for 2 instances of airplane crashes, [Cite_Footnote_3] instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://news.bbc.co.uk/shared/bsp/search2/advanced/news_ifs.stm",
    "section_title": "4 Data Description 4.2 Test Data",
    "add_info": "3 http://news.bbc.co.uk/shared/bsp/search2/advanced/news_ifs.stm",
    "text": "To test our system, we used document clusters from the Topic Detection and Tracking (TDT) cor-pus (Fiscus et al., 1999). Each TDT topic has a topic label, such as Accidents or Natural Disas-ters. 4 These categories are broader than our do-mains. Thus, we manually filtered the TDT topics relevant to our four training domains (e.g., Acci-dents matching Airplane Crashes). In this way, we obtained TDT document clusters for 2 instances of airplane crashes, [Cite_Footnote_3] instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrorist attacks). This variation in the number of documents per topic is typical for the TDT corpus. Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents."
  },
  {
    "id": 235,
    "name": "FREQuent Tree miner",
    "fullname": "N/A",
    "genericmention": [
      "This software"
    ],
    "description": [
      "This software is an implementation of the algorithm presented by (Abe et al., 2002; Zaki, 2002), which extracts frequent ordered subtrees from a set of ordered trees."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.org/\u02dctaku/software/freqt/",
    "section_title": "5 Creating Templates",
    "add_info": "5 http://chasen.org/\u02dctaku/software/freqt/",
    "text": "Step 3: Identify most frequent subtrees containing the top 50 verbs. A domain template should con-tain not only the most important actions for the do-main, but also the entities that are linked to these actions or to each other through these actions. The lexemes referring to such entities can potentially be used within the domain template slots. Thus, we analyze those portions of the syntactic trees which contain the verbs themselves plus other lex-emes used in the same subtrees as the verbs. To do this we use FREQuent Tree miner. [Cite_Footnote_5] This software is an implementation of the algorithm presented by (Abe et al., 2002; Zaki, 2002), which extracts frequent ordered subtrees from a set of ordered trees. Following (Sudo et al., 2003) we are inter-ested only in the lexemes which are near neighbors of the most frequent verbs. Thus, we look only for those subtrees which contain the verbs themselves and from four to ten tree nodes, where a node is either a syntactic tag or a lexeme with its tag. We analyze not only NPs which correspond to the sub-ject or object of the verb, but other syntactic con-stituents as well. For example, PPs can potentially link the verb to locations or dates, and we want to include this information into the template. Table 1 contains a sample of subtrees for the terrorist at-tack domain mined from the sentences containing the verb killed. The first column of Table 1 shows how many nodes are in the subtree."
  },
  {
    "id": 236,
    "name": "subreddits WRITINGPROMPTS",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.reddit.com/r/WritingPrompts/",
    "section_title": "2 SCOPE: Style Transfer through COmmonsense PropErty 2.1 Automatic Parallel Corpus Creation",
    "add_info": "5 https://www.reddit.com/r/WritingPrompts/",
    "text": "Simile Dataset Collection. One of the possible ways to collect similes would be to train a super-vised model using existing data and methods for simile detection but most data sets are very small in size (in the order of a few hundreds). The only large-scale dataset is that of (Niculae and Danescu-Niculescu-Mizil, 2014), however their data is from a rather restricted domain of product reviews on Amazon, which might lack variety, diversity and creativity needed for this task. For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS [Cite_Footnote_5] and FUNNY from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, \u201cThe boy was as strong as an ox\u201d gives strong as the PROPERTY shared by the boy and ox. But most similes do not give an explicit PROP-ERTY such as the Open Simile (e.g., \u201cThe boy was like an ox\u201d) leaving the reader to infer that the boy is strong/large/fast (Qadir et al., 2016). Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io to mine comments. Through this process we collect 87,843 from Reddit. For each example, we show the top five commonsense properties associated with the vehicle obtained from COMET, and the best literal sentence constructed from these properties. The blue italic texts in the literal sentences represent the property inferred from the vehicle in the simile (denoted in black italic). self-labeled human written similes, from which we use 82,697 samples for training and 5,146 for validation."
  },
  {
    "id": 237,
    "name": "FUNNY",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.reddit.com/r/funny/",
    "section_title": "2 SCOPE: Style Transfer through COmmonsense PropErty 2.1 Automatic Parallel Corpus Creation",
    "add_info": "6 https://www.reddit.com/r/funny/",
    "text": "Simile Dataset Collection. One of the possible ways to collect similes would be to train a super-vised model using existing data and methods for simile detection but most data sets are very small in size (in the order of a few hundreds). The only large-scale dataset is that of (Niculae and Danescu-Niculescu-Mizil, 2014), however their data is from a rather restricted domain of product reviews on Amazon, which might lack variety, diversity and creativity needed for this task. For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS and FUNNY [Cite_Footnote_6] from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, \u201cThe boy was as strong as an ox\u201d gives strong as the PROPERTY shared by the boy and ox. But most similes do not give an explicit PROP-ERTY such as the Open Simile (e.g., \u201cThe boy was like an ox\u201d) leaving the reader to infer that the boy is strong/large/fast (Qadir et al., 2016). Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io to mine comments. Through this process we collect 87,843 from Reddit. For each example, we show the top five commonsense properties associated with the vehicle obtained from COMET, and the best literal sentence constructed from these properties. The blue italic texts in the literal sentences represent the property inferred from the vehicle in the simile (denoted in black italic). self-labeled human written similes, from which we use 82,697 samples for training and 5,146 for validation."
  },
  {
    "id": 238,
    "name": "COMET",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "COMET is an adaptation framework for constructing common-sense knowledge based on pre-trained language models."
    ],
    "citationtag": [
      "Bosselut et al., 2019"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://mosaickg.apps.allenai.org/comet_conceptnet",
    "section_title": "2 SCOPE: Style Transfer through COmmonsense PropErty 2.1 Automatic Parallel Corpus Creation",
    "add_info": "9 https://mosaickg.apps.allenai.org/comet_conceptnet",
    "text": "To generate the common sense PROPERTY that is implied by the VEHICLE in the simile, we take advantage of the simple syntactic structure of a simile. We extract the VEHICLE by extract-ing the phrase after like a and feed it as input to COMET (Bosselut et al., 2019). COMET is an adaptation framework for constructing common-sense knowledge based on pre-trained language models. Our work only leverages the HasProp-erty relation from COMET [Cite_Footnote_9] ."
  },
  {
    "id": 239,
    "name": "pre-trained COMET model",
    "fullname": "N/A",
    "genericmention": [
      "the model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/atcbosselut/comet-commonsense",
    "section_title": "References",
    "add_info": "11 https://github.com/atcbosselut/comet-commonsense",
    "text": "For retrieving commonsense properties of the vehi-cle, we use the pre-trained COMET model [Cite_Footnote_11] and retrieve top 5 candidates for each input. Vehicle and Overall Quality. WORKERS denote num-ber of workers employed for each task and \u03b1 denotes Krippendorff\u2019s alpha (\u03b1 ) , reliability coefficient used for our study scheme (Fan et al., 2018). At each timestep, the model generates the probability of each word in the vocabulary being the likely next word. We randomly sample from the k = 5 most likely candidates from this distribution. We also use a softmax temperature of 0.7."
  },
  {
    "id": 240,
    "name": "FAIRSEQ",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ott et al., 2019"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "section_title": "References",
    "add_info": "12 https://github.com/pytorch/fairseq/tree/master/examples/bart",
    "text": "1. Number of Parameters: For BART we use the BART large checkpoint (400M parame-ters) and use the implementation by FAIRSEQ (Ott et al., 2019). [Cite_Footnote_12]"
  },
  {
    "id": 241,
    "name": "SentiWordNet",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "The set of semantic orientation scores of all WordNet synsets"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://sentiwordnet.isti.cnr.it/",
    "section_title": "2 Related Work",
    "add_info": "2 http://sentiwordnet.isti.cnr.it/",
    "text": "Esuli and Sebastiani (2006) used a supervised algorithm to attach semantic orientation scores to WordNet glosses. They train a set of ternary clas-sifiers using different training data and learning methods. The set of semantic orientation scores of all WordNet synsets is released by the name SentiWordNet. [Cite_Footnote_2] An evaluation of SentiWordNet by comparing orientation scores of about 1,000 WordNet glosses to scores assigned by human an-notators is presented in Esuli (2008). Our ap-proach uses a Roget-like thesaurus, and it does not use any supervised classifiers."
  },
  {
    "id": 242,
    "name": "MPQA corpus",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "The MPQA corpus contains news articles man-ually annotated for opinions and private states.",
      "it also has polarity annotations (posi-tive/negative) at the phrase-level",
      "collection of positive and neg-ative phrases (1,726 positive and 4,485 negative)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cs.pitt.edu/mpqa",
    "section_title": "4 Evaluation 4.2 Extrinsic: Identifying phrase polarity",
    "add_info": "4 http://www.cs.pitt.edu/mpqa",
    "text": "The MPQA corpus contains news articles man-ually annotated for opinions and private states. [Cite_Footnote_4] Notably, it also has polarity annotations (posi-tive/negative) at the phrase-level. We conducted an extrinsic evaluation of the manually-generated and automatically-generated lexicons by using them to determine the polarity of phrases in the MPQA version 1.1 collection of positive and neg-ative phrases (1,726 positive and 4,485 negative)."
  },
  {
    "id": 243,
    "name": "NodeXL network analy-sis tool",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Smith et al., 2009"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.codeplex.com/NodeXL",
    "section_title": "6 Visualizing the semantic orientation of thesaurus categories",
    "add_info": "5 Available from http://www.codeplex.com/NodeXL",
    "text": "As discussed in Section 3.1.1, the affix seeds set connects the thesaurus words with opposite se-mantic orientation. Usually these pairs of words occur in different thesaurus categories, but this is not necessary. We can think of these connections as relationships of contrast in meaning and seman-tic orientation, not just between the two words but also between the two categories. To better aid our understanding of the automatically deter-mined category relationships we visualized this network using the Fruchterman-Reingold force-directed graph layout algorithm (Fruchterman and Reingold, 1991) and the NodeXL network analy-sis tool (Smith et al., 2009) [Cite_Footnote_5] ."
  },
  {
    "id": 244,
    "name": "L6 Yahoo! An-swers Comprehensive Questions and Answers cor-pus",
    "fullname": "N/A",
    "genericmention": [
      "This dataset",
      "this larger dataset",
      "the L6 dataset"
    ],
    "description": [
      "This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://webscope.sandbox.yahoo.com/",
    "section_title": "3 Experiments 3.1 Data",
    "add_info": "4 http://webscope.sandbox.yahoo.com/",
    "text": "Our approach requires unlabelled data for unsu-pervised pre-training of the word and paragraph vec-tors. For these purposes we use the L6 Yahoo! An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [Cite_Footnote_4] This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. The Yahoo! Answers man-ner question dataset prepared by Jansen et al. (2014) and described in the previous paragraph, was ini-tially sampled from this larger dataset. We want to emphasize that the L6 dataset is only used for unsu-pervised pretraining \u2013 no meta-information is used in our experiments."
  },
  {
    "id": 245,
    "name": "English Gigaword corpus,",
    "fullname": "N/A",
    "genericmention": [
      "this cor-pus"
    ],
    "description": [
      "the English Gigaword corpus, [Cite_Footnote_6] which contains data from several English newswire sources"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2003T05",
    "section_title": "3 Experiments 3.1 Data",
    "add_info": "6 https://catalog.ldc.upenn.edu/LDC2003T05",
    "text": "We also experiment with the English Gigaword corpus, [Cite_Footnote_6] which contains data from several English newswire sources. Jansen et al. (2014) used this cor-pus to train word embeddings, which were then in-cluded as features in their answer reranker."
  },
  {
    "id": 246,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The data",
      "the gensim [Cite_Footnote_7] implementation of the DBOW and DM paragraph vector models"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://radimrehurek.com/gensim/models/doc2vec.html",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "7 https://radimrehurek.com/gensim/models/doc2vec.html",
    "text": "We use the gensim [Cite_Footnote_7] implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. The data was tokenized with the Stan-ford tokenizer and then lowercased."
  },
  {
    "id": 247,
    "name": "word2vec",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "8 https://code.google.com/p/word2vec/",
    "text": "We use the gensim implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. [Cite_Footnote_8] The data was tokenized with the Stan-ford tokenizer and then lowercased."
  },
  {
    "id": 248,
    "name": "Stan-ford tokenizer",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/tokenizer.shtml",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "9 http://nlp.stanford.edu/software/tokenizer.shtml",
    "text": "We use the gensim implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. The data was tokenized with the Stan-ford tokenizer [Cite_Footnote_9] and then lowercased."
  },
  {
    "id": 249,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code of our model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Justin1904/Low-rank-Multimodal-Fusion",
    "section_title": "4 Experimental Methodology 4.3 Model Architecture",
    "add_info": "2 The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion",
    "text": "In order to compare our fusion method with previ-ous work, we adopt a simple and straightforward model architecture [Cite_Footnote_2] for extracting unimodal rep-resentations. Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as f a , f v , f l , to extract unimodal representations z a , z v , z l from uni-modal input features x a , x v , x l . For acoustic and visual modality, the sub-embedding network is a simple 2-layer feed-forward neural network, and for language modality, we used an LSTM (Hochre-iter and Schmidhuber, 1997) to extract represen-tations. The model architecture is illustrated in Figure 1."
  },
  {
    "id": 250,
    "name": "TextEssence",
    "fullname": "N/A",
    "genericmention": [
      "the sys-tem"
    ],
    "description": [
      "an interactive system designed to enable comparative analysis of cor-pora using embeddings",
      "TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://textessence.github.io",
    "section_title": "References",
    "add_info": null,
    "text": "Embeddings of words and concepts capture syntactic and semantic regularities of lan-guage; however, they have seen limited use as tools to study characteristics of different cor-pora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of cor-pora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for cor-pus analysis. A case study on COVID-19 sci-entific literature illustrates the utility of the sys-tem. TextEssence can be found at  https://textessence.github.io."
  },
  {
    "id": 251,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our implementation and experimental code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/drgriffis/",
    "section_title": "7 Conclusion",
    "add_info": null,
    "text": "TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at  https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io."
  },
  {
    "id": 252,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the database derived from our CORD-19 analysis"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://doi.org/10.5281/zenodo.4432958",
    "section_title": "7 Conclusion",
    "add_info": null,
    "text": "TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at  https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io."
  },
  {
    "id": 253,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The dataset, along with associated scripts"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://infotabs.github.io/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "The dataset, along with associated scripts, are avail-able at  https://infotabs.github.io/."
  },
  {
    "id": 254,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code and the processed datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/mponza/SalIE",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/mponza/SalIE",
    "text": "The source code and the processed datasets are publicly available [Cite_Footnote_1] to encourage further develop-ments of the fact salience task."
  },
  {
    "id": 255,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/zdou0830/DAFE",
    "section_title": "References",
    "add_info": "1 Our code is publicly available at: https://github.com/zdou0830/DAFE.",
    "text": "The recent success of neural machine transla-tion models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised do-main adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these meth-ods use generic representations for text regard-less of domain shift, which makes it infeasible for translation models to control outputs con-ditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental set-tings. In addition, we show that combining our method with back translation can further improve the performance of the model. [Cite_Footnote_1]"
  },
  {
    "id": 256,
    "name": "TED",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Kevin Duh. 2018. The multitarget ted talks task."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/",
    "section_title": "3 Experiments 3.1 Setup",
    "add_info": "Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/.",
    "text": "Datasets. We validate our models in two differ-ent data settings. First, we train on the law, medi-cal and IT datasets of the German-English OPUS corpus (Tiedemann, 2012) and test our methods\u2019 ability to adapt from one domain to another. The dataset contain 2K development and test sentences in each domain, and about 715K, 1M and 337K training sentences respectively. These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018)  and law, medical OPUS datasets. For this setting, we con-sider two language pairs, namely Czech and Ger-man to English. The Czech-English and German-English datasets consist of 1M and 4.5M sentences and the development and test sets contain about 2K sentences."
  },
  {
    "id": 257,
    "name": "ProofWiki",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "ProofWiki is an online compendium of mathemati-cal proofs, with a goal to collect and classify math-ematical proofs.",
      "ProofWiki contains links between theorems, definitions and axioms in the context of a mathematical proof, determining which dependen-cies are present."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://proofwiki.org/",
    "section_title": "3 The Natural Language Premise Selection task",
    "add_info": "1 http://proofwiki.org/",
    "text": "In order to evaluate the premise selection, we used a corpus extracted from ProofWiki [Cite_Footnote_1] . ProofWiki is an online compendium of mathemati-cal proofs, with a goal to collect and classify math-ematical proofs. ProofWiki contains links between theorems, definitions and axioms in the context of a mathematical proof, determining which dependen-cies are present. Definitions and axioms are state-ments accepted without formal proof, while theo-rems, lemmas and corollaries require one (Solow, 2002). All entries are composed by a statement written in a combination of natural language and mathematical latex notation. The extracted cor-pus, which is named PS-ProofWiki, contains more than 18, 000 entries. We also computed how many times each statement is used as a premise, and we observed that most of the statements are used as dependencies for only a small subset of premises. A total of 6, 866 statements has between one and three dependants. On average, statements contain a total length of 289 symbols (characters and math-ematical symbols). The specific number of tokens will depend on the type of tokenisation used for the mathematical symbols. A complete analysis of this corpus is made available in (Ferreira and Freitas, 2020)."
  },
  {
    "id": 258,
    "name": "Ubuntu dialogue corpus",
    "fullname": "N/A",
    "genericmention": [
      "The original training data",
      "The validation data",
      "the test data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator",
    "section_title": "5 Experiments 5.1 Experimental Settings 5.1.1 Datasets",
    "add_info": "1 https://github.com/rkadlec/ubuntu-ranking-dataset-creator",
    "text": "We use two public datasets in our experiments. For the specific-requirement scenario, we use the Ubuntu dialogue corpus [Cite_Footnote_1] extracted from Ubuntu question-answering forum, named Ubuntu (Lowe et al., 2015). The original training data consists of 7 million conversational post-responses pairs from 2014 to April 27,2012. The validation data are conversational pairs from April 27,2014 to Au-gust 7,2012, and the test data are from August 7,2012 to December 1,2012. We set the number of positive examples as 4,000,000 in the Github to directly sample data from the whole corpus. Then we construct post and response pairs based on the period from both context and utterance. We also conduct some data pro-processing. For ex-ample, we use the official script to tokenize, stem and lemmatize, and the duplicates and sentences with length less than 5 or longer than 50 are re-moved. Finally, we obtain 3,200,000, 100,000 and 100,000 for training, validation and testing, re-spectively."
  },
  {
    "id": 259,
    "name": "STC",
    "fullname": "N/A",
    "genericmention": [
      "It"
    ],
    "description": [
      "It consists of 3,788,571 post-response pairs extracted from the Chinese Weibo website and cleaned by the data publishers.",
      "We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. "
    ],
    "citationtag": [
      "Shang et al., 2015"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/zhanghainan/TailoredSeq2Seq2DifferentConversationScenarios",
    "section_title": "5 Experiments 5.1 Experimental Settings 5.1.1 Datasets",
    "add_info": "2 https://github.com/zhanghainan/TailoredSeq2Seq2DifferentConversationScenarios",
    "text": "For the diverse-requirement scenario, we use the Chinese Weibo dataset, named STC (Shang et al., 2015). It consists of 3,788,571 post-response pairs extracted from the Chinese Weibo website and cleaned by the data publishers. We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. [Cite_Footnote_2]"
  },
  {
    "id": 260,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code",
      "Our approach",
      "our method",
      "our method",
      "our method"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/yangkevin2/emnlp2020-stream-beam-mt",
    "section_title": "1 Introduction",
    "add_info": "1 Code available at https://github.com/yangkevin2/emnlp2020-stream-beam-mt.",
    "text": "We apply our method to variable-width beam search. For variable-output-length decoding even in batched settings, variable-width beam search often modestly decreases accuracy in exchange for substantial speedups over fixed-width beam search (Freitag and Al-Onaizan, 2017; Wu et al., 2016). When decoding with Fairseq\u2019s state-of-the-art WMT\u201919 model (Ng et al., 2019), our method further improves over the speed of base-line variable-width beam search: up to 16.5% on a 32GB V100 GPU, without changing BLEU (Pa-pineni et al., 2002). Our approach also improves decoding efficiency in lightweight models for se-mantic and syntactic parsing. [Cite_Footnote_1] In principle, our method can be applied to any task which sequen-tially processes variable-length data."
  },
  {
    "id": 261,
    "name": "newstest2018 and new-stest2017",
    "fullname": "N/A",
    "genericmention": [
      "datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.statmt.org/wmt19/translation-task.html",
    "section_title": "- Hyperparameters",
    "add_info": null,
    "text": "In Tables 6 and 7, we present results from applying our method to the JOBS and GEO datasets. We use the same hyperparameters and heuristics as for ATIS, and operate under the same candidate-expansion constraint. V AR -S TREAM is substan-tially faster than Fixed and V AR -B ATCH under this setting. A.5 Dataset Details A.5.1 Machine Translation Evaluation datasets (newstest2018 and new-stest2017) are available at  http://www.statmt.org/wmt19/translation-task.html. new-stest2018 contains 2998 and 3000 examples for De-En and Ru-En respectively, while newstest2017 contains 3004 and 3001."
  },
  {
    "id": 262,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Datasets"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/Alex-Fabbri/",
    "section_title": "- A.5.2 Semantic Parsing",
    "add_info": null,
    "text": "Datasets can be obtained by running the data scripts at  https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch. We use Dong and Lapata (2016)\u2019s training, development (for ATIS), and test sets. ATIS, JOBS, and GEO contain 5410, 640, and 880 examples respectively."
  },
  {
    "id": 263,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our model",
      "Implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/neulab/NL2code",
    "section_title": "1 Introduction",
    "add_info": "1 Implementation available at https://github.com/neulab/NL2code",
    "text": "Experiments (\u00a7 5) on two Python code gener-ation tasks show 11.7% and 9.3% absolute im-provements in accuracy against the state-of-the-art system (Ling et al., 2016). Our model also gives competitive performance on a standard semantic parsing benchmark [Cite_Footnote_1] ."
  },
  {
    "id": 264,
    "name": "lamtram",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a strong neural machine translation ( NMT ) system",
      "A toolkit for lan-guage and translation modeling using neural net-works"
    ],
    "citationtag": [
      "Graham Neubig. 2015. lamtram: A toolkit for lan-guage and translation modeling using neural net-works.",
      "Neubig, 2015"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://www.github.com/neubig/lamtram",
    "section_title": "5 Experimental Evaluation 5.3 Results",
    "add_info": "Graham Neubig. 2015. lamtram: A toolkit for lan-guage and translation modeling using neural net-works. http://www.github.com/neubig/lamtram.",
    "text": "Evaluation results for Python code generation tasks are listed in Tab. 3. Numbers for our sys-tems are averaged over three runs. We compare primarily with two approaches: (1) Latent Pre-dictor Network ( LPN ), a state-of-the-art sequence-to-sequence code generation model (Ling et al., 2016), and (2) S EQ 2T REE , a neural semantic pars-ing model (Dong and Lapata, 2016). S EQ 2T REE generates trees one node at a time, and the tar-get grammar is not explicitly modeled a priori, but implicitly learned from data. We test both the original S EQ 2T REE model released by the au-thors and our revised one (S EQ 2T REE \u2013UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015). For completeness, we also compare with a strong neural machine translation ( NMT ) system (Neubig, 2015)  using a standard encoder-decoder architecture with atten-tion and unknown word replacement , and include numbers from other baselines used in Ling et al. (2016). On the HS dataset, which has relatively large ASTs, we use unary closure for our model and S EQ 2T REE , and for D JANGO we do not."
  },
  {
    "id": 265,
    "name": "MeSH",
    "fullname": "(Medical Subject Head-ings) lexical hierarchy",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "In MeSH, each concept is assigned one or more alphanumeric descriptor codes corresponding to particular positions in the hierarchy.",
      "For example, A (Anatomy), A01 (Body Regions), A01.456 (Head), A01.456.505 (Face), A01.456.505.420 (Eye)."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.nlm.nih.gov/mesh",
    "section_title": "2 MeSH and Medline",
    "add_info": "1 http://www.nlm.nih.gov/mesh",
    "text": "In this paper we use the MeSH (Medical Subject Head-ings) lexical hierarchy [Cite_Footnote_1] , but the approach should be equally applicable to other domains using other thesauri and ontologies. In MeSH, each concept is assigned one or more alphanumeric descriptor codes corresponding to particular positions in the hierarchy. For example, A (Anatomy), A01 (Body Regions), A01.456 (Head), A01.456.505 (Face), A01.456.505.420 (Eye). Eye is ambiguous according to MeSH and has a second code: A09.371 (A09 represents Sense Organs)."
  },
  {
    "id": 266,
    "name": "Hugging-Face Transformers (Wolf et al., 2020) [Cite_Footnote_2] library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2020"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "4 Experiment Settings",
    "add_info": "2 https://github.com/huggingface/transformers",
    "text": "Supervised Metrics For all supervised model-based approaches, we experiment with fine-tuning two multilingual pre-trained language models: 1. multilingual BERT , dubbed m BERT (Devlin et al., 2019)\u2014a transformer-based model pre-trained with a masked language model objective on the concatenation of monolingual Wikipedia corpora from the 104 languages with the largest Wikipedias. 2. XLM - R (Conneau et al., 2020)\u2014a transformer-based masked language model trained on 100 languages using monolingual Common-Crawl data. All models are based on the Hugging-Face Transformers (Wolf et al., 2020) [Cite_Footnote_2] library. We fine-tune with the Adam optimizer (Kingma and Ba, 2015), a batch size of 32, and a learning rate of 5e\u22125 for 3 and 5 epochs for classification and regression tasks, respectively. We perform a grid search on held-out validation sets over learning rate with values: 2e\u22123, 2e\u22124, 2e\u22125, and 5e\u22125 and over number of epochs with values: 3, 5, and 8."
  },
  {
    "id": 267,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the training data"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/mjpost/sacrebleu",
    "section_title": "4 Experiment Settings",
    "add_info": "3 https://github.com/mjpost/sacrebleu",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the [Cite_Footnote_3] ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 268,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the implementation of Salazar et al."
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "2020"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/awslabs/mlm-scoring",
    "section_title": "4 Experiment Settings",
    "add_info": "8 https://github.com/awslabs/mlm-scoring",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. [Cite_Footnote_8] PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 269,
    "name": "5-gram Ken LM model",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Heafield, 2011"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/kpu/kenlm",
    "section_title": "4 Experiment Settings",
    "add_info": "9 https://github.com/kpu/kenlm",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). [Cite_Footnote_9] Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 270,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "machine translated re-sources for STS",
      "The former"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/PhilipMay/stsb-multi-mt",
    "section_title": "4 Experiment Settings",
    "add_info": "10 https://github.com/PhilipMay/stsb-multi-mt",
    "text": "Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popovic\u0301 (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS [Cite_Footnote_10] and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -"
  },
  {
    "id": 271,
    "name": "Open-IE4",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://knowitall.github.io/openie",
    "section_title": "1 Introduction",
    "add_info": "1 http://knowitall.github.io/openie",
    "text": "Further, Stanovsky et al. (2015) compared the performance of several off-the-shelf parsers in dif-ferent semantic tasks. Most relevant to this work is the comparison between Open-IE and SRL. Specifically, they suggest that SRL\u2019s longer argu-ments introduce noise which hurts performance for downstream tasks. This is sustained empiri-cally by showing that extractions from Open-IE4 [Cite_Footnote_1] significantly outperform ClearNLP\u2019s SRL (Choi, 2012) in textual similarity, analogies, and reading comprehension tasks."
  },
  {
    "id": 272,
    "name": "Theano",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://deeplearning.net/software/theano/",
    "section_title": "5 Experiments and Results 5.2 Experiments on Veracity-based Datasets",
    "add_info": "3 http://deeplearning.net/software/theano/",
    "text": "We implement our models and DeClarE with Theano [Cite_Footnote_3] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation."
  },
  {
    "id": 273,
    "name": "Google Translate",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://translate.google.com",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "1 https://translate.google.com",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [Cite_Footnote_1] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 274,
    "name": "Microsoft Translator",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.bing.com/translator",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "2 https://www.bing.com/translator",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [Cite_Footnote_2] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 275,
    "name": "Amazon Translate",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://aws.amazon.com/translate",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "3 https://aws.amazon.com/translate",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [Cite_Footnote_3] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 276,
    "name": "SYSTRAN",
    "fullname": "N/A",
    "genericmention": [
      "the first four commercial MT systems"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.systransoft.com",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "4 http://www.systransoft.com",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, [Cite_Footnote_4] (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit."
  },
  {
    "id": 277,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the pre-trained models provided by the Fairseq toolkit"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "5 https://github.com/pytorch/fairseq",
    "text": "MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al. (2018), the WMT\u201918 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit. [Cite_Footnote_5]"
  },
  {
    "id": 278,
    "name": "RMN code",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/miyyer/rmn",
    "section_title": "6 Evaluation",
    "add_info": "8 Our implementation builds on the available RMN code https://github.com/miyyer/rmn.",
    "text": "Parameter settings Across all experiments and corpus-specific models, we set \u03b2=0.99 for MV-Plot, and for both MVPlot and RMN we set \u03b1=0.5, \u03bb=10 \u22125 , k=50. We train both RMN and MVPlot for 15 epochs using SGD and ADAM (Kingma and Ba, 2014). [Cite_Footnote_8]"
  },
  {
    "id": 279,
    "name": "mkcls",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "statis-tical machine translation systems"
    ],
    "citationtag": [
      "Och, 1999"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://fjoch.com/mkcls.html",
    "section_title": "2 Background",
    "add_info": "1 Available from http://fjoch.com/mkcls.html.",
    "text": "Past research in unsupervised PoS induction has largely been driven by two different motivations: a task based perspective which has focussed on induc-ing word classes to improve various applications, and a linguistic perspective where the aim is to induce classes which correspond closely to anno-tated part-of-speech corpora. Early work was firmly situtated in the task-based setting of improving gen-eralisation in language models. Brown et al. (1992) presented a simple first-order HMM which restricted word types to always be generated from the same class. Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the sig-nificant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language mod-elling and bilingual word alignment, and an imple-mentation with improved inference called mkcls (Och, 1999) [Cite_Footnote_1] has become a standard part of statis-tical machine translation systems."
  },
  {
    "id": 280,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The source code for the work presented in this paper",
      "this formulation",
      "this model architecture"
    ],
    "description": [
      "a sim-ple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://groups.csail.mit.edu/rbg/code/typetagging/",
    "section_title": "References",
    "add_info": "1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/.",
    "text": "Part-of-speech (POS) tag distributions are known to exhibit sparsity \u2014 a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incor-porating this sparsity constraint improves tag-ging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity. This paper proposes a sim-ple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parame-ters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial per-formance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems. [Cite_Footnote_1]"
  },
  {
    "id": 281,
    "name": "AQUAINT-2",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection",
      "Articles are in English and come from a variety of sources."
    ],
    "citationtag": [
      "AQUAINT-2, 2007",
      "2007"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents",
    "section_title": "4 Evaluation 4.2 Setup",
    "add_info": "AQUAINT-2 (2007). URL: http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents.",
    "text": "To estimate the timeliness and semantic cred-ibility indicators, we use AQUAINT-2, a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007)  . Articles are in English and come from a variety of sources."
  },
  {
    "id": 282,
    "name": "Splog blog dataset",
    "fullname": "N/A",
    "genericmention": [
      "For each classified blog d"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Kolari, P., Finin, T., Java, A., and Joshi, A. (2006).",
      "Kolari et al., 2006"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://ebiquity.umbc.edu/resource/html/id/212/Splog-Blog-Dataset",
    "section_title": "3 Modeling 3.3.2 Blog level credibility indicators",
    "add_info": "Kolari, P., Finin, T., Java, A., and Joshi, A. (2006). Splog blog dataset. URL: http://ebiquity.umbc.edu/resource/html/id/212/Splog-Blog-Dataset.",
    "text": "Spam filtering To estimate the spaminess of a blog, we take a simple approach. We train an SVM classifier on a labeled splog blog dataset (Kolari et al., 2006)  using the top 1500 words for both spam and non-spam blogs as features. For each classified blog d we have a confidence value s(d). If the clas-sifier cannot make a decision (s(d) = 0) we set p spam (d) to 0, otherwise we use the following to transform s(d) into a spam prior p spam (d): where n(r, d) is the number of comments on post d. Regularity To estimate the regularity prior we use where \u03c3 interval expresses the standard deviation of the temporal intervals between two successive posts. Topical consistency Here we use an approach similar to query clarity (Cronen-Townsend and Croft, 2002): based on the list of posts from the same blog we compare the topic distribution of blog B to the topic distribution in the collection C and assign a \u2018clarity\u2019 value to B; a score further away from zero indicates a higher topical consistency. We estimate the topical consistency prior as where clarity(d) is estimated by"
  },
  {
    "id": 283,
    "name": "Lycos Retriever",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a patent-pending information fusion engine",
      "Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia article.",
      "These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://www.lycos.com/retriever.html",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.lycos.com/retriever.html. Work on Retriever was done while author was employed at Lycos.",
    "text": "Lycos Retriever [Cite_Footnote_1] is something new on the Web: a patent-pending information fusion engine. That is, unlike a search engine, rather than returning ranked documents links in response to a query, Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia article. These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching."
  },
  {
    "id": 284,
    "name": "King Kong",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/king-kong-1933.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 285,
    "name": "Zoloft",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/zoloft.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 286,
    "name": "Public-Key Cryptography",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/public-key-cryptography.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 287,
    "name": "Lyme Disease",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/lyme-disease.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 288,
    "name": "Reggaeton",
    "fullname": "N/A",
    "genericmention": [
      "other categories"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.lycos.com/info/reggaeton.html",
    "section_title": "3 Lycos Retriever pages",
    "add_info": "4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html",
    "text": "Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux\u2019s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs."
  },
  {
    "id": 289,
    "name": "spidered DMOZ [Cite_Footnote_5] hierarchy",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.dmoz.com",
    "section_title": "4 Topic Selection",
    "add_info": "5 http://www.dmoz.com",
    "text": "After a topic was input to the system, the Retriever system assigned it a category using a na\u00efve Bayes classifier built on a spidered DMOZ [Cite_Footnote_5] hierarchy. Various heuristics were implemented to make the returned set of categories uniform in length and depth, up-to-date, and readable."
  },
  {
    "id": 290,
    "name": "CMU Link Parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.link.cs.cmu.edu/link/",
    "section_title": "7 Passage Extraction",
    "add_info": "6 http://www.link.cs.cmu.edu/link/",
    "text": "When a passage was identified as being potentially interesting, it was then fully parsed to see if an expression denoting the topic was the Discourse Topic of the passage. Discourse Topic is an under-theorized notion in linguistic theory: not all linguists agree that the notion of Discourse Topic is required in discourse analysis at all (cf. Asher, 2004). For our purposes, however, we for-mulated a set of patterns for identifying Discourse Topics on the basis of the output of the CMU Link Parser [Cite_Footnote_6] the system uses."
  },
  {
    "id": 291,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "an openly available blocklist"
    ],
    "citationtag": [
      "In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics."
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/LDNOOBW",
    "section_title": "6 Analysis of Safety and Gender Bias 6.3 Safety",
    "add_info": "3 https://github.com/LDNOOBWdataset. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics.",
    "text": "The MMB models may demonstrate offensiveness beyond gender bias for several reasons: (1) its gen-erative nature makes it rather difficult to define a limited set of utterances; (2) the model\u2019s training data contains real-world conversations from the Internet; and (3) the Image-Chat dataset has neg-ative styles to better capture the range of human styles. All of these factors could lead to an unsafe response given a multi-modal context. To mitigate this problem, we first measure our models\u2019 toxicity using an openly available blocklist [Cite_Footnote_3] and an offen-sive language classifier presented in Dinan et al. (2019b). We define the term \u201ctoxicity\" to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model. We evaluate our model on the Image-Chat validation set, with a fixed style trait to control the generation, presenting results for different choices of fixed trait. We first evaluate our model in the first round of the Image-Chat validation set. The results in Table 9 indicate that positive styles reduce the level of toxicity by a large margin for both metrics (classifier and blocklist). The results also align well with our previous experiments on degendering, as toxicity is reduced across all styles after applying the degendering process. After degendering, we can considerably improve our model\u2019s safety by en-forcing that it uses positive styles. We also evaluate our model in the second round of the conversation and collect the statistics based on the first round style, as shown in Table 23. This result suggests that even if the model is controlled with a positive style, it is less safe when responding to negative conversations."
  },
  {
    "id": 292,
    "name": "ResNeXt-IG-3.5B and Faster R-CNN image features",
    "fullname": "N/A",
    "genericmention": [
      "that model"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron.",
      "Girshick et al., 2018"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/facebookresearch/detectron",
    "section_title": "2 Related Work 2.3 Comparison to Existing Models",
    "add_info": "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron.",
    "text": "2AMMC: a retrieval model in which multiple Transformers are attended over in order to make use of a combination of ResNeXt-IG-3.5B and Faster R-CNN image features (Girshick et al., 2018)  . We specifically use the 2AMMC model from Ju et al. (2019) because that model has the best test-set per-formance on Image-Chat in that work."
  },
  {
    "id": 293,
    "name": "Detectron framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron."
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/facebookresearch/detectron",
    "section_title": "References",
    "add_info": "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron.",
    "text": "Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018)  ; specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\"."
  },
  {
    "id": 294,
    "name": "Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Amanpreet Singh, Vedanuj Goswami, Vivek Natara-jan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020a. Mmf: A multimodal framework for vision and language research.",
      "Singh et al., 2020a"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/facebookresearch/mmf",
    "section_title": "References",
    "add_info": "Amanpreet Singh, Vedanuj Goswami, Vivek Natara-jan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020a. Mmf: A multimodal framework for vision and language research. https://github.com/facebookresearch/mmf.",
    "text": "Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a)  . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\"."
  },
  {
    "id": 295,
    "name": "ImageNet1K dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Rus-sakovsky et al., 2015"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/",
    "section_title": "References",
    "add_info": "4 https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/",
    "text": "ResNeXt WSL We first experiment with im-age representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion pub-lic images (Mahajan et al., 2018), with subse-quent fine-tuning on the ImageNet1K dataset (Rus-sakovsky et al., 2015) [Cite_Footnote_4] . The output of this model is a 2048-dimensional vector, and we refer to these representations as \u201cResNeXt WSL\" features."
  },
  {
    "id": 296,
    "name": "Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Singh et al., 2020a"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/facebookresearch/vilbert-multi-task",
    "section_title": "References",
    "add_info": "5 https://github.com/facebookresearch/vilbert-multi-task",
    "text": "Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [Cite_Footnote_5] . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\"."
  },
  {
    "id": 297,
    "name": "Glove vec-tors",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pennington et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "5 Experiments 5.3 Hyperparameters and Training Details",
    "add_info": "5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.",
    "text": "We initialized our word representations using publicly available 300-dimensional Glove vec-tors [Cite_Footnote_5] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned."
  },
  {
    "id": 298,
    "name": "Tesseract OCR engine",
    "fullname": "N/A",
    "genericmention": [
      "OCR"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/tesseract-ocr/",
    "section_title": "3 Data Description 3.1 Parallel Data",
    "add_info": "6 https://github.com/tesseract-ocr/",
    "text": "Fifty-six percent of our parallel data is derived from the Cherokee New Testament. Other texts are novels, children\u2019s books, newspaper articles, etc. These texts vary widely in dates of publica-tion, the oldest being dated to 1860. Addition-ally, our data encompasses both existing dialects of Cherokee: the Overhill dialect, mostly spoken in Oklahoma (OK), and the Middle dialect, mostly used in North Carolina (NC). These two dialects are mainly phonologically different and only have a few lexical differences (Uchihara, 2016). In this work, we do not explicitly distinguish them during translation. The left pie chart of Figure 2 shows the parallel data distributions over text types and dialects, and the complete information is in Ta-ble 14 of Appendix A.1. Many of these texts were translations of English materials, which means that the Cherokee structures may not be 100% natural in terms of what a speaker might spontaneously produce. But each text was translated by people who speak Cherokee as the first language, which means there is a high probability of grammatical-ity. These data were originally available in PDF version. We apply the Optical Character Recog-nition (OCR) via Tesseract OCR engine [Cite_Footnote_6] to ex-tract the Cherokee and English text. Then our co-author, a proficient second-language speaker of Cherokee, manually aligned the sentences and fixed the errors introduced by OCR. This process is time-consuming and took several months."
  },
  {
    "id": 299,
    "name": "News Crawl 2017",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "English monolingual data"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://data.statmt.org/news-crawl/en/",
    "section_title": "5 Results 5.1 Experimental Details",
    "add_info": "10 http://data.statmt.org/news-crawl/en/",
    "text": "We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [Cite_Footnote_10] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 . We apply tokenizer and truecaser from Moses (Koehn et al., 2007). We also apply the BPE tokonization (Sennrich et al., 2016c), but instead of using it as default, we treat it as hyper-parameter. For systems with BERT, we apply the WordPiece tokenizer (Devlin et al., 2019). We compute detokenized and case-sensitive BLEU score (Papineni et al., 2002) using SacreBLEU (Post, 2018). 13"
  },
  {
    "id": 300,
    "name": "News Crawl 2017",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "English monolingual data"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://data.statmt.org/news-crawl/en/",
    "section_title": "B Experimental Details B.1 Data and Preprocessing",
    "add_info": "18 http://data.statmt.org/news-crawl/en/",
    "text": "For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. [Cite_Footnote_18] We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 19 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 20 , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary."
  },
  {
    "id": 301,
    "name": "News Commentary v13 of WMT2018",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.statmt.org/wmt18/index.html",
    "section_title": "B Experimental Details B.1 Data and Preprocessing",
    "add_info": "19 http://www.statmt.org/wmt18/index.html",
    "text": "For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. 18 We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 [Cite_Footnote_19] or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary."
  },
  {
    "id": 302,
    "name": "OPUS",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://opus.nlpl.eu/bible-uedin.php",
    "section_title": "B Experimental Details B.1 Data and Preprocessing",
    "add_info": "20 http://opus.nlpl.eu/bible-uedin.php",
    "text": "For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. 18 We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS [Cite_Footnote_20] , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary."
  },
  {
    "id": 303,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code",
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/jiesutd/SubwordEncoding-CWS",
    "section_title": "1 Introduction",
    "add_info": "1 Our code is released at https://github.com/jiesutd/SubwordEncoding-CWS.",
    "text": "In this paper, we fill this gap by proposing a subword-based neural word segmentor, by inte-grating two strands of works: the byte pair en-coding (BPE) algorithm (Gage, 1994) and the lat-tice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In partic-ular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code [Cite_Footnote_1] is based on NCRF++ (Yang and Zhang, 2018)."
  },
  {
    "id": 304,
    "name": "Chinese Gigaword",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2011T13",
    "section_title": "4 Experiments 4.1 Experimental Settings",
    "add_info": "3 https://catalog.ldc.upenn.edu/LDC2011T13.",
    "text": "Embeddings. We take the same character un-igram and bigram embeddings as Zhang et al. (2016), who pretrain embeddings using word2vec (Mikolov et al., 2013) on Chinese Gigaword [Cite_Footnote_3] . The vocabulary of subword is constructed with 200000 merge operations and the subword embeddings are also trained using word2vec (Heinzerling and Strube, 2018). Trie (Fredkin, 1960) is used to ac-celerate lattice building. All the embeddings are fine-tuned during training."
  },
  {
    "id": 305,
    "name": "20NG",
    "fullname": "20 Newsgroup dataset",
    "genericmention": [
      "all data sets"
    ],
    "description": [
      "a collection of approximately 20,000 20-category documents"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://people.csail.mit.edu/~jrennie/20Newsgroups/",
    "section_title": "4 Experimental Studies 4.1 Experimental Setup",
    "add_info": "1 http://people.csail.mit.edu/~jrennie/20Newsgroups/",
    "text": "Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [Cite_Footnote_1] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories."
  },
  {
    "id": 306,
    "name": "Cornell movie-review dataset",
    "fullname": "N/A",
    "genericmention": [
      "Both of them",
      "all data sets"
    ],
    "description": [
      "Both of them are 2-category tasks and each consists of 2,000 reviews."
    ],
    "citationtag": [
      "Pang and Lee, 2004"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.cornell.edu/People/pabo/movie-review-data/",
    "section_title": "4 Experimental Studies 4.1 Experimental Setup",
    "add_info": "2 http://www.cs.cornell.edu/People/pabo/movie-review-data/",
    "text": "Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset [Cite_Footnote_2] (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories."
  },
  {
    "id": 307,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "dataset from product reviews of domain DVD",
      "Both of them",
      "all data sets"
    ],
    "description": [
      "Both of them are 2-category tasks and each consists of 2,000 reviews."
    ],
    "citationtag": [
      "Blitzer et al., 2007"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.seas.upenn.edu/~mdredze/datasets/sentiment/",
    "section_title": "4 Experimental Studies 4.1 Experimental Setup",
    "add_info": "3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/",
    "text": "Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD [Cite_Footnote_3] (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories."
  },
  {
    "id": 308,
    "name": "Kyoto University Corpus [Cite_Footnote_2] 2.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html",
    "section_title": "1 Introduction",
    "add_info": "2 http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html",
    "text": "It takes a long time to construct high-quality an-notated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki\u2019s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus [Cite_Footnote_2] 2.0. These data are divided into two groups: gen-eral and editorial. General contains 30 general news articles, and editorial contains 30 editorial articles. According to his experiments, editorial is harder than general. Perhaps this is caused by the differ-ence in rhetorical styles and the lengths of articles. The average number of sentences in an editorial ar-ticle is 28.7, while that in a general article is 13.9."
  },
  {
    "id": 309,
    "name": "ChaSen 2.2.9",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.aist-nara.ac.jp/",
    "section_title": "1 Introduction",
    "add_info": "3 http://chasen.aist-nara.ac.jp/",
    "text": "In addition, we decided to use the output of ChaSen 2.2.9 [Cite_Footnote_3] and CaboCha 0.34 instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus."
  },
  {
    "id": 310,
    "name": "CaboCha 0.34",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cl.aist-nara.ac.jp/\u02dctaku-ku/software/cabocha/",
    "section_title": "1 Introduction",
    "add_info": "4 http://cl.aist-nara.ac.jp/\u02dctaku-ku/software/cabocha/",
    "text": "In addition, we decided to use the output of ChaSen 2.2.9 and CaboCha 0.34 [Cite_Footnote_4] instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus."
  },
  {
    "id": 311,
    "name": "Py-Torch",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Paszke et al., 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://pytorch.org/",
    "section_title": "- A.5 Implementation Details",
    "add_info": "15 https://pytorch.org/",
    "text": "Our models are implemented based on Py-Torch (Paszke et al., 2017). [Cite_Footnote_15] To speed up training, we use Nvidia Apex 16 for mixed precision train-ing. Gradient accumulation (Ott et al., 2018) is applied to reduce multi-GPU communication over-heads. All pre-training experiments are run on Nvidia V100 GPUs (32GB VRAM; NVLink con-nection). We use AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 3e\u22125 and weight decay of 0.01 to pre-train our model. The best pre-trained model is trained on 16 V100 GPUs for about 3 weeks. Finetuning experiments are implemented on the same hardware or Titan RTX GPUs (24GB VRAM) with AdamW optimizer but different learning rates."
  },
  {
    "id": 312,
    "name": "E CHO project",
    "fullname": "European CHronicles On-line",
    "genericmention": [
      "The project"
    ],
    "description": [
      "The E CHO project ( [Cite] http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives.",
      "The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://pc-erato2.iei.pi.cnr.it/echo",
    "section_title": "8 Recent Research Projects",
    "add_info": null,
    "text": "Two other related FP5 IST projects are: C ORE - TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line. C ORETEX (http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology. In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions. The E CHO project (  http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives. The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation."
  },
  {
    "id": 313,
    "name": "ukWaC corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://wacky.sslmit.unibo.it/",
    "section_title": "3 Method 3.1 Semantic space 3.1.1 Source corpus",
    "add_info": "2 http://wacky.sslmit.unibo.it/",
    "text": "Our source corpus is the concatenation of the ukWaC corpus [Cite_Footnote_2] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information."
  },
  {
    "id": 314,
    "name": "English Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://en.wikipedia.org",
    "section_title": "3 Method 3.1 Semantic space 3.1.1 Source corpus",
    "add_info": "3 http://en.wikipedia.org",
    "text": "Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [Cite_Footnote_3] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information."
  },
  {
    "id": 315,
    "name": "British National Corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.natcorp.ox.ac.uk/",
    "section_title": "3 Method 3.1 Semantic space 3.1.1 Source corpus",
    "add_info": "4 http://www.natcorp.ox.ac.uk/",
    "text": "Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia and the British National Corpus [Cite_Footnote_4] . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information."
  },
  {
    "id": 316,
    "name": "Color terms",
    "fullname": "N/A",
    "genericmention": [
      "two datasets of adjective-noun phrases",
      "one with color terms and one with intensional adjectives",
      "This dataset",
      "The dataset"
    ],
    "description": [
      "This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above.",
      "From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low)",
      "From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety).",
      "The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole).",
      "They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN.",
      "The dataset as used here consists of 239 intersective and 130 subsective ANs."
    ],
    "citationtag": [
      "Bruni et al. (to appear) for an analysis of the color term dataset from a multi-modal perspective."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip",
    "section_title": "p = Bv (5) 3.3 Datasets",
    "add_info": "6 Available at http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip. See Bruni et al. (to appear) for an analysis of the color term dataset from a multi-modal perspective.",
    "text": "We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. [Cite_Footnote_6] Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above. From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low), since the remaining (grey, orange, pink, and purple) are not in the 700 most frequent set of ad-jectives in the corpora used. From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety). The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole). They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN. In this paper, only the first label (most frequent interpretation, accord-ing to the judges) has been used. The \u03ba coefficient of the annotation on the three categories (first interpre-tation only) was 0.87 (conf. int. 0.82-0.92, according to Fleiss et al. (1969)), observed agreement 0.96. 7 There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. 8 The dataset as used here consists of 239 intersective and 130 subsective ANs."
  },
  {
    "id": 317,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code for the computation of inter-annotator agreement by Stefan Evert"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.collocations.de/temp/kappa_example.zip",
    "section_title": "p = Bv (5) 3.3 Datasets",
    "add_info": "7 Code for the computation of inter-annotator agreement by Stefan Evert, available at http://www.collocations.de/temp/kappa_example.zip.",
    "text": "We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. 6 Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above. From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low), since the remaining (grey, orange, pink, and purple) are not in the 700 most frequent set of ad-jectives in the corpora used. From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety). The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole). They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN. In this paper, only the first label (most frequent interpretation, accord-ing to the judges) has been used. The \u03ba coefficient of the annotation on the three categories (first interpre-tation only) was 0.87 (conf. int. 0.82-0.92, according to Fleiss et al. (1969)), observed agreement 0.96. [Cite_Footnote_7] There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. The dataset as used here consists of 239 intersective and 130 subsective ANs."
  },
  {
    "id": 318,
    "name": "ClueWeb",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Evgeniy Gabrilovich, Michael Ringgaard, , and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora.",
      "Gabrilovich et al., 2013"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://lemurproject.org/clueweb09/FACC1/",
    "section_title": "2 Related Work",
    "add_info": "Evgeniy Gabrilovich, Michael Ringgaard, , and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora. http://lemurproject.org/clueweb09/FACC1/.",
    "text": "All of the above work resorted to using the Free-base annotation of ClueWeb (Gabrilovich et al., 2013)  to gain extra advantage of paraphrasing QA pairs or dealing with data sparsity problem. How-ever, ClueWeb is proprietary data and costs hun-dreds of dollars to purchase. Moreover, even though the implementation systems from (Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014) are open-source, they all take considerable disk space (in tens of gigabytes) and training time (in days). In this paper we present a system that can be easily implemented in 300 lines of Python code with no compromise in accuracy and speed."
  },
  {
    "id": 319,
    "name": "Wikidata",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.wikidata.org/",
    "section_title": "2 Retrieving Facts from LMs",
    "add_info": "1 https://www.wikidata.org/",
    "text": "In this paper we follow the protocol of Petroni et al. (2019)\u2019s English-language LAMA bench-mark, which targets factual knowledge expressed in the form of subject-relation-object triples from Wikidata [Cite_Footnote_1] curated in the T-REx dataset (ElSahar et al., 2018). The cloze-style prompts used therein are manually created and consist of a sequence of tokens, where [X] and [Y] are placeholders for sub-jects and objects (e.g. \u201c[X] is a [Y] by profession.\u201d). To assess the existence of a certain fact, [X] is re-placed with the actual subject (e.g. \u201cObama is a hmaski by profession.\u201d) and the model predicts the object in the blank y\u0302 i = argmax y i p(y i |s i:i ), where s i:i is the sentence with the i-th token masked out. Finally, the predicted fact is compared to the ground truth. In the next section, we extend this setting to more languages and predict multiple tokens instead of a single one."
  },
  {
    "id": 320,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the appropriately inflected surface form of the bracketed words"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/antonisa/unimorph_inflect",
    "section_title": "3 Multilingual Multi-token Factual Retrieval Benchmark 3.3 Prompts",
    "add_info": "3 https://github.com/antonisa/unimorph_inflect",
    "text": "Once all the morphological features have been specified as detailed above, we use the unimorph_inflect package (Anastasopoulos and Neubig, 2019) to generate the appropriately inflected surface form of the bracketed words. [Cite_Footnote_3] We note that the target entity ([Y]) might also need to be inflected, as in the above Russian example, in which case we require the model\u2019s predictions to match the inflected target forms."
  },
  {
    "id": 321,
    "name": "HurtLex lexicon",
    "fullname": "N/A",
    "genericmention": [
      "This lexi-con"
    ],
    "description": [
      "HurtLex is a multilingual lexicon of hate words, originally built from 1,082 Italian hate words compiled in a manual fashion by the linguist Tullio De Mauro",
      "This lexi-con is semi-automatically extended and translated into 53 languages by using BabelNet (Navigli and Ponzetto, 2012), and the lexical items are divided into 17 categories such as homophobic slurs, eth-nic slurs, genitalia, cognitive and physical disabil-ities, animals and more"
    ],
    "citationtag": [
      "Bassig-nana et al., 2018"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://hatespeech.di.unito.it/resources.html",
    "section_title": "4 Cross-domain Classification",
    "add_info": "1 http://hatespeech.di.unito.it/resources.html",
    "text": "In this experiment, we investigate the performance of machine learning classifiers which are trained on a particular dataset and tested on different datasets ones. We focus on investigating the in-fluence of captured phenomena coverage between datasets. We hypothesize that a classifier which is trained on a broader coverage dataset and tested on narrower coverage dataset will give better perfor-mance than the opposite. Furthermore, we analyse the impact of using the HurtLex lexicon (Bassig-nana et al., 2018) to transfer knowledge between domains. HurtLex is a multilingual lexicon of hate words, originally built from 1,082 Italian hate words compiled in a manual fashion by the linguist Tullio De Mauro (De Mauro, 2016). This lexi-con is semi-automatically extended and translated into 53 languages by using BabelNet (Navigli and Ponzetto, 2012), and the lexical items are divided into 17 categories such as homophobic slurs, eth-nic slurs, genitalia, cognitive and physical disabil-ities, animals and more [Cite_Footnote_1] ."
  },
  {
    "id": 322,
    "name": "Google Translate",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "machine translation"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://translate.google.com/",
    "section_title": "5 Cross-lingual Classification",
    "add_info": "5 http://translate.google.com/",
    "text": "monolingual word embedding. We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate [Cite_Footnote_5] ) to translate training data from source to target language. In this model, we use pre-trained word embedding from FastText ."
  },
  {
    "id": 323,
    "name": "FastText",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://fasttext.cc/",
    "section_title": "5 Cross-lingual Classification",
    "add_info": "6 https://fasttext.cc/",
    "text": "monolingual word embedding. We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate ) to translate training data from source to target language. In this model, we use pre-trained word embedding from FastText [Cite_Footnote_6] ."
  },
  {
    "id": 324,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "mul-tilingual word embeddings"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/facebookresearch/MUSE",
    "section_title": "5 Cross-lingual Classification",
    "add_info": "7 https://github.com/facebookresearch/MUSE",
    "text": "(b). JL + ME. We also propose a joint-learning model with multilingual word embedding. We take advantage of the availability of mul-tilingual word embeddings [Cite_Footnote_7] to build a joint-learning model. Figure 1 summarize how the data is transformed and learned in this model. We create bilingual training data automatically by using Google Translate to translate the data in both directions (training from source to target language and testing from target to source language), then using it as training data for the two LSTM-based ar-chitectures (similar architecture of the model in cross-domain experiment). We concate-nate these two architectures before the output layer, which produces the final prediction. In the, we expect to reduce some of the noise from the translation while keeping the origi-nal structure of the training set."
  },
  {
    "id": 325,
    "name": "MS COCO image cap-tioning dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://cocodataset.org/",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "8 http://cocodataset.org/",
    "text": "We train our contextual token-image matching model (in Sec. 3.1) on MS COCO image cap-tioning dataset [Cite_Footnote_8] for 20 epochs. The concatena-tion of the last 4 layers of BERT outputs (fol-lowing Devlin et al. (2019)) and mean pooling of"
  },
  {
    "id": 326,
    "name": "English Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/withattardi/wikiextractor",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "9 Downloaded https://github.com/withattardi/wikiextractor",
    "text": "When pre-training the model on pure language corpus, we unify the training process to avoid pos-sible side effects from different training protocols. We follow previous work to conduct two simplifi-cations: 1. Removing the next-sentence-prediction task (Liu et al., 2019) 2. Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia [Cite_Footnote_9] for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison. It results in around 10% more training steps in pure MLM training. All models take batch sizes of 256 and a learning rate of 2e-4."
  },
  {
    "id": 327,
    "name": "Wiki103",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/",
    "text": "When pre-training the model on pure language corpus, we unify the training process to avoid pos-sible side effects from different training protocols. We follow previous work to conduct two simplifi-cations: 1. Removing the next-sentence-prediction task (Liu et al., 2019) 2. Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [Cite_Footnote_10] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison. It results in around 10% more training steps in pure MLM training. All models take batch sizes of 256 and a learning rate of 2e-4."
  },
  {
    "id": 328,
    "name": "PyTorch Trans-formers",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Wolf et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "A Appendices A.1 Full Implementation Details",
    "add_info": "11 https://github.com/huggingface/transformers",
    "text": "The whole framework is built on Py-Torch (Paszke et al., 2019). The implementations of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are borrowed from PyTorch Trans-formers (Wolf et al., 2019) [Cite_Footnote_11] . All evaluation code is from the PyTorch Transformers as well."
  },
  {
    "id": 329,
    "name": "CheckList",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a new eval-uation methodology and accompanying tool [Cite_Footnote_1] for comprehensive behavioral testing of NLP models",
      "CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks.",
      "To break down potential ca-pability failures into specific behaviors, CheckList introduces different test types, such as prediction invariance in the presence of certain perturbations, or performance on a set of \u201csanity checks.\u201d",
      "our implementation of CheckList includes multiple abstractions that help users generate large numbers of test cases easily, such as templates, lexi-cons, general-purpose perturbations, visualizations, and context-aware suggestions."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/marcotcr/checklist",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/marcotcr/checklist",
    "text": "In this work, we propose CheckList, a new eval-uation methodology and accompanying tool [Cite_Footnote_1] for comprehensive behavioral testing of NLP models. CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks. To break down potential ca-pability failures into specific behaviors, CheckList introduces different test types, such as prediction invariance in the presence of certain perturbations, or performance on a set of \u201csanity checks.\u201d Fi-nally, our implementation of CheckList includes multiple abstractions that help users generate large numbers of test cases easily, such as templates, lexi-cons, general-purpose perturbations, visualizations, and context-aware suggestions. (examples of each type in A, B and C)."
  },
  {
    "id": 330,
    "name": "CheckList",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "it contains var-ious visualizations, abstractions for writing test expectations (e.g. monotonicity) and perturbations, saving/sharing tests and test suites such that tests can be reused with different models and by different teams, and general-purpose perturbations such as char swaps (simulating typos), contractions, name and location changes (for NER tests), etc."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/marcotcr/",
    "section_title": "2 CheckList 2.3 Generating Test Cases at Scale",
    "add_info": null,
    "text": "Open source We release an implementation of CheckList at  https://github.com/marcotcr/checklist. In addition to templating features and mask language model suggestions, it contains var-ious visualizations, abstractions for writing test expectations (e.g. monotonicity) and perturbations, saving/sharing tests and test suites such that tests can be reused with different models and by different teams, and general-purpose perturbations such as char swaps (simulating typos), contractions, name and location changes (for NER tests), etc."
  },
  {
    "id": 331,
    "name": "CheckList",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/marcotcr/checklist",
    "section_title": "6 Conclusion",
    "add_info": null,
    "text": "Our user studies indicate that CheckList is easy to learn and use, and helpful both for expert users who have tested their models at length as well as for practitioners with little experience in a task. The tests presented in this paper are part of Check-List\u2019s open source release, and can easily be in-corporated into existing benchmarks. More impor-tantly, the abstractions and tools in CheckList can be used to collectively create more exhaustive test suites for a variety of tasks. Since many tests can be applied across tasks as is (e.g. typos) or with minor variations (e.g. changing names), we ex-pect that collaborative test creation will result in evaluation of NLP models that is much more ro-bust and detailed, beyond just accuracy on held-out data. CheckList is open source, and available at  https://github.com/marcotcr/checklist."
  },
  {
    "id": 332,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a dictionary"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.askoxford.com",
    "section_title": "4 Evaluating the GMM Approach 4.1 Data",
    "add_info": "2 We used http://www.askoxford.com.",
    "text": "To determine how well our model deals with dif-ferent types of figurative usage, we distinguish four phenomena: Phrase-level figurative means that the whole phrase is used figuratively. We further divide this class into expressions which are potentially am-biguous between literal and figurative usage (nsa), e.g., spill the beans, and those that are unambigu-ously figurative irrespective of the context (nsu), e.g., trip the light fantastic. The latter can, theoreti-cally, be detected by dictionary look-up, the former cannot. The label token-level figurative (nw) is used when part of the phrase is used figuratively (e.g., sparrow in (2)). Often it is difficult to determine whether a word is still used in a \u2019literal\u2019 sense or whether it is already used figuratively. Since we are interested in improving the performance of NLP ap-plications such as MT, we take a pragmatic approach and classify usages as \u2019figurative\u2019 if they are not lex-icalized, i.e., if the specific sense is not listed in a dictionary. [Cite_Footnote_2] For example, we would classify summit in the \u2019meeting\u2019 sense as \u2019literal\u2019 (l). In our data set, 7.3% of the instances were annotated as \u2019nsa\u2019, 1.9% as \u2019nsu\u2019, 9.2% as \u2019nw\u2019 and 81.5% as \u2019l\u2019. A randomly selected sample (100 instances) was annotated inde-pendently by a second annotator. The kappa score (Cohen, 1960) is 0.84, which suggest that the anno-tations are reliable."
  },
  {
    "id": 333,
    "name": "CoNLL-2000 dataset",
    "fullname": "N/A",
    "genericmention": [
      "the origi-nal training set",
      "the test set"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cnts.ua.ac.be/conll2000/chunking/",
    "section_title": "4 Experiments",
    "add_info": "4 http://www.cnts.ua.ac.be/conll2000/chunking/",
    "text": "To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task on the CoNLL-2000 dataset [Cite_Footnote_4] . We split the origi-nal training set into a dev set (top 1,000 sent.) and used the rest as train set (7,936 sent.); the test set was kept intact (2,012 sent.). For an input sentence x, each CRF node x i carries an observable word and its part-of-speech tag, and has to be assigned a chunk tag c i out of 3 labels: Beginning, Inside, or Outside (of a noun phrase). Chunk labels are not nested. As in Sha and Pereira (2003), we use second order Markov dependencies (bigram chunk tags), such that for sentence position i, the state is y i = c i\u22121 c i , increasing the label set size from 3 to 9. Out of the full list of Sha and Pereira (2003)\u2019s features we implemented all except two feature templates, y i = y and c(y i ) = c, to simplify im-plementation. Impossible bigrams (OI) and label transitions of the pattern ?O \u2192 I? were prohib-ited by setting the respective potentials to \u2212\u221e. As the active feature count in the train set was just un-der 2M, we hashed all features and weights into a sparse array of 2M entries. Despite the reduced train size and feature set, and hashing, our full in-formation baseline trained with log-likelihood at-tained the test F1-score of 0.935, which is compa-rable to the original result of 0.9438."
  },
  {
    "id": 334,
    "name": "Wikipedia",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.wikipedia.org",
    "section_title": "5 Experiments 5.1 Data",
    "add_info": "1 http://www.wikipedia.org",
    "text": "We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia [Cite_Footnote_1] and labeled a to-tal of 4701 relation instances. In addition to a large set of person-to-person relations, we also included links between people and organizations, as well as biographical facts such as birthday and jobTitle . In all, there are 53 labels in the training data (Table 1)."
  },
  {
    "id": 335,
    "name": "MALLET CRF implementation",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Mc-Callum, 2002",
      "Andrew McCallum. 2002. Mallet: A machine learning for language toolkit."
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "5 Experiments 5.1 Data",
    "add_info": "Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.",
    "text": "We use the MALLET CRF implementation (Mc-Callum, 2002)  with the default regularization pa-rameters."
  },
  {
    "id": 336,
    "name": "Reddit submissions and comments",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://files.pushshift.io/reddit/",
    "section_title": "4 Dataset 4.1 Users\u2019 utterances",
    "add_info": "4 https://files.pushshift.io/reddit/",
    "text": "We consider publicly-available Reddit submissions and comments [Cite_Footnote_4] from 2006 to 2018 as users\u2019 ut-terances. Given a Reddit user having a set of ut-terances U = u 0 ..u N , we aim to label the user with a set of profession and hobby values, based on explicit personal assertions (e.g., \u201cI work as a doctor\u201d) found in the user\u2019s posts. To label the candidate users with attribute values we utilized the Snorkel framework (Ratner et al., 2017). We provide details on our data labeling using Snorkel in Appendix A.1."
  },
  {
    "id": 337,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/Anna146/CHARM",
    "section_title": "4 Dataset 4.2 Document collection",
    "add_info": "5 Available at https://github.com/Anna146/CHARM",
    "text": "The scope of possible attribute values may be open-ended in nature, and thus, calls for an automatic method for collecting Web documents. In this work, we consider three different Web document collec-tions; summary statistics on the number of docu-ments per attribute value are provided in Table 1. Each document may be associated with multiple attribute values. To provide more diversity and comprehensiveness we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms. [Cite_Footnote_5]"
  },
  {
    "id": 338,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "All datasets used in the experiments"
    ],
    "description": [
      "We pro-vide IDs and texts of the posts used as training and test data for CHARM.",
      "we provide the posts containing explicit personal assertions, which have been used for ground truth labeling with the Snorkel framework"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/Anna146/CHARM",
    "section_title": "A Data",
    "add_info": null,
    "text": "All datasets used in the experiments are available at  https://github.com/Anna146/CHARM. We pro-vide IDs and texts of the posts used as training and test data for CHARM. All users are anonymized by replacing usernames with IDs. Additionally, we provide the posts containing explicit personal assertions, which have been used for ground truth labeling with the Snorkel framework."
  },
  {
    "id": 339,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www-nlp.stanford.edu/software/sempre/",
    "section_title": "4 Experiments 4.1 Data & evaluation metric",
    "add_info": "6 We used the official evaluation script from http://www-nlp.stanford.edu/software/sempre/.",
    "text": "We use the W EB Q UESTIONS dataset (Berant et al., 2013), which consists of 5,810 ques-tion/answer pairs. These questions were collected using Google Suggest API and the answers were obtained from Freebase with the help of Amazon MTurk. The questions are split into training and testing sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively. This dataset has several unique properties that make it appealing and was used in several recent papers on semantic parsing and question answering. For instance, although the questions are not directly sampled from search query logs, the selection pro-cess was still biased to commonly asked questions on a search engine. The distribution of this ques-tion set is thus closer to the \u201creal\u201d information need of search users than that of a small number of human editors. The system performance is ba-sically measured by the ratio of questions that are answered correctly. Because there can be more than one answer to a question, precision, recall and F 1 are computed based on the system output for each individual question. The average F 1 score is reported as the main evaluation metric [Cite_Footnote_6] ."
  },
  {
    "id": 340,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The discourse parser",
      "The discourse parser",
      "the parser"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://alt.qcri.org/tools/",
    "section_title": "3 Our Discourse-Based Measures 3.1 Generating Discourse Trees",
    "add_info": "2 The discourse parser is freely available from http://alt.qcri.org/tools/",
    "text": "The discourse parser uses a dynamic Condi-tional Random Field (Sutton et al., 2007) as a pars-ing model in order to infer the probability of all possible discourse tree constituents. The inferred (posterior) probabilities are then used in a proba-bilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F 1 -score of 79.8%, which is very close to the human agree-ment of 83%. These high scores allowed us to de-velop successful discourse similarity metrics. [Cite_Footnote_2]"
  },
  {
    "id": 341,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "This",
      "the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English"
    ],
    "description": [
      "the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.statmt.org/wmt{11,12}/results.html",
    "section_title": "4 Experimental Setup",
    "add_info": "3 http://www.statmt.org/wmt{11,12}/results.html",
    "text": "In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [Cite_Footnote_3] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references."
  },
  {
    "id": 342,
    "name": "A SIYA toolkit",
    "fullname": "N/A",
    "genericmention": [
      "the toolkit"
    ],
    "description": [
      "A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation."
    ],
    "citationtag": [
      "Gime\u0301nez and Ma\u0300rquez, 2010a"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.lsi.upc.edu/asiya/",
    "section_title": "4 Experimental Setup 4.1 MT Evaluation Metrics",
    "add_info": "4 http://nlp.lsi.upc.edu/asiya/",
    "text": "Metrics from A SIYA . We used the freely avail-able version of the A SIYA toolkit [Cite_Footnote_4] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation. For reproducibility, below we explain the individual metrics with the exact names required by the toolkit to calculate them."
  },
  {
    "id": 343,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code",
      "Our implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/pytorch/fairseq/pull/1095",
    "section_title": "References",
    "add_info": "1 Code can be found at https://github.com/pytorch/fairseq/pull/1095",
    "text": "The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, tradi-tional statistical word alignment models of-ten remain the go-to solution. In this pa-per, we present an approach to train a Trans-former model to produce both accurate trans-lations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine trans-lation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces com-petitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accu-racies compared to GIZA++ on three publicly available data sets. Our implementation has been open-sourced [Cite_Footnote_1] ."
  },
  {
    "id": 344,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the decoder",
      "the decoder"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/lilt/alignment-scripts",
    "section_title": "4 Proposed Method 4.3 Providing Full Target Context",
    "add_info": "2 https://github.com/lilt/alignment-scripts",
    "text": "The Transformer decoder computes the probabil-ity of the next target token conditioned on the past target tokens and all source tokens. This is imple-mented by masking the self attention probabilities, i.e. while computing the representation for the i th target token, the decoder can only self-attend to the representations of {1, [Cite_Footnote_2] . . . i \u2212 1} tokens from the previous layer. This auto-regressive behavior of the decoder is crucial for the model to repre-sent a valid probability distribution over the target sentence. However, conditioning on just the past target tokens is limiting for the alignment task. As described in Section 4.2, the alignment head is trained to model the alignment distribution for the i th target token given only the past target to-kens and all source tokens. Since the alignment head does not know the identity of the next tar-get token, it becomes difficult for it to learn this token\u2019s alignment to the source tokens. Previous work has also identified this problem and alleviate it by feeding the target token to be aligned as an input to the module computing the alignment (Pe-ter et al., 2017), or forcing the module to predict the target token (Zenkel et al., 2019) or its prop-erties, e.g. POS tags (Li et al., 2018). Feeding the next target token assumes that we know it in ad-vance and thus calls for separate translation and alignment models. Forcing the alignment module to predict target token\u2019s properties helps but still passes the information of the target token in an in-direct manner. We overcome these limitations by conditioning the two components of our loss func-tion on different amounts of context. The NLL loss L t is conditioned on the past target tokens to preserve the auto-regressive property:"
  },
  {
    "id": 345,
    "name": "NAACL\u201903 Building and Using Parallel Texts word align-ment",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://web.eecs.umich.edu/\u02dcmihalcea/wpt/index.html#resources",
    "section_title": "5 Experiments 5.1 Setup 5.1.1 Alignment Task",
    "add_info": "3 http://web.eecs.umich.edu/\u02dcmihalcea/wpt/index.html#resources",
    "text": "The purpose of the this task is to fairly com-pare with state-of-the-art results in terms of alignment quality and perform a hyperparame-ter search. We use the same experimental setup as described in (Zenkel et al., 2019). The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian\u2192English, English\u2192French and German\u2192English. Train-ing data and test data for Romanian\u2192English and English\u2192French are provided by the NAACL\u201903 Building and Using Parallel Texts word align-ment shared task [Cite_Footnote_3] (Mihalcea and Pedersen, 2003). The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German\u2192English we use the Europarl v7 cor-pus as training data and the gold alignments pro-vided by Vilar et al. (2006). The reference align-ments were created by randomly selecting a subset of the Europarl v7 corpus and manually annotating them following the guidelines suggested in (Och and Ney, 2003). Data statistics are shown in Ta-ble 1."
  },
  {
    "id": 346,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the gold alignments"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Vilar et al. (2006)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www-i6.informatik.rwth-aachen.de/goldAlignment/",
    "section_title": "5 Experiments 5.1 Setup 5.1.1 Alignment Task",
    "add_info": "4 https://www-i6.informatik.rwth-aachen.de/goldAlignment/",
    "text": "The purpose of the this task is to fairly com-pare with state-of-the-art results in terms of alignment quality and perform a hyperparame-ter search. We use the same experimental setup as described in (Zenkel et al., 2019). The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian\u2192English, English\u2192French and German\u2192English. Train-ing data and test data for Romanian\u2192English and English\u2192French are provided by the NAACL\u201903 Building and Using Parallel Texts word align-ment shared task (Mihalcea and Pedersen, 2003). The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German\u2192English we use the Europarl v7 cor-pus as training data and the gold alignments [Cite_Footnote_4] pro-vided by Vilar et al. (2006). The reference align-ments were created by randomly selecting a subset of the Europarl v7 corpus and manually annotating them following the guidelines suggested in (Och and Ney, 2003). Data statistics are shown in Ta-ble 1."
  },
  {
    "id": 347,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/moses-smt/mgiza/",
    "section_title": "5 Experiments 5.2 Statistical Baseline",
    "add_info": "5 https://github.com/moses-smt/mgiza/",
    "text": "For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit 5 implemented by Gao and Vo-gel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vo-gel et al., 1996), respectively. In particular, we perform [Cite_Footnote_5] iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and sym-metrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use the resulting word alignments to supervise the alignment loss for the method described in Section 4.4."
  },
  {
    "id": 348,
    "name": "CIA factsheet",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.cia.gov/cia/publications/factbook",
    "section_title": "2 References to people 2.3 Automatic semantic tagging",
    "add_info": "1 http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian.",
    "text": "As the task definition above suggests, our approach is to identify particular semantic attributes for a per-son, and generate a reference formally from this se-mantic input. Our analysis of human summaries tells us that the semantic attributes we need to identify are role, organization, country, state, location and temporal modifier. In addi-tion, we also need to identify the person name. We used BBN\u2019s I DENTI F INDER (Bikel et al., 1999) to mark up person names, organizations and lo-cations. We marked up countries and (American) states using a list obtained from the CIA factsheet [Cite_Footnote_1] . To mark up roles, we used a list derived from Word-Net (Miller et al., 1993) hyponyms of the person synset. Our list has 2371 entries including multi-word expressions such as chancellor of the exche-quer, brother in law, senior vice president etc. The list is quite comprehensive and includes roles from the fields of sports, politics, religion, military, busi-ness and many others. We also used WordNet to ob-tain a list of 58 temporal adjectives. WordNet classi-fies these as pre- (eg. occasional, former, incoming etc.) or post-nominal (eg. elect, designate, emeritus etc.). This information is used during generation. Further, we identified elementary noun phrases us-ing the LT TTT noun chunker (Grover et al., 2000), and combined NP of NP sequences into one com-plex noun phrase. An example of the output of our semantic tagging module on a portion of machine translated text follows:"
  },
  {
    "id": 349,
    "name": "BLEU (Papineni et al., 2002) and NIST [Cite_Footnote_2] MT metrics",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Papineni et al., 2002"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.nist.gov/speech/tests/mt/resources/scoring.htm",
    "section_title": "2 References to people 2.6 Evaluation",
    "add_info": "2 http://www.nist.gov/speech/tests/mt/resources/scoring.htm",
    "text": "We used 6 document sets from DUC\u201904 for devel-opment purposes and present the average P, R and F for the remaining 18 sets in Table 1. There were 210 generated references in the 18 testing sets. The table also shows the popular BLEU (Papineni et al., 2002) and NIST [Cite_Footnote_2] MT metrics. We also provide two base-lines - most frequent initial reference to the person in the input (Base1) and a randomly selected initial reference to the person (Base2). As Table 1 shows, Base1 performs better than random selection. This is intuitive as it also uses redundancy to correct er-rors, at the level of phrases rather than words. The generation module outperforms both baselines, par-ticularly on precision - which for unigrams gives an indication of the correctness of lexical choice, and for higher ngrams gives an indication of grammati-cality. The unigram recall of x\u2021}\u2030X\u0152C\u008d indicates that we are not losing too much information at the noise fil-tering stage. Note that we expect a low \u00a5\u00a7\u00a65\u00a8 for our approach, as we only generate particular attributes that are important for a summary. The important measure is , on which we do well. This is also reflected in the high scores on BLEU and NIST."
  },
  {
    "id": 350,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a public spell checker"
    ],
    "description": [
      "spell checker"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/",
    "section_title": "5 Experiments 5.2 Experimental setting",
    "add_info": "4 https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/",
    "text": "We resolve spelling errors with a public spell checker [Cite_Footnote_4] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do."
  },
  {
    "id": 351,
    "name": "NIST MT 06 dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.itl.nist.gov/iad/mig/tests/mt/",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "5 http://www.itl.nist.gov/iad/mig/tests/mt/",
    "text": "Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC cor-pora, with 27.9M Chinese words and 34.5M En-glish words respectively. We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. [Cite_Footnote_5] To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task."
  },
  {
    "id": 352,
    "name": "Berkeley Parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/slavpetrov/berkeleyparser",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "6 https://github.com/slavpetrov/berkeleyparser",
    "text": "Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC cor-pora, with 27.9M Chinese words and 34.5M En-glish words respectively. We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser [Cite_Footnote_6] (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task."
  },
  {
    "id": 353,
    "name": "cdecerarchical",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "an opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data."
    ],
    "citationtag": [
      "Dyerphrase-basedet al., 2010"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/redpony/cdec",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "7 https://github.com/redpony/cdec",
    "text": "\u2022 cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [Cite_Footnote_7]"
  },
  {
    "id": 354,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the usual evalb program"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Satoshi Sekine and Michael Collins. 2008. Evalb.",
      "Sekine and Collins, 2008"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://nlp.cs.nyu.edu/evalb/",
    "section_title": "2 Overview and Example 2.1 Creation of Dependency Representation",
    "add_info": "Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.",
    "text": "The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008)  , which ignores function tags and empty cate-gories. We return to this issue in the conclusion."
  },
  {
    "id": 355,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the standard evalb scoring code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Satoshi Sekine and Michael Collins. 2008. Evalb.",
      "Sekine and Collins, 2008"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "http://nlp.cs.nyu.edu/evalb/",
    "section_title": "4 Results of Dependency to Phrase Structure Conversion",
    "add_info": "Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.",
    "text": "To evaluate the correctness of conversion from de-pendency to phrase structure, we follow the same strategy as Xia and Palmer (2001) and Xia et al. (2009). We convert the phrase structure trees in the PTB to dependency structure and convert the depen-dency back to phrase structure. We then compare the original PTB trees with the newly-created phrase structure trees, using the standard evalb scoring code (Sekine and Collins, 2008)  . Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projec-tion chains, and where to attach dependent subtrees. They reported results for their system for Section 00 of the PTB, and we include in Table 2 only their highest scoring algorithm. The system of Xia et al. (2009) uses conversion rules learned from Section 19, and then tested on Sections 00 and Section 22."
  },
  {
    "id": 356,
    "name": "TensorFlow constrained optimization",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Cotter et al., 2019a,b"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/tensorflow_constrained_optimization",
    "section_title": "3 Proposed Approaches 3.2 Bias-constrained Model",
    "add_info": "3 https://github.com/google-research/tensorflow_constrained_optimization",
    "text": "Each group-wise constraint is denoted as \u03c8 g . The constraints involve a linear combination of indicator variables, which is not differentiable wrt \u03b8. A common approach to handle this constrained optimization problems is using the Lagrangian, which is minimized over \u03b8 and maximized over \u03bb. Similar formulations have been used for learn-ing fair models with structured data (Cotter et al., 2019b; Yang et al., 2020; Zafar et al., 2019). In this work, we apply this method to NLP tasks and use the two-player zero-sum game approach for optimization, where the first player chooses \u03b8 to minimize L(\u03b8, \u03bb), and the second player enforces fairness constraints by maximizing \u03bb (Kearns et al., 2018; Cotter et al., 2019b; Yang et al., 2020). Specifically we use the implementations available in TensorFlow constrained optimization (Cotter et al., 2019a,b). [Cite_Footnote_3]"
  },
  {
    "id": 357,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "this data",
      "their sil-ver data annotated with the high quality supertags"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://github.com/uwnlp/taggerflow",
    "section_title": "5 Tri-training",
    "add_info": "5 https://github.com/uwnlp/taggerflow",
    "text": "We simply combine the two previous ap-proaches. Lewis et al. (2016) obtain their sil-ver data annotated with the high quality supertags. Since they make this data publicly available [Cite_Footnote_5] , we obtain our silver data by assigning dependency"
  },
  {
    "id": 358,
    "name": "GloVe",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pen-nington et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "6 Experiments 6.1 English Experimental Settings",
    "add_info": "8 http://nlp.stanford.edu/projects/glove/",
    "text": "We use as word representation the concatena-tion of word vectors initialized to GloVe [Cite_Footnote_8] (Pen-nington et al., 2014), and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d."
  },
  {
    "id": 359,
    "name": "Jigg",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "shift-reduce CCG parser"
    ],
    "citationtag": [
      "Noji and Miyao, 2016"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/mynlp/jigg",
    "section_title": "6 Experiments 6.2 Japanese Experimental Settings",
    "add_info": "9 https://github.com/mynlp/jigg",
    "text": "We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [Cite_Footnote_9] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs."
  },
  {
    "id": 360,
    "name": "Japanese Wikipedia Entity Vector",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.cl.ecei.tohoku.ac.jp/\u02dcm-suzuki/jawiki_vector/",
    "section_title": "6 Experiments 6.2 Japanese Experimental Settings",
    "add_info": "10 http://www.cl.ecei.tohoku.ac.jp/\u02dcm-suzuki/jawiki_vector/",
    "text": "For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector [Cite_Footnote_10] , and 100-dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014). We do not use affix vectors as affixes are less informative in Japanese. All characters ap-pearing less than two times are mapped to \u201cUNK\u201d. We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization."
  },
  {
    "id": 361,
    "name": "CaboCha",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://taku910.github.io/cabocha/",
    "section_title": "6 Experiments 6.2 Japanese Experimental Settings",
    "add_info": "11 http://taku910.github.io/cabocha/",
    "text": "One issue in Japanese experiments is evalua-tion. The Japanese CCGbank is encoded in a dif-ferent format than the English bank, and no stan-dalone script for extracting semantic dependen-cies is available yet. For this reason, we evaluate the parser outputs by converting them to bunsetsu dependencies, the syntactic representation ordi-nary used in Japanese NLP (Kudo and Matsumoto, 2002). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha [Cite_Footnote_11] and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b. For ex-ample, the sentence in Figure 4e is segmented as \u201cBoku wa | eigo wo | hanashi tai\u201d, from which we extract two dependencies (Boku wa) \u2190 (hanashi tai) and (eigo wo) \u2190 (hanashi tai). We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy. Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser."
  },
  {
    "id": 362,
    "name": "C++ Tensor-Flow",
    "fullname": "N/A",
    "genericmention": [
      "Software"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Abadi et al., 2015"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://tensorflow.org/",
    "section_title": "6 Experiments 6.3 English Parsing Results",
    "add_info": "Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane\u0301, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Vie\u0301gas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Sys-tems. Software available from tensorflow.org. http://tensorflow.org/.",
    "text": "Efficiency Comparison We compare the ef-ficiency of our parser with neuralccg and EasySRL reimpl. The results are shown in Table 4. For the overall speed (the third row), our parser is faster than neuralccg al-though lags behind EasySRL reimpl. Inspect-ing the details, our supertagger runs slower than those of neuralccg and EasySRL reimpl, while in A* search our parser processes over 7 times more sentences than neuralccg. The delay in supertagging can be attributed to sev-eral factors, in particular the differences in net-work architectures including the number of bi- LSTM layers (4 vs. 2) and the use of bilin-ear transformation instead of linear one. There are also many implementation differences in our parser (C++ A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ Tensor-Flow (Abadi et al., 2015)  supertagger and recur-sive neural model in C++ DyNet (Neubig et al., 2017))."
  },
  {
    "id": 363,
    "name": "GloVe",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pen-nington et al., 2014",
      "Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543."
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.aclweb.org/anthology/D14-1162",
    "section_title": "6 Experiments 6.1 English Experimental Settings",
    "add_info": "Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.",
    "text": "We use as word representation the concatena-tion of word vectors initialized to GloVe (Pen-nington et al., 2014)  , and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d."
  },
  {
    "id": 364,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Pascanu et al., 2014"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/fxsjy/jieba",
    "section_title": "5 Experiments 5.2 Neural Hidden Markov Model",
    "add_info": "1 https://github.com/fxsjy/jieba",
    "text": "Apart from the basic projection layer, we also applied LSTM layers for the source and target words embedding. The embedding layers have 350 nodes and the size of the projection layer is 800 (400 + 200 + 200, Figure 1). We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [Cite_Footnote_1] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance."
  },
  {
    "id": 365,
    "name": "RNNLM toolkit",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models.",
      "Mikolov, 2012a"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://rnnlm.org",
    "section_title": "3 Experimental Setup 3.1 Unlabeled tweets",
    "add_info": "Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.",
    "text": "In order to train our SRN language model we col-lected a set of tweets using the Twitter sampling API. We use the raw sample directly without fil-tering it in any way, relying on the SRN to learn the structure of the data. The sample consists of 414 million bytes of UTF-8 encoded in a variety of languages and scripts text. We trained a 400-hidden-unit SRN, to predict the next byte in the sequence using backpropagation through time. In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a)  to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer. We did try to filter tweets by language and create specific em-beddings for English but this had negligible effect on tweet normalization performance."
  },
  {
    "id": 366,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the system",
      "The system"
    ],
    "description": [
      "a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models",
      "the first system for word sense induction and disambigua-tion, which is unsupervised, knowledge-free, and interpretable at the same time",
      "The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledge-free framework."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/uhh-lt/wsd",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/uhh-lt/wsd",
    "text": "We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambigua-tion, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledge-free framework. Implementation of the system is open source. [Cite_Footnote_1] A live demo featuring several dis-ambiguation models is available online."
  },
  {
    "id": 367,
    "name": "Apache Spark framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://spark.apache.org",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.1 Induction of the WSD Models",
    "add_info": "4 http://spark.apache.org",
    "text": "Figure 1 presents architecture of the WSD sys-tem. As one may observe, no human labor is used to learn interpretable sense representa-tions and the corresponding disambiguation mod-els. Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework [Cite_Footnote_4] , enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are la-beled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) pat-terns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction con-fidence. Top sentences are used as sense usage ex-amples. For more details about the model induc-tion process refer to (Panchenko et al., 2017). Cur-rently, the following WSD models induced from a text corpus are available:"
  },
  {
    "id": 368,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a rela-tional database",
      "the database",
      "the database"
    ],
    "description": [
      "each word sense is represented by its hypernyms, related words, and usage examples",
      "for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.postgresql.org",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.2 WSD API",
    "add_info": "5 https://www.postgresql.org",
    "text": "To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. [Cite_Footnote_5] In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework."
  },
  {
    "id": 369,
    "name": "Breeze",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/scalanlp/breeze",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.2 WSD API",
    "add_info": "6 https://github.com/scalanlp/breeze",
    "text": "To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. [Cite_Footnote_6] Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework."
  },
  {
    "id": 370,
    "name": "Play framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.playframework.com",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.2 WSD API",
    "add_info": "7 https://www.playframework.com",
    "text": "To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework. [Cite_Footnote_7]"
  },
  {
    "id": 371,
    "name": "React framework",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://facebook.github.io/react",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.3 User Interface for Interpretable WSD",
    "add_info": "8 https://facebook.github.io/react",
    "text": "The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [Cite_Footnote_8] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:"
  },
  {
    "id": 372,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "an image search API"
    ],
    "description": [
      "an image search API [Cite_Footnote_9] us-ing a query composed of the ambiguous word and its hypernym, e.g. \u201cjaguar animal\u201d"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://azure.microsoft.com/en-us/services/cognitive-services/search",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.3 User Interface for Interpretable WSD",
    "add_info": "9 https://azure.microsoft.com/en-us/services/cognitive-services/search",
    "text": "Single word disambiguation mode is illus-trated in Figure 2. In this mode, a user specifies an ambiguous word and its context. The output of the system is a ranked list of all word senses of the ambiguous word ordered by relevance to the input context. By default, only the best matching sense is displayed. The user can quickly understand the meaning of each induced sense by looking at the hypernym and the image representing the sense. Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API [Cite_Footnote_9] us-ing a query composed of the ambiguous word and its hypernym, e.g. \u201cjaguar animal\u201d. The first hit of this query is selected to represent the induced word sense. Interpretability of each sense is fur-ther ensured by providing to the user the list of related senses, the list of the most salient context clues, and the sense usage examples (cf. Figure 2). Note that all these elements are obtained without manual intervention."
  },
  {
    "id": 373,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a part-of-speech and a named entity taggers"
    ],
    "description": [
      "a part-of-speech and a named entity taggers"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.scalanlp.org",
    "section_title": "3 Unsupervised Knowledge-Free Interpretable WSD 3.3 User Interface for Interpretable WSD",
    "add_info": "10 http://www.scalanlp.org",
    "text": "All words disambiguation mode is illustrated in Figure 3. In this mode, the system performs dis-ambiguation of all nouns and entities in the input text. First, the text is processed with a part-of-speech and a named entity taggers. [Cite_Footnote_10] Next, each detected noun or entity is disambiguated in the same way as in the single word disambiguation mode described above, yet the disambiguation re-sults are represented as annotations of a running text. The best matching sense is represented by a hypernym and an image as depicted in Figure 3. This mode performs \u201csemantification\u201d of a text, which can, for instance, assist language learners with the understanding of a text in a foreign lan-guage: Meaning of unknown to the learner words can be deduced from hypernyms and images."
  },
  {
    "id": 374,
    "name": "Docker containers",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.docker.com",
    "section_title": "5 Conclusion",
    "add_info": "12 https://www.docker.com",
    "text": "We present the first openly available word sense disambiguation system that is unsupervised, knowledge-free, and interpretable at the same time. The system performs extraction of word and super sense inventories from a text corpus. The disambiguation models are learned in an unsuper-vised way for all words in the corpus on the ba-sis on the induced inventories. The user inter-face of the system provides efficient access to the produced WSD models via a RESTful API or via an interactive Web-based graphical user interface. The system is available online and can be directly used from external applications. The code and the WSD models are open source. Besides, in-house deployments of the system are made easy due to the use of the Docker containers. [Cite_Footnote_12] A prominent direction for future work is supporting more lan-guages and establishing cross-lingual sense links."
  },
  {
    "id": 375,
    "name": "IMDB",
    "fullname": "IMDB Movie Review Corpus",
    "genericmention": [
      "This cor-pus"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Maas et al. (2011)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://ai.stanford.edu/\u02dcamaas/data/sentiment/",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "1 http://ai.stanford.edu/\u02dcamaas/data/sentiment/",
    "text": "We use the IMDB Movie Review Corpus (IMDB) prepared by Maas et al. (2011). [Cite_Footnote_1] This cor-pus has 75k training reviews and 25k test reviews."
  },
  {
    "id": 376,
    "name": "BBC corpus",
    "fullname": "N/A",
    "genericmention": [
      "this corpus"
    ],
    "description": [
      "this corpus contains news articles which are almost always written in a formal style"
    ],
    "citationtag": [
      "Greene and Cunningham (2006)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mlg.ucd.ie/datasets/bbc.html",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "2 http://mlg.ucd.ie/datasets/bbc.html",
    "text": "BBC Similarly to movie reviews, each new ar-ticle tends to convey a single theme. We use the BBC corpus prepared by Greene and Cunningham (2006). [Cite_Footnote_2] Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro-posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages. We use the 10k most frequent words in the training corpus for recurrent language models."
  },
  {
    "id": 377,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl",
    "text": "Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization. [Cite_Footnote_3] Penn Treebank We evaluate a normal recurrent language model, count-based n-gram language model as well as the proposed RLM-BoW-EF-n and RLM-BoW-LF-n with varying n = 1, 2, 4, 8 on the Penn Treebank Corpus. We preprocess the corpus according to (Mikolov et al., 2011) and use a vocabulary of 10k words from the training cor-pus."
  },
  {
    "id": 378,
    "name": "Fil9",
    "fullname": "N/A",
    "genericmention": [
      "the corpus"
    ],
    "description": [
      "a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mattmahoney.net/dc/textdata",
    "section_title": "5 Experimental Settings 5.2 Datasets",
    "add_info": "4 http://mattmahoney.net/dc/textdata",
    "text": "Fil9 Fil9 is a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens, and is pro-vided on Matthew Mahoney\u2019s website. [Cite_Footnote_4] We tok-enized the corpus and used the 44k most frequent words in the training corpus for recurrent language models."
  },
  {
    "id": 379,
    "name": "Stanford POS Tagger",
    "fullname": "Stanford log-linear part-of-speech tagge",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Toutanova et al., 2003"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/tagger.shtml",
    "section_title": "6 Results and Analysis",
    "add_info": "5 http://nlp.stanford.edu/software/tagger.shtml",
    "text": "We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [Cite_Footnote_5] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb."
  },
  {
    "id": 380,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://rule.alibaba.com/rule/detail/2034.htm",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 381,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.apple.com/legal/privacy/en-ww/",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 382,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.cbsinteractive.com/legal/cbsi/privacy-policy",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 383,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "four website privacy policies"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://help.bet365.com/en/privacy-policy",
    "section_title": "1 Introduction",
    "add_info": "2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy",
    "text": "However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach."
  },
  {
    "id": 384,
    "name": "jsoup",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Jonathan Hedley. 2017. jsoup (1.11.3).",
      "Hedley, 2017"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://jsoup.org/",
    "section_title": "3 Approach 3.1 Domain-Independent Approach (DI)",
    "add_info": "Jonathan Hedley. 2017. jsoup (1.11.3). https://jsoup.org/.",
    "text": "Text Collection: Using jsoup (Hedley, 2017)  , we parse the HTML file and for each non-empty tag encountered we extract a tuple consisting of the text and its XPath."
  },
  {
    "id": 385,
    "name": "GVDB",
    "fullname": "Gun Violence Database",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://gun-violence.org/",
    "section_title": "3 The Gun Violence Database",
    "add_info": "2 http://gun-violence.org/",
    "text": "In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [Cite_Footnote_2] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence. The GVDB is the result of a large crowdsourced annotation effort. This an-notation is ongoing, and the GVDB will be regularly updated with new data and new layers of annotation, making it an interesting and challenging data set on which to evaluate state-of-the-art NLP tools."
  },
  {
    "id": 386,
    "name": "GVDB",
    "fullname": "Gun Violence Database",
    "genericmention": [
      "the database",
      "the database"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://gun-violence.org/",
    "section_title": "3 The Gun Violence Database",
    "add_info": null,
    "text": "At the time of writing, the GVDB contains 7,366 fully annotated articles (Table 1) coming from 1,512 US cities, and the database is continuing to grow. The latest version of the database will be main-tained and available for download at  http://gun-violence.org/."
  },
  {
    "id": 387,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/zhaozj89/TensorEmbeddingNLP",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at  https://github.com/zhaozj89/TensorEmbeddingNLP."
  },
  {
    "id": 388,
    "name": "Yelp dataset",
    "fullname": "N/A",
    "genericmention": [
      "The dataset"
    ],
    "description": [
      "The dataset consists of product reviews aligned with sentiment ratings from 1 to 5."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.yelp.com/dataset",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "2 https://www.yelp.com/dataset",
    "text": "We use one of the classic NLP tasks that commonly suffer profanity issues - text style transfer, to eval-uate the effectiveness of the proposed framework. Particularly, we conduct experiments on a subset of the widely used Yelp dataset [Cite_Footnote_2] . The dataset consists of product reviews aligned with sentiment ratings from 1 to 5. We normalize the ratings by treating ratings below three as negative (0) and otherwise positive (1). After data cleaning, we use the method presented in (Li et al., 2018b) to construct pseudo sentence pairs, which is commonly used in the style transfer field. Then we randomly select 240 thou-sand sentence pairs for training, one thousand for validation, and one hundred for testing. Our task is to transfer the sentence from positive opinion to negative. Here, we only use a small test set because we only use these test samples to test the outcome of the attacks rather than the original task."
  },
  {
    "id": 389,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/rivercold/BERT-unsupervised-OOD",
    "section_title": "References",
    "add_info": "1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.",
    "text": "Deployed real-world machine learning appli-cations are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety is-sues. Prior studies on out-of-domain detec-tion require in-domain task labels and are lim-ited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent represen-tations of pre-trained transformers and pro-pose a simple yet effective method to trans-form features across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further pro-posed to boost detection accuracy. Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [Cite_Footnote_1]"
  },
  {
    "id": 390,
    "name": "TensorFlow code",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Zhong-Yi Li. 2017.",
      "Li, 2017"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow",
    "section_title": "4 Case Study: Transformer on Different Tasks",
    "add_info": "Zhong-Yi Li. 2017. https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow.",
    "text": "The first task we explored is the variational autoencoder (VAE) language modeling (Bowman et al., 2015). We test two models, one with an LSTM RNN decoder which is traditionally used in the task, and the other with a Transformer de-coder. All other model configurations including parameter size are the same across the two mod-els. Table 1, top panel, shows the Transformer VAE consistently improves over the LSTM VAE. With Texar, changing the decoder from an LSTM to a Transformer is easily achieved by modifying only 3 lines of code. It is also worth noting that, building the VAE language model (including data reading, model construction, and optimization) on Texar uses only 70 lines of code (with the length of each line < 80 chars). As a (rough) reference, a popular public TensorFlow code (Li, 2017)  of the same model has used around 400 lines of code for the same part (without line length limit)."
  },
  {
    "id": 391,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The framework and the DTs for Google Books, News-paper and Wikipedia"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://sf.net/projects/jobimtext/",
    "section_title": "6 Conclusion",
    "add_info": "3 https://sf.net/projects/jobimtext/",
    "text": "We have introduced a highly scalable approach to DT computation and showed its adequacy for very large corpora. Evaluating against thesauri and WordNet, we demonstrated that our similarity mea-sure yields better-quality DTs and scales to corpora of billions of sentences, even on comparably small compute clusters. We achieve this by a number of pruning operations, and distributed processing. The framework and the DTs for Google Books, News-paper and Wikipedia are available online [Cite_Footnote_3] under the ASL 2.0 licence."
  },
  {
    "id": 392,
    "name": "Morfessor tool",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology.",
      "Creutz and Lagus, 2005"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cis.hut.fi/projects/morpho/",
    "section_title": "4 Speech recognition experiments 4.4 Turkish",
    "add_info": "Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.",
    "text": "Turkish is another a highly-inflected and agglutina-tive language with relatively free word order. The same Morfessor tool (Creutz and Lagus, 2005)  as in Finnish and Estonian was applied to Turkish texts as well. Using the 360k most common words from the training corpus, 34k morph units were obtained. The training corpus consists of approximately 27M words taken from literature, law, politics, social sciences, popular science, information technology, medicine, newspapers, magazines and sports news. N-gram language models for different orders with interpolated Kneser-Ney smoothing as well as en-tropy based pruning were built for this morph lexi-con using the SRILM toolkit (Stolcke, 2002). The number of n-grams for the highest order we tried (6-grams without entropy-based pruning) are reported in Table 4. In average, there are 2.37 morphs per word including the word break symbol."
  },
  {
    "id": 393,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a joint corpus",
      "the same text corpus"
    ],
    "description": [
      "a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words"
    ],
    "citationtag": [
      "CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other.",
      "CSC, 2001"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.csc.fi/kielipankki/",
    "section_title": "4 Speech recognition experiments 4.2 Finnish",
    "add_info": "CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other. http://www.csc.fi/kielipankki/.",
    "text": "Finnish is a highly inflected language, in which words are formed mainly by agglutination and com-pounding. Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001)  . We obtained a lexicon of 25k morphs by feeding the learning algorithm with the word list containing the 160k most common words. For language model training we used the same text corpus and the re-cently developed growing n-gram training algorithm (Siivola and Pellom, 2005). The amount of resulted n-grams are listed in Table 4. The average length of a morph is such that a word corresponds to 2.52 morphs including a word break symbol."
  },
  {
    "id": 394,
    "name": "ACE parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://sweaglesw.org/linguistics/ace/",
    "section_title": "3 System Description",
    "add_info": "3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.",
    "text": "The new system described here is what we call the MRS Crawler. This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below). We used the grammar together with one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures. Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an \u2018off the shelf\u2019 setting. We combine our system with the outputs from the best-performing 2012 submission, the system of Read et al. (2012), firstly by relying on the latter for system negation cue detection, 4 and secondly as a fall-back in sys-tem combination as described in \u00a7 3.4 below."
  },
  {
    "id": 395,
    "name": "DELPH-IN resources",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.delph-in.net",
    "section_title": "3 System Description",
    "add_info": "3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.",
    "text": "The new system described here is what we call the MRS Crawler. This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below). We used the grammar together with one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures. Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an \u2018off the shelf\u2019 setting. We combine our system with the outputs from the best-performing 2012 submission, the system of Read et al. (2012), firstly by relying on the latter for system negation cue detection, 4 and secondly as a fall-back in sys-tem combination as described in \u00a7 3.4 below."
  },
  {
    "id": 396,
    "name": "N/A",
    "fullname": "NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "1 http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip",
    "text": "The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset [Cite_Footnote_1] . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC. 94,651 unlabeled Chinese products re-views from corresponding categories are used as the development set, denoted as Dev_CHN. In the testing set, there are 12,000 Chinese product reviews (shown in Table.1). This dataset is de-signed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN : where c is either DVD, BOOK or MUSIC."
  },
  {
    "id": 397,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "2 http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf",
    "text": "The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC. 94,651 unlabeled Chinese products re-views from corresponding categories are used as the development set, denoted as Dev_CHN. In the testing set, there are 12,000 Chinese product reviews (shown in Table.1). This dataset is de-signed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN [Cite_Footnote_2] : where c is either DVD, BOOK or MUSIC."
  },
  {
    "id": 398,
    "name": "Google Translator",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://translate.google.com",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "3 https://translate.google.com",
    "text": "In the experiment, the basic transfer learning algorithm is co-training. The Chinese word seg-mentation tool is ICTCLAS (Zhang et al, 2003) and Google Translator [Cite_Footnote_3] is the MT for the source language. The monolingual opinion classifier is SVM light4 , word unigram/bigram features are em-ployed."
  },
  {
    "id": 399,
    "name": "Rus-tomata",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "a framework for weighted automata with storage written in the programming language Rust"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/tud-fop/",
    "section_title": "5 Evaluation and Conclusion",
    "add_info": "5 available on https://github.com/tud-fop/rustomata. We used commit 867a451 for evaluation.",
    "text": "We implemented the parser with the modifica-tions sketched in sec. 4 for \u03b5-free and simple wMCFGs, but no problems should arise gener-alising this implementation to arbitrary wMCFGs. The implementation is available as a part of Rus-tomata, [Cite_Footnote_5] a framework for weighted automata with storage written in the programming language Rust. We used the NeGra corpus (German newspaper articles, 20,602 sentences, 355,096 tokens; Skut et al., 1998) to compare our parser to Grammat-ical Framework (Angelov and Ljunglo\u0308f, 2014), rparse (Kallmeyer and Maier, 2013), and disco-dop (van Cranenburgh et al., 2016) with respect to parse time and accuracy. Our experiments were conducted on defoliated trees, i.e. we removed the leaves from each tree in the corpus. Parsing was performed on gold part-of-speech tags."
  },
  {
    "id": 400,
    "name": "InferWiki",
    "fullname": "N/A",
    "genericmention": [
      "Our datasets"
    ],
    "description": [
      "a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, as-sumptions, and patterns",
      "providing manually annotated nega-tive and unknown triples",
      "we include various inference patterns (e.g., reasoning path length and types) for comprehensive evalua-tion"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/TaoMiner/inferwiki",
    "section_title": "References",
    "add_info": null,
    "text": "We present InferWiki, a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, as-sumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. To ensure it, we propose to utilize rule-guided train/test generation, in-stead of conventional random split. Second, InferWiki initiates the evaluation following the open-world assumption and improves the in-ferential difficulty of the closed-world assump-tion, by providing manually annotated nega-tive and unknown triples. Third, we include various inference patterns (e.g., reasoning path length and types) for comprehensive evalua-tion. In experiments, we curate two settings of InferWiki varying in sizes and structures, and apply the construction process on CoDEx as comparative datasets. The results and em-pirical analyses demonstrate the necessity and high-quality of InferWiki. Nevertheless, the performance gap among various inferential as-sumptions and patterns presents the difficulty and inspires future research direction. Our datasets can be found in  https://github.com/TaoMiner/inferwiki."
  },
  {
    "id": 401,
    "name": "Wikidata",
    "fullname": "N/A",
    "genericmention": [
      "the September 2019 En-glish dump"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://www.wikidata.org/",
    "section_title": "3 Dataset Design 3.1 Data Preprocessing",
    "add_info": "1 https://www.wikidata.org/",
    "text": "More and more studies utilize Wikidata [Cite_Footnote_1] as a knowledge resource due to its high quality and large quantity. We utilize the September 2019 En-glish dump in experiments. Data preprocessing aims to define relation vocabulary and extract two sets of triples from Wikidata: a large one for rule mining T r and a relatively small one for dataset generation T d . The reason for using two sets is to avoid the leakage of rules. In other words, some frequent rules on the large set may be very few on the small set. The different distributions shall avoid that rule mining methods will easily achieve high performance. Besides, more triples can improve the quality of mined rules. In contrast, the relatively small set is enough for efficient KGC training and evaluation."
  },
  {
    "id": 402,
    "name": "AnyBURL",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Meilicke et al., 2019"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://web.informatik.uni-mannheim.de/AnyBURL/",
    "section_title": "3 Dataset Design 3.1 Data Preprocessing",
    "add_info": "2 http://web.informatik.uni-mannheim.de/AnyBURL/",
    "text": "3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Gala\u0301rraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL [Cite_Footnote_2] in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets \u03bb p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation \u2014 most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop 8 , and AnyBURL 9 . Because we utilize various"
  },
  {
    "id": 403,
    "name": "OpenKE",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/thunlp/OpenKE",
    "section_title": "F Experiment Setup",
    "add_info": "5 https://github.com/thunlp/OpenKE",
    "text": "Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [Cite_Footnote_5] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data. We then search the embedding size in the range of {256, 512}, number of negative samples in the range of {15, 25} and margin in the range of {4, 8}. The optimal parameters of each model on all of three datasets are listed in Table 10. The thresholds in triples classification are listed in Ta-ble 11"
  },
  {
    "id": 404,
    "name": "ConvE",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/TimDettmers/ConvE",
    "section_title": "F Experiment Setup",
    "add_info": "6 https://github.com/TimDettmers/ConvE",
    "text": "Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE [Cite_Footnote_6] , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data. We then search the embedding size in the range of {256, 512}, number of negative samples in the range of {15, 25} and margin in the range of {4, 8}. The optimal parameters of each model on all of three datasets are listed in Table 10. The thresholds in triples classification are listed in Ta-ble 11"
  },
  {
    "id": 405,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/salesforce/MultiHopKG",
    "section_title": "F Experiment Setup",
    "add_info": "8 https://github.com/salesforce/MultiHopKG",
    "text": "3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Gala\u0301rraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL 2 in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets \u03bb p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation \u2014 most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop [Cite_Footnote_8] , and AnyBURL . Because we utilize various"
  },
  {
    "id": 406,
    "name": "AnyBURL",
    "fullname": "N/A",
    "genericmention": [
      "it"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Meilicke et al., 2019"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://web.informatik.uni-mannheim.de/AnyBURL/",
    "section_title": "F Experiment Setup",
    "add_info": "9 http://web.informatik.uni-mannheim.de/AnyBURL/",
    "text": "3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Gala\u0301rraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL 2 in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets \u03bb p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation \u2014 most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop , and AnyBURL [Cite_Footnote_9] . Because we utilize various"
  },
  {
    "id": 407,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "this data set "
    ],
    "description": [
      "dialogue sessions",
      "For each session i, annotators create two correct responses and an arbitrary number(M i ) of incorrect responses based on the instruction described above.",
      "We set up one test case to consist of context, one correct response, and one incorrect response."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/kakaoenterprise/KorAdvMRSTestData",
    "section_title": "2 Adversarial Test Dataset",
    "add_info": null,
    "text": "Five annotators generate a total of 200 dialogue sessions. For each session i, annotators create two correct responses and an arbitrary number(M i ) of incorrect responses based on the instruction described above. All sessions and responses are reviewed and filtered by experts. We set up one test case to consist of context, one correct response, and one incorrect response. Therefore, 2 \u2217 M i test cases were extracted for each session, and a total of 2,220 test cases are constructed. It evaluates whether the model gives the correct answer a higher score than the incorrect one for a given context. Statistics and examples are described in Table 1. We release this data set at  https://github.com/kakaoenterprise/KorAdvMRSTestData."
  },
  {
    "id": 408,
    "name": "Ko-rean dialogue corpus",
    "fullname": "N/A",
    "genericmention": [
      "these corpora",
      "each",
      "each dataset"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294."
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://corpus.korean.go.kr",
    "section_title": "4 Experiments and Results 4.1 Experiment Setup",
    "add_info": "1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294.",
    "text": "We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [Cite_Footnote_1] . We split these corpora into three, and each is for training, validation, and test. Statistics of each dataset are described in Table 2. #pairs denote the number of context-response pairs, #cands denotes the number of candidates per context, pos:neg denotes the ratio of positive and negative responses in candidates, and #turns de-note the average turns per context. Details on the construction are as follows."
  },
  {
    "id": 409,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "our implementation",
      "our implementation"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://groups.csail.mit.edu/nlp/dpo3/",
    "section_title": "3. A free distribution of our implementation. 2",
    "add_info": "2 http://groups.csail.mit.edu/nlp/dpo3/",
    "text": "3. A free distribution of our implementation. [Cite_Footnote_2]"
  },
  {
    "id": 410,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code to reproduce our results"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/adhigunasurya/distillation_parser.git",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/adhigunasurya/distillation_parser.git",
    "text": "The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the con-ventional Hamming cost function, (ii) recently pub-lished strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graph-based dependency parsing for English, Chinese, and German. The code to reproduce our results is pub-licly available. [Cite_Footnote_1]"
  },
  {
    "id": 411,
    "name": "stack LSTM parser",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Dyer et al., 2015",
      "Dyer et al. (2015)"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/clab/lstm-parser",
    "section_title": "3 Consensus and Minimum Bayes Risk",
    "add_info": "3 We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.",
    "text": "Next, note that if we let s(h, m, x) = votes(h, m)/N, this has no effect on the parser (we have only scaled by a constant factor). We can there-fore view s as a posterior marginal, and the ensemble parser as an MBR parser (Eq. 2). Experiment. We consider this approach on the Stanford dependencies version 3.3.0 (De Marneffe and Manning, 2008) Penn Treebank task. As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [Cite_Footnote_3]"
  },
  {
    "id": 412,
    "name": "CNN neural network library",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/clab/cnn.git",
    "section_title": "6 Experiments",
    "add_info": "8 https://github.com/clab/cnn.git",
    "text": "Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library. [Cite_Footnote_8] Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncer-tainty under the ensemble. To obtain the ensemble uncertainty on each language, we use 21 base mod-els for English (see footnote 4), 17 for Chinese, and 11 for German."
  },
  {
    "id": 413,
    "name": "lex4all",
    "fullname": "N/A",
    "genericmention": [
      "its"
    ],
    "description": [
      "an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://lex4all.github.io/lex4all/",
    "section_title": "1 Introduction",
    "add_info": "1 http://lex4all.github.io/lex4all/",
    "text": "This is the motivation behind lex4all, [Cite_Footnote_1] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language."
  },
  {
    "id": 414,
    "name": "CMUSphinx",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.cmusphinx.org",
    "section_title": "2 Background and related work",
    "add_info": "3 http://www.cmusphinx.org",
    "text": "If, however, the target language is one of the many thousands of LRLs for which high-quality recognition engines have not yet been devel-oped, alternative strategies for developing speech-recognition interfaces must be employed. Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx [Cite_Footnote_3] ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit 4 (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low-or zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users."
  },
  {
    "id": 415,
    "name": "RLAT",
    "fullname": "Rapid Language Adaptation Toolkit",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Vu et al., 2010"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://i19pc5.ira.uka.de/rlat-dev",
    "section_title": "2 Background and related work",
    "add_info": "4 http://i19pc5.ira.uka.de/rlat-dev",
    "text": "If, however, the target language is one of the many thousands of LRLs for which high-quality recognition engines have not yet been devel-oped, alternative strategies for developing speech-recognition interfaces must be employed. Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx 3 ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit [Cite_Footnote_4] (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low-or zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users."
  },
  {
    "id": 416,
    "name": "NAudio",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "http://naudio.codeplex.com/",
    "section_title": "5 User interface 5.1 Audio input and recording",
    "add_info": "8 http://naudio.codeplex.com/",
    "text": "The recorder, built using the open-source library NAudio, [Cite_Footnote_8] takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1)."
  },
  {
    "id": 417,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Our code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/thu-coai/earl",
    "section_title": "4 Experiments 4.3 Implementation Details",
    "add_info": null,
    "text": "We used the stochastic gradient descent (SGD) algorithm with mini-batch. The batch size and learning rate are set to 100 and 0.5, respectively. The model was run at most 20 epochs, and the training stage of each model took about one day on a GPU machine. We selected the model performing best in the validation set to evaluate in test sets. Our code is available at:  https://github.com/thu-coai/earl."
  },
  {
    "id": 418,
    "name": "EQG-RACE dataset",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Jia et al. (2020)"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/jemmryx/EQG-RACE",
    "section_title": "4 Experiment 4.1 Experiment Setting",
    "add_info": "1 https://github.com/jemmryx/EQG-RACE",
    "text": "We carry out the training and inference on EQG-RACE dataset [Cite_Footnote_1] proposed by Jia et al. (2020). The passage numbers of training set, validation set and test set are respectively 11457, 642, 609."
  },
  {
    "id": 419,
    "name": "JUMAN",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Japanese morpholog-ical analyzer"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html",
    "section_title": "2 Japanese Morphology",
    "add_info": "1 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html",
    "text": "In order to understand the task of lexicon acquisi-tion, we briefly describe the Japanese morpholog-ical analyzer JUMAN. [Cite_Footnote_1] We explain Japanese mor-phemes in Section 2.1, morphological constraints in Section 2.2, and unknown morpheme processing in Section 2.3."
  },
  {
    "id": 420,
    "name": "KNP",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "dependency parser",
      "KNP is used to form a phrasal unit called bunsetsu by chunking morphemes."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html",
    "section_title": "3 Lexicon Acquisition 3.2 System Architecture",
    "add_info": "2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html",
    "text": "Figure 1 shows the system architecture. Each sen-tence in texts is processed by the morphological an-alyzer JUMAN and the dependency parser KNP. [Cite_Footnote_2] JUMAN consults a hand-crafted dictionary and an automatically constructed dictionary. KNP is used to form a phrasal unit called bunsetsu by chunking morphemes."
  },
  {
    "id": 421,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "the open-domain word embeddings"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/mmihaltz/word2vec-GoogleNews-vectors",
    "section_title": "6 Experiments 6.3 Training Details",
    "add_info": "1 https://github.com/mmihaltz/word2vec-GoogleNews-vectors",
    "text": "We use the open-domain word embeddings [Cite_Footnote_1] for the initialization of word vectors. We initialize other model parameters from a uniform distribu-tion U(-0.05, 0.05). The dimension of the word embedding and the size of the hidden layers are 300. The learning rate is set to 0.01 and the dropout rate is set to 0.1. Stochastic gradient de-scent is used as our optimizer. The position encod-ing is also used (Tang et al., 2016). We also com-pare the memory networks in their multiple com-putational layers version (i.e., multiple hops) and the number of hops is set to 3 as used in the men-tioned previous studies. We implemented all mod-els in the TensorFlow environment using same in-put, embedding size, dropout rate, optimizer, etc. so as to test our hypotheses, i.e., to make sure the achieved improvements do not come from else-where. Meanwhile, we can also report all evalua-tion measures discussed above 2 . 10% of the train-ing data is used as the development set. We report the best results for all models based on their F-1 Macro scores."
  },
  {
    "id": 422,
    "name": "Prote\u0301ge\u0301 4.0",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the freely available ontol-ogy editor"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://protege.stanford.edu/",
    "section_title": "3 Knowledge Representation and Semantic Inference",
    "add_info": "2 http://protege.stanford.edu/, as accessed 27 Oct 2009",
    "text": "The concepts and individuals of the particular domain are structured and organized in domain-specific ontologies. These ontologies are mod-elled in the Web Ontology Language (OWL). OWL allows us to define concept hierarchies, re-lations between concepts, domains and ranges of these relations, as well as specific relation in-stances between instances of a concept. Our on-tologies are defined by the freely available ontol-ogy editor Prote\u0301ge\u0301 4.0 [Cite_Footnote_2] . The advantage of using an ontology for structuring the domain knowledge is the modular non-redundant encoding. When com-bined with a reasoner, only a few statements about an individual have to be asserted explicitely, while the rest can be inferred from the ontology. We em-ploy several ontologies, among which the follow-ing are relevant for modelling the specific domains of our NPCs:"
  },
  {
    "id": 423,
    "name": "SwiftOwlim",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "It provides a forward-chaining inference engine which evaluates the domain definitions when loading the knowledge repository, and makes implicit knowledge explicit by asserting triples that must also hold true accord-ing to the ontology.",
      "SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.ontotext.com/owlim/",
    "section_title": "3 Knowledge Representation and Semantic Inference",
    "add_info": "3 http://www.ontotext.com/owlim/",
    "text": "We use SwiftOwlim [Cite_Footnote_3] for storing and querying the data. SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data. It provides a forward-chaining inference engine which evaluates the domain definitions when loading the knowledge repository, and makes implicit knowledge explicit by asserting triples that must also hold true accord-ing to the ontology. Once the reasoner is finished, the triple store can be queried directly using the RDF query language SPARQL."
  },
  {
    "id": 424,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "The code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/tingc9/LinearTemplatesSRW",
    "section_title": "9 Results",
    "add_info": "2 The code used for this research will be made available on https://github.com/tingc9/LinearTemplatesSRW",
    "text": "Computationally, the search time increases with increasing sentence lengths. On a reasonably modern machine, our implementation generated the above sentences in about 150 seconds while using 2.2 GB of memory. [Cite_Footnote_2]"
  },
  {
    "id": 425,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a publicly available preprocessing code"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/najoungkim/pdtb3",
    "section_title": "3 Proposed Evaluation Protocol",
    "add_info": "4 https://github.com/najoungkim/pdtb3",
    "text": "While Xue et al. (2015) lay out one possible pro-tocol, it does not fully address the issues we have raised in Section 2. Another limitation is the un-availability of the preprocessing code as of the date of this submission. We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [Cite_Footnote_4] In addition to accounting for the variation previously discussed, we take Shi and Demberg (2017)\u2019s concerns into consideration."
  },
  {
    "id": 426,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "a publicly available codebase"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface/pytorch-transformers",
    "section_title": "3 Proposed Evaluation Protocol 3.1 Baseline results",
    "add_info": "5 https://github.com/huggingface/pytorch-transformers",
    "text": "Following our proposed protocol, we report base-line results from two strong sentence encoder mod-els: BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), using a publicly available codebase. [Cite_Footnote_5] See Appendix C for training details. We present L2 results on PDTB 2.0 in Table 1 and results on PDTB 3.0 in Table 2 (see Appendix D for L1 re-sults). To maintain backwards compatibility to the literature, we also report PDTB 2.0 results on Ji, Lin and P&K splits (see Section 2.1). Ji & Lin are the most common splits, and P&K is the split used by Nie et al. (2019) who claim the current state-of-the-art for L2. For PDTB 2.0 (Table 1), our baselines showed strong performance on all splits. XLNet-large was the single best model, signifi-cantly outperforming every best reported result."
  },
  {
    "id": 427,
    "name": "WMT-15 corpora",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.statmt.org/wmt15/",
    "section_title": "4 Experimental Study 4.1 Experimental Setting",
    "add_info": "1 http://www.statmt.org/wmt15/",
    "text": "In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochas-tic gradient descent with Adam (Kingma and Ba, 2015). Chung et al. (2016) and Sennrich et al. (2016) demonstrated that bpe boosts NMT, so sim-ilar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora [Cite_Footnote_1] to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English\u2013Turkish (En\u2013Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016). The training side of the English\u2013German (En\u2013De), English\u2013Russian (En\u2013 Ru), and En\u2013Tr corpora include 4.5, 2.1, and 4 million parallel sentences, respectively. We ran-domly select 3K sentences for each of the develop-ment and test sets for En\u2013Tr. For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character."
  },
  {
    "id": 428,
    "name": "TIPSTER [Cite_Footnote_3] corpus",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words)"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93T3A",
    "section_title": "4 Experiments 4.1 Data",
    "add_info": "3 Description at http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93T3A.",
    "text": "We take a subset of the TIPSTER [Cite_Footnote_3] corpus \u2013 all Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words) \u2013 and automatically anno-tate them with sentence boundaries, part of speech tags and dependency relations using the Stanford parser (Klein & Manning, 2003). We reserve a small subset of about 600 articles (340,000 words) for testing and use the rest to build a trigram LM with the CMU toolkit (Clarkson & Rosenfeld, 1997, with Good-Turing smoothing and vocabulary size of 30,000). To train the maximum entropy classifiers we use about 41,000 sentences."
  },
  {
    "id": 429,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "Japanese short answer scoring dataset",
      "The dataset",
      "the dataset"
    ],
    "description": [
      "The dataset consists of six prompts.",
      "Each prompt has its rubric, student responses, and scores.",
      "Each response was manually scored using the multiple analytic criteria for the prompt, and the subscore for each criterion was rated individually on the basis of the correspond-ing rubric."
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://aip-nlu.gitlab.io/resources/sas-japanese",
    "section_title": "5 Experiments 5.1 Dataset",
    "add_info": "1 https://aip-nlu.gitlab.io/resources/sas-japanese",
    "text": "We use the Japanese short answer scoring dataset [Cite_Footnote_1] introduced by Mizumoto et al. (2019). The dataset consists of six prompts. Each prompt has its rubric, student responses, and scores. The prompts, rubrics, and student responses in the dataset were collected from the examinations conducted by a Japanese education company, Takamiya Gakuen Yoyogi Seminar. Each response was manually scored using the multiple analytic criteria for the prompt, and the subscore for each criterion was rated individually on the basis of the correspond-ing rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response."
  },
  {
    "id": 430,
    "name": "pretrained character-based BERT",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Devlin et al., 2019"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/cl-tohoku/bert-japanese",
    "section_title": "5 Experiments 5.2 Settings",
    "add_info": "3 We adopted pretrained character-based BERT which is known to be suitable for processing Japanese texts. This is available at https://github.com/cl-tohoku/bert-japanese. for Low-Resource NLP, pages 175\u2013182.",
    "text": "We split the dataset into training data (1, 600), val-idation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [Cite_Footnote_3] We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set. We trained five models with different random seeds and reported the aver-age of the results."
  },
  {
    "id": 431,
    "name": "Akamon system",
    "fullname": "N/A",
    "genericmention": [
      "this system"
    ],
    "description": [
      "The Akamon system [Cite_Footnote_2] , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding"
    ],
    "citationtag": [
      "Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a",
      "Liu et al., 2006; Mi et al., 2008"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://sites.google.com/site/xianchaowu2012",
    "section_title": "1 Introduction",
    "add_info": "2 Code available at https://sites.google.com/site/xianchaowu2012",
    "text": "However, few tree/forest-to-string systems have been made open source and this makes it diffi-cult and time-consuming to testify and follow exist-ing proposals involved in recently published papers. The Akamon system [Cite_Footnote_2] , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding (Liu et al., 2006; Mi et al., 2008). We hope this system will help re-lated researchers to catch up with the achievements of tree/forest-based translations in the past several years without re-implementing the systems or gen-eral algorithms from scratch."
  },
  {
    "id": 432,
    "name": "RIBES",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://www.kecl.ntt.co.jp/icl/lirg/ribes",
    "section_title": "2 Akamon Toolkit Features",
    "add_info": "3 Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes",
    "text": "Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score [Cite_Footnote_3] , , i.e., the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes."
  },
  {
    "id": 433,
    "name": "Google online transla-tion system",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "online transla-tion system"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://translate.google.com/",
    "section_title": "2 Akamon Toolkit Features",
    "add_info": "4 http://translate.google.com/",
    "text": "Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT sys-tem. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system [Cite_Footnote_4] are also listed in this table."
  },
  {
    "id": 434,
    "name": "SRILM",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "Stolcke, 2002"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.speech.sri.com/projects/srilm/",
    "section_title": "2 Akamon Toolkit Features",
    "add_info": "5 http://www.speech.sri.com/projects/srilm/",
    "text": "\u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM [Cite_Footnote_5] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003)."
  },
  {
    "id": 435,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://code.google.com/p/berkeleylm/",
    "section_title": "2 Akamon Toolkit Features",
    "add_info": "6 http://code.google.com/p/berkeleylm/",
    "text": "\u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 [Cite_Footnote_6] (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003)."
  },
  {
    "id": 436,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html",
    "section_title": "4 Using Deep Syntactic Structures",
    "add_info": "8 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html",
    "text": "In Akamon, we support the usage of deep syn-tactic structures for obtaining fine-grained transla-tion rules as described in our former work (Wu et al., 2010) . Similarly, Enju [Cite_Footnote_8] , a state-of-the-art and freely available HPSG parser for English, can be used to generate packed parse forests for source sentences . Deep syntactic structures are included in the HPSG trees/forests, which includes a fine-grained description of the syntactic property and a semantic representation of the sentence. We extract fine-grained rules from aligned HPSG forest-string pairs and use them in the forest-to-string decoder. The detailed algorithms can be found in (Wu et al., 2010; Wu et al., 2011a). Note that, in Akamon, we also provide the codes for generating HPSG forests from Enju."
  },
  {
    "id": 437,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Babylonpartners/corrsim",
    "section_title": "5 Experiments",
    "add_info": "1 https://github.com/Babylonpartners/corrsim",
    "text": "We now empirically demonstrate the power of the methods and statistical analysis presented in Sec-tion 4, through a set of evaluations on the Se-mantic Textual Similarity (STS) tasks series 2012- 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017). For methods involving pre-trained word embeddings, we use fastText (Bo-janowski et al., 2017) trained on Common Crawl (600B tokens), as previous evaluations have in-dicated that fastText vectors have uniformly the best performance on these tasks out of commonly-used pretrained unsupervised word vectors (Con-neau et al., 2017; Perone et al., 2018; Zhelezniak et al., 2019a,b). We provide experiments and sig-nificance analysis for additional word vector in the Appendix. The success metric for the STS tasks is the Pearson correlation between the sentence similarity scores provided by human annotators and the scores generated by a candidate algorithm. Note that the dataset for the STS13 SMT subtask is no longer publicly available, so the mean Pear-son correlation for STS13 reported in our exper-iments has been re-calculated accordingly. The code for our experiments builds on the SentEval toolkit (Conneau and Kiela, 2018) and is available on GitHub [Cite_Footnote_1] ."
  },
  {
    "id": 438,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://bert-as-service.github.com/hanxiao/bert-as-service",
    "section_title": "5 Experiments",
    "add_info": "Han Xiao. 2018. https://bert-as-service.github.com/hanxiao/bert-as-service.",
    "text": "Note that for BERT we evaluated all pooling strategies available in bert-as-service (Xiao, 2018)  applied to either the last or second-to-last layers and report results for the best-performing combi-nation, which was mean-pooling on the last layer for both model sizes. Our results are presented in Table 1. We can clearly see that deep learning-based methods do not shine on STS tasks, while simple compositions of word vectors can perform extremely well, especially when an appropriate correlation coefficient is used as the similarity measure. Indeed, the performance of max-pooled vectors with Spearman correlation approaches or exceeds that of more expensive or offline methods like that of Arora et al. (2017), which performs PCA computations on the entire test set. Addi-tionally, while the multivariate correlation meth-ods such as CKA are more computationally ex-pensive than pooling-based approaches (see Ta-ble 2), they can provide performance boost on some tasks, making the cost worth it depending on the application. Finally, we conducted an ex-ploratory error analysis and found that many errors are due to the well-known inherent weaknesses of word embeddings. For example, the proposed ap-proaches heavily overestimate similarity when two sentences contain antonyms or when one sentence is the negation of the other. We illustrate these and other cases in the Appendix."
  },
  {
    "id": 439,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html",
    "section_title": "1 Introduction",
    "add_info": "1 We provide the trained embeddings at http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html.",
    "text": "On the TED corpus, we obtained general pur-pose multilingual embeddings for 11 target lan-guages, by considering the (auxiliary) task of reconstructing pre-trained English word vectors. The resulting embeddings led to cross-lingual multi-label classifiers that achieved the highest re-ported scores on 10 out of these 11 languages. [Cite_Footnote_1]"
  },
  {
    "id": 440,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/dcferreira/multilingual-joint-embeddings",
    "section_title": "5 Experiments",
    "add_info": "5 Our code is available at https://github.com/dcferreira/multilingual-joint-embeddings.",
    "text": "We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus. [Cite_Footnote_5]"
  },
  {
    "id": 441,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Extend",
    "url": "http://www.clg.ox.ac.uk/tedcorpus",
    "section_title": "5 Experiments 5.2 TED Corpus",
    "add_info": null,
    "text": "To assess the ability of our framework to han-dle multiple target languages, we ran a second set of experiments on the TED corpus (Cettolo et al., 2012), using the training and test parti-tions created by Hermann and Blunsom (2014), downloaded from  http://www.clg.ox.ac.uk/tedcorpus. The corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference in 12 different languages, with 12,078 parallel docu-ments in the training partition (totalling 1,641,985 parallel sentences). Following their prior work, we used this corpus both as parallel data (D u ) and as the task dataset (D l ). There are L = 15 labels and documents can have multiple labels."
  },
  {
    "id": 442,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "5 Experiments 5.2 TED Corpus",
    "add_info": null,
    "text": "For the Joint w/ Aux strategy, we used the 300-dimensional GloVe-840B vectors (Penning-ton et al., 2014), downloaded from  http://nlp.stanford.edu/projects/glove/."
  },
  {
    "id": 443,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://godel.iis.sinica.edu.tw/ROCLING",
    "section_title": "5 Related Work",
    "add_info": "2 See http://godel.iis.sinica.edu.tw/ROCLING.",
    "text": "Bikel and Chiang (2000) and Xu et al. (2002) con-struct word-based statistical parsers on the first re-lease of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic context-free grammar (PCFG) similar to (Collins, 1997); the other is basedF-measureon statisticalis reportedTAGin((ChiangBikel and, 2000Chi-).About ang, 2000). Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus [Cite_Footnote_2] . Xu et al. (2002) reports an overall F-measure l when the same training and test set as (Bikel and Chiang, 2000) are used. Since our parser operates at character level, and more training data is used, the best results are not directly compa-rable. The middle point of the learning curve in Fig-ure 1, which is trained with roughly 100K words, is at the same ballpark of (Xu et al., 2002). The con-tribution of this work is that the proposed character-based parser does word-segmentation, POS tagging and parsing in a unified framework. It is the first at-tempt to our knowledge that syntactic information is used in word-segmentation."
  },
  {
    "id": 444,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io",
    "section_title": "4 Model Training 4.1 Corpora",
    "add_info": "1 https://spacy.io",
    "text": "Preprocessing Each dataset is tokenized using the spaCy [Cite_Footnote_1] tokenizer, converted to lowercase, and non-"
  },
  {
    "id": 445,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://docs.python.org/3/library/difflib.html",
    "section_title": "4 Model Training 4.1 Corpora",
    "add_info": "2 https://docs.python.org/3/library/difflib.html",
    "text": "ASCII symbols are removed. To restrain the vocab-ulary size and correct the typos, we use a default vocabulary of fixed size 42K words from spaCy. Each word in the dataset is then compared with the vocabulary using the difflib library [Cite_Footnote_2] in Python (algorithm based on the Levenshtein distance), and mapped to the most similar word in the vocabu-lary. If no word with more than 90% of similarity is found, the word is considered a rare word or a typo, and is mapped to the out-of-vocabulary (OOV) word. For Cornell, less than 1% of the uni-grams are OOV."
  },
  {
    "id": 446,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "3 Evaluation 3.1 Evaluation Setup",
    "add_info": "1 https://code.google.com/p/word2vec/",
    "text": "Two versions of queries are included in our ex-periments: a short keyword query (title query), and a longer description query that restates the cor-responding keyword query\u2019s information need in terms of natural language (description query). We evaluate each type of query separately using the metrics Mean Average Precision at 1,000 (MAP), Precision at 20 (P@20) (Manning et al., 2008), and NDCG@20 (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002). Preprocessing. Stopword removal and Porter\u2019s stemmer are applied (Manning et al., 2008). The word embeddings are pre-trained based on a pool of the top 2,000 documents returned by BM25 for individual queries as suggested by (Diaz et al., 2016). The implementation of Word2Vec [Cite_Footnote_1] from (Mikolov et al., 2013) is employed. In par-ticular, we employ CBOW with the dimension set to 300, window size to 10, minimum count to 5, and a subsampling threshold of 10 \u22123 . The CBOW model is trained for 10 iterations on the target cor-pus."
  },
  {
    "id": 447,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ucasir/NPRF",
    "section_title": "3 Evaluation 3.1 Evaluation Setup",
    "add_info": "2 https://github.com/ucasir/NPRF",
    "text": "Unsupervised ranking models serve as baselines for comparisons. We use the open source Terrier platform\u2019s (Macdonald et al., 2012) implementa-tion of these ranking models: the DRMM and K-NRM models, and report re-sults for all six variants. Our implementation of the NPRF framework is available to enable fu-ture comparisons [Cite_Footnote_2] ."
  },
  {
    "id": 448,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://data.gdeltproject.org/events/index.html",
    "section_title": "4 Experiments 4.1 Dataset",
    "add_info": "1 http://data.gdeltproject.org/events/index.html",
    "text": "We crawled and parsed the GDELT Even-t Database [Cite_Footnote_1] containing news articles published in May 2014. We manually annotated one-week da-ta containing 101,654 documents and identified 77 storylines for evaluation. We also report the result-s of our model on the one-month data containing 526,587 documents. But we only report the preci-sion and not recall of the storylines extracted since it is time consuming to identify all the true story-lines in such a large dataset. In our experiments, we used the Stanford Named Entity Recognizer for identifying the named entities. In addition, we removed common stopwords and only kept tokens which are verbs, nouns, or adjectives in these news articles."
  },
  {
    "id": 449,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear",
    "section_title": "5 Experiments",
    "add_info": "5 We used liblinear (Fan et al., 2008) at http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear with the solver type of 3.",
    "text": "We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO without our op-timization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with var-ied objectives, hinge loss or softmax loss . MERT learns parameters from forests (Kumar et al., 2009) with 4 restarts and 8 random directions in each it-eration. We experimented on a variant of PRO , in which the objective in Eq. 4 with the hinge loss of Eq. 5 was solved in each iteration in line 4 of Alg. 1 using an off-the-shelf solver [Cite_Footnote_5] . Our MIRA solves the problem in Equation 13 in line 7 of Alg. 2. For a sys-tematic comparison, we used our exhaustive oracle translation selection method in Section 3 for PRO, MIRA and ORO. For each learning algorithm, we ran 30 iterations and generated duplicate removed 1,000-best translations in each iteration. The hyper-parameter \u03bb for PRO and ORO was set to 10 \u22125 , se-lected from among {10 \u22123 , 10 \u22124 , 10 \u22125 }, and 10 2 for MIRA, chosen from {10, 10 2 , 10 3 } by preliminary testing on MT06. Both decoding and learning are parallelized and run on 8 cores. Each online learn-ing took roughly 12 hours, and PRO took one day. It took roughly 3 days for MERT with 20 iterations. Translation results are measured by case sensitive BLEU."
  },
  {
    "id": 450,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://lair.cse.msu.edu/lair/projects/actioneffect.html",
    "section_title": "1 Introduction",
    "add_info": "1 This dataset is available at http://lair.cse.msu.edu/lair/projects/actioneffect.html",
    "text": "The contributions of this paper are three folds. First, it introduces a new task on physical action-effect prediction, a first step towards an under-standing of causal relations between physical ac-tions and the state of the physical world. Such ability is central to robots which not only perceive from the environment, but also act to the environ-ment through planning. To our knowledge, there is no prior work that attempts to connect actions (in language) and effects (in images) in this na-ture. Second, our approach harnesses the large amount of image data available on the web with minimum supervision. It has shown that physi-cal action-effect models can be learned through a combination of a few annotated examples and a large amount of un-annotated web data. This opens up the possibility for humans to teach robots new tasks through language communication with a small number of examples. Third, we have cre-ated a dataset for this task, which is available to the community [Cite_Footnote_1] . Our bootstrapping approach can serve as a baseline for future work on this topic."
  },
  {
    "id": 451,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://www.nist.gov/speech/tests/mt/mt2001/resource/",
    "section_title": "2 Related Work",
    "add_info": "2 http://www.nist.gov/speech/tests/mt/mt2001/resource/",
    "text": "As well as Yasuda\u2019s method does, using W H is another way to calculate similarities between a summary to be evaluated and pooled summaries indirectly. Yasuda et al. (2003) tested DP matching (Su et al., 1992), BLEU (Papineni et al., 2002), and NIST [Cite_Footnote_2] , for the calculation of W H . However there are many other measures for summary evaluation."
  },
  {
    "id": 452,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://www.nist.gov/speech/tests/mt/mt2001/resource/",
    "section_title": "4 Experiments 4.3 Evaluation Methods",
    "add_info": "3 http://www.nist.gov/speech/tests/mt/mt2001/resource/",
    "text": "An arbitrary system was selected from the 10 systems, and Yasuda\u2019s method estimated its manual score from the other nine systems. Yasuda\u2019s method was evaluated by Gap, which is defined by Equation 7. where x k is the k th system, s(x k ) is a score of x k by Yasuda\u2019s method, and y k is the manual score for the k th system. Yasuda et al. (2003) tested DP matching (Su et al., 1992), BLEU (Papineni et al., 2002), and NIST [Cite_Footnote_3] , as automatic methods used in their evaluation. Instead of those methods, we tested ROUGE and cosine distance, both of which have been used for summary evaluation."
  },
  {
    "id": 453,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/abhyudaynj/LSTM-CRF-models",
    "section_title": "References",
    "add_info": "1 Code is available at https://github.com/abhyudaynj/LSTM-CRF-models",
    "text": "Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major ap-plication of sequence labeling involves ex-traction of relevant entities such as medica-tion, indication, and side-effects from Elec-tronic Health Record Narratives. Sequence la-beling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recur-rent Neural Networks. We extend the pre-viously studied CRF-LSTM model with ex-plicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods [Cite_Footnote_1] for structured prediction in or-der to improve the exact phrase detection of clinical entities."
  },
  {
    "id": 454,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Produce",
    "url": "http://www.freedict.com",
    "section_title": "5 Experiment: English-French 5.1 Translation Lexicon",
    "add_info": "9 This dictionary was generated using a dictionary de-rived from one available at http://www.freedict.com.",
    "text": "\u2022 An English-French dictionary (a total of 34,808 entries, 4,021 of which are not one-to-one). [Cite_Footnote_9] It contains morphological variants but does not include character accents. Each n-to-m entry was processed by stoplisting and then extracting all word-pairs in the remaining cross-product as in section 4.1. Result: 39,348 word pairs, 9,045 of which contain two words present in the corpora."
  },
  {
    "id": 455,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/saffsd/langid.c",
    "section_title": "6 Experiments 6.1 Language Identification",
    "add_info": "Marco Lui. 2014. Pure C natural language iden-tifier with support for 97 languages. https://github.com/saffsd/langid.c.",
    "text": "The langid.py model is 88.6\u201399.2% accurate (Lui and Baldwin, 2012). We tested the origi-nal Python, a Java implementation that \u201cshould be faster than anything else out there\u201d (Weiss, 2013), a C implementation (Lui, 2014)  , and our replica in hardware. We also tested CLD2 (Sites, 2013) writ-ten in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs of feature extraction and modeling."
  },
  {
    "id": 456,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/carrotsearch/langid-java",
    "section_title": "6 Experiments 6.1 Language Identification",
    "add_info": "Dawid Weiss. 2013. Java port of langid.py (language identifier). https://github.com/carrotsearch/langid-java.",
    "text": "The langid.py model is 88.6\u201399.2% accurate (Lui and Baldwin, 2012). We tested the origi-nal Python, a Java implementation that \u201cshould be faster than anything else out there\u201d (Weiss, 2013)  , a C implementation (Lui, 2014), and our replica in hardware. We also tested CLD2 (Sites, 2013) writ-ten in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs of feature extraction and modeling."
  },
  {
    "id": 457,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.speech.cs.cmu.edu/tools/lextool.html",
    "section_title": "4 Experiment 2: Speaker ID from ASR",
    "add_info": "1 http://www.speech.cs.cmu.edu/tools/lextool.html",
    "text": "\u2022 Lexicontions using: WethegeneratedLOGIOSthelexicalwordtoolpronuncia- [Cite_Footnote_1] . We decoded the audio in three ways:"
  },
  {
    "id": 458,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Neural Sparse Topical Coding 3.3 Neural Sparse Topical Coding",
    "add_info": "1 http://nlp.stanford.edu/projects/glove/",
    "text": "Word embedding layer (W E \u2208 R N\u00d7300 ): Sup-posing the word number of the vocabulary is N, this layer devotes to transform each word to a distributed embedding representation. Here, we adopt the pre-trained embeddings by GloVe based on a large Wikipedia dataset [Cite_Footnote_1] ."
  },
  {
    "id": 459,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.qwone.com/jason/20Newsgroups/",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "2 http://www.qwone.com/jason/20Newsgroups/",
    "text": "\u2022 20Newsgroups: is comprised of 18775 newsgroup articles with 20 categories, and contains 60698 unique words [Cite_Footnote_2] ."
  },
  {
    "id": 460,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://jwebpro.sourceforge.net/data-web-snippets.tar.gz",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "3 http://jwebpro.sourceforge.net/data-web-snippets.tar.gz",
    "text": "\u2022 Web Snippet: includes 12340 Web search snippets with 8 categories, we remove the words with fewer than 3 characters and with document frequency less than 3 in the dataset [Cite_Footnote_3] ."
  },
  {
    "id": 461,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://pypi.python.org/pypi/lda",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "4 https://pypi.python.org/pypi/lda",
    "text": "\u2022 LDA (Blei et al., 2001). A classical proba-bilistic topic model. We use the LDA pack-age [Cite_Footnote_4] for its implementation. We use the set-tings with iteration number n = 2000, the Dirichlet parameter for distribution over top-ics \u03b1 = 0.1 and the Dirichlet parameter for distribution over words \u03b7 = 0.01."
  },
  {
    "id": 462,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "5 http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/",
    "text": "\u2022 STC (Zhu and Xing, 2011). It is a sparsity-enhanced non-probabilistic topic model. We use the code released by the authors [Cite_Footnote_5] . We set the regularization constants as \u03bb = 0.3, \u03c1 = 0.0001 and the maximum number of itera-tions of hierarchical sparse coding, dictionary learning as 100."
  },
  {
    "id": 463,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/huashiyiqike/TMBP/tree/master/DocNADE",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "6 https://github.com/huashiyiqike/TMBP/tree/master/DocNADE",
    "text": "\u2022 DocNADE (Larochelle and Lauly, 2012b). An unsupervised neural network topic model of documents and has shown that it is a com-petitive model both as a generative model and as a document representation learning algo-rithm [Cite_Footnote_6] . In DocNADE, the hidden size is 50, the learning rate is 0.0004 , the bath size is 64 and the max training number is 50000."
  },
  {
    "id": 464,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/rajarshd/GaussianLDA",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "7 https://github.com/rajarshd/GaussianLDA",
    "text": "\u2022 GaussianLDA (Das et al., 2015). A new technique for topic modeling by treating the document as a collection of word embed-dings and topics itself as multivariate Gaus-sian distributions in the embedding space [Cite_Footnote_7] . We use default values for the parameters."
  },
  {
    "id": 465,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/askerlee/topicvec",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "8 https://github.com/askerlee/topicvec",
    "text": "\u2022 TopicVec (Li et al., 2016). A model incorpo-rates generative word embedding model with LDA [Cite_Footnote_8] . We also use default values for the pa-rameters."
  },
  {
    "id": 466,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.tensorflow.org/",
    "section_title": "4 Experiments 4.1 Data and Setting",
    "add_info": "9 https://www.tensorflow.org/",
    "text": "Our three models are implemented in Python using TensorFlow [Cite_Footnote_9] . For both datasets, we use the pre-trained 300-dimensional word embeddings from Wikipedia by GloVe, and it is fixed during train-ing. For each out-of-vocab word, we sample a random vector from a normal distribution. In practice, we use a regular learning rate 0.00001 for both dataset. We set the regularization factor \u03bb = 1,\u03b1 = 1,\u03bb 1 = 0.6,\u03bb 2 = 0.4. In initial-ization, all weight matrices are randomly initial-ized with the uniformed distribution in the inter-val [0, 0.001] for web snippet, and [0, 0.0001] for 20Newsgroups."
  },
  {
    "id": 467,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://git.io/MqyoIg",
    "section_title": "4 Emotion Lexicon Creation",
    "add_info": null,
    "text": "Finally, we transformed M WE by first apply-ing normalization column-wise (so to eliminate the over representation for happiness as discussed in Section 3) and then scaling the data row-wise so to sum up to one. An excerpt of the final Matrix M WE is presented in Table 3, and it can be in-terpreted as a list of words with scores that repre-sent how much weight a given word has in the af-fective dimensions we consider. So, for example, awe#n has a predominant weight in INSPIRED (0.38), comical#a has a predominant weight in AMUSED (0.51), while kill#v has a predomi-nant weight in AFRAID , ANGRY and SAD (0.23, 0.21 and 0.27 respectively). This matrix, that we call DepecheMood 2 , represents our emotion lex-icon, it contains 37k entries and is freely available for research purposes at  http://git.io/MqyoIg."
  },
  {
    "id": 468,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/shuningjin/discrete-text-rep",
    "section_title": "References",
    "add_info": "1 Code available on GitHub: https://github.com/shuningjin/discrete-text-rep",
    "text": "While much work on deep latent variable models of text uses continuous latent vari-ables, discrete latent variables are interesting because they are more interpretable and typi-cally more space efficient. We consider sev-eral approaches to learning discrete latent vari-able models for text in the case where ex-act marginalization over these variables is in-tractable. We compare the performance of the learned representations as features for low-resource document and sentence classification. Our best models outperform the previous best reported results with continuous representa-tions in these low-resource settings, while learning significantly more compressed repre-sentations. Interestingly, we find that an amor-tized variant of Hard EM performs particularly well in the lowest-resource regimes. [Cite_Footnote_1]"
  },
  {
    "id": 469,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/martin-arvidsson/InterpretableWordEmbeddings",
    "section_title": "3 Experiments",
    "add_info": "1 Code is available at: https://github.com/martin-arvidsson/InterpretableWordEmbeddings",
    "text": "We consider two semantic dimensions, gen-der, which is explored in SOTA, and sentiment, a dimension proven difficult to capture in stan-dard word embedding models (Tang et al., 2014). We follow SOTA when choosing prior anchors for gender, while using the AFINN dictionary (Nielsen, 2011) to find prior anchors for sentiment. To evaluate the effect of \u201cfew\u201d vs. \u201cmany\u201d prior anchors, we run experiments using between 2 and 276 words depending on the dimension of interest and the dataset at hand. All prior word types used in the experiments can be found in Sec. B in the supplementary material. We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling [Cite_Footnote_1] . We set the size of the embeddings K = 100, use a context window size of 8 and \u03c3 = 1 throughout all experiments."
  },
  {
    "id": 470,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://data.stanford.edu/congress_text",
    "section_title": "3 Experiments",
    "add_info": "M. Gentzkow, J. M. Shapiro, and M. Taddy. 2018. Congressional record for the 43rd-114th congresses: Parsed speeches and phrase counts. http://data.stanford.edu/congress_text. Ac-cessed: 2019-04-17.",
    "text": "We examine the proposed priors using three commonly sized English corpora for textual anal-ysis within CSSDH: the top 100 list of books in Project Gutenberg (2019), a sample from Twitter (Go et al., 2009) and the U.S. Congress Speeches 1981-2016 (Gentzkow et al., 2018)  . The num-ber of tokens ranges from 1.8M to 40M after pre-processing (see Sec. A in the supplementary ma-terial for details). The various origins, sizes and contents of these datasets work as a check of the effect of the priors in different types of corpora."
  },
  {
    "id": 471,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://AFINN.localhost/pubdb/p.php?6010",
    "section_title": "3 Experiments",
    "add_info": "F. A\u030a. Nielsen. 2011. http://AFINN.localhost/pubdb/p.php?6010.",
    "text": "We consider two semantic dimensions, gen-der, which is explored in SOTA, and sentiment, a dimension proven difficult to capture in stan-dard word embedding models (Tang et al., 2014). We follow SOTA when choosing prior anchors for gender, while using the AFINN dictionary (Nielsen, 2011)  to find prior anchors for sentiment. To evaluate the effect of \u201cfew\u201d vs. \u201cmany\u201d prior anchors, we run experiments using between 2 and 276 words depending on the dimension of interest and the dataset at hand. All prior word types used in the experiments can be found in Sec. B in the supplementary material. We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling . We set the size of the embeddings K = 100, use a context window size of 8 and \u03c3 = 1 throughout all experiments."
  },
  {
    "id": 472,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/usc-sail/mica-riskybehavior-identification",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/usc-sail/mica-riskybehavior-identification",
    "text": "2. MovieBERT [Cite_Footnote_1] : A domain-specific fine-tuned BERT model (Devlin et al., 2019) pre-trained over a large collection of film and TV scripts. We use this model to obtain better represen-tations for the semantics of a character\u2019s lan-guage"
  },
  {
    "id": 473,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.imdb.com/",
    "section_title": "3 Method 3.3 Role of Movie Genre",
    "add_info": "3 https://www.imdb.com/",
    "text": "Movie genres relate the elements of a story, plot, setting and characters to a specific category. Cate-gorizing a movie indirectly assists in shaping the characters and the story of the movie, and deter-mines the plot and best setting to use. Thus, movie genre contains information on the type of content one could expect in a movie (especially for the case of violent content (Martinez et al., 2019)). Thus, our models include movie genre as an additional feature. Genres for each movie were obtained from IMDb [Cite_Footnote_3] and transformed into a multi-hot encoding."
  },
  {
    "id": 474,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://keras.io",
    "section_title": "5 Experimental Setup 5.1 Model Implementation",
    "add_info": "7 https://keras.io",
    "text": "Our model was implemented in Keras [Cite_Footnote_7] . Although not common in most deep-learning approaches, we performed 10-fold cross-validation (CV) to obtain a more reliable estimation for our model\u2019s perfor-mance. In each fold, the model was trained until convergence (i.e. loss in consecutive epochs was less than 10 \u22128 difference). To prevent over-fitting, we used Adam optimizer with a small learning rate ( 0.001 ), batch size of 16 , and high dropout probabil-ity ( p = 0.5 ). For the RNN layer, we used Gated Re-current Units (GRU; Cho et al. 2014). For the senti-ment models, Bi-LSTM parameters were informed by the work of Tai et al. (2015): 50-dimensional hidden representation, dropout ( p = 0.1 ), trained with Adam optimizer on a batch size of 25 and a L 2 penalty of 10 \u22124 . To allow for a fair comparison, all the BERT pre-trained models and movieBERT had the same set of parameters as the BERT-base model: 12 layers, 768 dimensions, learning rate of 2 \u00d7 10 \u22125 , sequence length of 128 and batch size of 32. For the initial experiments, we set the model parameters to hidden dimension size of d = 16, to help prevent overfitting, and the sequence length m = 500, which is approximately the duration of one movie act (i.e., one third). This selection was informed by previous works (Martinez et al., 2019; Shafaei et al., 2019)."
  },
  {
    "id": 475,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "http://www.rulequest.com/Personal/",
    "section_title": "5 Experiments",
    "add_info": "3 http://www.rulequest.com/Personal/",
    "text": "Our results are compared against the FOIL algorithm [Cite_Footnote_3] , which learns first-order horn clauses. In order to evaluate FOIL using MAP, its candidate beliefs are first ranked by the number of FOIL rules they match. We further report results using Random Walks with Restart (RWR), also known as personalized PageRank (Haveliwala, 2002), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., (Agirre and Soroa, 2009; Moro et al., 2014)). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P (s \u2192 t; \u03c0)."
  },
  {
    "id": 476,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/",
    "section_title": "2 Related Work",
    "add_info": "Ashwin Srinivasan. 2001. The Aleph Manual. In http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.",
    "text": "Bi-directional search is a popular strategy in AI, and in the ILP literature. The Aleph algorithm (Srinivasan, 2001)  combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol. FORTE (Richards and Mooney, 1991) was another early ILP system which enumerated paths via a bi-directional seach. Computing backward random walks for PRA can be seen as a particular way of bi-directional search, which is also assigned a random walk probability semantics. Unlike in prior work, we will use this probability semantics directly for feature selection."
  },
  {
    "id": 477,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://openie.cs.washington.edu/",
    "section_title": "3 Textual Schema Matching 3.4 Comparing database relations with extracted relations",
    "add_info": "1 http://openie.cs.washington.edu/",
    "text": "M ATCHER uses an API for the ReVerb Open IE system [Cite_Footnote_1] (Fader et al., 2011) to collect I(r T ), for each r T . The API for ReVerb allows for rela-tional queries in which some subset of the entity strings, entity categories, and relation string are specified. The API returns all matching triples; types must match exactly, but relation or argument strings in the query will match any relation or ar-gument that contains the query string as a sub-string. M ATCHER queries ReVerb with three dif-ferent types of queries for each r T , specifying the types for both arguments, or just the type of the first argument, or just the second argument. Types for arguments are taken from the types of argu-ments for a potentially matching r D in Freebase. To avoid overwhelming the ReVerb servers, for our experiments we limited M ATCHER to queries for the top 80 r T \u2208 C(r D ), when they are ranked according to frequency during the candidate iden-tification process."
  },
  {
    "id": 478,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/wav2vec",
    "section_title": "3 Experimental Setup 3.2 Training",
    "add_info": "1 https://github.com/pytorch/fairseq/tree/master/examples/wav2vec.",
    "text": "Encoder. We initialize the encoder using the open-sourced [Cite_Footnote_1] wav2vec 2.0 large architecture pretrained on unlabelled English-only (XMEF-En) audio from LibriVox (Baevski et al., 2020). For many-to-one experiments, we also experiment with a multi-lingual wav2vec 2.0 (XMEF-X), which was pre-trained on raw audio from 53 languages (Conneau et al., 2020). Encoder output is followed by 3 1- D convolution layers with stride 2 to achieve 8x down-sampling of audio encoder outputs."
  },
  {
    "id": 479,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/multilingual",
    "section_title": "3 Experimental Setup 3.2 Training",
    "add_info": "2 https://github.com/pytorch/fairseq/tree/master/examples/multilingual preprint arXiv:2005.00052.",
    "text": "Decoder. We initialize the decoder with open-sourced [Cite_Footnote_2] mBART50 models and the same vocab-ulary (Tang et al., 2020). We use mBART50N1 (49 languages to English) for X-En ST directions and mBART501N (English to 49 languages) for translating En-X ST directions."
  },
  {
    "id": 480,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/facebookresearch/covost",
    "section_title": "- A.2 Data",
    "add_info": null,
    "text": "The CoVoST 2 dataset (Wang et al., 2020b) is a large-scale multilingual ST corpus which cov-ers translations from English into 15 languages\u2014 Arabic, Catalan, Welsh, German, Estonian, Per-sian, Indonesian, Japanese, Latvian, Mongolian, Slovenian, Swedish, Tamil, Turkish, Chinese, and translations from 21 languages into English, includ-ing Spanish, French, Italian, Dutch, Portuguese, Russian in addition to the 15 target languages. It has total 2,880 hours of speech from 78K speak-ers. The data could be downloaded from  https://github.com/facebookresearch/covost ."
  },
  {
    "id": 481,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/wav2vec",
    "section_title": "- A.3 Implementation Details",
    "add_info": null,
    "text": "Pretrained models. We use the open-sourced models from wav2vec 2.0 and mBART50 pretrained with multilingual par-allel text data. These models can be down-loaded from  https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and https://github.com/pytorch/fairseq/tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model. For XMEF-X, we use the 56K-hour XLSR-53 Large model. For decoder, we use the pretrained \u201cmMBART 50 finetuned many-to-one\u201d model for many-to-one experiments and \u201cmMBART 50 finetuned one-to-many\u201d for one-to-many experiments."
  },
  {
    "id": 482,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/multilingual",
    "section_title": "- A.3 Implementation Details",
    "add_info": null,
    "text": "Pretrained models. We use the open-sourced models from wav2vec 2.0 and mBART50 pretrained with multilingual par-allel text data. These models can be down-loaded from https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and  https://github.com/pytorch/fairseq/tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model. For XMEF-X, we use the 56K-hour XLSR-53 Large model. For decoder, we use the pretrained \u201cmMBART 50 finetuned many-to-one\u201d model for many-to-one experiments and \u201cmMBART 50 finetuned one-to-many\u201d for one-to-many experiments."
  },
  {
    "id": 483,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/tensorflow/models/tree/master/research/object_detection",
    "section_title": "5 The Experiment 5.2 Implementation Details",
    "add_info": "5 https://github.com/tensorflow/models/tree/master/research/object_detection",
    "text": "Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embed-ding (Pennington et al., 2014) and the ELMo (Pe-ters et al., 2018) embedding as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. We adopt the \u201cssd resnet 50 fpn coco\u201d model from Tensorflow detection model zoo [Cite_Footnote_5] as the object detection mod-ule. The size of hidden states in the LSTM module is set to 200, and the size of the projected embed-ding for computing similarity between text spans and object labels is 512. The feed-forward net-works for contextual scoring and visual scoring have two 150-dimension hidden layers and one 100-dimension hidden layer, respectively."
  },
  {
    "id": 484,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://stanfordnlp.github.io/CoreNLP/coref.html",
    "section_title": "5 The Experiment 5.3 Baseline Methods",
    "add_info": "6 https://stanfordnlp.github.io/CoreNLP/coref.html",
    "text": "As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit [Cite_Footnote_6] , we use their released model as baselines. For the End-to-end model, we also use their re-leased code ."
  },
  {
    "id": 485,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/kentonl/e2e-coref",
    "section_title": "5 The Experiment 5.3 Baseline Methods",
    "add_info": "7 https://github.com/kentonl/e2e-coref",
    "text": "As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit , we use their released model as baselines. For the End-to-end model, we also use their re-leased code [Cite_Footnote_7] ."
  },
  {
    "id": 486,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs",
    "section_title": "References",
    "add_info": "1 https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs",
    "text": "Neural models for NLP typically use large numbers of parameters to reach state-of-the-art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learn-ing sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of mod-els that is closely connected to weighted finite-state automata (WFSAs). We take advan-tage of rational RNNs\u2019 natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment anal-ysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parame-ters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of ra-tional RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90% of the weights. We publicly release our code. [Cite_Footnote_1]"
  },
  {
    "id": 487,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://riejohnson.com/cnn_data.html",
    "section_title": "3 Experiments",
    "add_info": "7 http://riejohnson.com/cnn_data.html",
    "text": "Data We experiment with the Amazon reviews binary sentiment classification dataset (Blitzer et al., 2007), composed of 22 product categories. We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015). [Cite_Footnote_7] We also examine three of the largest individual categories as separate datasets (kitchen, dvd, and books), following Johnson and Zhang (2015). The three category datasets do not overlap with each other (though they do with original mix), and are sig-nificantly different in size (see Appendix A), so we can see how our approach behaves with differ-ent amounts of training data."
  },
  {
    "id": 488,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "3 Experiments",
    "add_info": "8 https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "Implementation details To classify text, we concatenate the scores computed by each WFSA, then feed this d-dimensional vector of scores into a linear binary classifier. We use log loss. We ex-periment with both type-level word embeddings (GloVe.6B.300d; Pennington et al., 2014) and contextual embeddings (BERT large; Devlin et al., 2019). [Cite_Footnote_8] In both cases, we keep the embeddings fixed, so the vast majority of the learnable pa-rameters are in the WFSAs. We train mod-els using GloVe embeddings on all datasets. Due to memory constraints we evaluate BERT embed-dings (frozen, not fine-tuned) only on the smallest dataset (kitchen)."
  },
  {
    "id": 489,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/domluna/memn2n",
    "section_title": "5 Experiments",
    "add_info": "2 https://github.com/domluna/memn2n",
    "text": "For MemN2N, we use the publicly available im-plementation [Cite_Footnote_2] and train it exactly as all other mod-els (same optimizer, total epochs, and early stop-ping criteria) for fairness. While the reported best result for MemN2N is on the version with posi-tion encoding, linear start training, and random-injection of time index noise (Sukhbaatar et al., 2015), the version we use has only position encod-ing. Note that the comparison is still meaningful because linear start training and time index noise are not used in DMN+ (and as a result, neither in our proposed DSMN)."
  },
  {
    "id": 490,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/allenai/label_rationale_association",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/allenai/label_rationale_association.",
    "text": "In interpretable NLP, we require faithful ratio-nales that reflect the model\u2019s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text nat-ural language rationales. We demonstrate that pipelines, models for faithful rationaliza-tion on information-extraction style tasks, do not work as well on \u201creasoning\u201d tasks requir-ing free-text rationales. We turn to models that jointly predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the ex-tent to which the labels and rationales pre-dicted by these models are associated, a nec-essary property of faithful explanation. Via two tests, robustness equivalence and feature importance agreement, we find that state-of-the-art T5-based joint models exhibit desir-able properties for explaining commonsense question-answering and natural language infer-ence, indicating their potential for producing faithful free-text rationales. [Cite_Footnote_1]"
  },
  {
    "id": 491,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Compare",
    "url": "https://huggingface.co/docs/datasets/master/#",
    "section_title": "A Additional Information A.4 Implementation Details",
    "add_info": "10 https://huggingface.co/docs/datasets/master/#",
    "text": "We use Huggingface Datasets 10 to access all datasets, and Huggingface Transformers (Wolf et al., 2020) to access pretrained T5 weights and to-kenizer. To optimize, we use Adam with = 1e-8, \u03b2 1 = 0.9, and \u03b2 2 = 0.99. We use gradient clipping to a maximum norm of 1.0 and a dropout rate of 0.1. We train each model on a NVIDIA RTX 8000 GPU (48GB memory) for maximum 200 epochs with a batch size of 64 and a learning rate linearly decaying from 5e-5. Training ends if the validation set loss has not decreased for [Cite_Footnote_10] epochs. Early stopping occurs within 15 epochs for most mod-els. Most CoS-E models train in less than 1 hour and most E-SNLI models in around 30. At infer-ence time, we greedy-decode until an EOS token is generated (or for 200 tokens). Approximating the 64-batch model with a batch-size of 16 and 4 gradient accumulation steps on 8GB memory cloud GPUs, we sweep starting learning rates of 1e-2, 1e- 3, 1e-4, 5e-5, and 1e-5. The two largest learning rates never result in good performance. Among the smallest three rates, performance across all model variants (I\u2192R, I\u2192OR, R\u2192O, I\u2192O, and IR\u2192O) on E-SNLI and CoS-E v1.0 never varies by more than 1.58% accuracy or 0.34 BLEU."
  },
  {
    "id": 492,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.gjopen.com/",
    "section_title": "2 Linguistic Cues of Accurate Forecasting 2.1 Geopolitical Forecasting Data",
    "add_info": "2 https://www.gjopen.com/",
    "text": "To explore the connections between language and forecasting skill, we make use of data from Good Judgment Open, [Cite_Footnote_2] an online prediction forum. Users of this website share predictions in response to a number of pre-specified questions about future events with uncertain outcomes, such as: \u201cWill North Korea fire another intercontinental ballistic missile before August 2019?\u201d Users\u2019 predictions consist of an estimated chance the event will oc-cur (for example, 5%) in addition to an optional text justification that explains why the forecast was made. A sample is presented in Figure 1."
  },
  {
    "id": 493,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.cfraresearch.com/",
    "section_title": "3 Companies\u2019 Earnings Forecasts",
    "add_info": "8 https://www.cfraresearch.com/",
    "text": "Data. We analyze reports from the Center for Fi-nancial Research and Analysis (CFRA). [Cite_Footnote_8] These reports provide frequent updates for analysts\u2019 esti-mates and are also organized in a structured way, enabling us to accurately extract numerical fore-casts and corresponding text justifications."
  },
  {
    "id": 494,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.thomsonone.com/",
    "section_title": "3 Companies\u2019 Earnings Forecasts",
    "add_info": "9 https://www.thomsonone.com/",
    "text": "We collected CFRA\u2019s analyst reports from the Thomson ONE database [Cite_Footnote_9] from 2014 to 2018. All notes making forecasts are extracted under the \u201cAn-alyst Research Notes and other Company News\u201d section. The dataset contains a total of 32,807 notes from analysts, covering 1,320 companies."
  },
  {
    "id": 495,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://ckip.iis.sinica.edu.tw/DOMCAT/",
    "section_title": "References",
    "add_info": "1 http://ckip.iis.sinica.edu.tw/DOMCAT/",
    "text": "In this paper, we propose a web-based bilingual concordancer, DOMCAT [Cite_Footnote_1] , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively."
  },
  {
    "id": 496,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.npm.gov.tw",
    "section_title": "3 Experimental Results 3.1 Experimental Setting",
    "add_info": "2 http://www.npm.gov.tw",
    "text": "We use the Chinese/English web pages of the National Palace Museum [Cite_Footnote_2] as our underlying parallel corpus. It contains about 30,000 sentences in each language. We exploited the Champollion Toolkit (Ma et al., 2006) to align the sentence pairs. The English sentences are tokenized and lemmatized by using the NLTK (Bird and Loper, 2004) and the Chinese sentences are segmented by the CKIP Chinese segmenter (Ma and Chen, 2003)."
  },
  {
    "id": 497,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/rtmdrr/testSignificanceNLP.git",
    "section_title": "References",
    "add_info": "1 The code for all statistical tests detailed in this pa-per is found on: https://github.com/rtmdrr/testSignificanceNLP.git",
    "text": "Statistical significance testing is a standard sta-tistical tool designed to ensure that experimen-tal results are not coincidental. In this opin-ion/theoretical paper we discuss the role of statis-tical significance testing in Natural Language Pro-cessing (NLP) research. We establish the funda-mental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental se-tups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical pro-tocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental re-sults, statistical significance testing is often ig-nored or misused. We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [Cite_Footnote_1] ."
  },
  {
    "id": 498,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/ridiculouz/CKBQA",
    "section_title": "References",
    "add_info": "1 https://github.com/ridiculouz/CKBQA",
    "text": "We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding men-tions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outper-forms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset [Cite_Footnote_1] with such strat-egy is also published to promote further re-search. An online demo of NAMER is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion of NAMER is also available."
  },
  {
    "id": 499,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://kbqademo.gstore.cn",
    "section_title": "References",
    "add_info": "2 http://kbqademo.gstore.cn",
    "text": "We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding men-tions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outper-forms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset with such strat-egy is also published to promote further re-search. An online demo of NAMER [Cite_Footnote_2] is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion of NAMER is also available."
  },
  {
    "id": 500,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.biendata.xyz/competition/ccks_2019_6/data/",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "4 https://www.biendata.xyz/competition/ccks_2019_6/data/",
    "text": "Dataset We utilize the dataset published in CCKS Chinese KBQA Contest [Cite_Footnote_4] for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training."
  },
  {
    "id": 501,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://pkubase.gstore.cn/",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "5 A KB endpoint: http://pkubase.gstore.cn/",
    "text": "Dataset We utilize the dataset published in CCKS Chinese KBQA Contest for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE [Cite_Footnote_5] , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training."
  },
  {
    "id": 502,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/pkumod/gAnswer",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "7 https://github.com/pkumod/gAnswer",
    "text": "Baselines We compare our results with the top ranking team \"jchl\" (Luo et al., 2019) in the contest and a competitive KBQA system gAnswer [Cite_Footnote_7] (Hu et al., 2018) that reached first place in QALD-9 (Ngomo, 2018). Since the NE and RE module in gAnswer does not officially support Chinese, we replace them with those in our system. Hence, the gAnswer evaluated can be partly viewed as our system with a rule-based QG module and its comparison with us indicates the effectiveness of our generative QG module."
  },
  {
    "id": 503,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/ymcui/Chinese-BERT-wwm",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "8 Pretrained weights: https://github.com/ymcui/Chinese-BERT-wwm",
    "text": "Setup We adopt Chinese RoBERTa-large (Cui et al., 2020) in transformers library (Wolf et al., 2020) released by HFL [Cite_Footnote_8] as encoder and a 6-layer 8-head transformer as decoder. For our best results, we co-train the NE and QG models, remaining RE as a separate model. For NEQG, we train the encoder and decoder with learning rate 1e-6 and 4e-6 respectively with an Adam (Kingma and Ba, 2015) optimizer, setting hyperparameters to \u03b3 = 1, \u03b1 = \u03b2 = 2.5, \u03b8 = 0 and batch size to 40. For RE model, we set the learning rate and batch size to 1e-5 and 96 respectively with \u03b3 = \u03b1 = \u03b2 = 0, \u03b8 = 1. Both models are trained until no progress on validation accuracy for at most 10k steps."
  },
  {
    "id": 504,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://translate.google.com",
    "section_title": "3 Problem Definition and Baseline Ap-proaches",
    "add_info": "1 http://translate.google.com",
    "text": "The task is a regression problem and it is chal-lenging due to the language gap between the la-beled training dataset and the test dataset. Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation. We adopt Google Translate [Cite_Footnote_1] for review transla-tion. After review translation, the training re-views and the test reviews are now in the same language, and any regression algorithm (e.g. lo-gistic regression, least squares regression, KNN regressor) can be applied for learning and predic-tion. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the SVMLight toolkit 2 as the basic regressor. For comparative analysis, we simply use the default parameter values in SVMLight with linear kernel. The features include all unigrams and bigrams in the review texts, and the value of each feature is simply set to its frequency (TF) in a review."
  },
  {
    "id": 505,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://svmlight.joachims.org",
    "section_title": "3 Problem Definition and Baseline Ap-proaches",
    "add_info": "2 http://svmlight.joachims.org",
    "text": "The task is a regression problem and it is chal-lenging due to the language gap between the la-beled training dataset and the test dataset. Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation. We adopt Google Translate 1 for review transla-tion. After review translation, the training re-views and the test reviews are now in the same language, and any regression algorithm (e.g. lo-gistic regression, least squares regression, KNN regressor) can be applied for learning and predic-tion. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the SVMLight toolkit [Cite_Footnote_2] as the basic regressor. For comparative analysis, we simply use the default parameter values in SVMLight with linear kernel. The features include all unigrams and bigrams in the review texts, and the value of each feature is simply set to its frequency (TF) in a review."
  },
  {
    "id": 506,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://www.uni-weimar.de/medien/webis/research/corpora/corpus-webis-cls-10.html",
    "section_title": "5 Empirical Evaluation",
    "add_info": "3 http://www.uni-weimar.de/medien/webis/research/corpora/corpus-webis-cls-10.html",
    "text": "We used the WEBIS-CLS-10 corpus [Cite_Footnote_3] provided by (Prettenhofer and Stein, 2010) for evaluation. It consists of Amazon product reviews for three product categories (i.e. books, dvds and music) written in different languages including English, German, etc. For each language-category pair there exist three sets of training documents, test documents, and unlabeled documents. The train-ing and test sets comprise 2000 documents each, whereas the number of unlabeled documents var-ies from 9000 \u2013 170000. The dataset is provided with the rating score between 1 to 5 assigned by users, which can be used for the review rating prediction task. We extracted texts from both the summary field and the text field to represent a review text. We then extracted the rating score as a review\u2019s corresponding real-valued label. In the cross-language scenario, we regarded English as the source language, and regarded German as the target language. The experiments were con-ducted on each product category separately. Without loss of generality, we sampled and used only 8000 unlabeled documents for each product category. We use Mean Square Error (MSE) as the evaluation metric, which penalizes more se-vere errors more heavily."
  },
  {
    "id": 507,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/bdhingra/quasar",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "1 https://github.com/bdhingra/quasar",
    "text": "Quasar-T [Cite_Footnote_1] (Dhingra et al., 2017b) consists of 43, 000 open-domain trivia question, and their an-swers are extracted from ClueWeb09 data source, and the paragraphs are obtained by retrieving 50 sentences for each question from the ClueWeb09 data source using LUCENE."
  },
  {
    "id": 508,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/nyu-dl/SearchQA",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "2 https://github.com/nyu-dl/SearchQA",
    "text": "SearchQA [Cite_Footnote_2] (Dunn et al., 2017) is a large-scale open domain question answering dataset, which consists of question-answer pairs crawled from J! Archive, and the paragraphs are obtained by retrieving 50 webpages for each question from Google Search API."
  },
  {
    "id": 509,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://nlp.cs.washington.edu/triviaqa/",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "3 http://nlp.cs.washington.edu/triviaqa/",
    "text": "TriviaQA [Cite_Footnote_3] (Joshi et al., 2017) includes 95, 000 question-answer pairs authored by trivia enthusi-asts and independently gathered evidence docu-ments, six per question on average, and utilizes Bing Web search API to collect 50 webpages re-lated to the questions."
  },
  {
    "id": 510,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/brmson/dataset-factoid-curated/tree/master/trec",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "4 https://github.com/brmson/dataset-factoid-curated/tree/master/trec",
    "text": "CuratedTREC [Cite_Footnote_4] (Voorhees et al., 1999) is based on the benchmark from the TREC QA tasks, which contains 2, 180 questions extracted from the datasets from TREC1999, 2000, 2001 and 2002."
  },
  {
    "id": 511,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/brmson/dataset-factoid-webquestions",
    "section_title": "4 Experiments 4.1 Datasets and Evaluation Metrics",
    "add_info": "5 https://github.com/brmson/dataset-factoid-webquestions",
    "text": "WebQuestions [Cite_Footnote_5] (Berant et al., 2013b) is de-signed for answering questions from the Free-base knowledge base, which is built by crawl-ing questions through the Google Suggest API and the paragraphs are retrieved from the English Wikipedia using ."
  },
  {
    "id": 512,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://nlp.stanford.edu/data/glove.840B.300d.zip",
    "section_title": "4 Experiments 4.3 Experimental Settings",
    "add_info": "6 http://nlp.stanford.edu/data/glove.840B.300d.zip",
    "text": "For training, our Our+FULL model is first ini-tialized by pre-training using Our+AVG model, and we set the iteration number over all the train-ing data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe [Cite_Footnote_6] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data."
  },
  {
    "id": 513,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.openchannelfoundation.org/projects/Qanda/",
    "section_title": "2 Related Work",
    "add_info": "5 http://www.openchannelfoundation.org/projects/Qanda/",
    "text": "On a different line of research, several Textual-based Question Answering (QA) systems (Qanda [Cite_Footnote_5] , QANUS , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog 8 , NLBean 9 etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA."
  },
  {
    "id": 514,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.qanus.com/",
    "section_title": "2 Related Work",
    "add_info": "6 http://www.qanus.com/",
    "text": "On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS [Cite_Footnote_6] , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog 8 , NLBean 9 etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA."
  },
  {
    "id": 515,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.dzonesoftware.com/products/open-source-question-answer-software/",
    "section_title": "2 Related Work",
    "add_info": "7 http://www.dzonesoftware.com/products/open-source-question-answer-software/",
    "text": "On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS , QSQA [Cite_Footnote_7] etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog 8 , NLBean 9 etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA."
  },
  {
    "id": 516,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://technologies.kmi.open.ac.uk/aqualog/",
    "section_title": "2 Related Work",
    "add_info": "8 http://technologies.kmi.open.ac.uk/aqualog/",
    "text": "On a different line of research, several Textual-based Question Answering (QA) systems (Qanda 5 , QANUS 6 , QSQA 7 etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog [Cite_Footnote_8] , NLBean etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA."
  },
  {
    "id": 517,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.markwatson.com/opensource/",
    "section_title": "2 Related Work",
    "add_info": "9 http://www.markwatson.com/opensource/",
    "text": "On a different line of research, several Textual-based Question Answering (QA) systems (Qanda 5 , QANUS 6 , QSQA 7 etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog , NLBean [Cite_Footnote_9] etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA."
  },
  {
    "id": 518,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://lucene.apache.org/",
    "section_title": "6 Siamese Neural Network with Textual Similarity",
    "add_info": "10 https://lucene.apache.org/",
    "text": "Though SCQA can strongly model semantic relations between documents, it needs boosting in the area of textual similarity. The sense of word based similarity is infused to SCQA by using BM25 ranking algorithm. Lucene [Cite_Footnote_10] is used to cal-culate the BM25 scores for question pairs. The score from similarity metric of SCQA is com-bined with the BM25 score. A new similarity score is calculated by the weighted combination of the SCQA and BM25 score as: where \u03b1 control the weights given to SCQA and BM25 models. It range from 0 to 1. SCQA with this improved similarity metric is called Siamese Convolutional Neural Network for cQA with Textual Similartity (T-SCQA). Figure 4 de-picts the testing phase of T-SCQA. This model will give better performance in datasets with good mix of questions that are lexically and semantically similar. The value of \u03b1 can be tuned according to the nature of dataset."
  },
  {
    "id": 519,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://webscope.sandbox.yahoo.com/catalog.php?datatype=l",
    "section_title": "7 Experiments",
    "add_info": "11 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l",
    "text": "We collected Yahoo! Answers dataset from Yahoo! Labs Webscope [Cite_Footnote_11] . Each question in the dataset contains title, description, best an-swer, most voted answers and meta-data like categories, sub categories etc. For training dataset, we randomly selected 2 million data and extracted question-relevant answer pairs and question-irrelevant answer pairs from them to train SCQA. Similarly, our validation dataset contains 400,000 question answer pairs. The hyperparam-eters of the network are tuned on the validation dataset. The values of the hyperparameters for which we obtained the best results is shown in Ta-ble 1."
  },
  {
    "id": 520,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.statmt.org/moses/giza/GIZA++.html",
    "section_title": "8 Results",
    "add_info": "12 http://www.statmt.org/moses/giza/GIZA++.html",
    "text": "We did a comparative study of the results of the previous methods with respect to SCQA and T-SCQA. The baseline performance is shown by query likelihood language model (LM). For the translation based methods translation(word), translation+LM and translation(phrase) we implemented the papers by Jeon et al. (2005), Xue et al. (2008), Zhou et al. (2011) respec-tively. The first paper deals with word based trans-lation, the second enhanced the first by adding lan-guage model to it and the last paper implements phrase based translation method to bridge lexi-cal gap. As seen from Table 2, the translation based methods outperforms the baseline signifi-cantly. The models are trained using GIZA++ [Cite_Footnote_12] tool with the question and best answer pair as the parallel corpus. For the topic based Q-A topic model and Q-A topic model(s), we implemented the models QATM -PR (Question-Answer Topic Model) Ji et al.(2012) and T BLM SQATM\u2212V (Su-pervised Question-Answer Topic Model with user votes as supervision) Zhang et al. (2014) respec-tively. Again it is visible from the Table 2 that topic based approaches show slight improvement over translation based methods but they show sig-nificant improvement over baseline. The mod-els DSQA and T-DSQA were built using convo-lutional neural sub-networks joined by a distance measure at the top. There is no sharing of parame-ters involved between the sub-networks of these models. It is clear from the comparison of re-sults between T-DSQA and T-SCQA that param-eter sharing definitely helps in the task of similar question retrieval in cQA forums. T-SCQA outper-forms all the previous approaches significantly."
  },
  {
    "id": 521,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://ictclas.org/",
    "section_title": "5 Experimentation 5.1 Experimental Settings and Baseline",
    "add_info": "1 http://ictclas.org/",
    "text": "For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) [Cite_Footnote_1] with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model."
  },
  {
    "id": 522,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://code.google.com/p/berkeleyparser/",
    "section_title": "5 Experimentation 5.1 Experimental Settings and Baseline",
    "add_info": "2 http://code.google.com/p/berkeleyparser/",
    "text": "For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser [Cite_Footnote_2] and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model."
  },
  {
    "id": 523,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/lex-parser.shtml",
    "section_title": "5 Experimentation 5.1 Experimental Settings and Baseline",
    "add_info": "3 http://nlp.stanford.edu/software/lex-parser.shtml",
    "text": "For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser [Cite_Footnote_3] to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model."
  },
  {
    "id": 524,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu/",
    "section_title": "5 Experimentation 5.1 Experimental Settings and Baseline",
    "add_info": "4 http://mallet.cs.umass.edu/",
    "text": "For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) [Cite_Footnote_4] is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model."
  },
  {
    "id": 525,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://lpsolve.sourceforge.net/5.5/",
    "section_title": "5 Experimentation 5.1 Experimental Settings and Baseline",
    "add_info": "5 http://lpsolve.sourceforge.net/5.5/",
    "text": "For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver [Cite_Footnote_5] is used to construct our global argument inference model."
  },
  {
    "id": 526,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ShyamSubramanian/HESM",
    "section_title": "References",
    "add_info": null,
    "text": "Automated fact extraction and verification is a challenging task that involves finding rele-vant evidence sentences from a reliable cor-pus to verify the truthfulness of a claim. Ex-isting models either (i) concatenate all the ev-idence sentences, leading to the inclusion of redundant and noisy information; or (ii) pro-cess each claim-evidence sentence pair sepa-rately and aggregate all of them later, miss-ing the early combination of related sen-tences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple ev-idence sentences), and verify a claim to be supported, refuted or not enough info, by en-coding and attending the claim and evidence sets at different levels of hierarchy. Our ex-perimental results show that HESM outper-forms 7 state-of-the-art methods for fact ex-traction and claim verification. Our source code is available at  https://github.com/ShyamSubramanian/HESM."
  },
  {
    "id": 527,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/fred2008/TCMSA",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Experiments on Stanford Sentiment Treebank (SST; Socher et al. 2013) show that our model outperforms standard bottom-up tree-LSTM (Zhu et al., 2015; Looks et al., 2017) and also re-cent work on bidirectional tree-LSTM (Teng and Zhang, 2017). In addition, our model allows a more holistic prediction of phase-level sentiments over the tree with a high degree of node sen-timent consistency. To our knowledge, we are the first to investigate graph NNs for tree senti-ment classification, and the first to discuss phrase level sentiment consistency over a constituent tree for SST. We release our code and models at  https://github.com/fred2008/TCMSA."
  },
  {
    "id": 528,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php",
    "section_title": "4 Resources 4.1 Corpus",
    "add_info": "2 http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php",
    "text": "The French Treebank [Cite_Footnote_2] [FTB] (Abeille\u0301 et al., 2003) is a syntactically annotated corpus made up of jour-nalistic articles from Le Monde newspaper. We used the latest edition of the corpus (June 2010) that we preprocessed with the Stanford Parser pre-processing tools (Green et al., 2011). It contains 473,904 tokens and 15,917 sentences. One benefit of this corpus is that its compounds are marked. Their annotation was driven by linguistic criteria such as the ones in (Gross, 1986). Compounds are identified with a specific non-terminal symbol \u201dMWX\u201d where X is the part-of-speech of the expression. They have a flat structure made of the part-of-speech of their components as shown in figure 1."
  },
  {
    "id": 529,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://atoll.inria.fr/\u02dcsagot/lefff.html",
    "section_title": "4 Resources 4.2 Lexical resources",
    "add_info": "4 http://atoll.inria.fr/\u02dcsagot/lefff.html and the reranker models: n is the current position in the sentence, w(i) is the word at position i; t(i) is the part-of-speech tag of w(i); if the word at absolute position i is part of a compound in the Shortest Path Segmentation, mwt(i) and mws(i) are respectively the part-of-speech tag and the internal structure of the compound, mwpos(i) indicates its relative position in the compound (B or I).",
    "text": "French is a resource-rich language as attested by the existing morphological dictionaries which in-clude compounds. In this paper, we use two large-coverage general-purpose dictionaries: Dela (Cour-tois, 1990; Courtois et al., 1997) and Lefff (Sagot, 2010). The Dela was manually developed in the 90\u2019s by a team of linguists. We used the distribution freely available in the platform Unitex (Paumier, 2011). It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiword nouns). The compounds present in the resources re-spect the linguistic criteria defined in (Gross, 1986). The lefff is a freely available dictionary [Cite_Footnote_4] that has been automatically compiled by drawing from dif-ferent sources and that has been manually validated. We used a version with 553,138 lexical entries in-cluding 26,311 multiword ones (22,673 multiword nouns). Their different modes of acquisition makes those two resources complementary. In both, lexical entries are composed of a inflected form, a lemma, a part-of-speech and morphological features. The Dela has an additional feature for most of the mul-tiword entries: their syntactic surface form. For in-stance, eau de vie (brandy) has the feature NDN be-cause it has the internal flat structure noun \u2013 prepo-sition de \u2013 noun."
  },
  {
    "id": 530,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://alpage.inria.fr/statgram/frdep/fr",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "5 We used the version adapted to French in the software Bonsai (Candito and Crabbe\u0301, 2009): http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html. The original version is available at: http://code.google.com/p/berkeleyparser/. We trained the parser as follows: right binarization, no parent annotation, six split-merge cycles and default random seed initialisation (8).",
    "text": "We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser [Cite_Footnote_5] (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005)."
  },
  {
    "id": 531,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://code.google.com/p/berkeleyparser/",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "5 We used the version adapted to French in the software Bonsai (Candito and Crabbe\u0301, 2009): http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html. The original version is available at: http://code.google.com/p/berkeleyparser/. We trained the parser as follows: right binarization, no parent annotation, six split-merge cycles and default random seed initialisation (8).",
    "text": "We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser [Cite_Footnote_5] (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005)."
  },
  {
    "id": 532,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://wapiti.limsi.fr/",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).",
    "text": "We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [Cite_Footnote_6] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005)."
  },
  {
    "id": 533,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://igm.univ-mlv.fr/\u02dcmconstan/research/software/",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "7 Available at http://igm.univ-mlv.fr/\u02dcmconstan/research/software/.",
    "text": "We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [Cite_Footnote_7] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005)."
  },
  {
    "id": 534,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.mcs.anl.gov/petsc/",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/",
    "text": "We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [Cite_Footnote_8] as in (Charniak and Johnson, 2005)."
  },
  {
    "id": 535,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.mcs.anl.gov/research/projects/tao/",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/",
    "text": "We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [Cite_Footnote_8] as in (Charniak and Johnson, 2005)."
  },
  {
    "id": 536,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/evalb/",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "9 Evalb tool available at http://nlp.cs.nyu.edu/evalb/. We also used the evaluation by category implemented in the class EvalbByCat in the Stanford Parser.",
    "text": "Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] [Cite_Footnote_9] , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence 10 . And finally, the Leaf-Ancestor score [LA] 11 (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree. The global score of a generated parse is equal to the average score of all terminal nodes. Punctuation tokens are ignored in all met-rics. The quality of MWE identification was evalu-ated by computing the F 1 score on MWE nodes. We also evaluated the MWE segmentation by using the unlabeled F 1 score (U). In order to compare both ap-proaches, parse trees generated by BKYc were auto-matically transformed in trees with the same MWE annotation scheme as the trees generated by BKY."
  },
  {
    "id": 537,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://ilk.uvt.nl/conll/software.html",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "10 This score is computed by using the tool available at http://ilk.uvt.nl/conll/software.html. The constituent trees are automatically converted into dependency trees with the tool Bonsai.",
    "text": "Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] 9 , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence [Cite_Footnote_10] . And finally, the Leaf-Ancestor score [LA] (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree. The global score of a generated parse is equal to the average score of all terminal nodes. Punctuation tokens are ignored in all met-rics. The quality of MWE identification was evalu-ated by computing the F 1 score on MWE nodes. We also evaluated the MWE segmentation by using the unlabeled F 1 score (U). In order to compare both ap-proaches, parse trees generated by BKYc were auto-matically transformed in trees with the same MWE annotation scheme as the trees generated by BKY."
  },
  {
    "id": 538,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.grsampson.net/Resources.html",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "11 Leaf-ancestor assessment tool available at http://www.grsampson.net/Resources.html",
    "text": "Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] 9 , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence . And finally, the Leaf-Ancestor score [LA] [Cite_Footnote_11] (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree. The global score of a generated parse is equal to the average score of all terminal nodes. Punctuation tokens are ignored in all met-rics. The quality of MWE identification was evalu-ated by computing the F 1 score on MWE nodes. We also evaluated the MWE segmentation by using the unlabeled F 1 score (U). In order to compare both ap-proaches, parse trees generated by BKYc were auto-matically transformed in trees with the same MWE annotation scheme as the trees generated by BKY."
  },
  {
    "id": 539,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cis.upenn.edu/\u02dcdbikel/software.html",
    "section_title": "6 Evaluation 6.1 Experiment Setup",
    "add_info": "12 Dan Bikel\u2019s tool available at http://www.cis.upenn.edu/\u02dcdbikel/software.html.",
    "text": "In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples [Cite_Footnote_12] . The statistical significance between two MWE identification experiments was established by using the McNemar-s test (Gillick and Cox, 1989). The results of the two experiments are considered statistically significant with the com-puted value p < 0.01."
  },
  {
    "id": 540,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/danieliu/play-scraper",
    "section_title": "2 Construction of PolicyIE Corpus 2.1 Privacy Policies Selection",
    "add_info": "2 https://github.com/danieliu/play-scraper",
    "text": "Initial Collection Ramanath et al. (2014) intro-duced a corpus of 1,010 privacy policies of the top websites ranked on Alexa.com. We crawled those websites\u2019 privacy policies in November 2019 since the released privacy policies are outdated. For mobile application privacy policies, we scrape application information from Google Play Store us-ing play-scraper public API [Cite_Footnote_2] and crawl their privacy policy. We ended up with 7,500 mobile applications\u2019 privacy policies."
  },
  {
    "id": 541,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/microsoft/unilm",
    "section_title": "References",
    "add_info": "7 https://github.com/microsoft/unilm 8 https://github.com/microsoft/MASS",
    "text": "BART 3 (P:0.83, R: 1.0) [Cite_Footnote_7]"
  },
  {
    "id": 542,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/microsoft/MASS",
    "section_title": "References",
    "add_info": "7 https://github.com/microsoft/unilm 8 https://github.com/microsoft/MASS",
    "text": "BART 3 (P:0.83, R: 1.0) [Cite_Footnote_7]"
  },
  {
    "id": 543,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/INK-USC/KagNet",
    "section_title": "-",
    "add_info": "1 https://github.com/INK-USC/KagNet",
    "text": "Commonsense CapableOf reasoning aims ent to empower ReceiveAction machines with the human ub ability ev to make glue_stick presumptions about work ordinary HasS situations on in our daily life. In this paper, At we Locati propose AtLocation a textual inference framework office for answer- Schema Graph ing commonsense questions, which effec-Grounding sense knowledge graphs to perform Commonsense explain- Inference Commonsense Inference able inferences Where . The do framework adults use first glue grounds sticks? a question-answer A: classroom pair from B: the office semantic C: desk space drawer to the knowledge-based symbolic space as a schema graph, a related sub-graph of exter-nal knowledge Semantic graphs Space . It represents schema graphs with a novel knowledge-aware graph network module named K AG N ET , and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The interme-diate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for B ERT -based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning. We open-source our code [Cite_Footnote_1] to the community for future research in knowledge-aware commonsense reasoning."
  },
  {
    "id": 544,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Extend",
    "url": "https://github.com/tensorflow/models/tree/master/research/lm_1b",
    "section_title": "2 Number Agreement with LSTM Language Models",
    "add_info": "4 The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/tree/master/research/lm_1b.",
    "text": "Training was done using a language modeling objective that predicts the next word given the pre-fix; at test time we compute agreement error rates by comparing the probability of the correct verb form with the incorrect one. We report perfor-mance of a few different LSTM hidden layer con-figurations, while other hyper-parameters are se-lected based on a grid search. 2 Following Linzen ous LSTM language models, broken down by the number of attractors. The top two rows represent the random and majority class baselines, while the next row ( \u2020 ) is the reported result from Linzen et al. (2016) for an LSTM language model with 50 hidden units (some entries, denoted by \u2248, are approximately derived from a chart, since Linzen et al. (2016) did not provide a full table of results). We report results of our LSTM implementations of various hidden layer sizes, along with our re-run of the Jozefowicz et al. (2016) language model, in the next five rows. We lastly report the performance of a state of the art character LSTM baseline with a large model capacity (Melis et al., 2018). et al. (2016), we include the results of our repli-cation of the large-scale language model of Joze-fowicz et al. (2016) that was trained on the One Billion Word Benchmark. [Cite_Footnote_4] Hyper-parameter tun-ing is based on validation set perplexity."
  },
  {
    "id": 545,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/clab/rnng",
    "section_title": "3 Number Agreement with RNNGs 3.2 Experiments",
    "add_info": "8 https://github.com/clab/rnng",
    "text": "Experimental settings. We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [Cite_Footnote_8] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences. At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols. An example of the stack contents (i.e. the prefix) when predicting the verb is provided in Fig. 3(a). We similarly run a grid search over the same hyper-parameter range as the sequential LSTM and compare the results with the strongest sequential LSTM baseline from \u00a72."
  },
  {
    "id": 546,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/clab/rnng",
    "section_title": "3 Number Agreement with RNNGs 3.3 Further Analysis",
    "add_info": "11 https://github.com/clab/rnng",
    "text": "Perplexity. To what extent does the success of RNNGs in the number agreement task with mul-tiple attractors correlate with better performance under the perplexity metric? We answer this ques-tion by using an importance sampling marginal-ization procedure (Dyer et al., 2016) to obtain an estimate of p(x) under both RNNGs and the se-quential syntactic LSTM model. Following Dyer et al. (2016), for each sentence on the validation set we sample 100 candidate trees from a dis-criminative model [Cite_Footnote_11] as our proposal distribution. As demonstrated in Table 3, the LSTM language model has the lowest validation set perplexity de-spite substantially worse performance than RN-NGs in number agreement with multiple attrac-tors, suggesting that lower perplexity is not neces-sarily correlated with number agreement success."
  },
  {
    "id": 547,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html",
    "section_title": "3 Corpus, Features, and Data Creation 3.1 Corpus",
    "add_info": "6 The tagger is available free for academic research from http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html.",
    "text": "For these pairs we also annotated some more at-tributes, which are summarised in Table 1. Note that the average distance is slightly higher than that reported in (Schlangen and Lascarides, 2003) for (2-party) dialogue (1.8); this is presumably due to the presence of more speakers who are able to re-ply to an utterance. Finally, we automatically an-notated all utterances with part-of-speech tags, us-ing TreeTagger (Schmid, 1994), which we\u2019ve trained on the switchboard corpus of spoken lan-guage (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies. [Cite_Footnote_6]"
  },
  {
    "id": 548,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html",
    "section_title": "4 Experiments and Results 4.1 Experimental Setup",
    "add_info": "8 Available from http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html.",
    "text": "\u2022 M AX E NT , Zhang Le\u2019s C++ implementation [Cite_Footnote_8] of maximum entropy modelling (Berger et al., 1996). In our experiments, we used L-BFGS parameter es-"
  },
  {
    "id": 549,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://spacy.io",
    "section_title": "4 Experiments 4.2 The \u201cgood probes\u201d are good for both",
    "add_info": null,
    "text": "3  https://spacy.io is set to 0, probes with one hidden layer and 40 hidden neurons are better in both criteria."
  },
  {
    "id": 550,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/SPOClab-ca/InfoProbe",
    "section_title": "B Experiments details",
    "add_info": null,
    "text": "Note that we also swept hyperparameters for FastText, where probes with less parameters do not always outperform more complex probes in either accuracy, loss, selectivity, or information gain. Figures 8 and 9 illustrate these observations. C Reproducibility On a T4 GPU card, training one epoch takes around 20 seconds. Without setting maximum gra-dient steps, 98.6% of experiments finish within 400 epochs. We open source our codes at  https://github.com/SPOClab-ca/InfoProbe ."
  },
  {
    "id": 551,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/fxsjy/jieba",
    "section_title": "5 Processing steps 5.1 Preprocessing",
    "add_info": "3 https://github.com/fxsjy/jieba",
    "text": "We use the third-party tool jieba [Cite_Footnote_3] for word seg-mentation and POS tagging; both steps are cus-tomized in order to achieve a better performance on domain- and task-specific data. Specifically, the dictionary provided by the tool is inter-sected with a user-specified dictionary. This user-specified dictionary contains all words from our lexical resources. The user-added words are anno-tated with customized POS tags, such as \u2018F\u2019 for feature, \u2018EV\u2019 for evaluation etc. The following two examples depict the same sentence as output by jieba without and with customization:"
  },
  {
    "id": 552,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.gutenberg.org/",
    "section_title": "2 Data Collection 2.2 Open Vocabulary Condition",
    "add_info": "1 https://www.gutenberg.org/",
    "text": "The majority of our data was collected with open-vocabulary sentences from books. We use public domain books from Project Gutenberg. [Cite_Footnote_1] Unlike the closed-vocabulary data which is collected in a sin-gle sitting, the open-vocabulary data is broken into multiple sessions where electrodes are reattached before each session and may have minor changes in position between different sessions. In addition to sessions with parallel silent and vocalized utter-ances, we also collect non-parallel sessions with only vocalized utterances. A summary of dataset features is shown in Table 2. We select a validation and test set randomly from the silent parallel EMG data, with 30 and 100 utterances respectively. Note that during testing, we use only the silent EMG recordings E S , so the vocalized recordings of the test utterances are unused."
  },
  {
    "id": 553,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://pypi.org/project/noisereduce/",
    "section_title": "2 Data Collection 2.3 Recording Details",
    "add_info": "2 https://pypi.org/project/noisereduce/",
    "text": "Audio is recorded from a built-in laptop micro-phone at 16kHz. Background noise is reduced us-ing a spectral gating algorithm, [Cite_Footnote_2] and volume is nor-malized across sessions based on peak root-mean-square levels."
  },
  {
    "id": 554,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/mozilla/DeepSpeech",
    "section_title": "4 Experiments 4.2.2 Automatic Evaluation",
    "add_info": "5 https://github.com/mozilla/DeepSpeech",
    "text": "In addition to the human evaluation, we also per-form an automatic evaluation by transcribing sys-tem outputs with a large-vocabulary automatic speech recognition (ASR) system. Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [Cite_Footnote_5] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation."
  },
  {
    "id": 555,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://doi.org/10.5281/zenodo.4064408",
    "section_title": "5 Conclusion",
    "add_info": "6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.",
    "text": "Our results show that digital voicing of silent speech, while still challenging in open domain set-tings, shows promise as an achievable technology. We show that it is important to account for differ-ences in EMG signals between silent and vocal-ized speaking modes and demonstrate an effective method of doing so. On silent EMG recordings from closed vocabulary data our speech outputs achieve high intelligibility, with a 3.6% transcrip-tion word error rate and relative error reduction of 95% from our baseline. We also significantly improve intelligibility in an open vocabulary condi-tion, with a relative error reduction over 20%. We hope that our public release of data will encourage others to further improve models for this task. [Cite_Footnote_6]"
  },
  {
    "id": 556,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/dgaddy/silent_speech",
    "section_title": "5 Conclusion",
    "add_info": "6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.",
    "text": "Our results show that digital voicing of silent speech, while still challenging in open domain set-tings, shows promise as an achievable technology. We show that it is important to account for differ-ences in EMG signals between silent and vocal-ized speaking modes and demonstrate an effective method of doing so. On silent EMG recordings from closed vocabulary data our speech outputs achieve high intelligibility, with a 3.6% transcrip-tion word error rate and relative error reduction of 95% from our baseline. We also significantly improve intelligibility in an open vocabulary condi-tion, with a relative error reduction over 20%. We hope that our public release of data will encourage others to further improve models for this task. [Cite_Footnote_6]"
  },
  {
    "id": 557,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://www.cog.brown.edu/\u223cmj/software.htm",
    "section_title": "packed forest for each sentence. 3 5.1 Data Preparation",
    "add_info": "5 http://www.cog.brown.edu/\u223cmj/software.htm. We follow this version as it corrects some bugs from their 2005 paper which leads to a 0.4% increase in performance (see Table 4).",
    "text": "Following Charniak and Johnson (2005), we ex-tracted the features from the 50-best parses on the training set (sec. 02-21), and used a cut-off of 5 to prune away low-count features. There are 0.8M fea-tures in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M fea-tures in the updated version. [Cite_Footnote_5] However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Ta-bles 3 and 4). This result confirms that our feature set design is appropriate, and the averaged percep-tron learner is a reasonable candidate for reranking."
  },
  {
    "id": 558,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/ETIP-team/ETIP-Project/",
    "section_title": "5 Experiments 5.1 ETIP Dataset",
    "add_info": null,
    "text": "We collected 500 Chinese insurance contracts, which include life, disability, health, property, home, and auto insurance, where 350 contract-s are regarded as the corpus for training word embeddings (Mikolov et al., 2013) and the oth-er 150 contracts are manually labeled for ele-ment tagging testing. The maximum nested lev-el is three in ETIP. The dataset is available on-line (  https://github.com/ETIP-team/ETIP-Project/) without author information. This project cooperated with an information solu-tion provider of China Pacific Insurance Co., Lt-d. (CPIC). Tab. 1 shows the number (N), aver-age length (L) and average element length ratio (ELR) of seven categories in ETIP dataset. CP and IA are the two largest categories in the dataset. ELR of C, PC and E are 0.12, 0.63 and 0.76 re-spectively, which means that they are usually a phrase or clause embedded in a sentence and C is a 2-3 word phrase. ELR of CP, IA, and T are nearly 1.0, which denotes that they are always sentences."
  },
  {
    "id": 559,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/fxsjy/",
    "section_title": "5 Experiments 5.2 Experimental Settings",
    "add_info": "Jieba. 2017. https://github.com/fxsjy/jieba. Accessed: 2018-05-28.",
    "text": "Chinese texts are tokenized with Jieba (Jieba, 2017)  or NLPIR (NLPIR, 2018). 300-dimensional word vectors are trained on our insurance corpus. The size of the input layer in the CNN model is 60 \u00d7 300, and zeros are padded if the length of the training sample is less than 60. The kernel size of the convolution layer is 5\u00d7300, and the size of the feature maps is 36. the fixed length of TOI pooling layer output is 72 = 2 \u00d7 36."
  },
  {
    "id": 560,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/NLPIR-team/NLPIR",
    "section_title": "5 Experiments 5.2 Experimental Settings",
    "add_info": "NLPIR. 2018. https://github.com/NLPIR-team/NLPIR. Accessed: 2018-05- 28.",
    "text": "Chinese texts are tokenized with Jieba (Jieba, 2017) or NLPIR (NLPIR, 2018)  . 300-dimensional word vectors are trained on our insurance corpus. The size of the input layer in the CNN model is 60 \u00d7 300, and zeros are padded if the length of the training sample is less than 60. The kernel size of the convolution layer is 5\u00d7300, and the size of the feature maps is 36. the fixed length of TOI pooling layer output is 72 = 2 \u00d7 36."
  },
  {
    "id": 561,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Compare",
    "url": "https://dumps.wikimedia.org/",
    "section_title": "5 Experiments 5.3 Word Embedding Comparison",
    "add_info": "Wikipedia. 2018. Chinese wikipedia. https://dumps.wikimedia.org/.",
    "text": "350 contracts in ETIP Dataset are regarded as the corpus for training word embeddings (Mikolov et al., 2013). The augmented word2vec mod-el trained by our insurance contract corpus can improve the similarities of the insurance syn-onyms compared to the models trained by other corpora, e.g., Baidu Encyclopedia (Baidu, 2018), Wikipedia zh (Wikipedia, 2018)  , People\u2019s Daily News (People\u2019s Daily, 2018). Cosine similarity between word vectors of insurance synonyms is shown in Tab. 2. The Chinese words are trans-lated into English by Google Translate. Tab. 2 shows that the insurance corpus can greatly im-prove the word embedding similarity between in-surance synonyms compared with other corpora."
  },
  {
    "id": 562,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.ldc.upenn.edu/Projects/ACE/",
    "section_title": "3 Experiments and Results 3.1 Data",
    "add_info": "1 http://www.ldc.upenn.edu/Projects/ACE/",
    "text": "Our proposed graph-based method is evaluated on the ACE corpus [Cite_Footnote_1] , which contains 519 files from sources including broadcast, newswire, and news-paper. A break-down of the tagged data by different relation subtypes is given in Table 1."
  },
  {
    "id": 563,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://ilk.uvt.nl/\u223csabine/chunklink/",
    "section_title": "3 Experiments and Results 3.2 Features",
    "add_info": "2 Software available at http://ilk.uvt.nl/\u223csabine/chunklink/",
    "text": "We extract the following lexical and syntactic fea-tures from two entity mentions, and the contexts be-fore, between and after the entity pairs. Especially, we set the mid-context window as everything be-tween the two entities and the pre- and post- context as up to two words before and after the correspond-ing entity. Most of these features are computed from the parse trees derived from Charniak Parser (Char-niak, 1999) and the Chunklink script [Cite_Footnote_2] written by Sabine Buchholz from Tilburg University. three context windows."
  },
  {
    "id": 564,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm",
    "section_title": "3 Experiments and Results 3.3 Experimental Evaluation 3.3.1 Relation Detection 3.3.2 SVM vs. LP",
    "add_info": "3 LIBSV M: a library for support vector machines. Soft-ware available at http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm.",
    "text": "Table 2 reports the performance of relation detec-tion by using SVM and LP with different sizes of labled data. For SVM, we use LIBSVM tool with linear kernel function [Cite_Footnote_3] . And the same sampled la-beled data used in LP is used to train SVM mod-els. From Table 2, we see that both LP Cosine and LP JS achieve higher Recall than SVM. Especially, with small labeled dataset (percentage of labeled data \u2264 25%), this merit is more distinct. When the percentage of labeled data increases from 50% to 100%, LP Cosine is still comparable to SVM in F-measure while LP JS achieves better F-measure than SVM. On the other hand, LP JS consistently outper-forms LP Cosine ."
  },
  {
    "id": 565,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm",
    "section_title": "4 Experiment 4.1 Dataset Description",
    "add_info": "1 http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm",
    "text": "We conduct our experiments on the public Short Text Conversation (STC) dataset [Cite_Footnote_1] released in NTCIR-13. STC maintains a large reposit-ory of post-comment pairs from the Sina Weibo which is one of the popular Chinese social sites."
  },
  {
    "id": 566,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://pypi.python.org/pypi/jieba",
    "section_title": "4 Experiment 4.3 Implementation Details",
    "add_info": "2 https://pypi.python.org/pypi/jieba",
    "text": "We implemented our model in Tensorflow . We tuned the hyper-parameters via the development set. Specifically, we use one layer of bi-directional GRU for encoder and another uni-directional GRU for decoder, with the GRU hidden unit size set as 300 in both the encoder and decoder. The dimen-sion of semantic word embeddings in both utter-ances and responses is 300, while the dimension of usage word embeddings in responses is 50. We apply the Adam algorithm (Kingma and Ba, 2015) for optimization, where the parameters of Adam are set as in (Kingma and Ba, 2015). The variance \u03c3 [Cite_Footnote_2] of the Gaussian Kernel layer is set as 1, and all other trainable parameters are randomly initialized by uniform distribution within [-0.08,0.08]. The mini-batch size for the update is set as 128. We clip the gradient when its norm exceeds 5."
  },
  {
    "id": 567,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.speech.cs.cmu.edu/tools/lextool.html",
    "section_title": "3 Target Recovery 3.3 Phonetic Lexicon",
    "add_info": "1 http://www.speech.cs.cmu.edu/tools/lextool.html",
    "text": "The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998). This dictionary has an inventory of 39 phonemes. If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. [Cite_Footnote_1] The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited. But, when it is used to map the phoneme lattice into a word lattice of potential targets then the fixed vocabulary from the language model is used."
  },
  {
    "id": 568,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://ssli.ee.washington.edu/data/puns",
    "section_title": "4 Experiments 4.1 Data",
    "add_info": "2 Data available at http://ssli.ee.washington.edu/data/puns.",
    "text": "We collected 75 puns from various joke websites such as Tumblr, Reddit, and Twitter and soliciting examples from friends and colleagues. [Cite_Footnote_2] These were collected without reference to the sources used by Sobkowiak to assemble the puns used in building the phonetic edit model. This data was used for test data only and is completely separate from the training data used by the language model and the phonetic edit model. (It would be nice to have used some of the 1,182 puns from Sobkowiak for test data but only the isolated pun/target pairs were provided without the necessary word contexts.) Pun locations were marked in each sentence as the minimal set of words that change between the pun and the target."
  },
  {
    "id": 569,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.speech.cs.cmu.edu/cgibin/cmudict",
    "section_title": "3 Target Recovery 3.3 Phonetic Lexicon",
    "add_info": "Robert L Weide. 1998. The cmu pronouncing dictionary. URL: http://www.speech.cs.cmu.edu/cgibin/cmudict.",
    "text": "The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998)  . This dictionary has an inventory of 39 phonemes. If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited. But, when it is used to map the phoneme lattice into a word lattice of potential targets then the fixed vocabulary from the language model is used."
  },
  {
    "id": 570,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://catalog.ldc.upenn.edu/LDC2006T13",
    "section_title": "3 Method",
    "add_info": "1 http://catalog.ldc.upenn.edu/LDC2006T13",
    "text": "To perform lexical substitution, we follow the delex-icalization framework of Szarvas et al. (2013). We automatically build Distributional Thesauri (DTs) for the medical domain and use features from the Uni-fied Medical Language System (UMLS) ontology. The dataset for supervised lexical substitution consists of sentences, containing an annotated target word t. Con-sidering the sentence being the context for the target word, the target word might have different meanings. Thus annotated substitute candidates s g 1 . . . s g n \u2208 s g , need to be provided for each context. The negative ex-amples are substitute candidates that either are incor-rect for the target word, do not fit into the context or both. We will refer to these substitutes as false substi-tute candidates s f [Cite_Footnote_1] . . . s f m \u2208 s f with s f \u2229 s g = \u2205."
  },
  {
    "id": 571,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/",
    "section_title": "3 Method",
    "add_info": "2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/",
    "text": "For the generation of substitute candidates we do not use WordNet, as done in previous works (Szarvas et al., 2013), but use only substitutes from a DT. To train a single classifier, features that distinguishing the mean-ing of words in different context need to be considered. Such features could be e.g. n-grams, features from dis-tributional semantics or features which are extracted relative to the target word, such as the ratio between frequencies of the substitute candidate and the target word. After training, we apply the algorithm to un-seen substitute candidates and rank them according to their positive probabilities, given by the classifier. Con-trary to Szarvas et al. (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [Cite_Footnote_2] ."
  },
  {
    "id": 572,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.nlm.nih.gov/bsd/licensee/2014_stats/baseline_med_filecount.html",
    "section_title": "4 Resources 4.1 Distributional thesauri (DTs)",
    "add_info": "4 http://www.nlm.nih.gov/bsd/licensee/2014_stats/baseline_med_filecount.html",
    "text": "The first DT is computed based on Medline [Cite_Footnote_4] ab-stracts. This thesaurus uses the left and the right word as context features. To include multi-word expressions, we allow the number of tokens that form a term to be up to the length of three."
  },
  {
    "id": 573,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/jweb1t/",
    "section_title": "4 Resources 4.3 Google Web1T",
    "add_info": "5 https://code.google.com/p/jweb1t/",
    "text": "We use the Google Web1T to generate n-gram features as we expect this open domain resource to have consid-erable coverage for most specific domains as well. For accessing the resource, we use JWeb1T [Cite_Footnote_5] (Giuliano et al., 2007)."
  },
  {
    "id": 574,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/lancopku/text-autoaugment",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/lancopku/text-autoaugment",
    "text": "Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previ-ous methods, which decreases the diversity of the augmented data and thus restricts the per-formance gain. To overcome the above limi-tations, we propose a framework named Text AutoAugment (TAA) to establish a composi-tional and learnable paradigm for data aug-mentation. We regard a combination of vari-ous operations as an augmentation policy and utilize an efficient Bayesian Optimization al-gorithm to automatically search for the best policy, which substantially improves the gen-eralization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines. [Cite_Footnote_1]"
  },
  {
    "id": 575,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/AD3",
    "section_title": "5 Experiments 5.1 Experimental setup",
    "add_info": "8 We use the AD 3 implementation in http://www.ark.cs.cmu.edu/AD3, setting the maximum number of iterations to 200 at training time and 1000 at test time. We extended the code to handle the knapsack and budget factors; the modified code will be part of the next release (AD 3 2.1).",
    "text": "We evaluated our compressive summarizers on data from the Text Analysis Conference (TAC) evaluations. We use the same splits as previ-ous work (Berg-Kirkpatrick et al., 2011; Wood-send and Lapata, 2012): the non-update portions of TAC-2009 for training and TAC-2008 for test-ing. In addition, we reserved TAC-2010 as a dev-set. The test partition contains 48 multi-document summarization problems; each provides 10 related news articles as input, and asks for a summary with up to 100 words, which is evaluated against four manually written abstracts. We ignored all the query information present in the TAC datasets. Single-Task Learning. In the single-task exper-iments, we trained a compressive summarizer on the dataset disclosed by Berg-Kirkpatrick et al. (2011), which contains manual compressive sum-maries for the TAC-2009 data. We trained a struc-tured SVM with stochastic subgradient descent; the cost-augmented inference problems are re-laxed and solved with AD 3 , as described in \u00a73.3. [Cite_Footnote_8] We followed the procedure described in Berg-Kirkpatrick et al. (2011) to reduce the number of candidate sentences: scores were defined for each sentence (the sum of the scores of the concepts they cover), and the best-scored sentences were greedily selected up to a limit of 1,000 words. We then tagged and parsed the selected sentences with TurboParser. Our choice of a dependency parser was motivated by our will for a fast system; in par-ticular, TurboParser attains top accuracies at a rate of 1,200 words per second, keeping parsing times below 1 second for each summarization problem. Multi-Task Learning. For the multi-task ex-periments, we also used the dataset of Berg-Kirkpatrick et al. (2011), but we augmented the training data with extractive summarization and sentence compression datasets, to help train the compressive summarizer. For extractive sum-marization, we used the DUC 2003 and 2004 datasets (a total of 80 multi-document summariza-tion problems). We generated oracle extracts by maximizing bigram recall with respect to the man-ual abstracts, as described in Berg-Kirkpatrick et al. (2011). For sentence compression, we adapted the Simple English Wikipedia dataset of Wood-send and Lapata (2011), containing aligned sen-tences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletion-based compressions."
  },
  {
    "id": 576,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/TurboParser",
    "section_title": "5 Experiments 5.1 Experimental setup",
    "add_info": "9 http://www.ark.cs.cmu.edu/TurboParser",
    "text": "We evaluated our compressive summarizers on data from the Text Analysis Conference (TAC) evaluations. We use the same splits as previ-ous work (Berg-Kirkpatrick et al., 2011; Wood-send and Lapata, 2012): the non-update portions of TAC-2009 for training and TAC-2008 for test-ing. In addition, we reserved TAC-2010 as a dev-set. The test partition contains 48 multi-document summarization problems; each provides 10 related news articles as input, and asks for a summary with up to 100 words, which is evaluated against four manually written abstracts. We ignored all the query information present in the TAC datasets. Single-Task Learning. In the single-task exper-iments, we trained a compressive summarizer on the dataset disclosed by Berg-Kirkpatrick et al. (2011), which contains manual compressive sum-maries for the TAC-2009 data. We trained a struc-tured SVM with stochastic subgradient descent; the cost-augmented inference problems are re-laxed and solved with AD 3 , as described in \u00a73.3. We followed the procedure described in Berg-Kirkpatrick et al. (2011) to reduce the number of candidate sentences: scores were defined for each sentence (the sum of the scores of the concepts they cover), and the best-scored sentences were greedily selected up to a limit of 1,000 words. We then tagged and parsed the selected sentences with TurboParser. [Cite_Footnote_9] Our choice of a dependency parser was motivated by our will for a fast system; in par-ticular, TurboParser attains top accuracies at a rate of 1,200 words per second, keeping parsing times below 1 second for each summarization problem. Multi-Task Learning. For the multi-task ex-periments, we also used the dataset of Berg-Kirkpatrick et al. (2011), but we augmented the training data with extractive summarization and sentence compression datasets, to help train the compressive summarizer. For extractive sum-marization, we used the DUC 2003 and 2004 datasets (a total of 80 multi-document summariza-tion problems). We generated oracle extracts by maximizing bigram recall with respect to the man-ual abstracts, as described in Berg-Kirkpatrick et al. (2011). For sentence compression, we adapted the Simple English Wikipedia dataset of Wood-send and Lapata (2011), containing aligned sen-tences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletion-based compressions."
  },
  {
    "id": 577,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://lpsolve.sourceforge.net/",
    "section_title": "3 Experiments",
    "add_info": "3 From http://lpsolve.sourceforge.net/",
    "text": "We used lp solve [Cite_Footnote_3] to solve our ILP optimization problems. We ran experiments on two datasets. We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER , but we found that several documents where too long for lp solve to find a solution."
  },
  {
    "id": 578,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.nsdl.org",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.nsdl.org",
    "text": "Our research is motivated by the challenges we encountered in working with the National Science Digital Library (NSDL) collection. [Cite_Footnote_1] Each item in the collection is a scientific resource, such as a re-search paper, an educational video, or perhaps an entire website. In addition to its main content, each resource is annotated with metadata, which provides information such as the author or creator of the re-source, its subject area, format (text/image/video) and intended audience \u2013 in all over 90 distinct fields (though some are very related). Making use of such extensive metadata in a digital library paves the way for constructing highly-focused models of the user\u2019s information need. These models have the potential to dramatically improve the user experience in tar-geted applications, such as the NSDL portals. To illustrate this point, suppose that we are running an educational portal targeted at elementary school teachers, and some user requests teaching aids for an introductory class on gravity. An intelligent search system would be able to translate the request into a structured query that might look something like: subject=\u2019gravity\u2019 AND audience=\u2019grades 1-4\u2019 AND format=\u2019image,video\u2019 AND rights=\u2019free-for-academic-use\u2019. Such a query can be efficiently an-swered by a relational database system."
  },
  {
    "id": 579,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Compare",
    "url": "https://github.com/tuetschek/e2e-cleaning",
    "section_title": "2 Frequency of Surface Forms",
    "add_info": "1 https://github.com/tuetschek/e2e-cleaning",
    "text": "To compare the diversity of the E2E training data with that of the generated text, we looked at the surface forms used to express each attribute-value pair. This is enabled by a set of regular expres-sions released by Dus\u030cek et al. (2019a) [Cite_Footnote_1] . The reg-ular expressions capture the entire phrase used to express an attribute-value pair, focusing on the con-tent words and attempting as much as possible to leave out the function words, e.g."
  },
  {
    "id": 580,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/tuetschek/",
    "section_title": "References",
    "add_info": null,
    "text": "We thank the anonymous reviewers for their helpful comments. This research is supported by Science Foundation Ireland in the ADAPT Centre for Dig-ital Content Technology. The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional De-velopment Fund. A Replication Instructions Dataset The E2E dataset contains a training set of 42,061 pairs of meaning representations and hu-man authored utterances, 4,672 pairs in the devel-opment set, and 4,693 in the test set. Download the dataset from  https://github.com/tuetschek/ e2e-dataset ."
  },
  {
    "id": 581,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/tuetschek/e2e-cleaning",
    "section_title": "References",
    "add_info": null,
    "text": "1. First the delexicalised data is converted into source and target files. It uses modified regu-lar expressions from the e2e-cleaning repos-itory.  https://github.com/tuetschek/e2e-cleaning"
  },
  {
    "id": 582,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Henry-E/surface_realization_opennmt-py",
    "section_title": "References",
    "add_info": null,
    "text": "2. Inside the scripts/ folder of our main repository there are bash scripts for run-ning the preprocessing required by Open-NMT and the actual training. We use our own fork of OpenNMT; the only changes made were to the beam search decod-ing code.  https://github.com/Henry-E/surface_realization_opennmt-py"
  },
  {
    "id": 583,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/tuetschek/e2e-metrics",
    "section_title": "References",
    "add_info": null,
    "text": "1. We calculate n-gram metric scores using the E2E-metrics module  https://github.com/tuetschek/e2e-metrics"
  },
  {
    "id": 584,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/tuetschek/e2e-cleaning",
    "section_title": "References",
    "add_info": null,
    "text": "2. To calculate semantic accuracy we use a minimally modified version of the slot error.py module from  https://github.com/tuetschek/e2e-cleaning . We noticed it was incorrectly grouped together attributes in a small number of cases (we saw less than 5). This change improved Slug2Slug\u2019s results, as it now showed that it had fewer missing attributes."
  },
  {
    "id": 585,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cogcomp.cs.illinois.edu/page/softwareview/POS",
    "section_title": "2 The System 2.1 Temporal Expression Extraction",
    "add_info": "1 http://cogcomp.cs.illinois.edu/page/softwareview/POS",
    "text": "We built the temporal expression extraction module on top of the Heideltime system (Stro\u0308tgen and Gertz, 2010) to take advantage of a state-of-the-art tempo-ral extraction system in capturing basic expressions. We use the Illinois POS tagger [Cite_Footnote_1] (Roth and Zelenko, 1998) to provide part-of-speech tags for the input text before passing it to HeidelTime. Below is an example of the HeidelTime output of the example in the previous section:"
  },
  {
    "id": 586,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2006T08",
    "section_title": "3 Experimental Study 3.1 Data Preparation",
    "add_info": "3 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2006T08",
    "text": "We focus on scaling up temporal systems to deal with complex expressions. Therefore, we prepared an evaluation data set that consists of a list of sen-tences containing at least one of the five temporal connectives since, betwen, from, before and after. To do this, we extract all sentences that satisfy the condition from 183 articles in the TimeBank 1.2 corpus [Cite_Footnote_3] . This results in a total of 486 sentences. Each sentence in the data set comes with the doc-ument creation time (DCT) of its corresponding ar-ticle. The second and the third columns of Table 1 summarize the number of sentences and appear-ances of each temporal connective."
  },
  {
    "id": 587,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://cogcomp.cs.illinois.edu/page/demoview/TempSys",
    "section_title": "4 The Demonstration 4.1 Visualization",
    "add_info": "4 http://cogcomp.cs.illinois.edu/page/demoview/TempSys",
    "text": "We have implemented our system in a web-based demo [Cite_Footnote_4] . Figure 2 shows a screenshot of the input panel of the system. The input panel includes a main text box that allows users to input the text, and some other input fields that allow users to customize the system\u2019s outputs. Among the fields, the reference date serves as the document creation time (DCT) of the input text. All temporal expressions captured from the text will be normalized based on the ref-erence date and compared also to the reference date as illustrated in Figure 3."
  },
  {
    "id": 588,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03",
    "section_title": "3 Initial Experiments 3.1 Data",
    "add_info": "2 Available at: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03",
    "text": "The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC [Cite_Footnote_2] , containing about 28 millions of Chinese words. Since punc-tuations are rarely used to construct collocations, they were removed from the corpora. To auto-matically estimate the precision of extracted col-locations on the Chinese corpus, we built a gold set by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations."
  },
  {
    "id": 589,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.hcu.ox.ac.uk/BNC/",
    "section_title": "6 Evaluation on English corpus",
    "add_info": "6 Available at: http://www.hcu.ox.ac.uk/BNC/",
    "text": "We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [Cite_Footnote_6] . The English corpus contains about 20 millions of words."
  },
  {
    "id": 590,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cs.umd.edu/\u02dcmount/ANN/",
    "section_title": "6 Experiment 2: NN-based outlier detection",
    "add_info": "D. Mount and S. Arya. 2005. ANN: A library for approx-imate nearest neighbor searching. Download from http://www.cs.umd.edu/\u02dcmount/ANN/.",
    "text": "Modeling. We model unknown sense detection as an outlier detection task, using Tax and Duin\u2019s out-lier detection approach that we have outlined in the previous section. Nearest neighbors (by Eu-clidean distance) were computed using the ANN tool (Mount and Arya, 2005)  . We compute one out-lier detection model per lemma. With training and test sets constructed as described in Section 3, the average training set comprises 22.5 sentences."
  },
  {
    "id": 591,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://pypi.python.org/pypi/newspaper",
    "section_title": "3 Data Extraction and Preprocessing 3.2 Extracting Data",
    "add_info": "2 https://pypi.python.org/pypi/newspaper",
    "text": "We deduplicated the 16,000 extracted URLs into 6,003 unique addressed, then extracted and preprocessed their contents. The newspaper package [Cite_Footnote_2] was used to extract article text and the title from the web page. Since we are interested in text articles that can serve as the source text for summarization algorithms, we needed to re-move photos and video links such as those from Instagram and YouTube. To do so, we removed those links that contained fewer than a threshold of 150 words. After this preprocessing, the number of useful articles was reduced from 6003 to 3066. There were some further tweet-article pairs where the text of the tweets was identical, these were re-moved by further preprocessing and the number of unique tweet-article pairs came down to 2471."
  },
  {
    "id": 592,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.wikipedia.com",
    "section_title": "2 Related Work 2.4 Data",
    "add_info": "4 http://www.wikipedia.com",
    "text": "The third dataset (wikipedia, henceforth) comes from the Wikipedia [Cite_Footnote_4] discussion section. When a topic on Wikipedia is disputed, the editors of that topic start a discussion about it. We collected 117 Wikipeida discussion threads. The threads contains a total of 1,867 posts."
  },
  {
    "id": 593,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://github.com/coder352/HAHSum",
    "section_title": "1 Introduction",
    "add_info": "1 http://github.com/coder352/HAHSum",
    "text": "The contributions of this paper are as below: 1) We propose a hierarchical attentive heteroge-neous graph based model(HAHSum) to guide the redundancy information propagating between sen-tences and learn redundancy-aware sentence rep-resentation; 2) Our architecture is able to extract flexible quantity of sentences with a threshold, in-stead of top-k strategy; 3) We evaluate HAHSum on three popular benchmarks (CNN/DM, NYT, NEWSROOM) and experimental results show that HAHSum outperforms the existing state-of-the-art approaches. Our source code will be available on Github [Cite_Footnote_1] ."
  },
  {
    "id": 594,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "4 Experiments Setting 4.2 Evaluation Metric & Parameter Settings",
    "add_info": "2 https://github.com/huggingface/transformers",
    "text": "Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [Cite_Footnote_2] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set. Adam with \u03b2 1 = 0.9, \u03b2 2 = 0.999 is used as optimizer and learning rate schedule follows the strategies with warming-up on first 10,000 steps (Vaswani et al., 2017). The final threshold in ex-traction is 0.65 for CNN/DM, 0.58 for NYT and 0.64 for Newsroom, with the highest ROUGE-1 score individually. A higher threshold will be with more concise summary and the lower threshold will return more information."
  },
  {
    "id": 595,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/nlpyang/PreSumm",
    "section_title": "4 Experiments Setting 4.2 Evaluation Metric & Parameter Settings",
    "add_info": "3 https://github.com/nlpyang/PreSumm",
    "text": "Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm [Cite_Footnote_3] . We train our model (with about 400M parameters) one day for 100,000 steps on GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set. Adam with \u03b2 1 = 0.9, \u03b2 2 = 0.999 is used as optimizer and learning rate schedule follows the strategies with warming-up on first 10,000 steps (Vaswani et al., 2017). The final threshold in ex-traction is 0.65 for CNN/DM, 0.58 for NYT and 0.64 for Newsroom, with the highest ROUGE-1 score individually. A higher threshold will be with more concise summary and the lower threshold will return more information."
  },
  {
    "id": 596,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/neukg/GRTE",
    "section_title": "References",
    "add_info": null,
    "text": "Table filling based relational triple extraction methods are attracting growing research inter-ests due to their promising performance and their abilities on extracting triples from com-plex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ig-nore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information dur-ing triple extraction. To overcome this defi-ciency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a table feature for each relation. Then two kinds of global associations are mined from the generated ta-ble features. Next, the mined global associ-ations are integrated into the table feature of each relation. This \u201cgenerate-mine-integrate\u201d process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation\u2019s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the pro-posed model on three benchmark datasets. Ex-perimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at:  https://github.com/neukg/GRTE."
  },
  {
    "id": 597,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/gorokoba560/norm-analysis-of-transformer",
    "section_title": "References",
    "add_info": "1 https://github.com/gorokoba560/norm-analysis-of-transformer",
    "text": "Transformer architecture has become ubiqui-tous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer archi-tecture is not only composed of the multi-head attention; other components can also contribute to Transformers\u2019 progressive perfor-mance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual con-nection, and layer normalization. Our anal-ysis of Transformer-based masked language models shows that the token-to-token interac-tion performed via attention has less impact on the intermediate representations than pre-viously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention pat-terns tends not to adversely affect the perfor-mance. The codes of our experiments are pub-licly available. [Cite_Footnote_1]"
  },
  {
    "id": 598,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.ukp.tu-darmstadt.de/data/edit-classification",
    "section_title": "1 Introduction",
    "add_info": "2 http://www.ukp.tu-darmstadt.de/data/edit-classification",
    "text": "In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012). This taxonomy enables a fine-grained analysis of edit activity in revision histories. We present the results from an automatic classifica-tion experiment, based on an annotated corpus of ed-its in the English Wikipedia. Additional information necessary to reproduce our results, including word lists and training, development and test data, is re-leased online. [Cite_Footnote_2] To the best of our knowledge, this is the first approach allowing to classify each single edit in Wikipedia into one or more of 21 different edit categories using a supervised machine learning approach."
  },
  {
    "id": 599,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://sourceforge.net/projects/jazzydicts",
    "section_title": "3 Experiments 3.2 Features for Edit Category Classification",
    "add_info": "3 http://sourceforge.net/projects/jazzydicts",
    "text": "Table 1 includes the value of each feature for the example edit from Figure 1. This edit modifies the link [[Dactyl|Dactylic]] by adding a speci-fication to the target of that link. For spell-checking, we use British and US-American English Jazzy dic-tionaries. [Cite_Footnote_3] Markup elements are detected by the Sweble Wikitext parser (Dohrn and Riehle, 2011)."
  },
  {
    "id": 600,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://opennlp.apache.org",
    "section_title": "3 Experiments 3.2 Features for Edit Category Classification",
    "add_info": "5 Maxent model for English, http://opennlp.apache.org",
    "text": "Language Language features are calculated on the context s v\u22121 and s v of edits, any wiki markup is deleted. For the Explicit Semantic Analysis, we use Wiktionary (Zesch et al., 2008) and not Wikipedia assuming that the former has a better coverage with respect to different lexical classes. POS tagging was carried out using the OpenNLP POS tagger. [Cite_Footnote_5] The vandalism word list contains a hand-crafted set of around 100 vandalism and spam words from various places in the web."
  },
  {
    "id": 601,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://meka.sourceforge.net",
    "section_title": "3 Experiments 3.3 Experimental Setup",
    "add_info": "6 http://meka.sourceforge.net",
    "text": "We extract features with the help of ClearTK (Ogren et al., 2008). For the machine learning part, we use Weka (Hall et al., 2009) with the Meka [Cite_Footnote_6] and Mu-lan (Tsoumakas et al., 2010) extensions for multi-label classification. We use DKPro Lab (Eckart de Castilho et al., 2011) to test different parameter com-binations. We randomly split the gold standard data from WPEC into 80% training, 10% test and 10% development set, as shown in Table 2."
  },
  {
    "id": 602,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://en.wikipedia.org/wiki/Wikipedia:FA",
    "section_title": "5 A closer look at edit sequences: Mining collaboration patterns",
    "add_info": "7 http://en.wikipedia.org/wiki/Wikipedia:FA",
    "text": "An edit category classifier allows us to label en-tire article revision histories. We applied the best-performing model from Section 3.3 trained on the entire WPEC to automatically classify all edits in the Wikipedia Quality Assessment Corpus (WPQAC) as presented in previous work (Daxenberger and Gurevych, 2012). WPQAC consists of 10 fea-tured and 10 non-featured articles [Cite_Footnote_7] , with an over-all number of 21,578 revisions (9,986 revisions from featured articles and 11,592 from non-featured articles), extracted from the April 2011 English Wikipedia dump. The articles in WPQAC are care-fully chosen to form comparable pairs of featured and non-featured articles, which should reduce the noise of external influences on edit activity such as popularity or visibility. In Daxenberger and Gurevych (2012), we have shown significant dif-ferences in the edit category distribution of arti-cles with featured status before and after the articles were featured. We concluded that articles become more stable after being featured, as shown by the higher number of surface edits and lower number of meaning-changing edits."
  },
  {
    "id": 603,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.philippe-fournier-viger.com/spmf",
    "section_title": "5 A closer look at edit sequences: Mining collaboration patterns",
    "add_info": "8 http://www.philippe-fournier-viger.com/spmf",
    "text": "Different to our previous approach which is based on the mere distribution of edit categories, in the present study we include the chronological order of edits and use a 10 times larger amount of data for our experiments. We segmented all adjacent revisions in WPQAC into edits, following the approach ex-plained in Daxenberger and Gurevych (2012). Dur-ing the classification process, we discarded revisions where the classifier could not assign any of the 21 edit categories with a confidence higher than the threshold, cf. Table 3. This resulted in 17,640 re-maining revisions. We applied a sequential pattern mining algorithm with time constraints (Hirate and Yamana, 2006; Fournier-Viger et al., 2008) to the data. The latter is based on the PrefixSpan algorithm (Pei et al., 2004). Calculations have been carried out within the open-source SPMF Java data mining plat-form. [Cite_Footnote_8]"
  },
  {
    "id": 604,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/Maluuba/nlg-evalcan",
    "section_title": "A Appendices A.1 Hierarchical Fusion Decoder",
    "add_info": null,
    "text": "The final output state reaches through the feed-forward and add&norm layer like the general trans-former, calculated as the following equation: where W 1 , W 2 , b 1 and b 2 are trainable parameters. A.2 Evalution Metrics We use the nmtpytorch evaluation library  https: //github.com/lium-lst/nmtpytorch suggested by the How2 Challenge, which includes BLEU (1, 2, 3, 4), ROUGE-L, METEOR, and CIDEr eval-uation metrics. As an alternative, nlg-eval https://github.com/Maluuba/nlg-evalcan obtain the same evaluation score as nmtpytorch."
  },
  {
    "id": 605,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/Maluuba/nlg-eval",
    "section_title": "A Appendices A.1 Hierarchical Fusion Decoder",
    "add_info": null,
    "text": "The final output state reaches through the feed-forward and add&norm layer like the general trans-former, calculated as the following equation: where W 1 , W 2 , b 1 and b 2 are trainable parameters. A.2 Evalution Metrics We use the nmtpytorch evaluation library https: //github.com/lium-lst/nmtpytorch suggested by the How2 Challenge, which includes BLEU (1, 2, 3, 4), ROUGE-L, METEOR, and CIDEr eval-uation metrics. As an alternative, nlg-eval  https://github.com/Maluuba/nlg-eval can obtain the same evaluation score as nmtpytorch."
  },
  {
    "id": 606,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/neural-dialogue-metrics/rouge",
    "section_title": "A Appendices A.1 Hierarchical Fusion Decoder",
    "add_info": null,
    "text": "In addition, we also use a ROUGE evaluation library  https://github.com/neural-dialogue-metrics/rouge , which supports the evaluation of ROUGE series metrics (ROUGE-N, ROUGE-L and ROUGE-W)."
  },
  {
    "id": 607,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://homepages.inf.ed.ac.uk/s0677528/data.html",
    "section_title": "3 Problem Formulation",
    "add_info": "2 Available from http://homepages.inf.ed.ac.uk/s0677528/data.html.",
    "text": "In this section we give a brief description of the im-age database we employ and also define the image annotation and story picturing tasks we are attempt-ing here. As mentioned earlier, we use the dataset created in Feng and Lapata (2008). [Cite_Footnote_2] It contains 3,361 articles that have been downloaded from the BBC News website . Each article contains a news image which in turn is associated with a caption. The images are usually 203 pixels wide and 152 pix-els high. The average caption length is 5.35 tokens, and the average document length 133.85 tokens. The captions vocabulary is 2,167 words and the docu-ment vocabulary is 6,253. The vocabulary shared between captions and documents is 2,056 words. In contrast to the Corel database, this dataset con-tains more complex images (with many objects) and has a larger vocabulary (Corel\u2019s vocabulary is ap-proximately 300 words). An example of an abridged database entry is shown in Figure 1."
  },
  {
    "id": 608,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://code.google.com/p/thebeast",
    "section_title": "4 Models for Bridging Resolution 4.2 Markov Logic Networks",
    "add_info": "5 http://code.google.com/p/thebeast",
    "text": "MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich re-lations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast [Cite_Footnote_5] to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP infer-ence for Markov logic."
  },
  {
    "id": 609,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html",
    "section_title": "5 Features 5.1 Local features 5.1.2 Other features",
    "add_info": "6 http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html",
    "text": "First, we extract the three most highly associ-ated prepositions for each anaphor. Then for each anaphor-antecedent candidate pair, we use their head words to create the query \u201danaphor preposition an-tecedent\u201d. To improve recall, we take lowercase, uppercase, singular and plural forms of the head word into account, and replace proper names by fine-grained named entity types (using a gazetteer). All raw hit counts are converted into the Dunning Root Loglikelihood association measure, [Cite_Footnote_6] then nor-malized using Formula 2 within all antecedent can-didates of one anaphor."
  },
  {
    "id": 610,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.statmt.org/wmt19/translation-task.html",
    "section_title": "3 Experiment 3.1 Setup",
    "add_info": "4 http://www.statmt.org/wmt19/translation-task.html",
    "text": "Data Main experiments are conducted on four widely-used translation datasets: WMT14 English-German (En-De, Vaswani et al. 2017), WMT16 Romanian-English (Ro-En, Gu et al. 2018), WMT17 Chinese-English (Zh-En, Hassan et al. 2018), and WAT17 Japanese-English (Ja-En, Mor-ishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [Cite_Footnote_4] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Pa-pineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical sig-nificance test. The translation accuracy of low-frequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established based on the widely-used automatic alignment tool GIZA++ (Och and Ney, 2003)."
  },
  {
    "id": 611,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.nlm.nih.gov/databases/download/pubmed_medline.html",
    "section_title": "2 Data",
    "add_info": "2 https://www.nlm.nih.gov/databases/download/pubmed_medline.html",
    "text": "For minimally supervised extraction of n-gram patterns, we use structured abstracts in which the authors describe different aspects of their work under targeted headings. We retrieved the headings and associated sections automatically from abstracts in XML format (downloaded from PubMed [Cite_Footnote_2] ). In general abstracts are structured id-iosyncratically (often as Introduction, Methods, Results, Discussion). We capitalized on the minor-ity of abstracts that used the explicit Participants, Intervention and Outcome headings. We obtained 50,000 segments for each of these three categories."
  },
  {
    "id": 612,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/gyauney/domain-specific-lexical-grounding",
    "section_title": "1 Introduction",
    "add_info": "3 Code is at https://github.com/gyauney/domain-specific-lexical-grounding.",
    "text": "Our second contribution is a simple but per-formant clustering algorithm for this setting, EntSharp. [Cite_Footnote_3] We intend this method to learn from himage, wordi co-occurrences collected from multi-image, multi-sentence document collections."
  },
  {
    "id": 613,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/salesforce/TopicBERT",
    "section_title": "References",
    "add_info": "1 Code will be available at https://github.com/salesforce/TopicBERT.",
    "text": "While participants in a multi-party multi-turn conversation simultaneously engage in multi-ple conversation topics, existing response se-lection methods are developed mainly focus-ing on a two-party single-conversation sce-nario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and rel-evant conversation context. With this new for-mulation, we propose a novel multi-task learn-ing framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretrain-ing step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selec-tion and topic disentanglement tasks outper-forming existing methods by a good margin. [Cite_Footnote_1]"
  },
  {
    "id": 614,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/dstc8-track2/NOESIS-II/tree/master/subtask4",
    "section_title": "5 Experiments 5.3 Experiment III: Disentanglement",
    "add_info": "3 https://github.com/dstc8-track2/NOESIS-II/tree/master/subtask4",
    "text": "\u2022 Feed-Forward. This is the baseline model [Cite_Footnote_3] from DSTC-8 task organizers that has the best result for task 4 (Kummerfeld et al., 2019), which is trained by employing a two-layer feed-forward neural network on a set of 77 hand engineered features combined with word average embed-dings from pretrained Glove embeddings."
  },
  {
    "id": 615,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://arxiv.org/abs/1803.07416",
    "section_title": "7:453\u2013466.",
    "add_info": "8 https://arxiv.org/abs/1803.07416",
    "text": "As future work, we would like to investigate complementary attention mechanisms like those of Reformer (Kitaev et al., 2020) or Routing Trans-former (Roy et al., 2020), push scalability with ideas like those from RevNet (Gomez et al., 2017), and study the performance of ETC in datasets with even richer structure. Kanagal, Ilya Eckstein, Manan Shah, Nich Kwon, Vikram Rao Sudarshan, Joshua Maynez, Manzil Zaheer, Kelvin Guu, Tom Kwiatkowski, Kristina Toutanova, and D. Sivakumar for helpful discus-sions, support, comments, and feedback on earlier versions of this work. We would also like to thank the Longformer authors (Iz Beltagy, Matthew E. Peters, Arman Cohan) for their useful feedback on earlier versions of this paper and for sharing parameter counts. Appendix A: Implementation Details Global-Local Attention Implementation This appendix provides further details on the TPU/GPU-friendly implementation of global-local attention. Our implementation of sliding win-dow local attention is similar to the approach in the local attention 1d layer in Ten-sor2Tensor [Cite_Footnote_8] , but with the addition of flexible mask-ing, relative position encoding, and global tokens as side keys/values. We use a simple example to describe the internal blocking logic. Let\u2019s say the input corresponds to embeddings for the fol-lowing word pieces, each represented by a letter: ABCDEFG."
  },
  {
    "id": 616,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://qangaroo.cs.ucl.ac.uk/",
    "section_title": "Appendix C: Training Details NQ",
    "add_info": null,
    "text": "Inference: In order to make predictions, sup-porting facts are predicted using a single dense layer taking the global input embeddings as input with a threshold over the output logits. Output type is predicted with a single dense layer from the global CLS token. Answer spans where predicted also with dense layers, but using the long input embeddings as inputs, using the following crite-ria: begin/end positions must be in sentences or titles, begin/end must be in the same sentence/title, spans must belong to a supporting fact, begin must be before end, and spans cannot exceed a maxi-mum answer length of 30 tokens. Within spans satisfying those criteria, a single span with top begin prob \u2217 end prob is selected. WikiHop Data Download Link:  https://qangaroo.cs.ucl.ac.uk/"
  },
  {
    "id": 617,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/microsoft/ContextualSP",
    "section_title": "1 Introduction",
    "add_info": "1 Our code is available at https://github.com/microsoft/ContextualSP.",
    "text": "To deal with IUR, a natural idea is to trans-fer models from coreference resolution (Clark and Manning, 2016). However, this idea is not easy to realize, as ellipsis also accounts for a large pro-portion. Despite being different, coreference and ellipsis both can be resolved without introducing out-of-dialogue words in most cases. That is to say, words of the rewritten utterance are nearly from either the context utterances or the incom-plete utterance. Observing it, most previous works employ the pointer network (Vinyals et al., 2015) or the sequence to sequence model with copy mechanism (Gu et al., 2016; See et al., 2017). However, they generate the rewritten utterance from scratch, neglecting a key trait that the main structure of a rewritten utterance is always the same as the incomplete utterance. To highlight it, we imagine the rewritten utterance as the outcome after a series of edit operations (i.e. substitute and insert) on the incomplete utterance. Taking the ex-ample from Table 1, x 3\u2217 can be obtained by sub-stituting \u201c\u8fd9\u6837\u201d(this) in x 3 with \u201c\u9634\u5929\u201d(cloudy) in x 2 and inserting \u201c\u5317\u4eac\u201d(Beijing) before \u201c\u4e3a\u4ec0 \u4e48\u201d(Why), much easier than producing x \u22173 via de-coding word by word. These edit operations are carried out between word pairs of the context ut-terances and the incomplete utterance, analogous to semantic segmentation (a well-known task in computer vision): Given relevance features be-tween word pairs as an image, the model is to pre-dict the edit type for each word pair as a pixel-level mask (elaborated in Section 3). Inspired by the above, in this paper, we propose a novel and exten-sive approach which formulates IUR as semantic segmentation [Cite_Footnote_1] . Our contributions are as follows:"
  },
  {
    "id": 618,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/usage/linguistic-features#pos-tagging",
    "section_title": "5 Task-based evaluation 5.2 Baselines",
    "add_info": "1 https://spacy.io/usage/linguistic-features#pos-tagging",
    "text": "\u2022 A NTONYMS : We tag each phrase in the KB as either a verb, noun, or adjective phrase us-ing the SpaCy POS tagger. [Cite_Footnote_1] Then, for each verb (noun, adjective) phrase, we replace the first verb (noun, adjective) token with a ran-"
  },
  {
    "id": 619,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://code.google.com/p/word2vec/",
    "section_title": "3 Task & System Description 3.2 Building Clusters for Comparison",
    "add_info": "4 We use the pre-trained 300 dimension vectors available at http://code.google.com/p/word2vec/",
    "text": "In response, we exploit the availability of pre-trained word vectors as a source of background se-mantic knowledge for every phrase, and generalize the pLSA model to Gaussian pLSA (G-pLSA). We construct a vector representation for each descriptive phrase by averaging the word-vectors of individual words in a phrase (Mikolov et al., 2013) [Cite_Footnote_4] . Thus, this model is pLSA with each topic-word distribution represented as a Gaussian distribution over descrip-tive phrases in the embedding space. This model is also similar to the recently introduced Gaussian LDA model (Das et al., 2015), but without LDA\u2019s Dirichlet priors as discussed above."
  },
  {
    "id": 620,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://snap.stanford.edu/data,June",
    "section_title": "4 Human Subject Evaluations",
    "add_info": "Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data,June.",
    "text": "For Movies dataset, we used the Amazon review data set (Leskovec and Krevl, 2014)  . It has over 7.9 million reviews for 250,000 movies. We combined all the reviews for a movie, thus, generating a large review document per movie. This dataset is much noisier compared to WikiTravel due to presence of slang, incorrect grammar, sarcasm, etc. In addition, users also tend to compare and contrast while re-viewing movies so there are even references to other movies. As a result, the descriptive phrases extracted were much more noisy."
  },
  {
    "id": 621,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/jungokasai/graph_parser",
    "section_title": "3 Results and Discussion",
    "add_info": "3 Our code is available online for easy replication of our results at https://github.com/jungokasai/graph_parser.",
    "text": "We follow the protocol of Bangalore et al. (2009), Chung et al. (2016), Kasai et al. (2017), and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005). Following that work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set. The training, dev, and test sets comprise 39832, 1921, and 2415 sentences, respectively. We implement all of our models in TensorFlow (Abadi et al., 2016). [Cite_Footnote_3]"
  },
  {
    "id": 622,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.aclweb.org/anthology/P16-1101",
    "section_title": "2 Our Models 2.1 Supertagging Model 2.1.1 Input Representations",
    "add_info": "Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.",
    "text": "The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word. We initialize the word embeddings to be the pre-trained GloVe vectors (Pennington et al., 2014); for words not in GloVe, we initialize their embedding to a zero vec-tor. The other embeddings are randomly initial-ized. We obtain predicted POS tags from a BiL-STM POS tagger with the same configuration as in Ma and Hovy (2016)  ."
  },
  {
    "id": 623,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/timvdc/poetry",
    "section_title": "5 Conclusion",
    "add_info": null,
    "text": "In order to facilitate reproduction of the results and encourage further research, the poetry genera-tion system is made available as open source soft-ware. The current version can be downloaded at  https://github.com/timvdc/poetry."
  },
  {
    "id": 624,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://tdil-dc.in/index.php?lang=en",
    "section_title": "4 Experimental Setup",
    "add_info": "1 The corpus is available on request from http://tdil-dc.in/index.php?lang=en",
    "text": "Datasets: For training English-Hindi NMT sys-tems, we use the IITB English-Hindi parallel cor-pus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010) [Cite_Footnote_1] spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB par-allel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set."
  },
  {
    "id": 625,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md",
    "section_title": "4 Experimental Setup",
    "add_info": "2 https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md",
    "text": "Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the stan-dard encoder-attention-decoder architecture (Bah-danau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fast- Text embeddings (Grave et al., 2018) [Cite_Footnote_2] ."
  },
  {
    "id": 626,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/anoopkunchukuttan/cfilt_preorder",
    "section_title": "4 Experimental Setup",
    "add_info": "3 https://github.com/anoopkunchukuttan/cfilt_preorder",
    "text": "Pre-ordering: We use CFILT-preorder [Cite_Footnote_3] for pre-reordering English sentences. It contains two pre-ordering configurations: (1) generic rules (G) that apply to all Indian languages (Ramanathan et al., 2008), and (2) hindi-tuned rules (HT) which im-proves generic rules by incorporating improve-ments found through error analysis of English-Hindi reordering (Patel et al., 2013). The Hindi-tuned rules improve translation for other English to Indian language pairs too (Kunchukuttan et al., 2014)."
  },
  {
    "id": 627,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://research.microsoft.com/mct",
    "section_title": "5 Experiments",
    "add_info": "1 http://research.microsoft.com/mct",
    "text": "Datasets: We use two datasets for our evaluation. (1) First is the MCTest-500 dataset [Cite_Footnote_1] , a freely available set of 500 stories (split into 300 train, 50 dev and 150 test) and associated questions (Richardson et al., 2013). The stories are fictional so the answers can be found only in the story it-self. The stories and questions are carefully lim-ited, thereby minimizing the world knowledge re-quired for this task. Yet, the task is challenging for most modern NLP systems. Each story in MCTest has four multiple choice questions, each with four answer choices. Each question has only one cor-rect answer. Furthermore, questions are also anno-tated with \u2018single\u2019 and \u2018multiple\u2019 labels. The ques-tions annotated \u2018single\u2019 only require one sentence in the story to answer them. For \u2018multiple\u2019 ques-tions it should not be possible to find the answer to the question in any individual sentence of the passage. In a sense, the \u2018multiple\u2019 questions are harder than the \u2018single\u2019 questions as they typically require complex lexical analysis, some inference and some form of limited reasoning. Cucerzan-converted questions can also be downloaded from the MCTest website."
  },
  {
    "id": 628,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://research.facebook.com/researchers/1543934539189348",
    "section_title": "5 Experiments",
    "add_info": "2 https://research.facebook.com/researchers/1543934539189348 sider two evaluation metrics: accuracy (propor-",
    "text": "(2) The second dataset is a synthetic dataset released under the bAbI project [Cite_Footnote_2] (Weston et al., 2015). The dataset presents a set of 20 \u2018tasks\u2019, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner. For each \u2018task\u2019, 1000 questions are used for training and 1000 for testing. The \u2018tasks\u2019 refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg. relations, yes/no questions, counting ques-tions, etc. Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appear-ing in the text, etc. We write simple rules to con-vert the question and answer candidate pairs to hy-potheses. Baselines: We have five baselines. (1) The first three baselines are inspired from Richardson et al. (2013). The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text. (2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well. As far as we know, SW+D is the best previ-ously published result on this task. 4 (3) The third baseline (called RTE) uses textual entail-ment to answer MCTest questions. For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015). The base-line uses LSTMs (Hochreiter and Schmidhuber, 1997) to accomplish the task. LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) is taken from Iyyer et al. (2014). QANTA too uses a recursive neural net-work for question answering. Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments. First, we look at question classification. We use a simple question classi-fication based on the question word (what, why, what, etc.). We call this QClassification. Next, we also use a question/answer classification from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought. We call this QAClassification. Finally, we also learn a clas-sifier for the 20 tasks in the Machine Compre-hension gamut described in Weston et al. (2015). The classification algorithm (called TaskClassifi-cation) was built on the bAbI training set. It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the ques-tion and answer. The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting. In our ex-periments we will see that learning these strategies is better than learning the question answer classi-fication which is in turn better than learning the question classification."
  },
  {
    "id": 629,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://cs.umd.edu/miyyer/qblearn/",
    "section_title": "5 Experiments",
    "add_info": "6 http://cs.umd.edu/miyyer/qblearn/",
    "text": "(2) The second dataset is a synthetic dataset released under the bAbI project (Weston et al., 2015). The dataset presents a set of 20 \u2018tasks\u2019, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner. For each \u2018task\u2019, 1000 questions are used for training and 1000 for testing. The \u2018tasks\u2019 refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg. relations, yes/no questions, counting ques-tions, etc. Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appear-ing in the text, etc. We write simple rules to con-vert the question and answer candidate pairs to hy-potheses. Baselines: We have five baselines. (1) The first three baselines are inspired from Richardson et al. (2013). The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text. (2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well. As far as we know, SW+D is the best previ-ously published result on this task. 4 (3) The third baseline (called RTE) uses textual entail-ment to answer MCTest questions. For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015). The base-line uses LSTMs (Hochreiter and Schmidhuber, 1997) to accomplish the task. LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) [Cite_Footnote_6] is taken from Iyyer et al. (2014). QANTA too uses a recursive neural net-work for question answering. Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments. First, we look at question classification. We use a simple question classi-fication based on the question word (what, why, what, etc.). We call this QClassification. Next, we also use a question/answer classification from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought. We call this QAClassification. Finally, we also learn a clas-sifier for the 20 tasks in the Machine Compre-hension gamut described in Weston et al. (2015). The classification algorithm (called TaskClassifi-cation) was built on the bAbI training set. It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the ques-tion and answer. The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting. In our ex-periments we will see that learning these strategies is better than learning the question answer classi-fication which is in turn better than learning the question classification."
  },
  {
    "id": 630,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://cogcomp.cs.illinois.edu/Data/QA/QC/",
    "section_title": "5 Experiments",
    "add_info": "7 http://cogcomp.cs.illinois.edu/Data/QA/QC/",
    "text": "(2) The second dataset is a synthetic dataset released under the bAbI project (Weston et al., 2015). The dataset presents a set of 20 \u2018tasks\u2019, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner. For each \u2018task\u2019, 1000 questions are used for training and 1000 for testing. The \u2018tasks\u2019 refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg. relations, yes/no questions, counting ques-tions, etc. Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appear-ing in the text, etc. We write simple rules to con-vert the question and answer candidate pairs to hy-potheses. Baselines: We have five baselines. (1) The first three baselines are inspired from Richardson et al. (2013). The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text. (2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well. As far as we know, SW+D is the best previ-ously published result on this task. 4 (3) The third baseline (called RTE) uses textual entail-ment to answer MCTest questions. For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015). The base-line uses LSTMs (Hochreiter and Schmidhuber, 1997) to accomplish the task. LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) is taken from Iyyer et al. (2014). QANTA too uses a recursive neural net-work for question answering. Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments. First, we look at question classification. We use a simple question classi-fication based on the question word (what, why, what, etc.). We call this QClassification. Next, we also use a question/answer classification [Cite_Footnote_7] from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought. We call this QAClassification. Finally, we also learn a clas-sifier for the 20 tasks in the Machine Compre-hension gamut described in Weston et al. (2015). The classification algorithm (called TaskClassifi-cation) was built on the bAbI training set. It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the ques-tion and answer. The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting. In our ex-periments we will see that learning these strategies is better than learning the question answer classi-fication which is in turn better than learning the question classification."
  },
  {
    "id": 631,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum",
    "section_title": "A.1 Data",
    "add_info": "3 We take the processed Wikipedia articles from https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum released on April 25th 2018.",
    "text": "WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. [Cite_Footnote_3] We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018)."
  },
  {
    "id": 632,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://wiki.dbpedia.org/downloads-2016-10",
    "section_title": "A.1 Data",
    "add_info": "4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).",
    "text": "WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [Cite_Footnote_4] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018)."
  },
  {
    "id": 633,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://mappings.dbpedia.org/server/ontology/classes/",
    "section_title": "A.1 Data",
    "add_info": "4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).",
    "text": "WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [Cite_Footnote_4] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018)."
  },
  {
    "id": 634,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://barbar.cs.lth.se:8081",
    "section_title": "1 Introduction",
    "add_info": "2 http://barbar.cs.lth.se:8081",
    "text": "SRL4ORL. The semantic roles of the predicate fear (marked blue bold) correspond to the opin-ion roles H and T, according to MPQA. For this reason, the output of SRL systems has been com-monly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Ad-ditionally, a considerable amount of training data is available for training SRL models (Table [Cite_Footnote_2] in Sec. 3), which made neural SRL models success-ful (Zhou and Xu, 2015; Yang and Mitchell, 2017)."
  },
  {
    "id": 635,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/neulab/cmu-multinlp",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "The simple hypothesis behind our paper is: if humans can perform natural language analysis in a single unified format, then perhaps machines can as well. Fortunately, there already exist NLP mod-els that perform span prediction and prediction of relations between pairs of spans, such as the end-to-end coreference model of Lee et al. (2017). We extend this model with minor architectural mod-ifications (which are not our core contributions) and pre-trained contextualized representations (e.g., Guo et al. (2016) 7 3 7 Swayamdipta et al. (2018) 7 7 3 BERT; Devlin et al. (2019) 1 ) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference reso-lution (Coref.), open information extraction (Ope-nIE), part-of-speech tagging (POS), dependency parsing (Dep.), constituency parsing (Consti.), se-mantic role labeling (SRL), aspect based sentiment analysis (ABSA), and opinion role labeling (ORL). While previous work has used similar formalisms to understand the representations learned by pre-trained embeddings (Tenney et al., 2019a,b), to the best of our knowledge this is the first work that uses such a unified model to actually perform analysis. Moreover, we demonstrate that despite the model\u2019s simplicity, it can achieve comparable performance with special-purpose state-of-the-art models on the tasks above (Table 1). We also demonstrate that this framework allows us to easily perform multi-task learning (MTL), leading to improvements when there are related tasks to be learned from or data is sparse. Further analysis shows that dissimilar tasks exhibit divergent attention patterns, which explains why MTL is harmful on certain tasks. We have released our code and the General Language Analysis Datasets (GLAD) benchmark with 8 datasets covering 10 tasks in the BRAT format at  https://github.com/neulab/cmu-multinlp , and provide a leaderboard to facilitate future work on generalized models for NLP."
  },
  {
    "id": 636,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.iesl.cs.umass.edu/data/bibtex",
    "section_title": "4 Experiments: Author Coreference",
    "add_info": "3 http://www.iesl.cs.umass.edu/data/bibtex",
    "text": "For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries. [Cite_Footnote_3] We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset labeled for coref-erence, in order to assess accuracy. We also include \u223c5 million mentions from DBLP."
  },
  {
    "id": 637,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html",
    "section_title": "4 Experiments: Author Coreference",
    "add_info": "4 http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html",
    "text": "For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries. We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset [Cite_Footnote_4] labeled for coref-erence, in order to assess accuracy. We also include \u223c5 million mentions from DBLP."
  },
  {
    "id": 638,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Lambda-3/underDiscourseSimplification",
    "section_title": "3 Recursive Sentence Splitting",
    "add_info": "1 The source code of our framework is avail-able https://github.com/Lambda-3/underDiscourseSimplification.",
    "text": "We present D IS S IM , a recursive sentence splitting approach that creates a semantic hierarchy of sim-plified sentences. [Cite_Footnote_1] The goal of our approach is to generate an intermediate representation that presents a simple and more regular structure which is easier to process for downstream se-mantic applications and may support a faster generalization in ML tasks. For this purpose, we cover a wider range of syntactic constructs (10 in total) than state-of-the-art rule-based syn-tactic frameworks. In particular, our approach is not limited to breaking up clausal components, but also splits and rephrases a variety of phrasal elements, resulting in a much more fine-grained output where each proposition represents a mini-mal semantic unit that is typically composed of a simple subject-predicate-object structure. Though tackling a larger set of linguistic constructs, our framework operates on a much smaller set of only 35 manually defined rules as compared to existing syntax-driven rule-based approaches."
  },
  {
    "id": 639,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material",
    "section_title": "3 Recursive Sentence Splitting 3.1 Transformation Stage",
    "add_info": "2 For reproducibility purposes, the complete set of trans-formation patterns is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.",
    "text": "The transformation patterns are based on syn-tactic and lexical features that can be derived from a sentence\u2019s phrase structure. They were heuris-tically determined in a rule engineering process whose main goal was to provide a best-effort set of patterns, targeting the challenge of being applied in a recursive fashion and to overcome biased or incorrectly structured parse trees. We empirically determined a fixed execution order of the rules by examining which sequence achieved the best sim-plification results in a manual qualitative analysis conducted on a development test set of 100 ran-domly sampled Wikipedia sentences. The gram-mar rules are applied recursively in a top-down fashion on the source sentence, until no more sim-plification pattern matches. In that way, the in-put is turned into a discourse tree, consisting of a set of hierarchically ordered and semantically interconnected sentences that present a simpli-fied syntax. Table 2 displays some examples of our transformation patterns, [Cite_Footnote_2] which are specified in terms of Tregex patterns."
  },
  {
    "id": 640,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/senisioi/NeuralTextSimplification",
    "section_title": "4 Experimental Setup",
    "add_info": "6 For the computation of the BLEU and SARI scores we used the implementation of Nisioi et al. (2017) which is available under https://github.com/senisioi/NeuralTextSimplification.",
    "text": "Automatic Evaluation. The automatic metrics that were calculated in the evaluation procedure comprise a number of basic statistics, including (i) the average sentence length of the simplified sentences in terms of the average number of to-kens per output sentence (#T/S); (ii) the average number of simplified output sentences per com-plex input (#S/C); (iii) the percentage of sentences that are copied from the source without perform-ing any simplification operation (%SAME), serv-ing as an indicator for system conservatism; and (iv) the averaged Levenshtein distance from the in-put (LD SC ), which provides further evidence for a system\u2019s conservatism. Furthermore, in accor-dance with prior work on TS, we report average BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016) scores for the rephrasings of each system. [Cite_Footnote_6] Finally, we computed the SAMSA and SAMSA abl score of each system, which are the first met-rics that explicitly target syntactic aspects of TS (Sulem et al., 2018b)."
  },
  {
    "id": 641,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material",
    "section_title": "References",
    "add_info": "10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.",
    "text": "Tables 9 and 10 show the results of the recall-based qualitative analysis of the transformation patterns, together with the findings of the error analysis. These analyses were carried out on a dataset which we compiled. [Cite_Footnote_10] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules."
  },
  {
    "id": 642,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://files.pushshift.io/reddit/comments/",
    "section_title": "3 Dataset of Derivatives",
    "add_info": "3 We draw upon the entire Baumgartner Reddit Corpus, a collection of all public Reddit posts available at https://files.pushshift.io/reddit/comments/.",
    "text": "We base our study on a new dataset of derivatives in context similar in form to the one released by Vylomova et al. (2017), i.e., it is based on sen-tences with a derivative (e.g., this jacket is unwearable .) that are altered by masking the derivative (this jacket is .). Each item in the dataset consists of (i) the altered sen-tence, (ii) the derivative (unwearable) and (iii) the base (wear). The task is to generate the cor-rect derivative given the altered sentence and the base. We use sentential contexts rather than tags to represent derivational meanings because they better reflect the semantic variability inherent in derivational morphology (Section 2). While Vylo-mova et al. (2017) use Wikipedia, we extract the dataset from Reddit. [Cite_Footnote_3] Since productively formed derivatives are not part of the language norm ini-tially (Bauer, 2001), social media is a particularly fertile ground for our study."
  },
  {
    "id": 643,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/ivri/dmorph",
    "section_title": "4 Experiments 4.3 Models",
    "add_info": "8 The dataset is available at https://github.com/ivri/dmorph. While Vylomova et al. (2017) take morpho-orthographic changes into account, we only predict affixes, not the accompanying changes in orthography (Section 4.1).",
    "text": "In order to provide a strict comparison to Vy-lomova et al. (2017), we also evaluate our LSTM and best BERT-based model on the suffix dataset released by Vylomova et al. (2017) against the reported performance of their encoder-decoder model. [Cite_Footnote_8] Notice Vylomova et al. (2017) show that providing the LSTM with the POS of the deriva-tive increases performance. Here, we focus on the more general case where the POS is not known and hence do not consider this setting."
  },
  {
    "id": 644,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "http://en.wikipedia.org/",
    "section_title": "1 Introduction",
    "add_info": "2 http://en.wikipedia.org/",
    "text": "To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia [Cite_Footnote_2] , or DBpedia. Only facts that are as-serted in and gleaned from the source documents are posited."
  },
  {
    "id": 645,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "http://www.dbpedia.org/",
    "section_title": "1 Introduction",
    "add_info": "3 http://www.dbpedia.org/",
    "text": "To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia , or DBpedia. [Cite_Footnote_3] Only facts that are as-serted in and gleaned from the source documents are posited."
  },
  {
    "id": 646,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://nlp.stanford.edu/software",
    "section_title": "1 Introduction",
    "add_info": "2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp",
    "text": "The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language pro-cessing components, including part-of-speech tag-gers, chunkers, and parsers. There remain many differences in how these components are built, re-sulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differ-ently-trained linguistic component, such as tokeni-zation. The community recognizes this difficulty, and shared task organizers are now providing ac-companying parses and other analyses of the shared task data. For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components."
  },
  {
    "id": 647,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp",
    "section_title": "1 Introduction",
    "add_info": "2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp",
    "text": "The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language pro-cessing components, including part-of-speech tag-gers, chunkers, and parsers. There remain many differences in how these components are built, re-sulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differ-ently-trained linguistic component, such as tokeni-zation. The community recognizes this difficulty, and shared task organizers are now providing ac-companying parses and other analyses of the shared task data. For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components."
  },
  {
    "id": 648,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://incubator.apache.org/opennlp",
    "section_title": "1 Introduction",
    "add_info": "2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp",
    "text": "The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language pro-cessing components, including part-of-speech tag-gers, chunkers, and parsers. There remain many differences in how these components are built, re-sulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differ-ently-trained linguistic component, such as tokeni-zation. The community recognizes this difficulty, and shared task organizers are now providing ac-companying parses and other analyses of the shared task data. For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components."
  },
  {
    "id": 649,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT",
    "section_title": "1 Introduction",
    "add_info": "1 The source code can be found in this repos-itory: https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT",
    "text": "Empirical results on popular benchmarks show that exact-edit consistently outperforms the state-of-the-art unsupervised NMT system (Lample et al., 2018b) with back-translation across four dif-ferent languages pairs. In summary, our main con-tributions are three-fold [Cite_Footnote_1] :"
  },
  {
    "id": 650,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/facebookresearch/faiss",
    "section_title": "4 Experiments 4.2 Implementation Details",
    "add_info": "2 https://github.com/facebookresearch/faiss",
    "text": "Model Structure In this work, the NMT mod-els can be built upon long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) cells. For LSTM cells, both the encoder and decoder have 3 layers. As for Transformer, we use 4 lay-ers both in the encoder and the decoder. As for both LSTM and Transformer, all encoder param-eters are shared across two languages. Similarly, we share all decoder parameters across two lan-guages. Both two model structure are optimized using Adam (Kingma and Ba, 2014) with a batch size of 32. The rate for LSTM cell is 0.0003 while Transformer\u2019s is set as 0.0001. The weights in Equation 7 are \u03c9 lm = \u03c9 ext = 1. The \u03bb for cal-culating ranking scores is 0.5. As for the evalu-ation network R, we use a multilayer perceptron with two hidden layers of size 512. For efficient nearest neighbor search in the extracting step, we use the open-source Faiss library (Johnson et al., 2017) [Cite_Footnote_2] . We calculate the similarity of sentences in each episode instead of each batch for computa-tional efficiency. At decoding time, sentences are generated using greedy decoding."
  },
  {
    "id": 651,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/vid-koci/bert-commonsense",
    "section_title": "1 Introduction",
    "add_info": "1 The code can be found at https://github.com/vid-koci/bert-commonsense. The dataset and the models can be obtained from https://ora.ox.ac.uk/objects/uuid: c83e94bb-7584-41a1-aef9-85b0e764d9e3",
    "text": "In this work, we address the lack of large train-ing sets for pronoun disambiguation by introduc-ing a large dataset that can be easily extended. To generate this dataset, we find passages of text where a personal name appears at least twice and mask one of its non-first occurrences. To make the disambiguation task more challenging, we also ensure that at least one other distinct personal name is present in the text in a position before the masked occurrence. We instantiate our method on English Wikipedia and generate the Wikipedia Co-REferences Masked (W IKI CREM) dataset with 2.4M examples, which we make publicly available for further usage [Cite_Footnote_1] . We show its value by using it to fine-tune the B ERT language model (Devlin et al., 2018) for pronoun resolution."
  },
  {
    "id": 652,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://dumps.wikimedia.org/enwiki/",
    "section_title": "3 The W IKI CREM Dataset",
    "add_info": "2 https://dumps.wikimedia.org/enwiki/ dump id: enwiki-20181201",
    "text": "In this section, we describe how we obtained W I - KI CREM. Starting from English Wikipedia [Cite_Footnote_2] , we search for sentences and pairs of sentences with the following properties: at least two distinct per-sonal names appear in the text, and one of them is repeated. We do not use pieces of text with more than two sentences to collect concise examples only. Personal names in the text are called \u201ccan-didates\u201d. One non-first occurrence of the repeated candidate is masked, and the goal is to predict the masked name, given the correct and one incorrect candidate. In case of more than one incorrect can-didate in the sentence, several datapoints are con-structed, one for each incorrect candidate."
  },
  {
    "id": 653,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://spacy.io/usage/linguistic-features#named-entities",
    "section_title": "3 The W IKI CREM Dataset",
    "add_info": "3 https://spacy.io/usage/linguistic-features#named-entities",
    "text": "We used the Spacy Named Entity Recognition library [Cite_Footnote_3] to find the occurrences of names in the text. The resulting dataset consists of 2, 438, 897 samples. 10, 000 examples are held out to serve as the validation set. Two examples from our dataset can be found on Figure 1."
  },
  {
    "id": 654,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://pypi.org/project/gender-guesser/",
    "section_title": "3 The W IKI CREM Dataset",
    "add_info": "4 https://pypi.org/project/gender-guesser/",
    "text": "W IKI CREM statistics. We analyze our dataset for gender bias. We use the Gender guesser li-brary [Cite_Footnote_4] to determine the gender of the candidates. To mimic the analysis of pronoun genders per-formed in the related works (Webster et al., 2018; Rudinger et al., 2018; Zhao et al., 2018), we ob-serve the gender of the correct candidates only. There were 0.8M \u201cmale\u201d or \u201cmostly male\u201d names and 0.42M \u201cfemale\u201d or \u201cmostly female\u201d names, the rest were classified as \u201cunknown\u201d. The ratio between female and male candidates is thus esti-mated around 0.53 in favour of male candidates. We will see that this gender imbalance does not have any negative impact on bias, as shown in Sec-tion 6.2."
  },
  {
    "id": 655,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "4 Model 4.1 B ERT",
    "add_info": "5 https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "In this work, we only focus on the masked token prediction. We use the PyTorch implementation of B ERT [Cite_Footnote_5] and the pre-trained weights for B ERT -large released by Devlin et al. (2018)."
  },
  {
    "id": 656,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/AIPHES/ACL20-Reference-Free-MT-Evaluation",
    "section_title": "References",
    "add_info": "1 https://github.com/AIPHES/ACL20-Reference-Free-MT-Evaluation",
    "text": "Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual simi-larity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system trans-lations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systemati-cally investigate a range of metrics based on state-of-the-art cross-lingual semantic repre-sentations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limita-tions, namely, (a) a semantic mismatch be-tween representations of mutual translations and, more prominently, (b) the inability to punish \u201ctranslationese\u201d, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 cor-relation points. We make our MT evaluation code available. [Cite_Footnote_1]"
  },
  {
    "id": 657,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/AIPHES/",
    "section_title": "6 Conclusion",
    "add_info": null,
    "text": "We release our metrics under the name XMover-Score publicly:  https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation ."
  },
  {
    "id": 658,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://github.com/tuvuumass/task-transferability",
    "section_title": "1 Introduction",
    "add_info": "2 Library and code available at http://github.com/tuvuumass/task-transferability.",
    "text": "We publicly release our task library, which con-sists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability. [Cite_Footnote_2]"
  },
  {
    "id": 659,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs",
    "section_title": "References Christopher Clark, Kenton Lee, Ming-Wei Chang,",
    "add_info": "20 https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs",
    "text": "Treebank (POS-PTB; Marcus et al., 1993) and B; Cer et al., 2017) and Quora Question the Universal Dependencies English Web Tree-Pairs [Cite_Footnote_20] (QQP); natural language inference (NLI) bank (POS-EWT; Silveira et al., 2014); syntactic with Multi-Genre NLI (MNLI; Williams et al., constituency ancestor tagging, i.e., predicting the 2018), SQuAD (Rajpurkar et al., 2016) con-constituent label of the parent (Parent), grandpar-verted into Question-answering NLI (QNLI; Wang ent (GParent), and great-grandparent (GGParent) et al., 2019b), Recognizing Textual Entailment of each word in the PTB phrase-structure tree; 1,2,3,5 (RTE; Dagan et al., 2005, et seq.), and the semantic tagging task (ST; Bjerva et al., 2016; Winograd Schema Challenge (Levesque, 2011) re-"
  },
  {
    "id": 660,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/alontalmor/MultiQA",
    "section_title": "References Christopher Clark, Kenton Lee, Ming-Wei Chang,",
    "add_info": "21 https://github.com/alontalmor/MultiQA",
    "text": "2016); and conjunct identification, i.e., identify-ing the tokens that comprise the conjuncts in a Question answering (eleven tasks): We use coordination construction, with the coordination eleven QA datasets from the MultiQA (Tal-annotated PTB dataset (Conj; Ficler and Goldberg, mor and Berant, 2019a) repository [Cite_Footnote_21] , includ-"
  },
  {
    "id": 661,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.lemurproject.org/clueweb12/",
    "section_title": "1 Introduction",
    "add_info": "3 http://www.lemurproject.org/clueweb12/",
    "text": "One particularly interesting source of unstruc-tured text data is CQA websites (e.g. Yahoo! An-swers, 1 Answers.com, 2 etc.), which became very popular resources for question answering. The in-formation expressed there can be very useful, for example, to answer future questions (Shtok et al., 2012), which makes it attractive for knowledge base population. Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb [Cite_Footnote_3] about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents). There are certain limitations in applying existing re-lation extraction algorithms to CQA data, i.e., they typically consider sentences independently and ig-nore the discourse of QnA pair text. However, of-ten it is impossible to understand the answer without knowing the question. For example, in many cases users simply give the answer to the question with-out stating it in a narrative sentence (e.g. \u201cWhat does \u201dxoxo\u201d stand for? Hugs and kisses.\u201c), in some other cases the answer contains a statement, but some im-portant information is omitted (e.g. \u201cWhat\u2019s the cap-ital city of Bolivia? Sucre is the legal capital, though the government sits in La Paz\u201c)."
  },
  {
    "id": 662,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://webscope.sandbox.yahoo.com/catalog.php?datatype=l",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "4 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l",
    "text": "For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers [Cite_Footnote_4] and a crawl of WikiAnswers (Fader et al., 2014). The Yahoo! Answers dataset contains 4,483,032 questions (3,894,644 in English) with the corresponding answers collected on 10/25/2007. The crawl of WikiAnswers has 30,370,994 question clusters, tagged by WikiAn-swers users as paraphrases, and only 3,386,256 them have answers. From these clusters we used all possi-ble pairs of questions and answers (19,629,443 pairs in total)."
  },
  {
    "id": 663,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://wiki.answers.com",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "5 http://wiki.answers.com",
    "text": "For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers and a crawl of WikiAnswers [Cite_Footnote_5] (Fader et al., 2014). The Yahoo! Answers dataset contains 4,483,032 questions (3,894,644 in English) with the corresponding answers collected on 10/25/2007. The crawl of WikiAnswers has 30,370,994 question clusters, tagged by WikiAn-swers users as paraphrases, and only 3,386,256 them have answers. From these clusters we used all possi-ble pairs of questions and answers (19,629,443 pairs in total)."
  },
  {
    "id": 664,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf",
    "section_title": "3 Experiments 3.1 Data Preprocessing",
    "add_info": "Anoop Kunchukuttan. 2020. The IndicNLP Library. https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf.",
    "text": "For transliterating the Urdu and Konkani to a com-mon script, we used the Indic Trans library (Bhat et al., 2014), and for the others, we used Indic NLP library (Kunchukuttan, 2020)  (as Urdu and Konkani not supported). In addition, there is an exception with Urdu because it follows a right to left writing system and all other Indian languages follow left to right writing order. Hence, in the pro-cessing step, we also changed the order of Urdu to maintain consistency among all languages, and do-ing this also made our string similarity algorithms work more efficiently."
  },
  {
    "id": 665,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Compare",
    "url": "https://github.com/airsplay/R2R-EnvDrop",
    "section_title": "5 Results and Analysis 5.1 Room-to-Room Dataset",
    "add_info": "3 The baseline is the re-implementation of their model without back translation based on code: https://github.com/airsplay/R2R-EnvDrop",
    "text": "We compare our agent with the baseline agent (Tan et al., 2019) [Cite_Footnote_3] on the R2R test leaderboard. Our syntax-aware agent achieves 47.8% in success rate and 0.45 in SPL, improving the baseline model by 2.1% in success rate and 2% in SPL."
  },
  {
    "id": 666,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://github.com/thunlp/OpenKE",
    "section_title": "4 Implementations",
    "add_info": "1 http://github.com/thunlp/OpenKE",
    "text": "In this section, we mainly present the implemen-tations of acceleration modules and special sam-pling algorithm in OpenKE. OpenKE has been available to the public on GitHub [Cite_Footnote_1] and is open-source under the MIT license."
  },
  {
    "id": 667,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/thunlp/Fast-TransX",
    "section_title": "4 Implementations 4.2 Parallel Learning",
    "add_info": "2 https://github.com/thunlp/Fast-TransX",
    "text": "Abundant computing resources (e.g Servers with multiple GPUs) do not exist all the time. In fact, we often rely on simple personal computers for model validation. Hence, we enable OpenKE to adapt models for parallel learning on CPU [Cite_Footnote_2] be-sides employing GPU learning, which allow users to make full use of all available computing re-sources. The parallel learning method is shown in Algorithm 1. The main idea of parallel learning method is based on data parallelism mechanism, which divides training triples into several parts and trains each part of triples with a corresponding thread. In parallel learning, there are two strate-gies implemented to update gradients. One of the methods is the lock-free strategy, which means all threads share the unified embedding space and update embeddings directly without synchronized operations. We also implement a central syn-chronized method, where each thread calculates its own gradient and results will be updated after summing up the gradients from all threads."
  },
  {
    "id": 668,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://everest.hds.utc.fr/doku.php?id=en:transe",
    "section_title": "5 Evaluations",
    "add_info": "3 https://everest.hds.utc.fr/doku.php?id=en:transe",
    "text": "Some datasets are usually used as benchmarks for link prediction, such as FB15K and WN18. FB15K is the relatively dense subgraph of Free-base; WN18 is the subset of WordNet. These pub-lic datasets are available online [Cite_Footnote_3] . Following pre-vious works, We adopt them in our experiments. The statistics of FB15K and WN18 are listed in Table 2, including the number of entities, rela-tions, and facts."
  },
  {
    "id": 669,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/thunlp/KB2E",
    "section_title": "5 Evaluations",
    "add_info": "4 https://github.com/thunlp/KB2E",
    "text": "As mentioned above, OpenKE supports mod-els with efficient learning on both CPU and GPU. For CPU, the benchmarks are run on an Intel(R) Core(TM) i7-6700K @ 3.70GHz, with 4 cores and 8 threads. For GPU, the models in both TensorFlow and PyTorch versions are trained by GeForce GTX 1070 (Pascal), with CUDA v.8.0 (driver 384.111) and cuDNN v.6.5. To compare with the previous works, we simply follow the pa-rameter settings used before and traverse all train-ing triples for 1000 rounds. Other detailed pa-rameters and training strategies are shown in our source code. We show these results in Table 3 and Table 4. In these tables, the difference between our implementations and the paper reported results are listed in the parentheses. To demonstrate the efficiency of OpenKE, we select TransE as a rep-resentative and implement it with both OpenKE and KB2E [Cite_Footnote_4] , and then compare their training time. KB2E is a widely-used toolkit for KE models on GitHub. These results can be found in Table 5."
  },
  {
    "id": 670,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cs.stanford.edu/\u223cpliang/software/",
    "section_title": "7 Results",
    "add_info": "13 In our experiments, we employ Liang\u2019s implementation http://cs.stanford.edu/\u223cpliang/software/. The number of clus-ters is set to 30.",
    "text": "Application to Automatically Induced POS Tags A potential benefit of the proposed method is to re-late automatically induced clusters in the target lan-guage to universal tags. In our experiments, we in-duce such clusters using Brown clustering, [Cite_Footnote_13] which has been successfully used for similar purposes in parsing research (Koo et al., 2008). We then map these clusters to the universal tags using our algo-rithm."
  },
  {
    "id": 671,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://alchemy.cs.washington.edu/",
    "section_title": "3 Markov Logic",
    "add_info": "Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.",
    "text": "The open-source Alchemy package (Kok et al., 2007)  provides implementations of existing algo-rithms for Markov logic. In Section 5, we develop the first general-purpose unsupervised learning al-gorithm for Markov logic by extending the existing algorithms to handle hidden predicates."
  },
  {
    "id": 672,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/heartexlabs/label-studio",
    "section_title": "6 Results and Discussion 6.3.2 Automated Conversion",
    "add_info": "Maxim Tkachenko, Mikhail Malyuk, Nikita Shevchenko, Andrey Holmanyuk, and Nikolai Liubimov. 2020. Label Studio: Data labeling software. Open source software available from https://github.com/heartexlabs/label-studio.",
    "text": "SizeOf(Vocab j ) mechanism itself becomes a confounding factor, enabling the RACE converted model to perform bet-ter on the MCRC task. To assess this nuance, we manually annotate a subset of the MCRC datasets using Label Studio (Tkachenko et al., 2020)  , with a random set of examples annotated by each of the authors. To create a setting where the differ-ence is vivid, we design the annotation subsets such that the RACE converted model gives an accuracy of around 50% using the hybrid conversion strategy. The independent manual annotations prevent any exploitable signal from leaking into the training data of the model through the conversion mech-anism. We compare the performance of models trained on converted forms of the RACE dataset using both our hybrid strategy as well as manual annotation. \u2020\u2020 We manually annotate 100 examples from MultiRC and 50 each from ComsosQA and DREAM. MultiRC is evaluated at an option-level with each question-answer pair considered an in-dividual example. On the other hand, CosmosQA and DREAM are evaluated at a question-level, with each example consisting of three question-answer pairs, and one label corresponding to the correct answer option."
  },
  {
    "id": 673,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html",
    "section_title": "C Reproducibility Checklist C.2 Neural Conversion",
    "add_info": null,
    "text": "\u2022 The threshold for CFCS as classifica-tion experiments (Section 6.2 (1)) we calculated by tuning for best balanced accuaracy  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html."
  },
  {
    "id": 674,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Mayer123/CS_Model_Adaptation",
    "section_title": "3 Experimental Setup 3.2 Strategies",
    "add_info": "1 Our code is available at https://github.com/Mayer123/CS_Model_Adaptation wards human-like linguistic generalization? In Pro-ceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5210\u20135217, On-line. Association for Computational Linguistics.",
    "text": "We describe how we adapt pre-trained GPT-2 and BART models to a target task with three methods [Cite_Footnote_1] : (S1) Fine-tuning is the classic model adaptation ap-proach, where all its parameters are updated using the training signal from the ground truth. (S2) Prefix-tuning (Li and Liang, 2021) is a method which fixes the pre-trained model\u2019s parameters dur-ing adaptation. This method adds trainable param-eters, called prefix states, to the self-attention com-ponent (Vaswani et al., 2017) of every transformer layer in the model; only these prefix states are up-dated during training. Essentially, the prefix states act as conditioning variables that contextualize the representation of the inputs, such that the model can generate the desired outputs."
  },
  {
    "id": 675,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/segmenter.shtml",
    "section_title": "4 Experiments 4.1 Experimental Settings",
    "add_info": "7 http://nlp.stanford.edu/software/segmenter.shtml",
    "text": "The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) [Cite_Footnote_7] ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out."
  },
  {
    "id": 676,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.phontron.com/kytea/",
    "section_title": "4 Experiments 4.1 Experimental Settings",
    "add_info": "8 http://www.phontron.com/kytea/",
    "text": "The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) 7 ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea [Cite_Footnote_8] ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out."
  },
  {
    "id": 677,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://www2.nict.go.jp/univ-com/multitrans/cicada/",
    "section_title": "4 Experiments 4.1 Experimental Settings",
    "add_info": "9 http://www2.nict.go.jp/univ-com/multitrans/cicada/",
    "text": "The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watan-abe, 2012; Tamura et al., 2013) [Cite_Footnote_9] . We named the implemented aligner AGRIPPA, to support our in-house decoders OCTAVIAN and AUGUSTUS."
  },
  {
    "id": 678,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "http://www.isi.edu/\u02dcavaswani/giza-pp-l0.html",
    "section_title": "4 Experiments 4.5 Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods",
    "add_info": "11 http://www.isi.edu/\u02dcavaswani/giza-pp-l0.html",
    "text": "The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) [Cite_Footnote_11] and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem. This enables the probability distributions on rare words to be estimated more effectively. In this way, these two GIZA++ variants are related to the proposed method. smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES. For the settings of l 0 -normalized GIZA++ that are not in common with GIZA++ were the default settings. As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1 \u2013 4 and HMM model were turned on."
  },
  {
    "id": 679,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/hznlp/giza-kn",
    "section_title": "4 Experiments 4.5 Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods",
    "add_info": "12 https://github.com/hznlp/giza-kn",
    "text": "The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) [Cite_Footnote_12] . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem. This enables the probability distributions on rare words to be estimated more effectively. In this way, these two GIZA++ variants are related to the proposed method. smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES. For the settings of l 0 -normalized GIZA++ that are not in common with GIZA++ were the default settings. As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1 \u2013 4 and HMM model were turned on."
  },
  {
    "id": 680,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/ryanzhumich/editsql",
    "section_title": "4 Experiments 4.1 Baseline Results",
    "add_info": "3 https://github.com/ryanzhumich/editsql",
    "text": "We first evaluate KaggleDBQA using models that were developed for the Spider dataset. EditSQL (Zhang et al., 2019): EditSQL (with BERT) is the highest-performing model on the Spi-der dataset that also provides an open-source im-plementation along with a downloadable trained model. [Cite_Footnote_3] The model was built for edit-based multi-turn parsing tasks, but can also be used as a single-turn parser for Spider or KaggleDBQA. It employs a sequence-to-sequence model with a question-table co-attention encoder for schema encoding."
  },
  {
    "id": 681,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://aka.ms/KaggleDBQA",
    "section_title": "5 Conclusion & Future Work",
    "add_info": null,
    "text": "KaggleDBQA provides two resources to facili-tate real-world applications of text-to-SQL pars-ing. First, it encourages an evaluation regime that bridges the gap between academic and indus-trial settings, leveraging in-domain knowledge and more realistic database distribution. We encour-age adopting this regime for established text-to- SQL benchmarks. Second, it is a new dataset of more realistic databases and questions, present-ing a challenge to state-of-the-art parsers. De-spite the addition of domain knowledge in the form of database documentation, our baselines reach only 26.77% accuracy, struggling to generalize to harder questions. We hope that better use of docu-mentation and new modeling and domain adapta-tion techniques will help further advance state of the art. The KaggleDBQA dataset is available at  https://aka.ms/KaggleDBQA."
  },
  {
    "id": 682,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip",
    "section_title": "A.3 Implementation Details",
    "add_info": "6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip",
    "text": "For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [Cite_Footnote_6] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting. We keep the learning rate of non-BERT parameters the same at 7.44e-4. We also increase the dropout rate of the transformers from 0.1 to 0.3 to provide further regularization."
  },
  {
    "id": 683,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.stanford.edu/data/glove.twitter.27B.zip",
    "section_title": "3 Methods 3.2 Model",
    "add_info": "5 http://nlp.stanford.edu/data/glove.twitter.27B.zip",
    "text": "Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter [Cite_Footnote_5] for each token of ex-tracted discourse arguments from messages. For the distributional representation of discourse ar-guments, we run a Word-level LSTM on the words\u2019 embeddings within each discourse argu-ment and concatenate\u2212\u2192last hidden state vectors of\u2190\u2212forward LSTM (h) and backward LSTM ( h ) which\u2212\u2192is\u2190\u2212suggested by (Ji and Smith, 2017) (DA = [ h ; h ]). Then, we feed the sequence of the vector representation of discourse arguments to the Discourse-argument-level LSTM (DA-level LSTM) to make a final prediction with log soft-max function. With this structure, the model can learn the representation of interaction of tokens inside each discourse argument, then capture dis-course relations across all of the discourse argu-ments in each message (Figure 2). In order to prevent the overfitting, we added a dropout layer between the Word-level LSTM and the DA-level LSTM layer."
  },
  {
    "id": 684,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.sle.sharp.co.uk/senseval2/",
    "section_title": "2 Objective Functions: Naive-Bayes",
    "add_info": "2 http://www.sle.sharp.co.uk/senseval2/",
    "text": "We performed the following WSD experiments with Naive-Bayes models. We took as data the col-lection of S ENSEVAL -2 English lexical sample WSD corpora. [Cite_Footnote_2] We set the NB model parameters in several ways. We optimized JL (using the RFE s). We also optimized SCL and (the log of) CL , using a conju-gate gradient ( CG ) method (Press et al., 1988). For CL and SCL , we optimized each objective both over the space of all distributions and over the subspace of non-deficient models (giving CL \u2217 and SCL \u2217 ). Acc was not directly optimized."
  },
  {
    "id": 685,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/rekriz11/sockeye-recipes",
    "section_title": "References",
    "add_info": "1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes.",
    "text": "Sentence simplification is the task of rewriting texts so they are easier to understand. Recent research has applied sequence-to-sequence (Seq2Seq) models to this task, focusing largely on training-time improvements via reinforce-ment learning and memory augmentation. One of the main problems with applying generic Seq2Seq models for simplification is that these models tend to copy directly from the origi-nal sentence, resulting in outputs that are rel-atively long and complex. We aim to alle-viate this issue through the use of two main techniques. First, we incorporate content word complexities, as predicted with a leveled word complexity model, into our loss function dur-ing training. Second, we generate a large set of diverse candidate simplifications at test time, and rerank these to promote fluency, adequacy, and simplicity. Here, we measure simplicity through a novel sentence complexity model. These extensions allow our models to per-form competitively with state-of-the-art sys-tems while generating simpler sentences. We report standard automatic and human evalua-tion metrics. [Cite_Footnote_1]"
  },
  {
    "id": 686,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://newsela.com/data/",
    "section_title": "3 Seq2Seq Approach 3.1 Complexity-Weighted Loss Function 3.1.1 Word Complexity Prediction",
    "add_info": "3 Newsela is an education company that provides reading materials for students in elementary through high school. The Newsela corpus can be requested at https://newsela.com/data/",
    "text": "Extending the complex word identification model of Kriz et al. (2018), we train a linear regression model using length, number of syllables, and word frequency; we also include Word2Vec embeddings (Mikolov et al., 2013). To collect data for this task, we consider the Newsela corpus, a collection of 1,840 news articles written by professional edi-tors at 5 reading levels (Xu et al., 2015). 3 We ex-tract word counts in each of the five levels; in this dataset, we denote 4 as the original complex doc-ument, [Cite_Footnote_3] as the least simplified re-write, and 0 as the most simplified re-write. We propose using Al-gorithm 1 to obtain the complexity label for each word w, where l w represents the level given to the word, and c w i represents the number of times that word occurs in level i."
  },
  {
    "id": 687,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cs.cmu.edu/",
    "section_title": "4 Speaker Annotated TED Talks Dataset",
    "add_info": null,
    "text": "This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks. The data it-self can be found at  http://www.cs.cmu.edu/ \u223c pmichel1/sated/."
  },
  {
    "id": 688,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1",
    "section_title": "5 Experiments 5.3 Further experiments on the Europarl corpus",
    "add_info": "6 available here: https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1",
    "text": "One of the quirks of the TED talks is that the speaker annotation correlates with the topic of their talk to a high degree. Although the topics that a speaker talks about can be considered as a man-ifestation of speaker traits, we also perform a con-trol experiment on a different dataset to verify that our model is indeed learning more than just topical information. Specifically, we train our models on a speaker annotated version of the Europarl corpus (Rabinovich et al., 2017), on the en-de language pair [Cite_Footnote_6] ."
  },
  {
    "id": 689,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.ee.columbia.edu/dvmm/newDownloads.htm",
    "section_title": "1 Introduction",
    "add_info": "1 Dataset can be found at http://www.ee.columbia.edu/dvmm/newDownloads.htm",
    "text": "In this paper, we propose to jointly incorporate features from both speech (textual) and video (vi-sual) channels for the first time. We also build a newscast crawling system that can automatically accumulate video records and transcribe closed captions. With the crawler, we created a bench-mark dataset which is fully annotated with cross-document coreferential events [Cite_Footnote_1] ."
  },
  {
    "id": 690,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.statmt.org/moses/",
    "section_title": "6 Related work",
    "add_info": "2 http://www.statmt.org/moses/",
    "text": "Among early statistical methods for AMR-to-text generation, Flanigan et al. (2016b) convert input graphs to trees by splitting re-entrances, and then translate the trees into sentences with a tree-to-string transducer. Song et al. (2017) use a syn-chronous node replacement grammar to parse in-put AMRs and generate sentences at the same time. Pourdamghani et al. (2016) linearize input graphs by breadth-first traversal, and then use a phrase-based machine translation system [Cite_Footnote_2] to gen-erate results by translating linearized sequences."
  },
  {
    "id": 691,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://paraphrase.org",
    "section_title": "1 Introduction",
    "add_info": "1 Freely available at http://paraphrase.org.",
    "text": "In this work, we release version 1.0 of the Para-Phrase DataBase PPDB, [Cite_Footnote_1] a collection of ranked En-glish and Spanish paraphrases derived by:"
  },
  {
    "id": 692,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://projects.ldc.upenn.edu/gale/data/Catalog.html",
    "section_title": "4 English Paraphrases \u2013 PPDB:Eng",
    "add_info": "2 http://projects.ldc.upenn.edu/gale/data/Catalog.html",
    "text": "We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [Cite_Footnote_2] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS."
  },
  {
    "id": 693,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/snipsco/nlu-benchmark/",
    "section_title": "3 Experiment Setup",
    "add_info": "1 https://github.com/snipsco/nlu-benchmark/",
    "text": "Datasets For each task, we evaluate our proposed models by applying it on two real-word datasets: SNIPS Natural Language Understanding bench-mark [Cite_Footnote_1] (SNIPS-NLU) and the Airline Travel Infor-mation Systems (ATIS) dataset (Tur et al., 2010). The statistical information on two datasets are shown in Table 1."
  },
  {
    "id": 694,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://dialogflow.com/",
    "section_title": "3 Experiment Setup",
    "add_info": "2 https://dialogflow.com/",
    "text": "We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) [Cite_Footnote_2] , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex ."
  },
  {
    "id": 695,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://www.ibm.com/cloud/watson-assistant/",
    "section_title": "3 Experiment Setup",
    "add_info": "3 https://www.ibm.com/cloud/watson-assistant/",
    "text": "We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant [Cite_Footnote_3] , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex ."
  },
  {
    "id": 696,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://www.luis.ai/",
    "section_title": "3 Experiment Setup",
    "add_info": "4 https://www.luis.ai/",
    "text": "We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis [Cite_Footnote_4] , wit.ai , snips.ai , recast.ai , and Amazon Lex ."
  },
  {
    "id": 697,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://wit.ai/",
    "section_title": "3 Experiment Setup",
    "add_info": "5 https://wit.ai/",
    "text": "We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai [Cite_Footnote_5] , snips.ai , recast.ai , and Amazon Lex ."
  },
  {
    "id": 698,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://snips.ai/",
    "section_title": "3 Experiment Setup",
    "add_info": "6 https://snips.ai/",
    "text": "We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai [Cite_Footnote_6] , recast.ai , and Amazon Lex ."
  },
  {
    "id": 699,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://recast.ai/",
    "section_title": "3 Experiment Setup",
    "add_info": "7 https://recast.ai/",
    "text": "We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai [Cite_Footnote_7] , and Amazon Lex ."
  },
  {
    "id": 700,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://aws.amazon.com/lex/",
    "section_title": "3 Experiment Setup",
    "add_info": "8 https://aws.amazon.com/lex/",
    "text": "We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex [Cite_Footnote_8] ."
  },
  {
    "id": 701,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/UKPLab/acl2017-non-factoid-qa",
    "section_title": "References",
    "add_info": "1 https://github.com/UKPLab/acl2017-non-factoid-qa",
    "text": "Advanced attention mechanisms are an im-portant part of successful neural network approaches for non-factoid answer selec-tion because they allow the models to focus on few important segments within rather long answer texts. Analyzing attention mechanisms is thus crucial for understand-ing strengths and weaknesses of particular models. We present an extensible, highly modular service architecture that enables the transformation of neural network mod-els for non-factoid answer selection into fully featured end-to-end question answer-ing systems. The primary objective of our system is to enable researchers a way to in-teractively explore and compare attention-based neural networks for answer selec-tion. Our interactive user interface helps researchers to better understand the capa-bilities of the different approaches and can aid qualitative analyses. The source-code of our system is publicly available. [Cite_Footnote_1]"
  },
  {
    "id": 702,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.clg.ox.ac.uk/tedcorpus",
    "section_title": "4 Datasets 4.1 Multlingual TED corpus",
    "add_info": "1 http://www.clg.ox.ac.uk/tedcorpus",
    "text": "Hermann and Blunsom (2014b) provide a multilin-gual corpus based on the TED corpus for IWSLT 2013 (Cettolo et al., 2012). It contains English tran-scriptions of several talks from the TED conference and their translations in multiple languages. We use the parallel data between English and other lan-guages for training Bridge Corrnet (English, thus, acts as the pivot langauge). Hermann and Blunsom (2014b) also propose a multlingual document classi-fication task using this corpus. The idea is to use the keywords associated with each talk (document) as class labels and then train a classifier to predict these classes. There are one or more such keywords asso-ciated with each talk but only the 15 most frequent keywords across all documents are considered as class labels. We used the same pre-processed splits [Cite_Footnote_1] as provided by (Hermann and Blunsom, 2014b). The training corpus consists of a total of 12,078 par-allel documents distributed across 12 language pairs."
  },
  {
    "id": 703,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.statmt.org/moses/",
    "section_title": "6 Experiment 2: Cross modal access using a pivot language",
    "add_info": "4 http://www.statmt.org/moses/",
    "text": "5. CorrNet + MT: Here, we train an En-Image Cor-rNet using Z 1 and an Fr/De-En MT system [Cite_Footnote_4] using Z 2 . For the task of retrieving images given a French"
  },
  {
    "id": 704,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://nlp.stanford.edu/software/srparser.html",
    "section_title": "3 Models 3.1 Chunkings and Chunk Embeddings",
    "add_info": "John Bauer. 2014. Shift-reduce constituency parser. https://nlp.stanford.edu/software/srparser.html. Accessed: 2018-11-30.",
    "text": "We chunk the source and target sentences using constituent parsing (Bauer, 2014)  . We consider all nodes with phrase-level tags (XP) to be con-stituents. Beginning with the leaves, we move up the tree, deleting any node that is wholly contained in a larger constituent but that is neither a con-stituent itself, nor the sibling of a constituent. Fig-ure 2 shows a simplified constituent tree."
  },
  {
    "id": 705,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/bzhangXMU/transformer-aan",
    "section_title": "References",
    "add_info": "1 Source code is available at https://github.com/bzhangXMU/transformer-aan.",
    "text": "With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive archi-tecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average atten-tion network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to en-hance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Trans-former to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model en-ables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 dif-ferent language pairs, we obtain robust and consistent speed-ups in decoding. [Cite_Footnote_1]"
  },
  {
    "id": 706,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/thumt/THUMT",
    "section_title": "5 Experiments 5.1 WMT14 English-German Translation 5.1.1 Model Settings",
    "add_info": "2 https://github.com/thumt/THUMT",
    "text": "The maximum number of training steps was set to 100K. Weights of target-side embedding and out-put weight matrix were tied for all models. We implemented our model with masking tricks based on the open-sourced thumt (Zhang et al., 2017b) [Cite_Footnote_2] , and trained and evaluated all models on a single NVIDIA GeForce GTX 1080 GPU. For evalua-tion, we averaged last five models saved with an interval of 1500 training steps."
  },
  {
    "id": 707,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://data.statmt.org/wmt17/translation-task/preprocessed/",
    "section_title": "5 Experiments 5.2 WMT17 Translation Tasks",
    "add_info": "3 http://data.statmt.org/wmt17/translation-task/preprocessed/",
    "text": "Interestingly, these translation tasks involves train-ing corpora with different scales (ranging from 0.21M to 52M sentence pairs). This help us thor-oughly examine the ability of our model on differ-ent sizes of training data. All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [Cite_Footnote_3]"
  },
  {
    "id": 708,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://matrix.statmt.org/matrix",
    "section_title": "5 Experiments 5.2 WMT17 Translation Tasks 5.2.1 Translation Results",
    "add_info": "4 http://matrix.statmt.org/matrix",
    "text": "Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [Cite_Footnote_4] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus."
  },
  {
    "id": 709,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/successar/FRESH",
    "section_title": "References",
    "add_info": "1 Code is available at https://github.com/successar/FRESH",
    "text": "In many settings it is important for one to be able to understand why a model made a partic-ular prediction. In NLP this often entails ex-tracting snippets of an input text \u2018responsible for\u2019 corresponding model output; when such a snippet comprises tokens that indeed informed the model\u2019s prediction, it is a faithful explana-tion. In some settings, faithfulness may be crit-ical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful ratio-nales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method com-plicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary fea-ture importance scores (e.g., gradients from a trained model) are used to induce binary la-bels over token inputs, which an extractor can be trained to predict. An independent classi-fier module is then trained exclusively on snip-pets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predic-tive performance superior to \u2018end-to-end\u2019 ap-proaches, while being more general and easier to train. [Cite_Footnote_1]"
  },
  {
    "id": 710,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/xiaochang13/AMR-generation",
    "section_title": "2 Method 2.3 Rule Acquisition",
    "add_info": "1 Our code for grammar induction can be downloaded from https://github.com/xiaochang13/AMR-generation",
    "text": "We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC pro-cedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discard-ing function words, while our task is AMR-to-text generation, and we need to be able to fill in these un-aligned words. Since incorporating unaligned words will introduce noise, we rank the translation candi-dates for each AMR fragment by their counts in the training data, and select the top N candidates. [Cite_Footnote_1]"
  },
  {
    "id": 711,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt",
    "section_title": "2 Method 2.3 Rule Acquisition",
    "add_info": "2 http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt",
    "text": "Some concepts (such as \u201chave-rel-role-91\u201d) in an AMR graph do not contribute to the final translation, and we skip them when generating concept rules. Besides that, we use a verbalization list [Cite_Footnote_2] for concept rule generation. For rule \u201cVERBALIZE peacekeep-ing TO keep-01 :ARG1 peace\u201d, we will create a con-cept rule \u201c(k/keep-01 :ARG1 (p/peace)) ||| peace-keeping\u201d if the left-hand-side fragment appears in the target graph."
  },
  {
    "id": 712,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://developers.google.com/optimization/",
    "section_title": "3 Experiments 3.1 Setup",
    "add_info": "3 https://developers.google.com/optimization/",
    "text": "We use the dataset of SemEval-2016 Task8 (Mean-ing Representation Parsing), which contains 16833 training instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same mean-ing. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [Cite_Footnote_3] ."
  },
  {
    "id": 713,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://www.statmt.org/moses/",
    "section_title": "3 Experiments 3.1 Setup",
    "add_info": "4 http://www.statmt.org/moses/",
    "text": "Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses [Cite_Footnote_4] to translate the linearized AMR into a sen-tence. To traverse the children of an AMR con-cept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMR-gen (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07)."
  },
  {
    "id": 714,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/jflanigan/jamr/tree/Generator",
    "section_title": "3 Experiments 3.1 Setup",
    "add_info": "5 https://github.com/jflanigan/jamr/tree/Generator",
    "text": "Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses to translate the linearized AMR into a sen-tence. To traverse the children of an AMR con-cept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMR-gen [Cite_Footnote_5] (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07)."
  },
  {
    "id": 715,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "2 The PACRR Model 2.1 Relevance Matching",
    "add_info": "1 https://code.google.com/archive/p/word2vec/",
    "text": "We first encode the query-document relevance matching via query-document similarity matri-ces sim |q|\u00d7|d| that encodes the similarity be-tween terms from a query q and a document d, where sim ij corresponds to the similarity be-tween the i-th term from q and the j-th term from d. When using cosine similarity, we have sim \u2208 [\u22121,1] |q|\u00d7|d| . As suggested in (Hui et al., 2017), query-document similarity matrices pre-serve a rich signal that can be used to perform relevance matching beyond unigram matches. In particular, n-gram matching corresponds to con-secutive document terms that are highly similar to at least one of the query terms. Query coverage is reflected in the number of rows in sim that include at least one cell with high similarity. The similar-ity between a query term q and document term d is calculated by taking the cosine similarity using the pre-trained [Cite_Footnote_1] word2vec (Mikolov et al., 2013)."
  },
  {
    "id": 716,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://trec.nist.gov/tracks.html",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "3 http://trec.nist.gov/tracks.html",
    "text": "We rely on the widely-used 2009\u20132014 T REC Web Track ad-hoc task benchmarks [Cite_Footnote_3] . The benchmarks are based on the C LUE W EB 09 and C LUE W EB 12 datasets as document collections. In total, there are 300 queries and more than 100k judgments (qrels). Three years (2012\u201314) of query-likelihood baselines provided by T REC serve as baseline runs in the R ERANK S IMPLE benchmark. In the R ERANK ALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55 (2010), 62 (2011), 48 (2012), 50 (2013), and 27 (2014) runs. ERR@20 (Chapelle et al., 2009) and nDCG@20 (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002) are employed as evaluation measures, and both are computed with the script from T REC 6 ."
  },
  {
    "id": 717,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://trec.nist.gov/data/web/12/gdeval.pl",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "6 http://trec.nist.gov/data/web/12/gdeval.pl",
    "text": "We rely on the widely-used 2009\u20132014 T REC Web Track ad-hoc task benchmarks 3 . The benchmarks are based on the C LUE W EB 09 and C LUE W EB 12 datasets as document collections. In total, there are 300 queries and more than 100k judgments (qrels). Three years (2012\u201314) of query-likelihood baselines 4 provided by T REC 5 serve as baseline runs in the R ERANK S IMPLE benchmark. In the R ERANK ALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55 (2010), 62 (2011), 48 (2012), 50 (2013), and 27 (2014) runs. ERR@20 (Chapelle et al., 2009) and nDCG@20 (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002) are employed as evaluation measures, and both are computed with the script from T REC [Cite_Footnote_6] ."
  },
  {
    "id": 718,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/fchollet/keras",
    "section_title": "3 Evaluation 3.1 Experimental Setup",
    "add_info": "Franc\u0327ois Chollet et al. 2015. Keras. https://github.com/fchollet/keras.",
    "text": "All models are implemented with Keras (Chol-let et al., 2015)  using Tensorflow as backend, and are trained on servers with multiple CPU cores. In particular, the training of PACRR takes 35 seconds per iteration on average, and in total at most 150 iterations are trained for each model variant."
  },
  {
    "id": 719,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.statmt.org/moses/",
    "section_title": "5 Experiments",
    "add_info": "3 http://www.statmt.org/moses/",
    "text": "We validated our simple implementation using a phrase table of 38,488,777 lines created with the Moses toolkit [Cite_Footnote_3] (Koehn et al., 2007) phrase-based SMT system, corresponding to 15,764,069 entries for distinct source phrases ."
  },
  {
    "id": 720,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/corenlp.shtml",
    "section_title": "3 A Contrastive Experiment",
    "add_info": "5 See http://nlp.stanford.edu/software/corenlp.shtml, run in \u2018strictTreebank3\u2019 mode.",
    "text": "PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools [Cite_Footnote_5] ; and (c) tokenization from the parser of Charniak & Johnson (2005). Table 1 shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance (Leven-shtein, 1966) over tokens (for a total of 49,208 sen-tences and 1,173,750 gold-standard tokens)."
  },
  {
    "id": 721,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://www.cog.brown.edu/mj/Software.htm",
    "section_title": "3 System Description 3.5 Inference",
    "add_info": "2 Available at http://www.cog.brown.edu/mj/Software.htm",
    "text": "Our implementation of adaptor grammars is a mod-ified version of the Pitman-Yor adaptor grammar sampler [Cite_Footnote_2] , altered to deal with the infinite number of entities. It carries out inference using a Metropolis-within-Gibbs algorithm (Johnson et al., 2007), in which it repeatedly parses each input line using the CYK algorithm, samples a parse, and proposes this as the new tree."
  },
  {
    "id": 722,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://cogcomp.cs.illinois.edu",
    "section_title": "2 The Models 2.1 Averaged Perceptron",
    "add_info": "4 LBJ can be downloaded from http://cogcomp.cs.illinois.edu.",
    "text": "We use the regularized version of AP in Learn-ing Based Java [Cite_Footnote_4] (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with a generaliza-tion bound related to the margin of the data, Aver-aged Perceptron also comes with a PAC-like gener-alization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoreti-cally and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, while being more efficient in training. It also has been shown to produce state-of-the-art results on many natural language applica-tions (Punyakanok et al., 2008)."
  },
  {
    "id": 723,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://cogcomp.cs.illinois.edu",
    "section_title": "3 Comparison of Algorithms 3.1 Evaluation Data",
    "add_info": "7 The annotation of the ESL corpus can be downloaded from http://cogcomp.cs.illinois.edu.",
    "text": "We evaluate the models using a corpus of ESL es-says, annotated [Cite_Footnote_7] by native English speakers (Ro-zovskaya and Roth, 2010a). For each preposition (article) used incorrectly, the annotator indicated the correct choice. The data include sentences by speak-ers of five first languages. Table 3 shows statistics by the source language of the writer."
  },
  {
    "id": 724,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.imsdb.com",
    "section_title": "5 Datasets 5.3 Television Series Transcripts",
    "add_info": "4 http://www.imsdb.com",
    "text": "Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). [Cite_Footnote_4] We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns."
  },
  {
    "id": 725,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://lit.eecs.umich.edu/downloads.html",
    "section_title": "7 Conclusion and Recommendations",
    "add_info": null,
    "text": "The code used in the experiments described in this paper is publicly available from  http://lit.eecs.umich.edu/downloads.html."
  },
  {
    "id": 726,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285",
    "section_title": "4 Factors Influencing Stability 4.2 Word Properties",
    "add_info": "Christiane Fellbaum. 1998. WordNet. Wiley Online Library. https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285.",
    "text": "To get a coarse-grained representation of the polysemy of the word, we consider the number of different POS present. For a finer-grained repre-sentation, we use the number of different Word-Net senses associated with the word (Miller, 1995; Fellbaum, 1998  )."
  },
  {
    "id": 727,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/ldc2008t19",
    "section_title": "4 Factors Influencing Stability 4.3 Data Properties",
    "add_info": "Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consor-tium, Philadelphia 6(12):e26752. https://catalog.ldc.upenn.edu/ldc2008t19.",
    "text": "Data features capture properties of the training data (and the word in relation to the training data). For this model, we gather data from two sources: New York Times (NYT) (Sandhaus, 2008)  and Eu-roparl (Koehn, 2005). Overall, we consider seven domains of data: (1) NYT - U.S., (2) NYT - New York and Region, (3) NYT - Business, (4) NYT - Arts, (5) NYT - Sports, (6) All of the data from domains 1-5 (denoted \u201cAll NYT\u201d), and (7) All of English Europarl. Table 3 shows statistics about these datasets. The first five domains are chosen because they are the top five most common cate-gories of news articles present in the NYT corpus. They are smaller than \u201cAll NYT\u201d and Europarl, and they have a narrow topical focus. The \u201cAll NYT\u201d domain is more diverse topically and larger than the first five domains. Finally, the Europarl domain is the largest domain, and it is focused on a single topic (European Parliamentary politics). These varying datasets allow us to consider how data-dependent properties affect stability."
  },
  {
    "id": 728,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.speech.cs.cmu.edu/cgibin/cmudict",
    "section_title": "4 Factors Influencing Stability 4.2 Word Properties",
    "add_info": "Robert L Weide. 1998. The CMU pronouncing dictionary http://www.speech.cs.cmu.edu/cgibin/cmudict.",
    "text": "We also consider the number of syllables in a word, determined using the CMU Pronuncing Dic-tionary (Weide, 1998)  . If the word is not present in the dictionary, then this is set to zero."
  },
  {
    "id": 729,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.di.unito.it/~tutreeb/",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.di.unito.it/~tutreeb/",
    "text": "Meanwhile, motivated by different syntactic theories and practices, major languages in the world often possess multiple large-scale hetero-geneous treebanks, e.g., Tiger (Brants et al., 2002) and T\u00fcBa-D/Z (Telljohann et al., 2004) treebanks for German, Talbanken (Einarsson, 1976) and Syntag (J\u00e4rborg, 1986) treebanks for Swedish, ISST (Montemagni et al., 2003) and TUT [Cite_Footnote_1] treebanks for Italian, etc. Ta-ble 1 lists several large-scale Chinese tree-banks. In this work, we take HIT-CDT as a case study. Our next-step plan is to annotate bi-tree aligned data for PKU-CDT and then convert PKU-CDT to our guideline. For non-dependency treebanks, the straight-forward choice is to convert such treebanks to dependency treebanks based on heuris-tic head-finding rules. The second choice is to directly extend our proposed approaches by adapting the patterns and treeLSTMs for non-dependency structures, which should be straightforward as well."
  },
  {
    "id": 730,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/tdozat/Parser-v1",
    "section_title": "5 Experiments 5.1 Experiment Settings",
    "add_info": "6 https://github.com/tdozat/Parser-v1",
    "text": "Implementation. In order to more flexibly realize our ideas, we re-implement the baseline biaffine parser in C++ based on the lightweight neural network library of Zhang et al. (2016). On the Chinese CoNLL-2009 data, our parser achieves 85.80% in LAS, whereas the origi-nal tensorflow-based parser [Cite_Footnote_6] achieves 85.54% (85.38% reported in their paper) under the same parameter settings and external word embedding."
  },
  {
    "id": 731,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.uexpress.com/dearabby/archives",
    "section_title": "3 S OCIAL -C HEM -101 Dataset 3.1 Situations",
    "add_info": "3 https://www.uexpress.com/dearabby/archives",
    "text": "We use a situation to denote the one-sentence prompt given to a worker as the basis for writ-ing RoTs. We gather a total of 104k real life situations from four domains: scraped titles of posts in the subreddits r/confessions (32k) and r/amitheasshole (r/AITA, 30k), which largely focus on moral quandaries and interper-sonal conflicts; 30k sentences from the ROCSto-ries corpus (rocstories, Mostafazadeh et al., 2016); and scraped titles from the Dear Abby ad-vice column archives [Cite_Footnote_3] (dearabby, 12k)."
  },
  {
    "id": 732,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://mediabiasfactcheck.com",
    "section_title": "6 Morality & Political Bias",
    "add_info": "10 We use the MediaBias/FactCheck ratings: https://mediabiasfactcheck.com. Shoumen, BULGARIA. know\u201d \u2013 Follows: \u201cIt\u2019s devastating to be excluded from a wedding you were invited to\u201d",
    "text": "To demonstrate a use case of our proposed formal-ism, we analyze the social norms and expectations evoked in news headlines from news sources of for headlines and the news source\u2019s political leaning (left: neg., right: pos.) and reliability (controlled for political leaning). Results shown are significant after Holm-correction for multiple comparisons (p < 0.001: \u2217\u2217\u2217 , p < 0.01: \u2217\u2217 , p < 0.05: \u2217 , p > 0.05: n.s.) . Takeaway: We see evidence that a model trained on the S OCIAL -C HEM -101 Dataset can naturally uncover moral and topical leanings in news sources, mirroring results found in previous news studies. various political leanings and trustworthiness, us-ing the N EURAL N ORM T RANSFORMER (GPT-2 XL). Specifically, we generate ROTs and attributes for 50,000 news headlines randomly selected from N\u00f8rregaard et al. (2019), a large corpus of political headlines from 2018 paired with news source rat-ings of political leaning (5-point scale from left- to right-leaning) and factual reliability (5-point scale from least reliable to most reliable). [Cite_Footnote_10]"
  },
  {
    "id": 733,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://cs.rochester.edu/nlp/rocstories/",
    "section_title": "2 Approach",
    "add_info": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839\u2013 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https://cs.rochester.edu/nlp/rocstories/ .",
    "text": "Punching a friend who stole from me. RoT 1: It is unacceptable to injure a person. RoT 2: People should not steal from others. RoT 3: It is bad to betray a friend. RoT 4: It is OK to want to take revenge. often with interpersonal conflicts, such as \u201cI feel threat-ened by women prettier than me.\u201d As with r/AITA, we scrape only the titles of these posts. This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) \u2014 The ROCStories corpus from (Mostafazadeh et al., 2016)  . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., \u201cThey weren\u2019t sure either so he started asking friends.\u201d. We select a subset of the sentences from ROCStories which are likely to column. These titles are usually information dense sum-maries of interpersonal situations written in the style of news headlines, e.g., \u201cPushy Party Guests Make Them-selves Too Much at Home.\u201d We scrape all of the titles found in the archives, and use heuristics to attempt to filter out all posts that do not match this style, such as announcements and holiday greetings. once. For example, in a sentence like \u201cWe went to the park.\u201d we would pick we. Or for a sentence like \u201cThey spent hours talking to us and we had a good time.\u201d we would pick they and us. underlying expectations. RoTs that are too vague often do describe norms, but the link to the situa-tion can be so distant as to be misleading. Good RoTs may be somewhat specific, but explain both the underlying norms at play, and apply to other situations. \u2013 Distinct ideas. When multiple RoTs are provided for a situation, each should contain a distinct idea. This includes inversions of the same idea. \u2013 Example situation: Never taking out the trash \u2013 Violates: \u201cIt\u2019s irresponsible to avoid the chores you are assigned\u201d with \u201cIt\u2019s bad to not do chores you\u2019re supposed to do\u201d"
  },
  {
    "id": 734,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Liang-Qiu/SVRNN-dialogues",
    "section_title": "References",
    "add_info": "1 The code is released at https://github.com/Liang-Qiu/SVRNN-dialogues.",
    "text": "Inducing a meaningful structural representa-tion from one or a set of dialogues is a cru-cial but challenging task in computational lin-guistics. Advancement made in this area is critical for dialogue system design and dis-course analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured atten-tion layers into a Variational Recurrent Neu-ral Network (VRNN) model with discrete la-tent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sen-tence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with struc-tured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dia-logue datasets, our model learns an interac-tive structure demonstrating its capability of distinguishing speakers or addresses, automat-ically disentangling dialogues without explicit human annotation. [Cite_Footnote_1]"
  },
  {
    "id": 735,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://radimrehurek.com/gensim/",
    "section_title": "5 Experimental Setup",
    "add_info": "2 https://radimrehurek.com/gensim/",
    "text": "Training Details Word embeddings were pre-trained with the Gensim [Cite_Footnote_2] implementation of word2vec (Mikolov et al., 2013) on the English GigaWord corpus (with case left intact). The di-mensionality of the word embeddings was set to 50. Following Li et al. (2016), the embed-dings were fine-tuned using a mapping matrix W \u2208 R 50\u00d750 trained with the following criterion: where LT tuned , and LT pre are lookup tables for fine-tuned and pre-trained word embeddings in the training set. Matrix W can be subsequently used to to estimate fine-tuned embeddings for words in the test set."
  },
  {
    "id": 736,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Domain Adapted Word Embeddings for Improved Sentiment Classification 3.2 Experimental evaluation and results",
    "add_info": "1 https://nlp.stanford.edu/projects/glove/",
    "text": "\u2022 Genericembeddingswordusedembeddingsare GloVe: Generic [Cite_Footnote_1] from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings . These generic embeddings will be denoted as Glv, GlvCC and w2v."
  },
  {
    "id": 737,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "3 Domain Adapted Word Embeddings for Improved Sentiment Classification 3.2 Experimental evaluation and results",
    "add_info": "2 https://code.google.com/archive/p/word2vec/",
    "text": "\u2022 Genericembeddingswordusedembeddingsare GloVe: Generic from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings [Cite_Footnote_2] . These generic embeddings will be denoted as Glv, GlvCC and w2v."
  },
  {
    "id": 738,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://radimrehurek.com/gensim/",
    "section_title": "3 Domain Adapted Word Embeddings for Improved Sentiment Classification 3.2 Experimental evaluation and results",
    "add_info": "3 https://radimrehurek.com/gensim/",
    "text": "\u2022 DSobtainedwordviaembeddingsLatent Semantic: DS embeddingsAnalysis (LSAare) and via retraining word2vec on the test data sets using the implementation in gensim [Cite_Footnote_3] . DS embeddings via LSA are denoted by LSA and DS embeddings via word2vec are de-noted by DSw2v."
  },
  {
    "id": 739,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/atschneid/DebugSL",
    "section_title": "6 Demonstration",
    "add_info": null,
    "text": "We present two main scenarios to demonstrate the practical usefulness of DebugSL, screenshots of which are shown in Figure 3. The source code is available at  https://github.com/atschneid/DebugSL."
  },
  {
    "id": 740,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/atschneid/DebugSL",
    "section_title": "7 Conclusion",
    "add_info": null,
    "text": "In this paper we have presented the system DebugSL and described its usage. The project source code is available at  https://github.com/atschneid/DebugSL and a screencast can be viewed at https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public."
  },
  {
    "id": 741,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://spacy.io/",
    "section_title": "- 8.1 Definitions of Complexity Measures",
    "add_info": "Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed-dings, convolutional neural networks and incremen-tal parsing. https://spacy.io/.",
    "text": "4. Parse tree depth: dependency parse tree depth using spaCy (Honnibal and Montani, 2017)  ."
  },
  {
    "id": 742,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://textinspector.com/help/lexical-diversity/",
    "section_title": "- 8.1 Definitions of Complexity Measures",
    "add_info": "1 See https://textinspector.com/help/lexical-diversity/ for McCarthy\u2019s recommendation on vocd-D vs HD-D.",
    "text": "Here, we describe in detail the linguistic complex-ity measures we used, which span lexical diversity ( ), syntactic simplicity ( ), readability ( ), and prototypicality ( ). samples of different length, is merely a com-plex approximation (R \u201c 0.971) of a hypergeo-metric distribution, which they use in an index called HD-D. [Cite_Footnote_1] HD-D measures the mean con-tribution that each type makes to the TTR of all possible combinations of a samples of size 35-"
  },
  {
    "id": 743,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php",
    "section_title": "- 8.1 Definitions of Complexity Measures",
    "add_info": "2 Words not on a list of 3,000 familiar words at https://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php",
    "text": "6. Dale-Chall readability score (Dale and Chall, 1948, 1995): DCRS \u201c 0.1579p DWR \u00a8 100q ` 0.0496 WPS , where DWR is the ratio of difficult words [Cite_Footnote_2] and WPS is the average words per sen-"
  },
  {
    "id": 744,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/clab/lstm-parser",
    "section_title": "4 Training Procedure",
    "add_info": "6 Software for replicating the experiments is available from https://github.com/clab/lstm-parser.",
    "text": "We trained our parser to maximize the conditional log-likelihood (Eq. 1) of treebank parses given sentences. Our implementation constructs a com-putation graph for each sentence and runs forward-and backpropagation to obtain the gradients of this objective with respect to the model parameters. The computations for a single parsing model were run on a single thread on a CPU. Using the dimen-sions discussed in the next section, we required between 8 and 12 hours to reach convergence on a held-out dev set. [Cite_Footnote_6]"
  },
  {
    "id": 745,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://github.com/naist-cl-parsing/",
    "section_title": "1 Introduction",
    "add_info": "2 We release our dependency corpus at https://github.com/naist-cl-parsing/mwe-aware-dependency.MWE-aware phrase struc-tures will be distributed from LDC as a part of LDC2017T01. Computational Linguistics (Short Papers), pages 427\u2013432",
    "text": "To pursue this direction further, we construct a corpus such that dependency structures are consis-tent with MWEs, by extending Kato et al. (2016)\u2019s corpus [Cite_Footnote_2] . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware de-pendency structure from our corpus (Figure 1b). Moreover, our corpus includes not only functional MWEs but also NEs. Because NEs are highly pro-ductive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary."
  },
  {
    "id": 746,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://catalog.ldc.upenn.edu/LDC2017T01",
    "section_title": "2 MWE-aware Dependency Corpus",
    "add_info": "5 https://catalog.ldc.upenn.edu/LDC2017T01",
    "text": "To ensure consistency between MWE annotations and dependency structures, we first integrate NE annotations on Ontonotes into phrase structures such that functional MWEs are established as sub-trees. Subsequently, we convert phrase structures to dependency structures. We construct our corpus by extending Kato et al. (2016)\u2019s corpus [Cite_Footnote_5] , which is itself built on a corpus by Shigeto et al. (2013). Regarding MWE annotations, Shigeto et al. (2013) first constructed an MWE dictionary by extract-ing functional MWEs from the English-language Wiktionary , and classified their occurrences in Ontonotes into either MWE or literal usage. Kato et al. (2016) integrated these MWE annotations into phrase structures and established functional MWEs as subtrees."
  },
  {
    "id": 747,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://en.wiktionary.org",
    "section_title": "2 MWE-aware Dependency Corpus",
    "add_info": "6 https://en.wiktionary.org",
    "text": "To ensure consistency between MWE annotations and dependency structures, we first integrate NE annotations on Ontonotes into phrase structures such that functional MWEs are established as sub-trees. Subsequently, we convert phrase structures to dependency structures. We construct our corpus by extending Kato et al. (2016)\u2019s corpus , which is itself built on a corpus by Shigeto et al. (2013). Regarding MWE annotations, Shigeto et al. (2013) first constructed an MWE dictionary by extract-ing functional MWEs from the English-language Wiktionary [Cite_Footnote_6] , and classified their occurrences in Ontonotes into either MWE or literal usage. Kato et al. (2016) integrated these MWE annotations into phrase structures and established functional MWEs as subtrees."
  },
  {
    "id": 748,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://members.unine.ch/jacques.savoy/clef/englishST.txt",
    "section_title": "3 Models for MWE identification and MWE-aware dependency parsing 3.1 Pipeline Model",
    "add_info": "9 http://members.unine.ch/jacques.savoy/clef/englishST.txt",
    "text": "The pipeline model involves the following three steps. First, BIO tags encoding MWE-spans and MWE POS tags, such as \u201cB NNP\u201d and \u201cI DT\u201d are predicted by a sequential labeler based on Con-ditional Random Fields (CRFs) (Lafferty et al., 2001). Second, tokens belonging to each pre-dicted MWE-span are concatenated into a sin-gle node. Finally, an MWE-based dependency structure (Figure 1b) is predicted by an arc-eager transition-based parser. For the CRFs, in addi-tion to word-form and character-based features, we use 1- to 3-gram features based on dictionaries of functional MWEs and NEs within 5-word win-dows from a target token. For a dictionary of func-tional MWEs, we use the dictionary by Shigeto et al. (2013) (Section 2). Meanwhile, we create a dictionary of NEs from a title list of English Wikipedia articles, excepting stop words, provided by UniNE [Cite_Footnote_9] . Regarding parsing features, we use baseline features and rich non-local features pro-posed by Zhang and Nivre (2011)."
  },
  {
    "id": 749,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://www.github.com/",
    "section_title": "3 A Bayesian Approach to Information 3.2 Bayesian Mutual Information Proof. See App. D. 3.4.2 No Data-Processing Inequality",
    "add_info": "8 Our code is available in https://www.github.com/ 10 L2 weight decay regularisation is equivalent to a Gaussian rycolab/bayesian-mi. prior on the parameter space (pg. 350, Bishop, 1995).",
    "text": "MI learning curves. In this regard, our analysis As previously discussed, the Gaussian and is similar to the learning curves used by Talmor Dirichlet priors on the parameters will cause these et al. (2020) and the complexity\u2013accuracy trade- models to initially place a uniform distribution on offs from Pimentel et al. (2020a). the output classes\u2014as such, they will have an ini-tial Bayesian MI of zero. We then expose the probe 5 Experiments and Results [Cite_Footnote_8] agent to increasingly larger sets of data from the 5.1 Data and Representations task. Unfortunately, the posterior of eq. (28) has no closed form solution, so we approximate it with We focus on part-of-speech (POS) tagging and dependency-arc labelling in our experiments. the maximum-a-posteriori probability p \u03b8 (t | r, \u03b8 ), With this in mind, we make use of the universal where \u03b8 \u2217 = argmax \u03b8\u2208\u0398 p \u03b8 (\u03b8 | d n ). We ob-tain this MAP estimate using the gradient descent dependencies (UD 2.6; Zeman et al., 2020); method AdamW (Loshchilov and Hutter, 2019) analysing the treebanks of four typologically with a cross-entropy loss and L2 norm regularisa-diverse languages, namely: Basque, English, tion. 10 The posterior predictive belief of eq. (29) Marathi, and Turkish. As our object of analysis, has a closed-form solution 11 we look at the contextual representations from ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019) and BERT (Devlin et al., 2019), using as a count(d N , t) + 1"
  },
  {
    "id": 750,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://scikit-learn.org/stable/",
    "section_title": "4 Experiments 4.2 Document Representation",
    "add_info": "2 See http://scikit-learn.org/stable/",
    "text": "We classify these selected tweets by Random Forest classifier (Breiman, 2001) implemented in sklearn [Cite_Footnote_2] python module with 10-fold cross valida-tion. Using accuracy as the evaluation metric, we report the classification performance of different topic models in Figure 2. With the increase of the topic number K, all the models\u2019 accuracies are tending to increase. BTM is worse than all other models, which confirms the effectiveness of user based aggregation. Twitter-BTM and BTM-U always outperform LDA-U, Twitter-LDA and TwitterUB-LDA. Twitter-BTM\u2019s accuracy is a lit-tle higher than BTM-U, which demonstrates that the background topic is helpful to capture more accurate topic representation of documents."
  },
  {
    "id": 751,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://www.marekrei.com/projects/mltagger",
    "section_title": "5 Implementation Details",
    "add_info": "1 http://www.marekrei.com/projects/mltagger",
    "text": "The code used for performing these experi-ments is made available online. [Cite_Footnote_1]"
  },
  {
    "id": 752,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://doi.org/10.1038/nn.3331",
    "section_title": "5 Implementation Details",
    "add_info": "Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Viegas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: Large-Scale Machine Learning on Heterogeneous Dis-tributed Systems. Arxiv preprint arXiv:1603.04467 https://doi.org/10.1038/nn.3331.",
    "text": "The model was implemented using Tensorflow (Abadi et al., 2016)  . The network weights were randomly initialized using the uniform Glorot initialization method (Glorot and Bengio, 2010) and optimization was performed using AdaDelta (Zeiler, 2012) with learning rate 1.0. Dropout (Srivastava et al., 2014) with probability 0.5 was applied to word representations w i and the com-posed representations h i after the LSTMs. The training was performed in batches of 32 sentences. Sentence-level performance was observed on the development data and the training was stopped if performance did not improve for 7 epochs. The best overall model on the development set was then used to report performance on the test data, both for sentence classification and sequence la-beling. In order to avoid random outliers, we per-formed each experiment with 5 random seeds and report here the averaged results."
  },
  {
    "id": 753,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://research-lab.yahoo.co.jp/en/software/",
    "section_title": "3 Experiments 3.1 Datasets",
    "add_info": "2 The tweet IDs will be provided from https://research-lab.yahoo.co.jp/en/software/.",
    "text": "The dialog dataset contains about 22.3 million tweet-reply pairs extracted from Twitter Firehose data. In its preprocessing, we filtered out spam and bot posts by using user-level signals such as the follower count, the friend count, the favorite count, and whether a profile image is set or not. Also, we replaced all the URLs in the text with \u201c[u]\u201d and all the user mentions with \u201c[m]\u201d, consid-ering them as noise. The rest of the text was used as it was. On average, source and target (or re-ply) tweets after preprocessing were 31.5 and 27.8 characters long, respectively. While redistribution of tweets is prohibited, we are planning to publi-cize tweet IDs of this dataset for reproducibility. [Cite_Footnote_2]"
  },
  {
    "id": 754,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://u.cs.biu.ac.il/~nlp/resources/downloads/",
    "section_title": "2 Experiment Setup",
    "add_info": "1 http://u.cs.biu.ac.il/~nlp/resources/downloads/",
    "text": "Due to various differences (e.g. corpora, train/test splits), we do not list previously reported results, but apply a large space of state-of-the-art supervised methods and review them comparatively. We ob-serve similar trends to previously published results, and make the dataset splits available for replication. [Cite_Footnote_1]"
  },
  {
    "id": 755,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://bitbucket.org/yoavgo/word2vecf",
    "section_title": "2 Experiment Setup 2.1 Word Representations 2.1.2 Representation Models",
    "add_info": "3 http://bitbucket.org/yoavgo/word2vecf",
    "text": "SVD We reduced M\u2019s dimensionality to k = 500 using Singular Value Decomposition (SVD). SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 nega-tive samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014). [Cite_Footnote_3]"
  },
  {
    "id": 756,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://jobimtext.org",
    "section_title": "3 Negative Results",
    "add_info": "4 http://jobimtext.org",
    "text": "Based on the above setup, we present three nega-tive empirical results, which challenge the claim that the methods presented in \u00a72.3 are learning a rela-tion between x and y. In addition to our setup, these results were also reproduced in preliminary exper-iments by applying the JoBimText framework [Cite_Footnote_4] for scalable distributional thesauri (Biemann and Riedl, 2013) using Google\u2019s syntactic N-grams (Goldberg and Orwant, 2013) as a corpus."
  },
  {
    "id": 757,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://people.csail.mit.edu/jacobe/naacl09.html",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Source code is available at  http://people.csail.mit.edu/jacobe/naacl09.html."
  },
  {
    "id": 758,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://onlinebooks.library.upenn.edu",
    "section_title": "5 Experimental Setup",
    "add_info": "2 The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. Malioutov, and Masao Utiyama for making their topic segmentation systems publicly available, and to the anonymous reviewers for useful feedback. This research is supported by the Beckman Postdoctoral Fellowship.",
    "text": "Corpora The dataset for evaluation is drawn from a medical textbook (Walker et al., 1990). [Cite_Footnote_2] The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections. Evaluation is performed sep-arately on each of the twelve parts, with the task of correctly identifying the chapter and section bound-aries. Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries."
  },
  {
    "id": 759,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.freelang.net/dictionary",
    "section_title": "3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping",
    "add_info": null,
    "text": "In order to translate the English semantic lexi-cons into other languages, we needed a bilingual lexicon for each of the target languages, Italian, Chinese and Portuguese in our particular case. For this purpose, we first used two corpus-based fre-quency dictionaries compiled for Chinese (Xiao et al., 2009) and Portuguese (Davies and Preto-Bay, 2007), which cover the 5,000 most frequent Chi-nese and Portuguese words respectively. These dictionaries provided high-quality manually edited word translations. In addition, we used large Eng-lish-Italian and English-Portuguese bilingual lexi-cons available from FreeLang site (  http://www.freelang.net/dictionary) as well as an English-Chinese bilingual word list available from LDC (Linguistic Data Consortium). Compiled without professional editing, these bilingual word lists contain errors and inaccurate translations, and hence they introduced noise into the mapping pro-cess. However, they provided wider lexical cover-age of the languages involved and complemented the limited sizes of the high-quality dictionaries used in our experiment. Table 1 lists the bilingual lexical resources employed for translating the Eng-lish lexicons into each of the three languages in-volved in our experiment."
  },
  {
    "id": 760,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://ucrel.lancs.ac.uk/claws7tags.html",
    "section_title": "3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping",
    "add_info": "1 For definitions of the POS and semantic tags, see websites http://ucrel.lancs.ac.uk/claws7tags.html and http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf",
    "text": "USAS semantic categories [Cite_Footnote_1] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1"
  },
  {
    "id": 761,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf",
    "section_title": "3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping",
    "add_info": "1 For definitions of the POS and semantic tags, see websites http://ucrel.lancs.ac.uk/claws7tags.html and http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf",
    "text": "USAS semantic categories [Cite_Footnote_1] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1"
  },
  {
    "id": 762,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "http://www.elsevier.com",
    "section_title": "3 Linguistic Resources 3.1 Specialized Comparable Corpora",
    "add_info": "2 http://www.elsevier.com",
    "text": "Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website [Cite_Footnote_2] . The documents were taken from the medical domain within the sub-domain of \u201cbreast cancer\u201d. We have auto-matically selected the documents published between 2001 and 2008 where the title or the keywords contain the term cancer du sein in French and breast cancer in English. We col-lected 130 French documents (about 530,000 words) and 1,640 English documents (about"
  },
  {
    "id": 763,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.ncbi.nlm.nih.gov/pubmed/",
    "section_title": "3 Linguistic Resources 3.1 Specialized Comparable Corpora",
    "add_info": "3 http://www.ncbi.nlm.nih.gov/pubmed/",
    "text": "Diabetes corpus The documents making up the French part of the comparable corpus have been craweled from the web using three keywords: diabe\u0300te (diabetes), alimentation (food), and obe\u0301site\u0301 (obesity). After a man-ual selection, we only kept the documents which were relative to the medical domain. As a result, 65 French documents were ex-tracted (about 257,000 words). The English part has been extracted from the medical website PubMed [Cite_Footnote_3] using the keywords: dia-betes, nutrition and feeding. We only kept the free fulltext available documents. As a re-sult, 2,339 English documents were extracted (about 3,5 million words). We also split the English documents into 14 parts each con-taining about 250,000 words."
  },
  {
    "id": 764,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://code.google.com/p/ttc-project",
    "section_title": "3 Linguistic Resources 3.1 Specialized Comparable Corpora",
    "add_info": "4 http://code.google.com/p/ttc-project",
    "text": "The French and English documents were then normalised through the following linguistic pre-processing steps: tokenisation, part-of-speech tag-ging, and lemmatisation. These steps were car-ried out using the TTC TermSuite [Cite_Footnote_4] that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded. Table 3 shows the number of dis-tinct words (# words) after these steps. It also indicates the comparability degree in percentage (comp.) between the French part and each English part of each comparable corpus. The comparabil-ity measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable. We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora. In the remainder of this article, [breast cancer corpus i] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i (i \u2208 [1, 14])."
  },
  {
    "id": 765,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "http://www.elra.info/",
    "section_title": "3 Linguistic Resources 3.2 Bilingual Dictionary",
    "add_info": "5 http://www.elra.info/",
    "text": "The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M0033 available from the ELRA catalogue [Cite_Footnote_5] . This re-source is a general language dictionary which con-tains only a few terms related to the medical do-main."
  },
  {
    "id": 766,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.nlm.nih.gov/research/umls",
    "section_title": "3 Linguistic Resources 3.3 Terminology Reference Lists",
    "add_info": "6 http://www.nlm.nih.gov/research/umls",
    "text": "To evaluate the quality of terminology extrac-tion, we built a bilingual terminology reference list for each comparable corpus. We selected all French/English single words from the UMLS [Cite_Footnote_6] meta-thesaurus. We kept only i) the French sin-gle words which occur more than four times in the French part and ii) the English single words which occur more than four times in each English part i . As a result of filtering, 169 French/English single words were extracted for the breast can-cer corpus and 244 French/English single words were extracted for the diabetes corpus. It should be noted that the evaluation of terminology ex-traction using specialized comparable corpora of-ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2007), 125 and 79 in Bouamor et al. (2013)."
  },
  {
    "id": 767,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/monologg/KoBERT-Transformers",
    "section_title": "3 Backchannel Prediction Model 3.3 Experimental Setup",
    "add_info": "2 https://github.com/monologg/KoBERT-Transformers",
    "text": "The pre-training language model used in BPM was KoBERT [Cite_Footnote_2] , and ReLU was used as the activa-tion function for each hidden layer; the dropout rate was 0.3. The batch size was 64, and the number of epochs was 60. Optimization was performed us-ing SGD as the parameter of the Transformers and Adam for the other parameters. The learning rate was 0.0005. The data were divided into training, validation, and test sets at a ratio of 3:1:1. The best model 3 was saved by early stopping regularization based on the validation result."
  },
  {
    "id": 768,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "5 Experiments 5.2 Training Details and Implementation",
    "add_info": "1 https://code.google.com/p/word2vec/",
    "text": "As for training cost, our system processes around 4000 tokens per second on a single GTX 670 GPU. As an example, this amounts to [Cite_Footnote_1] minute per epoch on the TREC dataset, converging within 50 epochs."
  },
  {
    "id": 769,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "5 Experiments 5.2 Training Details and Implementation",
    "add_info": "2 http://nlp.stanford.edu/projects/glove/",
    "text": "We use two sets of 300-dimensional pre-trained embeddings, word2vec 1 and GloVe [Cite_Footnote_2] , forming two channels for our network. For all datasets, we use 100 convolution filters each for window sizes of 3, 4, 5. Rectified Linear Units (ReLU) is chosen as the nonlinear function in the convolutional layer."
  },
  {
    "id": 770,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html",
    "section_title": "4 Syntactic Pruning",
    "add_info": "3 See http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html.",
    "text": "WordNet provides an important resource for ob-taining the set of clause types that are compatible with each sense of a verb. In particular, each verb sense in WordNet is annotated with a set of frames (e.g., \u201csomebody verb something\u201d) in which they may occur, capturing both syntactic and semantic constraints. There are 35 different frames in to-tal. [Cite_Footnote_3] We manually assigned a set of clause types to each frame (e.g., SVO to frame \u201csomebody verb something\u201d). Table 1 shows an example frame for each of the seven clause types. On average, each WordNet-3.0 verb sense is associated with 1.57 frames; the maximum number of frames per sense is 9. The distribution of frames is highly skewed: More than 61% of the 21,649 frame annotations belong to one of four simple SVO frames (num-bers 8, 9, 10 and 11), and 22 out of the 35 frames have less than 100 instances. This skew makes the syntactic pruning step effective for non-SVO clauses, but less effective for SVO clauses."
  },
  {
    "id": 771,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://wordnet.princeton.edu/glosstag.shtml",
    "section_title": "6 Verb-Object Sense Repository",
    "add_info": "4 http://wordnet.princeton.edu/glosstag.shtml",
    "text": "We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses [Cite_Footnote_4] as well as the sense-annotated SemCor corpus (Landes et al., 1998)."
  },
  {
    "id": 772,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://web.eecs.umich.edu/\u02dcmihalcea/downloads.html",
    "section_title": "6 Verb-Object Sense Repository",
    "add_info": "5 http://web.eecs.umich.edu/\u02dcmihalcea/downloads.html",
    "text": "We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses as well as the sense-annotated SemCor corpus (Landes et al., 1998). [Cite_Footnote_5]"
  },
  {
    "id": 773,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://www.sussex.ac.uk/Users/drh21/",
    "section_title": "7 Evaluation",
    "add_info": "8 http://www.sussex.ac.uk/Users/drh21/",
    "text": "Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the en-try\u2019s context (words in the sentence) and both the sense\u2019s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Baner-jee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1\u201320 in our experi-ments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004), [Cite_Footnote_8] which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled."
  },
  {
    "id": 774,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.neo4j.org/",
    "section_title": "7 Evaluation",
    "add_info": "9 http://www.neo4j.org/",
    "text": "Degree Centrality. Proposed by Navigli and La-pata (2010). The method collects all paths con-necting each candidate sense of an entry to the set of candidate senses of the words the entry\u2019s con-text. The candidate sense with the highest degree in the resulting subgraph is selected. We imple-mented this algorithm using the Neo4j library. [Cite_Footnote_9] We used a fixed threshold of 1 and vary the search depth in range 1\u201320. We used the candidate senses of all nouns and verbs in a sentence as context."
  },
  {
    "id": 775,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "A Appendix A.3.1 Adaptation to Chinese Inputs",
    "add_info": "7 https://github.com/google-research/bert",
    "text": "Since all the benchmark approaches use BERT for encodings and C HASE is constructed for Chinese, we replace BERT with Chinese-BERT. [Cite_Footnote_7] During the adaptations of EditSQL and IGSQL, we iden-tified and fixed 3 bugs in their pre-process and post-process procedures. The string-match based schema linking method in RAT-SQL utilizes the Stanford CoreNLP Toolkit (Manning et al., 2014) to tokenize a question, and the method performs string matches between the resulting words and schema items. To adapt this method to Chinese, we try to use the Chinese package of CoreNLP to tokenize questions. However, we find that doing so fails to link a lot of schema items. Consider the question \u201c\u8fd9\u9996\u6b4c\u66f2\u7684\u540d\u5b57\u662f?\u201d and the col-umn \u201c\u6b4c\u540d\u201d which is an abbreviation for \u201c\u6b4c\u66f2 \u7684\u540d\u5b57\u201d. The question is tokenized by CoreNLP into h \u8fd9, \u9996, \u6b4c\u66f2, \u7684, \u540d\u5b57, \u662f, ? i. None of the resulting words can be matched with \u201c\u6b4c\u540d\u201d. Consequently, the method cannot link the column to the question. To solve this problem, we simply tokenize a Chinese question character by charac-ter. In this way, the character \u2018\u6b4c\u2019 and \u2018\u540d\u2019 can be partly matched to \u201c\u6b4c\u540d\u201d. Although this solution would introduce a lot of noises, our experimental results show that this solution outperforms the one using CoreNLP. It would be very useful to explore the ways to conduct schema linking in Chinese."
  },
  {
    "id": 776,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://github.com/andreasvc/discodop",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "4 http://github.com/andreasvc/discodop",
    "text": "For the evaluation, we use the corresponding module of discodop. [Cite_Footnote_4] We report several metrics (as implemented in discodop):"
  },
  {
    "id": 777,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://github.com/wmaier/rparse",
    "section_title": "3 Experiments 3.2 Experimental Setup",
    "add_info": "5 http://github.com/wmaier/rparse",
    "text": "We run further experiments with rparse [Cite_Footnote_5] (Kallmeyer and Maier, 2013) to facilitate a com-parison with a grammar-based parser."
  },
  {
    "id": 778,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "3 Method 3.1 Contextual Utterance Embedding",
    "add_info": "1 See https://github.com/google-research/bert for details.",
    "text": "We generate contextual utterance features from the tokens by following the method in (Luo and Wang, 2019). First, every utterance u 1 , u 2 , \u00b7 \u00b7 \u00b7 , u N is tokenized by the BPE tokenizer (Sennrich et al., 2015), i.e., u i = (u i,1 , u i,2 , \u00b7 \u00b7 \u00b7 , u i,T i ), where T i denotes the number of tokens. The tokens are embedded through WordPiece embeddings (Wu et al., 2016). The pre-trained uncased BERT-Base [Cite_Footnote_1] model converts the token embeddings into con-textualized token representations, which can be converted to the vector representations via max pooling, so that they are regarded as the contex-tual utterance embeddings h (i0) \u2208 R D m for i = 1, \u00b7 \u00b7 \u00b7 , M, where D m denotes the dimension of the utterance embeddings. This BERT model is fine-tuned through a training process."
  },
  {
    "id": 779,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.uic.edu/\u02dcliub/FBS/CustomerReviewData.zip",
    "section_title": "5 Data and Annotation Procedure",
    "add_info": "4 http://www.cs.uic.edu/\u02dcliub/FBS/CustomerReviewData.zip",
    "text": "We applied our annotation scheme to the product re-view dataset [Cite_Footnote_4] released by Hu and Liu (2004). We annotated the data for 3 out of 5 products. Each comment in the review is evaluated as being quali-fied or bald claim. The data has been made available for research purposes ."
  },
  {
    "id": 780,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://minorthird.sourceforge.net/",
    "section_title": "6 Experiments and Results",
    "add_info": "William Cohen. 2004. Minorthird: Methods for Iden-tifying Names and Ontological Relations in Text us-ing Heuristics for Inducing Regularities from Data. http://minorthird.sourceforge.net/",
    "text": "For our supervised machine learning experiments on automatic classification of comments as qualified or bald, we used the Support Vector Machine classifier in the MinorThird toolkit (Cohen, 2004)  with the de-fault linear kernel. We report average classification accuracy and average Cohen\u2019s Kappa using 10-fold cross-validation."
  },
  {
    "id": 781,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/tuzhaopeng/NMT-Coverage",
    "section_title": "References",
    "add_info": "1 Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.",
    "text": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vec-tor is fed to the attention model to help ad-just future attention, which lets NMT sys-tem to consider more about untranslated source words. Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [Cite_Footnote_1]"
  },
  {
    "id": 782,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/",
    "section_title": "6 http://Infomap-nlp.sourceforge.net/",
    "add_info": "7 http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/",
    "text": "An input pair (A i ,A s ) is then associated with a grade g(A i , A s ) = u T \u03c8(A i , A s ) computed as a lin-ear combination of features. The weight vector u is trained to optimize performance in two scenarios: Regression: An SVM model for regression (SVR) is trained using as target function the grades as-signed by the instructors. We use the libSVM [Cite_Footnote_7] im-plementation of SVR, with tuned parameters."
  },
  {
    "id": 783,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://svmlight.joachims.org/",
    "section_title": "6 http://Infomap-nlp.sourceforge.net/",
    "add_info": "8 http://svmlight.joachims.org/",
    "text": "Ranking: An SVM model for ranking (SVMRank) is trained using as ranking pairs all pairs of stu-dent answers (A s ,A t ) such that grade(A i ,A s ) > grade(A i ,A t ), where A i is the corresponding in-structor answer. We use the SVMLight [Cite_Footnote_8] implemen-tation of SVMRank with tuned parameters."
  },
  {
    "id": 784,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://www.eccox.io/",
    "section_title": "References",
    "add_info": null,
    "text": "Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco \u2013 an open-source 1 li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models. This includes (1) gradient-based fea-ture attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and exami-nation tools for neuron activations in the under-explored Feed-Forward Neural Network sub-layer of Transformer layers. (4) convenient ex-amination of activation vectors via canonical correlation analysis (CCA), non-negative ma-trix factorization (NMF), and probing classi-fiers. We find that syntactic information can be retrieved from BERT\u2019s FFNN representa-tions in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntac-tic information. Ecco is available at  https://www.eccox.io/ . 2"
  },
  {
    "id": 785,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/jalammar/ecco",
    "section_title": "References",
    "add_info": "1 The code is available at https://github.com/jalammar/ecco",
    "text": "Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco \u2013 an open-source [Cite_Footnote_1] li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models. This includes (1) gradient-based fea-ture attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and exami-nation tools for neuron activations in the under-explored Feed-Forward Neural Network sub-layer of Transformer layers. (4) convenient ex-amination of activation vectors via canonical correlation analysis (CCA), non-negative ma-trix factorization (NMF), and probing classi-fiers. We find that syntactic information can be retrieved from BERT\u2019s FFNN representa-tions in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntac-tic information. Ecco is available at https: //www.eccox.io/ ."
  },
  {
    "id": 786,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google/svcca",
    "section_title": "6 System Design",
    "add_info": "4 https://github.com/google/svcca",
    "text": "Ecco is built on top of open source libraries including Scikit-Learn (Pedregosa et al., 2011), Matplotlib (Hunter, 2007), NumPy (Walt et al., 2011), PyTorch (Paszke et al., 2019) and Trans-formers (Wolf et al., 2020). Canonical Correla-tion Analysis is calculated using the code open-sourced [Cite_Footnote_4] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019)."
  },
  {
    "id": 787,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/jalammar/ecco",
    "section_title": "8 Conclusion",
    "add_info": "5 https://github.com/jalammar/ecco",
    "text": "Ecco is open-source software [Cite_Footnote_5] and contributions are welcome. Acknowledgments This work was improved thanks to feedback pro-vided by Abdullah Almaatouq, Anfal Alatawi, Christopher Olah, Fahd Alhazmi, Hadeel Al-Negheimish, Hend Al-Khalifa, Isabelle Augen-stein, Jasmijn Bastings, Najwa Alghamdi, Pepa Atanasova, and Sebastian Gehrmann."
  },
  {
    "id": 788,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://bit.ly/2ApmLcz",
    "section_title": "4 Evaluation 4.1 Datasets and Indexing",
    "add_info": "2 https://bit.ly/2ApmLcz",
    "text": "We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID [Cite_Footnote_2] , is based on the CORD-19 dataset , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels). Documents in this dataset have three fields (title, abstract and content). The two other bench-marks are based on news articles datasets: AP (As-sociation Press, about 242K docs) and WSJ (Wall Street Journal, about 160K docs). These datasets are part of the TREC ad-hoc retrieval newswire collection . Here we used topics 51-150 and topics 151-200 (with their respective qrels) for the AP and WSJ datasets, respectively. Those two datasets have only title and content so we created the ab-stract by taking the first 512 tokens of the content."
  },
  {
    "id": 789,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://bit.ly/3dxyZ1i",
    "section_title": "4 Evaluation 4.1 Datasets and Indexing",
    "add_info": "3 https://bit.ly/3dxyZ1i",
    "text": "We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID , is based on the CORD-19 dataset [Cite_Footnote_3] , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels). Documents in this dataset have three fields (title, abstract and content). The two other bench-marks are based on news articles datasets: AP (As-sociation Press, about 242K docs) and WSJ (Wall Street Journal, about 160K docs). These datasets are part of the TREC ad-hoc retrieval newswire collection . Here we used topics 51-150 and topics 151-200 (with their respective qrels) for the AP and WSJ datasets, respectively. Those two datasets have only title and content so we created the ab-stract by taking the first 512 tokens of the content."
  },
  {
    "id": 790,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://bit.ly/3gJcF6X",
    "section_title": "4 Evaluation 4.1 Datasets and Indexing",
    "add_info": "5 https://bit.ly/3gJcF6X",
    "text": "We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID , is based on the CORD-19 dataset , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels). Documents in this dataset have three fields (title, abstract and content). The two other bench-marks are based on news articles datasets: AP (As-sociation Press, about 242K docs) and WSJ (Wall Street Journal, about 160K docs). These datasets are part of the TREC ad-hoc retrieval newswire collection [Cite_Footnote_5] . Here we used topics 51-150 and topics 151-200 (with their respective qrels) for the AP and WSJ datasets, respectively. Those two datasets have only title and content so we created the ab-stract by taking the first 512 tokens of the content."
  },
  {
    "id": 791,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://bit.ly/2Me0Gk1",
    "section_title": "4 Evaluation 4.2 Experimental Setup",
    "add_info": "6 https://bit.ly/2Me0Gk1",
    "text": "IR-Base. The following Lucene similarities config-urations were used: i) BM25Similarity (Robertson and Zaragoza, 2009) with k1 = 1.2 and b = 0.7. ii) LMDirichletSimilarity (Zhai, 2009) with Dirichlet-smoothing parameter \u00b5 = 200 and \u00b5 = 1000 for TREC-COVID and news datasets, respectively. iii) DFRSimilarity (Amati and Van Rijsbergen, 2002) with BasicModelIF, AfterEffectB and Normaliza-tionH3. iv) AxiomaticF1LOG (Fang and Zhai, 2005) with growth parameter s = 0.25 and s = 0.1 for TREC-COVID and news datasets, respectively. BERT models. We used the pytorch huggingface implementation of BERT and GPT2 [Cite_Footnote_6] . For the two BERT models we used bert-base-uncased (12-layers, 768-hidden, 12-heads, 110M parameters). Fine-tuning was done with a learning rate of 2e- 5 and 3 training epochs. For training BERT-Q-a on each of the three datasets, we used a subset of their first 20K documents. For TREC-COVID, we used SciBERT model (Beltagy et al., 2019) (that was pre-trained on 1M scientific documents), as it yields better results than using the vanilla pre-trained BERT model. This is mainly due to the scientific nature of the documents in this bench-mark."
  },
  {
    "id": 792,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/YosiMass/ad-hoc-retrieval",
    "section_title": "4 Evaluation 4.2 Experimental Setup",
    "add_info": "7 The filtered paraphrases can be downloaded from https://github.com/YosiMass/ad-hoc-retrieval",
    "text": "GPT2. For generating title paraphrases we used GPT2 small model (12-layers, 768-hidden, 12-heads, 110M parameters). For fine-tuning we used (title, abstract) pairs from all documents of TREC-COVID and a subset of the first 20K documents of the other two datasets. We generated 10 para-phrases for the first 20K documents of each of the three datasets. After filtering the generated para-phrases, we were left with 18K, 4.5K and 3.5K paraphrases for TREC-COVID, WSJ and AP re-spectively. [Cite_Footnote_7] Fusion. We fine-tuned the PoolRank (Roitman, 2018) method\u2019s parameters for all datasets as fol-lows: For Base fusion we used CombSUM (Nuray and Can, 2006) with sum-normalization. The other parameters were set as: Pseudo-relevance set size: 5 documents. Term clip size: 100. Document re-ranking using KL-score (equally interpolated with the CombSUM score) with Dirichlet-smoothing pa-rameter \u00b5 = 200 and \u00b5 = 1000 for TREC-COVID and news datasets, respectively."
  },
  {
    "id": 793,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://bit.ly/2XjkE2T",
    "section_title": "4 Evaluation 4.3 Results",
    "add_info": "8 The details of these systems as well as other competing systems are available in https://bit.ly/2XjkE2T",
    "text": "To demonstrate the relative effectiveness of our proposed approach, we compared its quality to state-of-the-art alternative baselines. On TREC-COVID, we directly compared against the three best automatic performing systems [Cite_Footnote_8] (out of 141 system runs submitted to the Round-1 challenge by 56 different teams), namely: sabir, IRIT markers and unipd.it."
  },
  {
    "id": 794,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.chokkan.org/software/crfsuite/",
    "section_title": "4 Experimental setup 4.1 MT and NLU systems",
    "add_info": "Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs). http://www.chokkan.org/software/crfsuite/.",
    "text": "For building NLU models, we use Conditional Random Fields (Lafferty et al., 2001; Okazaki, 2007  ) for Named Entity Recognition and a Max-imum Entropy classifier (Berger et al., 1996) for Intent Classification; we keep the sets of features, hyper-parameters and configuration constant for our experiments."
  },
  {
    "id": 795,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://sourceforge.net/projects/mstparser",
    "section_title": "2 Unlabeled Dependency Parsing using Global Features 2.3 Local Features",
    "add_info": "3 http://sourceforge.net/projects/mstparser",
    "text": "The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [Cite_Footnote_3] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent. The direction and the distance from a parent to its child, and the FEATS fields of the parent and the child which are split into elements and then combined are also included. Features that appeared less than 5 times in training data are ignored."
  },
  {
    "id": 796,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/sanketvmehta/efficient-meta-lifelong-learning",
    "section_title": "4 Experiments 4.2 Experimental Setup",
    "add_info": "1 Source code is available at https://github.com/sanketvmehta/efficient-meta-lifelong-learning.",
    "text": "For our framework, Meta-MbPA [Cite_Footnote_1] , unless stated otherwise, we set the number of neighbors K = 32 and control the memory size through a write rate r M = 1%. We use L = 30 local adaptation steps and perform local adaptation for whole testing set. That is, we randomly draw K = 32 examples from the memory and perform a local adaptation step. Through this, the computational cost is equivalent to MbPA++ but we only need to perform the whole process once while MbPA++ requires conducting local adaptation independently for each testing ex-ample. We set \u03b1 = 1e \u22125 (in Eq. (5), (6)), \u03b2 = 10 (in Eq. (7)) and \u03bb l = 0.001 (in Eq. (4)). All of the experiments are performed using PyTorch (Paszke et al., 2017), which allows for automatic differentiation through the gradient update as re-quired for optimizing the meta-task loss Eq. (5) and meta-replay loss Eq. (6)."
  },
  {
    "id": 797,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://dumps.wikimedia.org/",
    "section_title": "4 Experiments on Word/Phrase Similarity",
    "add_info": "1 https://dumps.wikimedia.org/",
    "text": "In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algo-rithms, word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), based on the Oct. 2013 Wikipedi-a dump. [Cite_Footnote_1] We first compare levels of truncation of word2vec embeddings, and then evaluate the s-tochastic rounding and the auxiliary vectors based methods for training word2vec vectors."
  },
  {
    "id": 798,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.wordvectors.org/",
    "section_title": "4 Experiments on Word/Phrase Similarity 4.1 Datasets",
    "add_info": "2 http://www.wordvectors.org/",
    "text": "Word Similarity. Word similarity datasets have been widely used to evaluate word embed-ding results. We use the datasets summarized by Faruqui and Dyer (Faruqui and Dyer, 2014): wordsim-353, wordsim-sim, wordsim-rel, MC-30, RG-65, MTurk-287, MTurk-771, MEN 3000, YP-130, Rare-Word, Verb-143, and SimLex-999. [Cite_Footnote_2] We compute the similarities between pairs of words and check the Spearman\u2019s rank correlation coeffi-cient (Myers and Well., 1995) between the com-puter and the human labeled ranks."
  },
  {
    "id": 799,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://ntunlpsg.github.io/project/rst-parser",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We make our code available at  https://ntunlpsg.github.io/project/rst-parser"
  },
  {
    "id": 800,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum",
    "section_title": "2 Curriculum Learning for Adaptation 2.2 Curriculum Learning Training Strategy",
    "add_info": "1 https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum",
    "text": "In this paper, we use the same probabilistic cur-riculum strategy and code base [Cite_Footnote_1] as Zhang et al. (2018). The main difference here is the applica-tion to domain adaptation. The proposed strategy is summarized as follows:"
  },
  {
    "id": 801,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum",
    "section_title": "3 Experiments and Results",
    "add_info": "2 https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum",
    "text": "We evaluate on four domain adaptation tasks. The code base is provided to ensure reproducibility. [Cite_Footnote_2]"
  },
  {
    "id": 802,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.paracrawl.eu/",
    "section_title": "3 Experiments and Results 3.1 Data and Setup",
    "add_info": "4 https://www.paracrawl.eu/",
    "text": "We randomly sample 15k parallel sentences from the original corpora as our in-domain bitext. We also have around 2k sentences of de-velopment and test data for TED and 3k for patent. Unlabeled-domain For additionalData unlabeled-domain data, we use web-crawled bitext from the Paracrawl project. [Cite_Footnote_4] We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively."
  },
  {
    "id": 803,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/HKUST-KnowComp/RINANTE",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Our code is available at  https://github.com/HKUST-KnowComp/RINANTE."
  },
  {
    "id": 804,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.yelp.com/dataset/challenge",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "1 https://www.yelp.com/dataset/challenge",
    "text": "Besides the above datasets, we also use a Yelp dataset [Cite_Footnote_1] and an Amazon Electronics dataset (He and McAuley, 2016) as auxiliary data to be anno-tated with the mined rules. They are also used to train word embeddings. The Yelp dataset is used for the restaurant datasets SE14-R and SE15-R. It includes 4,153,150 reviews that are for 144,072 different businesses. Most of the businesses are restaurants. The Amazon Electronics dataset is used for the laptop dataset SE14-L. It includes 1,689,188 reviews for 63,001 products such as lap-tops, TV, cell phones, etc."
  },
  {
    "id": 805,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://jmcauley.ucsd.edu/data/amazon/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "2 http://jmcauley.ucsd.edu/data/amazon/",
    "text": "Besides the above datasets, we also use a Yelp dataset and an Amazon Electronics dataset (He and McAuley, 2016) [Cite_Footnote_2] as auxiliary data to be anno-tated with the mined rules. They are also used to train word embeddings. The Yelp dataset is used for the restaurant datasets SE14-R and SE15-R. It includes 4,153,150 reviews that are for 144,072 different businesses. Most of the businesses are restaurants. The Amazon Electronics dataset is used for the laptop dataset SE14-L. It includes 1,689,188 reviews for 63,001 products such as lap-tops, TV, cell phones, etc."
  },
  {
    "id": 806,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://en.wiktionary.org",
    "section_title": "1 Introduction",
    "add_info": "1 http://en.wiktionary.org",
    "text": "For natural languages with rich morphology, knowl-edge of how to inflect base forms is critical for both text generation and analysis. Hand-engineered, rule-based methods for predicting inflections can offer extremely high accuracy, but they are laborious to construct and do not exist with full lexical cover-age in all languages. By contrast, a large number of example inflections are freely available in a semi-structured format on the Web. The English Wik-tionary [Cite_Footnote_1] is a crowd-sourced lexical resource that in-cludes complete inflection tables for many lexical items in many languages. We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language\u2019s in-flectional morphology. 2"
  },
  {
    "id": 807,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/emorynlp/nlp4j",
    "section_title": "3 Corpus 3.1 Passage Generation",
    "add_info": "2 https://github.com/emorynlp/nlp4j",
    "text": "The plot summaries collected from the fan sites are associated with episodes, not scenes. To break down the episode-level summaries into scene-level, they are segmented into sentences by the tokenizer in NLP4J. [Cite_Footnote_2] Each sentence in the plot summaries is then queried to Elasticsearch that has indexed the selected scenes, and the scene with the highest relevance is retrieved. Finally, the retrieved scene along with the queried sentence are sent to a crowd worker who is asked to determine whether or not they are relevant, and perform anaphora resolution to replace all pronouns in the sentence with the corresponding character names. The sentence that is checked for the relevancy and processed by the anaphora resolution is considered a passage."
  },
  {
    "id": 808,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.aclweb.org/anthology/D14-1162",
    "section_title": "5 Experiments",
    "add_info": "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.",
    "text": "The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014)  are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25. For the 2/1D convolutions in Sections 4.1 and 4.3, f = e = 50 filters are used, and the ReLu acti-vation is applied to all convolutional layers. The dimension of the LSTM outputs ~h \u2193\u2191 \u2217 is 32, and the tanh activation is applied to all hidden states of LSTMs. Finally, the Adam optimizer with the learning rate of 0.001 is used to learn the weights of all models. Table 4 shows the dataset split for our experiments that roughly gives 80/10/10% for training/development/evaluation sets."
  },
  {
    "id": 809,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://commoncrawl.org/",
    "section_title": "2 Unsupervised Extractive QA 2.4 Unsupervised Cloze Translation",
    "add_info": "3 http://commoncrawl.org/",
    "text": "Question Corpus We mine questions from En-glish pages from a recent dump of common crawl using simple selection criteria: [Cite_Footnote_3] We select sen-tences that start in one of a few common wh* words, (\u201chow much\u201d, \u201chow many\u201d, \u201cwhat\u201d, \u201cwhen\u201d, \u201cwhere\u201d and \u201cwho\u201d) and end in a ques-tion mark. We reject questions that have repeated question marks or \u201c?!\u201d, or are longer than 20 to-kens. This process yields over 100M english ques-tions when deduplicated. Corpus Q is created by sampling 5M questions such that there are equal numbers of questions starting in each wh* word."
  },
  {
    "id": 810,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/",
    "section_title": "3 Experiments 3.1 Unsupervised QA Experiments",
    "add_info": "5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa",
    "text": "For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [Cite_Footnote_5] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics."
  },
  {
    "id": 811,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/allenai/document-qa",
    "section_title": "3 Experiments 3.1 Unsupervised QA Experiments",
    "add_info": "5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa",
    "text": "For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [Cite_Footnote_5] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics."
  },
  {
    "id": 812,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://bit.ly/semi-supervised-qa",
    "section_title": "3 Experiments 3.1 Unsupervised QA Experiments",
    "add_info": "6 http://bit.ly/semi-supervised-qa",
    "text": "We shall compare our results to some published baselines. Rajpurkar et al. (2016) use a super-vised logistic regression model with feature en-gineering, and a sliding window approach that finds answers using word overlap with the ques-tion. Kaushik and Lipton (2018) train (supervised) models that disregard the input question and sim-ply extract the most likely answer span from the context. To our knowledge, ours is the first work to deliberately target unsupervised QA on SQuAD. Dhingra et al. (2018) focus on semi-supervised QA, but do publish an unsupervised evaluation. To enable fair comparison, we re-implement their approach using their publicly available data, and train a variant with BERT-Large. [Cite_Footnote_6] Their approach also uses cloze questions, but without translation, and heavily relies on the structure of wikipedia ar-ticles."
  },
  {
    "id": 813,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2013T19",
    "section_title": "5 Discussion",
    "add_info": "7 Ontonotes 5: https://catalog.ldc.upenn.edu/LDC2013T19",
    "text": "It is worth noting that to attain our best perfor-mance, we require the use of both an NER system, indirectly using labelled data from OntoNotes 5, and a constituency parser for extracting sub-clauses, trained on the Penn Treebank (Marcus et al., 1994). [Cite_Footnote_7] Moreover, a language-specific wh* heuristic was used for training the best perform-ing NMT models. This limits the applicability and flexibility of our best-performing approach to do-mains and languages that already enjoy extensive linguistic resources (named entity recognition and treebank datasets), as well as requiring some hu-man engineering to define new heuristics."
  },
  {
    "id": 814,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/glample/fastBPE",
    "section_title": "- Details",
    "add_info": null,
    "text": "Here we describe experimental details for un-supervised NMT setup. We use the English tokenizer from Moses (Koehn et al., 2007), and use FastBPE (  https://github.com/glample/fastBPE) to split into subword units, with a vocabulary size of 60000. The architec-ture uses a 4-layer transformer encoder and 4-layer transformer decoder, where one layer is language specific for both the encoder and decoder, the rest are shared. We use the standard hyperparameter settings recommended by Lample et al. (2018). The models are initialised with random weights, and the input word embedding matrix is initialised using FastText vectors (Bojanowski et al., 2016) trained on the concatenation of the C and Q cor-pora. Initially, the auto-encoding loss and back-translation loss have equal weight, with the auto-encoding loss coefficient reduced to 0.1 by 100K steps and to 0 by 300k steps. We train using 5M cloze questions and natural questions, and cease training when the BLEU scores between back-translated and input questions stops improving, usually around 300K optimisation steps. When generating, we decode greedily, and note that de-coding with a beam size of 5 did not significantly change downstream QA performance, or greatly change the fluency of generations."
  },
  {
    "id": 815,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/facebookresearch/XLM",
    "section_title": "A.6 Language Model Pretraining",
    "add_info": null,
    "text": "We experimented with Masked Language Model (MLM) pretraining of the translation mod-els, p s\u2192t (q|q 0 ) and p t\u2192s (q 0 |q). We use the XLM implementation (  https://github.com/facebookresearch/XLM) and use de-fault hyperparameters for both MLM pretraining and and unsupervised NMT fine-tuning. The UNMT encoder is initialized with the MLM model\u2019s parameters, and the decoder is randomly initialized. We find translated questions to be qualitatively more fluent and abstractive than the those from the models used in the main paper. Table 6 supports this observation, demonstrating that questions produced by models with MLM pre-training are classified as well-formed 10.5% more often than those without pretraining, surpassing the rule-based question generator of Heilman and Smith (2010). However, using MLM pretraining did not lead to significant differences for question answering performance (the main focus of this pa-per), so we leave a thorough investigation into lan-guage model pretraining for unsupervised ques-tion answering as future work."
  },
  {
    "id": 816,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://arxiv.org/help/api",
    "section_title": "3 Experiments 3.2 Multi-Label Classification",
    "add_info": "6 https://arxiv.org/help/api",
    "text": "Because the arXiv dataset released by Yang et al. (2018) removed all line breaks, we created one ourselves. We collected abstracts and categories of papers submitted to arXiv from January 1st, 2019 to June 4th, 2019 using arXiv API. [Cite_Footnote_6]"
  },
  {
    "id": 817,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "3 Experiments 3.3 Settings",
    "add_info": "7 https://github.com/google-research/bert",
    "text": "As a text encoder, we employed BERT and a Hi-erarchical Attention Network (HAN) (Yang et al., 2016) for generating sentence and document rep-resentation, respectively. For BERT, we used the pre-trained BERT-base [Cite_Footnote_7] (d = 768). We imple-mented the HAN following Yang et al. (2016) who used the bi-directional Gated Recurrent Unit as the encoder with the hidden size of 50 (d = 50). The embedding layer of the HAN was initialised using CBOW (Mikolov et al., 2013) embeddings (with dimensions of 200), which were trained using nega-tive sampling on the training and development sets of each task."
  },
  {
    "id": 818,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://gordonscruton.blogspot.in/2011/08/",
    "section_title": "1 Introduction",
    "add_info": "2 We took the examples from this site for explaining coherence and cohesion: http://gordonscruton.blogspot.in/2011/08/what-is-cohesion-coherence-cambridge. html",
    "text": "Table 1: Examples of coherence and cohesion [Cite_Footnote_2] ."
  },
  {
    "id": 819,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cfilt.iitb.ac.in/cognitive-nlp/",
    "section_title": "1 Introduction",
    "add_info": "3 The dataset can be downloaded from http://www.cfilt.iitb.ac.in/cognitive-nlp/",
    "text": "Our work has the following contributions. Firstly, we propose a novel way to predict read-ers\u2019 rating of text by recording their eye move-ments as they read the texts. Secondly, we show that if a reader has understood the text com-pletely, their gaze behaviour is more reliable. Thirdly, we also release our dataset [Cite_Footnote_3] to help in further research in using gaze features in other tasks involving predicting the quality of texts."
  },
  {
    "id": 820,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://www.sr-research.com/products/eyelink-portable-duo/",
    "section_title": "2 Motivation",
    "add_info": "5 https://www.sr-research.com/products/eyelink-portable-duo/",
    "text": "One of the major concerns is How are we go-ing to get the gaze data? This is because capa-bility to gather eye-tracking data is not available to the masses. However, top mobile device man-ufacturers, like Samsung, have started integrat-ing basic eye-tracking software into their smart-phones (Samsung Smart Scroll) that are able to detect where the eye is fixated, and can be used in applications like scrolling through a web page. Start-ups, like Cogisen , have started using gaze features in their applications, such as using gaze information to improve input to image processing systems. Recently, SR Research has come up with a portable eye-tracking system [Cite_Footnote_5] ."
  },
  {
    "id": 821,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.nltk.org/",
    "section_title": "4 Features 4.1 Text-based Features",
    "add_info": "7 http://www.nltk.org/",
    "text": "The third set of features that we use are stylis-tic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK [Cite_Footnote_7] (Loper and Bird, 2002)."
  },
  {
    "id": 822,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://pypi.python.org/pypi/pyenchant/",
    "section_title": "4 Features 4.1 Text-based Features",
    "add_info": "9 https://pypi.python.org/pypi/pyenchant/",
    "text": "The fifth set of features that we use are lan-guage modeling features. We use the count of words that are absent in Google News word vec-tors and misspelled words using the PyEnchant [Cite_Footnote_9] library. In order to check the grammaticality of the text, we construct a 5-gram language model, using the Brown Corpus (Francis and Kucera, 1979)."
  },
  {
    "id": 823,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://simple.wikipedia.org",
    "section_title": "5 Experiment Details Details of Texts",
    "add_info": "10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com",
    "text": "To the best of our knowledge there isn\u2019t a pub-licly available dataset with gaze features for tex-tual quality. Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [Cite_Footnote_10] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each. The original arti-cles ranged from a couple hundred words (Simple English Wikipedia) to over a thousand words (En-glish Wikipedia). We first summarized the longer articles manually. Then, for the many articles over 200 words, we removed a few of the paragraphs and sentences. In this way, despite all the texts being published, we were able to introduce some poor quality texts into our dataset. The articles were sampled from a variety of genres, such as History, Science, Law, Entertainment, Education, Sports, etc."
  },
  {
    "id": 824,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://en.wikipedia.org",
    "section_title": "5 Experiment Details Details of Texts",
    "add_info": "10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com",
    "text": "To the best of our knowledge there isn\u2019t a pub-licly available dataset with gaze features for tex-tual quality. Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [Cite_Footnote_10] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each. The original arti-cles ranged from a couple hundred words (Simple English Wikipedia) to over a thousand words (En-glish Wikipedia). We first summarized the longer articles manually. Then, for the many articles over 200 words, we removed a few of the paragraphs and sentences. In this way, despite all the texts being published, we were able to introduce some poor quality texts into our dataset. The articles were sampled from a variety of genres, such as History, Science, Law, Entertainment, Education, Sports, etc."
  },
  {
    "id": 825,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://newsela.com",
    "section_title": "5 Experiment Details Details of Texts",
    "add_info": "10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com",
    "text": "To the best of our knowledge there isn\u2019t a pub-licly available dataset with gaze features for tex-tual quality. Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [Cite_Footnote_10] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each. The original arti-cles ranged from a couple hundred words (Simple English Wikipedia) to over a thousand words (En-glish Wikipedia). We first summarized the longer articles manually. Then, for the many articles over 200 words, we removed a few of the paragraphs and sentences. In this way, despite all the texts being published, we were able to introduce some poor quality texts into our dataset. The articles were sampled from a variety of genres, such as History, Science, Law, Entertainment, Education, Sports, etc."
  },
  {
    "id": 826,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.tensorflow.org/",
    "section_title": "5 Experiment Details 5.5 Classification Details",
    "add_info": "Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane\u0301, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Vie\u0301gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.",
    "text": "We split the data into a training - test split of sizes 70% and 30%. We used a Feed Forward Neural Network with 1 hidden layer containing 100 neurons (Bebis and Georgiopoulos, 1994) . The size of the input vector was 361 features. Out of these, there were 49 text features, plus 300 dimension word embeddings features, 11 gaze features, and 1 class label. The data was split using stratified sampling, to ensure that there is a similar distribution of classes in each of the train-ing and test splits. The Feed Forward Neural Net-work was implemented using TensorFlow (Abadi et al., 2015)  in Python. We ran the neural network over 10,000 epochs, with a learning rate of 0.001 in 10 batches. The loss function that we used was the mean square error."
  },
  {
    "id": 827,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.aclweb.org/anthology/P/P14/P14-5010",
    "section_title": "4 Features 4.1 Text-based Features",
    "add_info": "Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations. pages 55\u201360. http://www.aclweb.org/anthology/P/P14/P14-5010.",
    "text": "The next set of features that we use are com-plexity features, namely the degree of polysemy, coreference distance, and the Flesch Reading Ease Score (FRES) (Flesch, 1948). These features help in normalizing the gaze features for text complex-ity. These features were extracted using Stan-ford CoreNLP (Manning et al., 2014)  , and Mor-phAdorner (Burns, 2013)."
  },
  {
    "id": 828,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/ziyi-yang/GEM",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/ziyi-yang/GEM",
    "text": "The rest of this paper is organized as following. In Section 2, we describe our sentence embedding algorithm GEM. We evaluate our model on vari-ous tasks in Section 3 and Section 4. Finally, we summarize our work in Section 5. Our implemen-tation is available online [Cite_Footnote_1] ."
  },
  {
    "id": 829,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://lucene.apache.org/core/",
    "section_title": "3 Crosslingual Vector-Based Writing Assistance (CroVeWA)",
    "add_info": "1 https://lucene.apache.org/core/",
    "text": "Our system encourages refining retrieved results and viewing relations in different contexts by sup-porting multiple queries. All queries and their cor-responding results are visualized together to aid a better understanding of their relationships. To il-lustrate the differences to phrase vector-based sen-tence retrieval, we also offer a retrieval option based on direct word-to-text matching using the EDICT Japanese-English dictionary (Breen, 2004) and Apache Lucene [Cite_Footnote_1] for sentence retrieval."
  },
  {
    "id": 830,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://orchid.kuee.kyoto-u.ac.jp/ASPEC/",
    "section_title": "3 Crosslingual Vector-Based Writing Assistance (CroVeWA) 3.1 Inducing Crosslingually Constrained Word Representations",
    "add_info": "2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/",
    "text": "Currently we use Japanese and English resources to learn word embeddings, but plan to add more lan-guages in the future. The bilingual sentence-parallel resource used is the ASPEC corpus [Cite_Footnote_2] , which fea-tures sentence-aligned text from scientific paper ab-stracts. For monolingual data, we use subsets of the Japanese and English Wikipedia."
  },
  {
    "id": 831,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/JetRunner/beyond-preserved-accuracy",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/JetRunner/beyond-preserved-accuracy.",
    "text": "Recent studies on compression of pretrained language models (e.g., BERT) usually use pre-served accuracy as the metric for evaluation. In this paper, we propose two new metrics, la-bel loyalty and probability loyalty that mea-sure how closely a compressed model (i.e., stu-dent) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowl-edge distillation and progressive module re-placing with loyalty and robustness. By com-bining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness. [Cite_Footnote_1]"
  },
  {
    "id": 832,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/shawnwun/RNNLG",
    "section_title": "2 End-to-End NLG Systems",
    "add_info": "2 https://github.com/shawnwun/RNNLG",
    "text": "\u2022 RNNLG : [Cite_Footnote_2] The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface re-alisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far."
  },
  {
    "id": 833,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/UFAL-DSG/tgen",
    "section_title": "2 End-to-End NLG Systems",
    "add_info": "3 https://github.com/UFAL-DSG/tgen",
    "text": "\u2022 TG EN : [Cite_Footnote_3] The system by Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2015) learns to incrementally generate deep-syntax dependency trees of candidate sentence plans (i.e. which MR elements to mention and the overall sentence structure). Surface realisation is performed using a separate, domain-independent rule-based module."
  },
  {
    "id": 834,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/glampouras/JLOLS_NLG",
    "section_title": "2 End-to-End NLG Systems",
    "add_info": "4 https://github.com/glampouras/JLOLS_NLG",
    "text": "\u2022 LOLS : [Cite_Footnote_4] The system by Lampouras and Vlachos (2016) learns sentence planning and surface reali-sation using Locally Optimal Learning to Search ( LOLS ), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions."
  },
  {
    "id": 835,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.nist.gov/speech/tests/mt",
    "section_title": "3 Metrics and Test Beds 3.2 Test Beds",
    "add_info": "2 http://www.nist.gov/speech/tests/mt",
    "text": "We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005) [Cite_Footnote_2] . Both campaigns include two dif-ferent translations exercises: Arabic-to-English (\u2018AE\u2019) and Chinese-to-English (\u2018CE\u2019). Human as-sessments of adequacy and fluency, on a 1-5 scale, are available for a subset of sentences, each eval-uated by two different human judges. A brief nu-merical description of these test beds is available in Table 1. The corpus AE05 includes, apart from five automatic systems, one human-aided system that is only used in our last experiment."
  },
  {
    "id": 836,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09",
    "section_title": "1 Introduction",
    "add_info": "2 Available at http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09",
    "text": "However, we identify several limitations in the schemas produced by their system. [Cite_Footnote_2] Their schemas often lack coherence: mixing unrelated events and having actors whose entities do not play the same role in the schema. Table 2 shows an event schema from Chambers that mixes the events of fire spread-ing and disease spreading."
  },
  {
    "id": 837,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://relgrams.cs.washington.edu",
    "section_title": "1 Introduction 1.1 Contributions",
    "add_info": "3 Available at http://relgrams.cs.washington.edu",
    "text": "We release our open domain event schemas and the Rel-grams database [Cite_Footnote_3] for further use by the NLP community."
  },
  {
    "id": 838,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://knowitall.github.io/ollie/",
    "section_title": "3 Modeling Relational Co-occurrence 3.1 Relations Extraction and Representation",
    "add_info": "4 Available at: http://knowitall.github.io/ollie/",
    "text": "We extract relational triples from each sentence in a large corpus using the OLLIE Open IE system (Mausam et al., 2012). [Cite_Footnote_4] This provides relational tu-ples in the format (Arg1, Relation, Arg2) where each tuple element is a phrase from the sentence. The sentence \u201cHe cited a new study that was released by UCLA in 2008.\u201d produces three tuples:"
  },
  {
    "id": 839,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/CRF-NER.shtml",
    "section_title": "3 Modeling Relational Co-occurrence 3.1 Relations Extraction and Representation",
    "add_info": "5 We used the system downloaded from: http://nlp.stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it.",
    "text": "To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005) [Cite_Footnote_5] , and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target se-mantic types. We use regular expressions to recog-nize dates and numeric expressions, and map per-sonal pronouns to . We associate all types found by this mechanism with each argument. The tuples in the example above are normalized to the following:"
  },
  {
    "id": 840,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/dcoref.shtml",
    "section_title": "3 Modeling Relational Co-occurrence 3.2 Co-occurrence Tabulation",
    "add_info": "6 Available for download at: http://nlp.stanford.edu/software/dcoref.shtml",
    "text": "Equality Constraints: Along with the co-occurrence counts, we record the equality of argu-ments in Rel-grams pairs. We assert an argument pair is equal if they are from the same token se-quence in the source sentence or one argument is a co-referent mention of the other. We use the Stan-ford Co-reference system (Lee et al., 2013) [Cite_Footnote_6] to de-tect co-referring mentions. There are four possible equalities depending on the specific pair of argu-ments in the tuples are the same, shown as E11, E12, E21 and E22 in Figure 1. For example, the E21 col-umn has counts for the number of times the Arg2 of T1 was determined to be the same as the Arg1 of T2. Implementation and Query Language: We pop-ulated the Rel-grams database using OLLIE extrac-tions from a set of 1.8 Million New York Times arti-cles drawn from the Gigaword corpus. The database consisted of approximately 320K tuples that have frequency \u2265 3 and 1.1M entries in the bigram table."
  },
  {
    "id": 841,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Compare",
    "url": "http://www.usna.edu/Users/cs/",
    "section_title": "5 Evaluation 5.2 Schemas Evaluation",
    "add_info": "8 Available at http://www.usna.edu/Users/cs/",
    "text": "We compare Rel-grams schemas against the state-of-the-art narrative schemas released by Cham-bers (Chambers and Jurafsky, 2009). [Cite_Footnote_8] Chambers\u2019 schemas are less expressive than ours \u2013 they do not associate types with actors and each schema has a constant pre-specified number of relations. For a fair comparison we use a similarly expressive ver-sion of our schemas that strips off argument types and has the same number of relations per schema (six) as their highest quality output set."
  },
  {
    "id": 842,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://alt.qcri.org/semeval2014/task4/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "4 http://alt.qcri.org/semeval2014/task4/",
    "text": "To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 [Cite_Footnote_4] (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017)."
  },
  {
    "id": 843,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://alt.qcri.org/semeval2015/task12/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "5 http://alt.qcri.org/semeval2015/task12/",
    "text": "To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 [Cite_Footnote_5] (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017)."
  },
  {
    "id": 844,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.pitt.edu/mpqa/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "6 http://www.cs.pitt.edu/mpqa/",
    "text": "To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus [Cite_Footnote_6] (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017)."
  },
  {
    "id": 845,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://verbs.colorado.edu/jdpacorpus/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "7 http://verbs.colorado.edu/jdpacorpus/",
    "text": "To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora [Cite_Footnote_7] (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017)."
  },
  {
    "id": 846,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/wenhuchen/Variational-Vocabulary-Selection.git",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/wenhuchen/Variational-Vocabulary-Selection.git",
    "text": "2. We propose a novel vocabulary selection algorithm based on variational dropout by re-formulating text classification under the Bayesian inference framework. The code will be released in Github [Cite_Footnote_1] ."
  },
  {
    "id": 847,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/dennybritz/cnn-text-classification-tf",
    "section_title": "2 Vocabulary Selection 2.1 Problem Definition",
    "add_info": "2 https://github.com/dennybritz/cnn-text-classification-tf",
    "text": "We now formally define the problem setting and introduce the notations for our problem. Conven-tionally, we assume the neural classification model vectorizes the discrete language input into a vec-tor representation via an embedding matrix W \u2208 R V\u2217D , where V denotes the size of the vocabu-lary, and D denotes the vector dimension. The em-bedding is associated with a pre-defined word-to-index dictionary V = {w i : i|1 \u2264 i \u2264 V } where w i denotes a literal word corresponding to i th row in the embedding matrix. The embedding matrix W covers the subset of a vocabulary of interests for a particular NLP task, note that the value of V is known to be very large due to the rich variations in human languages. Here we showcase the em-bedding matrix size of a popular text classification model [Cite_Footnote_2] on AG-news dataset (Zhang et al., 2015) in Table 1. From which we can easily observe that the embedding matrix is commonly occupy-ing most of the parameter capacity, which could be the bottleneck in many real-world applications with limited computation resources."
  },
  {
    "id": 848,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/coetaur0/ESIM",
    "section_title": "4 Experiments 4.1 Datasets & Architectures",
    "add_info": "3 https://github.com/coetaur0/ESIM",
    "text": "The main datasets we are using are listed in Ta-ble 2, which provides an overview of its descrip-tion and capacities. Specifically, we follow (Zhang et al., 2015; Goo et al., 2018; Williams et al., 2018) to pre-process the document classification datasets, natural language understanding dataset and natural language inference dataset. We ex-actly replicate their experiment settings to make our method comparable with theirs. Our mod-els is implemented with TensorFlow (Abadi et al., 2015). In order to evaluate the generalization abil-ity of VVD selection algorithm in deep learning architectures, we study its performance under dif-ferent established architectures (depicted in Fig-ure 3). In natural language understanding, we use the most recent attention-based model for in-tention tracking (Goo et al., 2018), this model first uses BiLSTM recurrent network to leverage left-to-right and right-to-left context information to form the hidden representation, then computes self-attention weights to aggregate the hidden rep-resentation and predicts user intention. In doc-ument classification, we mainly follow the CNN architecture (Kim, 2014) to extract n-gram fea-tures and then aggregate these features to pre-dict document category. In natural language in-ference, we follow the popular ESIM architec-ture (Williams et al., 2018; Chen et al., 2017) us-ing the Github implementation [Cite_Footnote_3] . In this structure, three main components input encoding, local in-ference modeling, and inference composition are used to perform sequential inference and composi-tion to simulate the interaction between premises and hypothesis. Note that, we do not apply the syntax-tree based LSTM proposed in (Chen et al., 2017) because we lost the parse tree (Klein and Manning, 2003) after the vocabulary compression, instead, we follow the simpler sequential LSTM framework without any syntax parse as input. Be-sides, the accuracy curve is obtained using the publicly available test split rather than the official online evaluation because we need to evaluate lots of times at different vocabulary capacity."
  },
  {
    "id": 849,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://wit3.fbk.eu/",
    "section_title": "4 Experiment Setup",
    "add_info": "6 https://wit3.fbk.eu/",
    "text": "Datasets We evaluate our method on the IWSLT datasets which contain multiple languages from TED talks. We collect datasets from the IWSLT evaluation campaign [Cite_Footnote_6] from years 2011 to 2018, which consist of the translation pairs of 23 languages\u2194English. The detailed description of the training/validation/test set of the 23 transla-tion pairs can be found in Supplementary Mate-rials (Section 1). All the data has been tokenized and segmented into sub-word symbols using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE operations for all languages to-gether, which results in a shared vocabulary of 90K BPE tokens."
  },
  {
    "id": 850,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl",
    "section_title": "4 Experiment Setup",
    "add_info": "7 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl",
    "text": "During inference, each source token is also added with the corresponding language embed-ding in order to give the model a sense of the lan-guage it is currently processing. We decode with beam search and set beam size to 6 and length penalty \u03b1 = 1.1 for all the languages. We evaluate the translation quality by tokenized case sensitive BLEU (Papineni et al., 2002) with multi-bleu.pl [Cite_Footnote_7] ."
  },
  {
    "id": 851,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://github.com/tensorflow/tensor2tensor",
    "section_title": "5 Results 5.1 Results of Language Clustering",
    "add_info": "8 https://github.com/tensorflow/tensor2tensor",
    "text": "The language clustering based on language family is shown in Figure 2, which results in [Cite_Footnote_8] groups given our 23 languages. All the language names and their corresponding ISO-639-1 code can be found in Supplementary Materials (Section 2)."
  },
  {
    "id": 852,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://www.iso.org/iso-639-language-codes.html",
    "section_title": "5 Results 5.1 Results of Language Clustering",
    "add_info": "9 https://www.iso.org/iso-639-language-codes.html.",
    "text": "The language clustering based on language family is shown in Figure 2, which results in groups given our 23 languages. All the language names and their corresponding ISO-639-1 code [Cite_Footnote_9] can be found in Supplementary Materials (Section 2)."
  },
  {
    "id": 853,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html",
    "section_title": "1 Introduction",
    "add_info": "Anthony Kroch and Ann Taylor. 2000. Penn-Helsinki Parsed Corpus of Middle English, second edition. http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html.",
    "text": "1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000)  (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words)."
  },
  {
    "id": 854,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.ling.upenn.edu/hist-corpora/PPCEME-RELEASE-2/index.html",
    "section_title": "1 Introduction",
    "add_info": "Anthony Kroch, Beatrice Santorini, and Ariel Diertani. 2004. Penn-Helsinki Parsed Cor-pus of Early Modern English. http://www.ling.upenn.edu/hist-corpora/PPCEME-RELEASE-2/index.html.",
    "text": "1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004)  (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words)."
  },
  {
    "id": 855,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html",
    "section_title": "1 Introduction",
    "add_info": "Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.",
    "text": "We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010)  , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct com-parison problematic. Some first steps at analysis of the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material."
  },
  {
    "id": 856,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/berkeleyparser/",
    "section_title": "5 Parsing Experiments",
    "add_info": "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2008. The Berkeley Parser. https://code.google.com/p/berkeleyparser/.",
    "text": "The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008)  and score using the standard evalb program (Sekine and Collins, 2008). We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006). Since the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags. Table 4 shows the results for both modes."
  },
  {
    "id": 857,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/evalb/",
    "section_title": "5 Parsing Experiments",
    "add_info": "Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.",
    "text": "The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) and score using the standard evalb program (Sekine and Collins, 2008)  . We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006). Since the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags. Table 4 shows the results for both modes."
  },
  {
    "id": 858,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm",
    "section_title": "2 Corpus description 2.1 Part-of-speech tags",
    "add_info": "Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm.",
    "text": "3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003)  , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words."
  },
  {
    "id": 859,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www-users.york.ac.uk/\u02dclang22/PCEEC-manual/index.htm",
    "section_title": "1 Introduction",
    "add_info": "Ann Taylor, Arja Nurmi, Anthony Warner, Susan Pintzuk, and Terttu Nevalainen. 2006. Parsed Corpus of Early English Correspondence. Com-piled by the CEEC Project Team. York: Uni-versity of York and Helsinki: University of Helsinki. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/PCEEC-manual/index.htm.",
    "text": "1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006)  (2.2 million words)."
  },
  {
    "id": 860,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.linguist.is/icelandic_treebank",
    "section_title": "2 Corpus description 2.1 Part-of-speech tags",
    "add_info": "Joel Wallenberg, Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson. 2011. Icelandic Parsed Historical Corpus (IcePaHC) version 0.4. http://www.linguist.is/icelandic_treebank.",
    "text": "3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011)  , French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words."
  },
  {
    "id": 861,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "3 Experiments 3.1 Experimental Setup",
    "add_info": "2 https://github.com/huggingface/transformers",
    "text": "Experiment Details We adopt the T5 base model from huggingface Transformer library [Cite_Footnote_2] for all experiments. T5 closely follows the original encoder-decoder architecture of the Transformer model, with some slight differences such as differ-ent position embedding schemes. Therefore, the encoder and decoder of it have similar parameter size as the B ERT -B ASE model. For all tasks, we use similar experimental settings for simplicity: we train the model with the batch size of 16 and accu-mulate gradients every two batches. The learning rate is set to be 3e-4. The model is trained up to 20 epochs for the AOPE, UABSA, and ASTE task and 30 epochs for the TASD task."
  },
  {
    "id": 862,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ryankiros/skip-thoughts/",
    "section_title": "3 Skip-Thought Generative Adversarial Network 3.3 Model Architecture",
    "add_info": "1 https://github.com/ryankiros/skip-thoughts/",
    "text": "The Skip-Thought encoder for the model en-codes sentences using 2400 GRU units (Chung et al., 2014) with a word vector dimensionality of 620. The encoder combines the sentence em-beddings to produce 4800-dimensional combine-skip vectors with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model. This work uses the 4800-dimensional vectors as they have been found to be the best performing in experiments [Cite_Footnote_1] . For training of the STGAN, the Skip-Thought encoder produces sentence embed-ding vectors which are labelled as real samples for GAN discriminator."
  },
  {
    "id": 863,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/clab/sp2016.11-731/tree/master/hw4/data",
    "section_title": "4 Results and Discussion 4.2 Language Generation.",
    "add_info": "2 https://github.com/clab/sp2016.11-731/tree/master/hw4/data",
    "text": "Language generation is performed on a dataset comprising simple English sentences referred to as CMU-SE [Cite_Footnote_2] (Rajeswar et al., 2017). The CMU-SE dataset consists of 44,016 sentences with a vocabulary of 3,122 words. The vectors are ex-tracted in batches of same-lengthed sentences for encoding. The samples represent how mode col-lapse is manifested when using least-squares dis-tance (Mao et al., 2016) f-measure without mini-batch discrimination. Table 3(a) contains sen-tences generated from a vanilla STGAN which mode collapse is observed, while 3(b) contains examples wherein it is not observed when using minibatch discrimination. Table 3(c) shows gen-erated samples from STGAN when using Wasser-stein distance f-measure as WGAN (Arjovsky et al., 2017)) and 3(d) contains samples when us-ing a gradient penalty regularizer term as WGAN-GP (Gulrajani et al., 2017). The two models gen-erate longer human-like sentences and over a more diverse vocabulary."
  },
  {
    "id": 864,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "4 Evaluation setting",
    "add_info": "3 Available at https://code.google.com/p/word2vec/",
    "text": "Construction of vector spaces We test two types of vector representations. The cbow model introduced in Mikolov et al. (2013a) learns vec-tor representations using a neural network archi-tecture by trying to predict a target word given the words surrounding it. We use the word2vec soft-ware [Cite_Footnote_3] to build vectors of size 300 and using a con-text window of 5 words to either side of the target. We set the sub-sampling option to 1e-05 and esti-mate the probability of a target word with the neg-ative sampling method, drawing 10 samples from the noise distribution (see Mikolov et al. (2013a) for details). We also implement a standard count-based bag-of-words distributional space (Turney and Pantel, 2010) which counts occurrences of a target word with other words within a symmetric window of size 5. We build a 300Kx300K sym-metric co-occurrence matrix using the top most frequent words in our source corpus, apply posi-tive PMI weighting and Singular Value Decompo-sition to reduce the space to 300 dimensions. For both spaces, the vectors are finally normalized to unit length."
  },
  {
    "id": 865,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://wacky.sslmit.unibo",
    "section_title": "4 Evaluation setting",
    "add_info": "5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk",
    "text": "For both types of vectors we use 2.8 billion to-kens as input (ukWaC + Wikipedia + BNC). The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [Cite_Footnote_5] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures. We sanity-check the two spaces on MEN (Bruni et al., 2012), a 3,000 items word similarity data set. cbow sig-nificantly outperforms count (0.80 vs. 0.72 Spear-man correlations with human judgments). count performance is consistent with previously reported results. 6 (De)composition function training The train-ing data sets consist of the 50K most frequent hu, v, pi tuples for each phrase type, for example, hred, car, red.cari or hin, car, in.cari. 7 We con-catenate ~u and v~ vectors to obtain the [U; V ] ma-trix and we use the observed ~p vectors (e.g., the corpus vector of the red.car bigram) to obtain the phrase matrix P. We use these data sets to solve the least squares regression problems in eqs. (1) and (2), obtaining estimates of the composition and decomposition matrices, respectively. For the decomposition function in eq. (3), we replace the observed phrase vectors with those composed with f comp R (u~,~v), where f comp R is the previously esti-mated composition function for relation R."
  },
  {
    "id": 866,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.natcorp.ox.ac.uk",
    "section_title": "4 Evaluation setting",
    "add_info": "5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk",
    "text": "For both types of vectors we use 2.8 billion to-kens as input (ukWaC + Wikipedia + BNC). The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [Cite_Footnote_5] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures. We sanity-check the two spaces on MEN (Bruni et al., 2012), a 3,000 items word similarity data set. cbow sig-nificantly outperforms count (0.80 vs. 0.72 Spear-man correlations with human judgments). count performance is consistent with previously reported results. 6 (De)composition function training The train-ing data sets consist of the 50K most frequent hu, v, pi tuples for each phrase type, for example, hred, car, red.cari or hin, car, in.cari. 7 We con-catenate ~u and v~ vectors to obtain the [U; V ] ma-trix and we use the observed ~p vectors (e.g., the corpus vector of the red.car bigram) to obtain the phrase matrix P. We use these data sets to solve the least squares regression problems in eqs. (1) and (2), obtaining estimates of the composition and decomposition matrices, respectively. For the decomposition function in eq. (3), we replace the observed phrase vectors with those composed with f comp R (u~,~v), where f comp R is the previously esti-mated composition function for relation R."
  },
  {
    "id": 867,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://clic.cimec.unitn.it/composes",
    "section_title": "5 Noun phrase generation 5.2 Recursive decomposition",
    "add_info": "8 This dataset is available at http://clic.cimec.unitn.it/composes",
    "text": "We continue by testing generation through recur-sive decomposition on the task of generating noun-preposition-noun (NPN) paraphrases of adjective-nouns (AN) phrases. We introduce a dataset con-taining 192 AN-NPN pairs (such as pre-election promises\u2192 promises before election), which was created by the second author and additionally cor-rected by an English native speaker. The data set was created by analyzing a list of randomly se-lected frequent ANs. 49 further ANs (with adjec-tives such as amazing and great) were judged not NPN-paraphrasable and were used for the experi-ment reported in Section 7. The paraphrased sub-set focuses on preposition diversity and on includ-ing prepositions which are rich in semantic content and relevant to paraphrasing the AN. This has led to excluding of, which in most cases has the purely syntactic function of connecting the two nouns. The data set contains the following 14 preposi-tions: after, against, at, before, between, by, for, from, in, on, per, under, with, without. [Cite_Footnote_8]"
  },
  {
    "id": 868,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://opus.lingfil.uu.se/",
    "section_title": "6 Noun phrase translation",
    "add_info": null,
    "text": "Adjective-noun translation dataset We ran-domly extract 1,000 AN-AN En-It phrase pairs from a phrase table built from parallel movie sub-titles, available at  http://opus.lingfil.uu.se/ (OpenSubtitles2012, en-it) (Tiedemann, 2012). translation: phrases are composed in source lan-guage and decomposed in target language. Train-ing on composed phrase representations (eq. (3)) (with observed phrase training (eq. 2) results are \u224850% lower)."
  },
  {
    "id": 869,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.usf.edu/Freeassociation",
    "section_title": "4 Experimental Setup",
    "add_info": "3 http://www.usf.edu/Freeassociation.",
    "text": "In order to simulate word association, we used the human norms collected by Nelson et al. (1999). [Cite_Footnote_3] These were established by presenting a large num-ber of participants with a cue word (e.g., rice) and asking them to name an associate word in response (e.g., Chinese, wedding, food, white). For each word, the norms provide a set of associates and the fre-quencies with which they were named. We can thus compute the probability distribution over associates for each cue. Analogously, we can estimate the de-gree of similarity between a cue and its associates using our model (and any of the measures in Sec-tion 3.3). And consequently examine (using corre-lation analysis) the degree of linear relationship be-tween the human cue-associate probabilities and the automatically derived similarity values. We also re-port how many times the word with the highest prob-ability under the model was the first associate in the norms. The norms contain 10,127 unique words in total. Of these, we created semantic representations for the 3,895 words that appeared in our corpus."
  },
  {
    "id": 870,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.codeproject.com/KB/recipes/englishparsing.aspx",
    "section_title": "4 Experimental results",
    "add_info": "R. Northedge. 2005. OpenNLP software http://www.codeproject.com/KB/recipes/englishparsing.aspx",
    "text": "Similar to SLM (Chelba and Jelinek, 2000), af-ter the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the \u201copenNLP\u201d software (Northedge, 2005)  to parse a large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that contain 230 million tokens, then use them to initial-ize model parameters. The parser at \u201dopenNLP\u201d is trained by Upenn treebank with 1 million tokens and there is a mismatch between Upenn treebank and LDC English Gigaword corpus. Nevertheless, ex-perimental results show that this approach is effec-tive to provide initial values of model parameters."
  },
  {
    "id": 871,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py",
    "section_title": "3 Experimental Setup",
    "add_info": "1 https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py",
    "text": "For evaluation we use labeled attachment score (LAS). Significance testing is performed using a randomization test, with the script from the CoNLL 2017 Shared Task. [Cite_Footnote_1]"
  },
  {
    "id": 872,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/UppsalaNLP/uuparser",
    "section_title": "3 Experimental Setup 3.1 The Parser",
    "add_info": "2 https://github.com/UppsalaNLP/uuparser",
    "text": "We use UUParser [Cite_Footnote_2] (de Lhoneux et al., 2017a), which is based on the transition-based parser of Kiperwasser and Goldberg (2016), and adapted to UD. It uses the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). This model allows the construction of non-projective dependency trees (Nivre, 2009)."
  },
  {
    "id": 873,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://nlp.cs.lth.se/lth_srl",
    "section_title": "4 Conclusion",
    "add_info": "1 Our system is freely available for download at http://nlp.cs.lth.se/lth_srl.",
    "text": "We have described a dependency-based system [Cite_Footnote_1] for semantic role labeling of English in the PropBank framework. Our evaluations show that the perfor-mance of our system is close to the state of the art. This holds regardless of whether a segment-based or a dependency-based metric is used. In-terestingly, our system has a complete proposition accuracy that surpasses other systems by nearly 3 percentage points. Our system is the first semantic role labeler based only on syntactic dependency that achieves a competitive performance."
  },
  {
    "id": 874,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "http://www.natcorp.ox.ac.uk",
    "section_title": "2 Pro3Gres and its Design Policy",
    "add_info": null,
    "text": "The parser is fast enough for large-scale appli-cation to unrestricted texts, and it delivers depen-dency relations which are a suitable base for a range of applications. We have used it to parse the entire 100 million words British National Corpus (  http://www.natcorp.ox.ac.uk) and similar amounts of biomedical texts. Its parsing speed is about 500,000 words per hour. The flowchart of the parser can be seen in figure 1."
  },
  {
    "id": 875,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://languagetool.org/",
    "section_title": "4 Experiments 4.4 Evaluation Metrics",
    "add_info": "2 https://languagetool.org/",
    "text": "We use (1) attack success rate \u2013 the ratio of the suc-cessful attacks to total number of attacks, (2) query count \u2013 the number of queries, (3) perturbation rate \u2013 the percentage of words substituted in an in-put and (4) grammatical correctness \u2013 the average grammatical error increase rate (calculated using Language-Tool [Cite_Footnote_2] ) to verify the quality of generated adversarial examples. For all the metrics except attack success rate, lower the value better the re-sult. Also, for all metrics, we report the average score across all the generated adversarial examples on each dataset. Further, we also conducted hu-man evaluation to assess the quality of generated adversarial examples."
  },
  {
    "id": 876,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.itpark.mn/",
    "section_title": "4 Experiments 4.1 Method",
    "add_info": "2 http://www.itpark.mn/ (May, 2006)",
    "text": "We collected 1,118 technical reports published in Mongolian from the \u201cMongolian IT Park\u201d [Cite_Footnote_2] and used them as a Mongolian corpus. The number of phrase types and phrase tokens in our corpus were 110,458 and 263,512, respectively."
  },
  {
    "id": 877,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/SAG-KeLP",
    "section_title": "2 Framework Overview",
    "add_info": "4 https://github.com/SAG-KeLP",
    "text": "K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [Cite_Footnote_4] . Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group of the University of Roma, Tor Vergata."
  },
  {
    "id": 878,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://sag.art.uniroma2.it/demo-software/kelp/",
    "section_title": "2 Framework Overview",
    "add_info": "5 http://sag.art.uniroma2.it/demo-software/kelp/",
    "text": "K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github . Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [Cite_Footnote_5] of the University of Roma, Tor Vergata."
  },
  {
    "id": 879,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://sag.art.uniroma2.it/demo-software/kelp/",
    "section_title": "3 Case Studies in NLP",
    "add_info": "7 http://sag.art.uniroma2.it/demo-software/kelp/",
    "text": "In this Section, the functionalities and use of the learning platform are shown. We apply K E LP to very different NLP tasks, i.e. Sentiment Analysis in Twitter, Text Categorization and Question Clas-sification, providing examples of kernel-based and linear learning algorithms. Further examples are available on the K E LP website [Cite_Footnote_7] where it is shown how to instantiate each algorithm or kernel via JSON and how to add new algorithms, represen-tations and kernels."
  },
  {
    "id": 880,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "3 Case Studies in NLP 3.1 Sentiment Analysis in Twitter",
    "add_info": "8 https://code.google.com/p/word2vec/",
    "text": "The task of Sentiment Analysis in Twitter has been proposed in 2013 during the SemEval competi-tion (Nakov et al., 2013). We built a classifier for the subtask B, i.e. the classification of a tweet with respect to the positive, negative and neutral classes. The contribution of different kernel func-tions is evaluated using the Support Vector Ma-chine learning algorithm. As shown in Table 1, we apply linear (Lin), polynomial (Poly) and Gaus-sian (Rbf) kernels on two different data represen-tations: a Bag-Of-Words model of tweets (BoW ) and a distributional representation (WS). The last is obtained by linearly combining the distri-butional vectors corresponding to the words of a message; these vectors are obtained by applying a Skip-gram model (Mikolov et al., 2013) with the word2vec tool [Cite_Footnote_8] over 20 million of tweets. The lin-ear combination of the proposed kernel functions is also applied, e.g. Poly Bow +Rbf WS . The mean F1-measure of the positive and negative classes (pn) as well as of all the classes (pnn) is shown in Table 1."
  },
  {
    "id": 881,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/",
    "section_title": "3 Case Studies in NLP 3.2 Text Categorization",
    "add_info": "10 http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/",
    "text": "We selected the Text Categorization task on the RCV1 dataset (Lewis et al., 2004) with the setting that can be found on the LibLinear website [Cite_Footnote_10] . In this version of the dataset, CCAT and ECAT are collapsed into a positive class, while GCAT and MCAT are the negative class, resulting in a dataset composed by 20, 242 examples. As shown in Ta-ble 2, we applied the LibLinear, Pegasos and Lin-ear Passive-Aggressive implementations, comput-ing the accuracy and the standard deviation with respect to a 5-fold cross validation strategy."
  },
  {
    "id": 882,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://cogcomp.cs.illinois.edu/Data/QA/QC/",
    "section_title": "3 Case Studies in NLP 3.3 Question Classification",
    "add_info": "11 http://cogcomp.cs.illinois.edu/Data/QA/QC/",
    "text": "The third case study explores the application of Tree Kernels to Question Classification (QC), an inference task required in many Question Answer-ing processes. In this problem, questions writ-ten in natural language are assigned to different classes. A QC system should select the correct class given an instance question. In this setting, Tree Kernels allow to directly model the examples in terms of their parse trees. The reference cor-pus is the UIUC dataset (Li and Roth, 2002), in-cluding 5,452 questions for training and 500 ques-tions for test [Cite_Footnote_11] , organized in six coarse-grained classes, such as HUMAN or LOCATION. Again, Kernel-based SVM has been evaluated adopting the same setup of (Croce et al., 2011). A pure lex-ical model based on a linear kernel over a Bag-of- Words (BoW) is considered a baseline. The con-tribution of the syntactic information is demon-strated by the results achieved by the Partial Tree Kernel (PTK), the Smoothed Partial Tree Kernels (SPTK) and the Compositionally Smoothed Par-tial Tree Kernel (CSPTK), as shown in Table 3."
  },
  {
    "id": 883,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "ATIS and SNIPS, respectively), named entity recog- The results in Table 1 (above) support the observa-nition, part-of-speech tagging and semantic role tion that, in general, bidirectional models do have labeling (Weischedel et al., 2013); and four for a better non-incremental performance than LSTMs sentence classification: intent (Hemphill et al., (except for IntentATIS and ProsCons) and that there marginal difference in the evaluation metrics. We thus present 4 The code is available at  https://github.com/ the results using the pre-trained model only, and leave more briemadu/inc-bidirectional. For more details on exploration of fine-tuning for future work. implementation and data for reproducibility, see Appendix. is an overall considerable improvement in the use of BERT model for all tasks. Truncated training re-duces overall performance but even so BERT with truncated training outperforms all models, even with usual training, in most tasks (except for slot filling and IntentATIS)."
  },
  {
    "id": 884,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.comet.ml/docs/python-sdk/introduction-optimizer/",
    "section_title": "References",
    "add_info": "6 https://www.comet.ml/docs/python-sdk/introduction-optimizer/",
    "text": "\u2022 We use Comet\u2019s Bayes algorithm [Cite_Footnote_6] , which bal- Learning rate"
  },
  {
    "id": 885,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/megagonlabs/opiniondigest",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/megagonlabs/opiniondigest.",
    "text": "We present O PINION D IGEST , an abstrac-tive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained Trans-former model, which verbalizes them into an opinion summary. O PINION D IGEST can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sen-timent. Automatic evaluation on Y ELP data shows that our framework outperforms com-petitive baselines. Human studies on two cor-pora verify that O PINION D IGEST produces informative summaries and shows promising customization capabilities [Cite_Footnote_1] ."
  },
  {
    "id": 886,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://extremereader.megagon.info/",
    "section_title": "3 Evaluation 3.4 Results",
    "add_info": "5 http://extremereader.megagon.info/",
    "text": "Finally, we provide qualitative analysis of the controllable summarization abilities of O PINION D IGEST , which are enabled by input opinion filtering. As discussed in Section 2.2, we filtered input opinions based on predicted aspect categories and sentiment polarity. The examples of controlled summaries (last 4 rows of Table 5) show that O PINION D IGEST can generate aspect/sentiment-specific summaries. These examples have redundant opinions and incorrect extractions in the input, but O PINION D IGEST is able to convert the input opinions into natural summaries. Based on O PINION D IGEST , we have built an online demo (Wang et al., 2020) [Cite_Footnote_5] that allows users to customize the generated summary by specifying search terms."
  },
  {
    "id": 887,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www-personal.umich.edu/\u02dcbenking/resources/mixed-language-annotations-release-v1.0.tgz",
    "section_title": "3 Task Definition 3.1 Evaluation Data",
    "add_info": "1 http://www-personal.umich.edu/\u02dcbenking/resources/mixed-language-annotations-release-v1.0.tgz",
    "text": "For researchers who wish to make use this data, the set of annotations used in this paper is available from the first author\u2019s website [Cite_Footnote_1] ."
  },
  {
    "id": 888,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.unicode.org/udhr/",
    "section_title": "3 Task Definition 3.4 Training Data",
    "add_info": "2 The Universal Declaration of Human Rights is a document created by the United Nations and translated into many lan-guages. As of February 2011 there were 365 versions available from http://www.unicode.org/udhr/",
    "text": "Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights [Cite_Footnote_2] , non-English Wikipedias , the Jehovah\u2019s Witnesses website , and the Rosetta project (Lands-bergen, 1989)."
  },
  {
    "id": 889,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://meta.wikimedia.org/wiki/List",
    "section_title": "3 Task Definition 3.4 Training Data",
    "add_info": "3 As of February 2011, there were 113 Wikipedias in differ-ent languages. Current versions of Wikipedia can be accessed from http://meta.wikimedia.org/wiki/List of Wikipedias",
    "text": "Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights , non-English Wikipedias [Cite_Footnote_3] , the Jehovah\u2019s Witnesses website , and the Rosetta project (Lands-bergen, 1989)."
  },
  {
    "id": 890,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.watchtower.org",
    "section_title": "3 Task Definition 3.4 Training Data",
    "add_info": "4 As of February 2011, there were 310 versions of the site available at http://www.watchtower.org",
    "text": "Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights , non-English Wikipedias , the Jehovah\u2019s Witnesses website [Cite_Footnote_4] , and the Rosetta project (Lands-bergen, 1989)."
  },
  {
    "id": 891,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.gsk.or.jp/catalog/gsk2014-a/",
    "section_title": "3 Experiments 3.1 Settings",
    "add_info": "3 https://www.gsk.or.jp/catalog/gsk2014-a/",
    "text": "Dataset We use the Extended Named Entity Cor-pus for English and Japanese. [Cite_Footnote_3] fine-grained NER (Mai et al., 2018) In this dataset, each NE is assigned one of 200 entity labels defined in the Extended Named Entity Hierarchy (Sekine et al., 2002). For the English dataset, we follow the train-ing/development/test split defined by Mai et al. (2018). For the Japanese dataset, we follow the training/development/test split of Universal Depen-dencies (UD) Japanese-BCCWJ. (Asahara et al., 2018) Table 1 shows the statistics of the dataset."
  },
  {
    "id": 892,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/UniversalDependencies/UD_Japanese-BCCWJ",
    "section_title": "3 Experiments 3.1 Settings",
    "add_info": "4 https://github.com/UniversalDependencies/UD_Japanese-BCCWJ",
    "text": "Dataset We use the Extended Named Entity Cor-pus for English and Japanese. fine-grained NER (Mai et al., 2018) In this dataset, each NE is assigned one of 200 entity labels defined in the Extended Named Entity Hierarchy (Sekine et al., 2002). For the English dataset, we follow the train-ing/development/test split defined by Mai et al. (2018). For the Japanese dataset, we follow the training/development/test split of Universal Depen-dencies (UD) Japanese-BCCWJ. (Asahara et al., 2018) [Cite_Footnote_4] Table 1 shows the statistics of the dataset."
  },
  {
    "id": 893,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/kamalkraj/BERT-NER",
    "section_title": "3 Experiments 3.1 Settings",
    "add_info": "5 We use the open-source NER model utilizing BERT: https://github.com/kamalkraj/BERT-NER.",
    "text": "Model setup As the encoder f(x,X) in Equa-tion 2 in Section 2.1, we use BERT [Cite_Footnote_5] (Devlin et al., 2019), which is a state-of-the-art language model. As the baseline model, we use the general label embedding matrix without considering label com-ponents, i.e., each label embedding W[y] in Equa-tion 2 is randomly initialized and independently learned. In contrast, our proposed model calculates the label embedding matrix from label components (Equations 3 and 4). The only difference between these models is the label embedding matrix, so if a performance gap between them is observed, it stems from this point."
  },
  {
    "id": 894,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We are currently making preparations for the project to be released with an open-source license. The code will be available at  http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/."
  },
  {
    "id": 895,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://kheafield.com/code/kenlm/",
    "section_title": "4 Decoding",
    "add_info": "3 http://kheafield.com/code/kenlm/",
    "text": "The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a varia-tion of cube-pruning (Chiang, 2007). We use KenLM [Cite_Footnote_3] (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estima-tions(Heafield et al., 2012)."
  },
  {
    "id": 896,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.umiacs.umd.edu/~hal/megam/",
    "section_title": "5 Features and Tuning",
    "add_info": "4 http://www.umiacs.umd.edu/~hal/megam/",
    "text": "The optimal weights for each feature are estimated using the Pairwise Ranking Op-timization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM [Cite_Footnote_4] . We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM."
  },
  {
    "id": 897,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://orchid.kuee.kyoto-u.ac.jp/ASPEC/",
    "section_title": "6 Experiments",
    "add_info": "5 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/",
    "text": "For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the official baseline scores. For Japanese-Chinese, we used parallel scientific paper ex-cerpts from the ASPEC [Cite_Footnote_5] corpus and com-pared against the same baseline system as for Japanese-English. The corpora contain 3M parallel sentences for Japanese-English and 670K for Japanese-Chinese."
  },
  {
    "id": 898,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/lex-parser.shtml",
    "section_title": "4 EDU Model",
    "add_info": "1 http://nlp.stanford.edu/software/lex-parser.shtml",
    "text": "Parse trees are obtained using the Stanford Parser [Cite_Footnote_1] , and each clause is treated as an EDU. For a given parent p in the tree and its two children c 1 (associated with vector representation h c 1 ) and c 2 (associated with vector representation h c 2 ), stan-dard recursive networks calculate the vector for parent p as follows: where [h c 1 , h c 2 ] denotes the concatenating vector for children representations h c 1 and h c 2 ; W is a K \u00d7 2K matrix and b is the 1 \u00d7 K bias vector; and f(\u00b7) is the function tanh. Recursive neural models compute parent vectors iteratively until the root node\u2019s representation is obtained, and use the root embedding to represent the whole sentence."
  },
  {
    "id": 899,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/",
    "section_title": "4 Experiments 4.1 Baselines",
    "add_info": "1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/",
    "text": "1-vs-Rest multiclass SVM (1-vs-rest-SVM). This is the standard 1-vs-Rest multiclass SVM with Platt Probability Estimation (Platt, 2000), and it is implemented based on LIBSVM [Cite_Footnote_1] (version 3.20) (Chang and Lin, 2011). It works in the same way as the proposed cbsSVM (Section 3.4) except that it uses the document space classification. Linear ker-nel is used as it is shown by many researchers that linear SVM performs the best for text classification (Joachims, 1998; Colas and Brazdil, 2006). 1-vs-Set Machine (1-vs-set-linear). For this base-line (Scheirer et al., 2013), we use all the default parameter settings in the original paper. That is, the near and far plane pressures are set at \ud835\udc5d ! = 1.6 and \ud835\udc5d ! = 4 respectively; regularization constant \ud835\udf06 ! = 1 and no explicit hard constraints are used on the training error (\ud835\udefc = 0, \ud835\udefd = 1)."
  },
  {
    "id": 900,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://github.com/Vastlab/liblinear.git",
    "section_title": "4 Experiments 4.1 Baselines",
    "add_info": "2 https://github.com/Vastlab/liblinear.git",
    "text": "All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine [Cite_Footnote_2] , W-SVM and P I - SVM , and Exploratory learning ) was provided by the authors of their original papers."
  },
  {
    "id": 901,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://github.com/ljain2/libsvm-openset",
    "section_title": "4 Experiments 4.1 Baselines",
    "add_info": "3 https://github.com/ljain2/libsvm-openset",
    "text": "All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM [Cite_Footnote_3] , and Exploratory learning ) was provided by the authors of their original papers."
  },
  {
    "id": 902,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip",
    "section_title": "4 Experiments 4.1 Baselines",
    "add_info": "4 http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip",
    "text": "All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [Cite_Footnote_4] ) was provided by the authors of their original papers."
  },
  {
    "id": 903,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/microsoft/Multilingual-Model-Transfer",
    "section_title": "References",
    "add_info": "1 The code is available at https://github.com/microsoft/Multilingual-Model-Transfer.",
    "text": "Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applica-ble to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by lever-aging labeled data from other (source) lan-guages. In this work, we focus on the multi-lingual transfer setting where training data in multiple source languages is leveraged to fur-ther boost target language performance. Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at in-stance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target lan-guage and each individual source language [Cite_Footnote_1] . This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource set-ting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classifi-cation and sequence tagging tasks including a large-scale industry dataset."
  },
  {
    "id": 904,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/",
    "section_title": "4 Experiments 4.1 Cross-Lingual Semantic Slot Filling 4.1.1 Results",
    "add_info": "4 https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/",
    "text": "MT baselines employ machine translation (MT) for cross-lingual transfer. In particular, the train-on-trans(lation) method translates the entire En-glish training set into each target language which are in turn used to train a supervised system on the target language. On the other hand, the test-on-trans(lation) method trains an English sequence tagger, and utilizes MT to translate the test set of each target language into English in order to make predictions. In this work, we adopt the Mi-crosoft Translator [Cite_Footnote_4] , a strong commercial MT sys-tem. Note that for a MT system to work for se-quence tagging tasks, word alignment informa-tion must be available, in order to project word-level annotations across languages. This rules out many MT systems such as Google Translate since they do not provide word alignment information through their APIs."
  },
  {
    "id": 905,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://aka.ms/WikiQA",
    "section_title": "3 Method 3.3 SIN with Convolution (SIN-CONV)",
    "add_info": "2 http://aka.ms/WikiQA",
    "text": "In SIN-CONV, we first use a convolution layer to obtain phrase representations for the two sen-tences s 1 and s 2 , and the SIN interaction proce-dure is then applied to these phrase representations as before to model phrase interactions. The aver-age of all hidden states are treated as sentence vec-tors v scnn 1 and v scnn [Cite_Footnote_2] . Thus, SIN-CONV is SIN with word vectors substituted by phrase vectors. The two phrase-based sentence vectors are then fed to a classifier along with the two word-based sentence vectors together for classification."
  },
  {
    "id": 906,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "4 Experiments 4.1 Answer Selection 4.1.2 Setup",
    "add_info": "3 http://nlp.stanford.edu/projects/glove/",
    "text": "We use the 100-dimensional GloVe vectors [Cite_Footnote_3] (Pen-nington et al., 2014) to initialize our word embed-dings, and those words that do not appear in Glove vectors are treated as unknown. The dimension of all hidden states is set to 100 as well. The window size of the convolution layer is 2. To avoid overfit-ting, dropout is introduced to the sentence vectors, namely setting some dimensions of the sentence vectors to 0 with a probability p (0.5 in our experi-ment) randomly. No handcrafted features are used in our methods and the baselines."
  },
  {
    "id": 907,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://compprag.christopherpotts.net/swda.html",
    "section_title": "4 Experiments 4.2 Dialogue Act Analysis 4.2.1 Dataset",
    "add_info": "5 http://compprag.christopherpotts.net/swda.html.",
    "text": "We use the Switch-board Dialogue Act (SwDA) corpus (Calhoun et al., 2010) in our experiments [Cite_Footnote_5] . SwDA contains the transcripts of several people discussing a given topic on the telephone. There are 42 dialogue act tags in SwDA, and we list the 10 most frequent tags in Table 3."
  },
  {
    "id": 908,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://web.stanford.edu/%7ejurafsky/ws97/",
    "section_title": "4 Experiments 4.2 Dialogue Act Analysis 4.2.1 Dataset",
    "add_info": "7 http://web.stanford.edu/%7ejurafsky/ws97/",
    "text": "The same data split as in Stolcke et al. (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [Cite_Footnote_7] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues)."
  },
  {
    "id": 909,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://nlp.lsi.upc.edu/asiya/",
    "section_title": "1 Introduction",
    "add_info": "1 http://nlp.lsi.upc.edu/asiya/",
    "text": "Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to in-duce models from examples of sentence transla-tions annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statis-tical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA [Cite_Footnote_1] and Q U E ST (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014)."
  },
  {
    "id": 910,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.quest.dcs.shef.ac.uk/",
    "section_title": "1 Introduction",
    "add_info": "2 http://www.quest.dcs.shef.ac.uk/",
    "text": "Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to in-duce models from examples of sentence transla-tions annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statis-tical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA and Q U E ST [Cite_Footnote_2] (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014)."
  },
  {
    "id": 911,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://scikit-learn.org/",
    "section_title": "2 Architecture",
    "add_info": "3 http://scikit-learn.org/",
    "text": "Figure 1 depicts the architecture of Q U E ST ++ . Document and Paragraph classes are used for document-level feature extraction. A Document is a group of Paragraphs, which in turn is a group of Sentences. Sentence is used for both word- and sentence-level feature extraction. A Feature Pro-cessing Module was created for each level. Each processing level is independent and can deal with the peculiarities of its type of feature. Machine learning Q U E ST ++ provides scripts to interface the Python toolkit scikit-learn [Cite_Footnote_3] (Pedregosa et al., ). This module is indepen-dent from the feature extraction code and uses the extracted feature sets to build and test QE models. The module can be configured to run different regression and classification algorithms, feature selection methods and grid search for hyper-parameter optimisation. Algorithms from scikit-learn can be easily integrated by modifying existing scripts."
  },
  {
    "id": 912,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/lex-parser.shtml",
    "section_title": "3 Features 3.1 Word level",
    "add_info": "4 http://nlp.stanford.edu/software/lex-parser.shtml",
    "text": "The syntactic backoff behavior is calculated in an analogous fashion: it verifies for the existence of n-grams of POS tags in a POS-tagged LM. The POS tags of target sentence are produced by the Stanford Parser [Cite_Footnote_4] (integrated in Q U E ST ++ )."
  },
  {
    "id": 913,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.chokkan.org/software/crfsuite/",
    "section_title": "2 Architecture",
    "add_info": "N. Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields. http://www.chokkan.org/software/crfsuite/.",
    "text": "For word-level prediction, Q U E ST ++ provides an interface for CRFSuite (Okazaki, 2007)  , a se-quence labelling C++ library for Conditional Ran-dom Fields (CRF). One can configure CRFSuite training settings, produce models and test them."
  },
  {
    "id": 914,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cs.princeton.edu/\u02dcblei/lda-c/index.html",
    "section_title": "5 Experimental Setup",
    "add_info": "3 Available from http://www.cs.princeton.edu/\u02dcblei/lda-c/index.html.",
    "text": "The model presented in Section 4 has a few pa-rameters that must be selected empirically on the development set. These include the vocabulary size, which is dependent on the n words with the high-est tf \u2217idf scores in each document, and the num-ber of topics for the LDA-based re-ranker. We ob-tained best performance with n set to 100 (no cutoff was applied in cases where the vocabulary was less than 100). We trained an LDA model with 20 top-ics on our document collection using David Blei\u2019s implementation. [Cite_Footnote_3] We used this model to re-rank the output of our annotation model according to the three most likely topics in each document."
  },
  {
    "id": 915,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://bit.ly/fsdg_emnlp2019",
    "section_title": "4 Dialogue Knowledge Transfer Network 4.2 Stage 2. Transfer",
    "add_info": "1 https://bit.ly/fsdg_emnlp2019",
    "text": "DiKTNet is visualized in Figure 2. The model (as well as its variants listed above) is imple-mented in PyTorch (Paszke et al., 2017), and the code is openly available [Cite_Footnote_1] ."
  },
  {
    "id": 916,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Elbria/utdsm_naacl2018",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/Elbria/utdsm_naacl2018",
    "text": "To our knowledge, this is the first time that map-pings between semantic spaces are applied to the problem of learning multiple embeddings for pol-ysemous words. Our multi-topic word representa-tions are evaluated on the contextual semantic sim-ilarity task and yield state-of-the-art performance compared to other unsupervised multi-prototype word embedding approaches. We further per-form experiments on two NLP downstream tasks: text classification and paraphrase identification and demonstrate that our learned word represen-tations consistently provide higher performance than single-prototype word embedding models. The code of the present work is publicly avail-able [Cite_Footnote_1] ."
  },
  {
    "id": 917,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://qwone.com/jason/20Newsgroups/",
    "section_title": "4 Experimental Setup 4.5 Downstream NLP Tasks",
    "add_info": "6 http://qwone.com/jason/20Newsgroups/",
    "text": "Text classification. We used the 20NewsGroup [Cite_Footnote_6] dataset, which consists of about 20000 docu-ments. Our goal is to classify each document into one of the 20 different newsgroups based on its content."
  },
  {
    "id": 918,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://contrib.scikit-learn.org/lightning/",
    "section_title": "4 Early Prediction of Controversy 4.1 Comparing Text Models",
    "add_info": null,
    "text": "16 We cross-validate regularization strength 10\u02c6(-100,-5,- 4,-3,-2,-1,0,1), model type (SVM vs. Logistic L1 vs. Logistic L2 vs. Logistic L1/L2), and whether or not to apply feature standardization for each feature set and cross-validation split separately. These are trained using lightning (  http://contrib.scikit-learn.org/lightning/). W2V. We consider a mean, 300D word2vec (Mikolov et al., 2013) embedding representation, computed from a GoogleNews corpus."
  },
  {
    "id": 919,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/trangvu/alil-dream",
    "section_title": "1 Introduction",
    "add_info": "1 Source code is available at https://github.com/trangvu/alil-dream",
    "text": "Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [Cite_Footnote_1] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task."
  },
  {
    "id": 920,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/cambridgeltl/BioNLP-2016",
    "section_title": "4 Experiments 4.3 Biomedical Named Entity Recognition",
    "add_info": "4 https://github.com/cambridgeltl/BioNLP-2016",
    "text": "The experiment setup for policy transfer and un-derlying model is kept the same as in the NER ex-periments in Section 4.2. For the word embed-ding, we use the pre-trained English BioNLP em-bedding [Cite_Footnote_4] (Chiu et al., 2016) with 200 dimension. Vocabulary size is set to 20,000."
  },
  {
    "id": 921,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://nlp.jhu.edu/rams/",
    "section_title": "4 Experiments 4.1 Experiment Setup",
    "add_info": "1 https://nlp.jhu.edu/rams/",
    "text": "Dataset. We conduct experiments on the RAMS [Cite_Footnote_1] dataset, which is annotated with 139 event types and 65 corresponding argument roles. Each in-stance consists of a 5-sentences context around the typed event trigger, and there are several typed ar-guments to be extracted. RAMS dataset consists of 7329, 924, and 871 instances in the training, devel-opment, and test set, respectively."
  },
  {
    "id": 922,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://di.unipi.it/\u02dcgulli/AG_corpus_of_news_articles.html",
    "section_title": "6 Experiments and Results 6.1 Data",
    "add_info": "9 https://di.unipi.it/\u02dcgulli/AG_corpus_of_news_articles.html",
    "text": "We use five datasets for text classification follow-ing Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al., 2011); (iii) ADR Tweets (Sarker et al., 2015); (iv) AG News; [Cite_Footnote_9] and (v) MIMIC Anemia (Johnson et al., 2016). See Table 1 for detailed data statistics."
  },
  {
    "id": 923,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://archive.org/details/twitterstream",
    "section_title": "3 Experimental Setup",
    "add_info": "4 https://archive.org/details/twitterstream",
    "text": "Social Media Corpora Our English Twitter corpus is obtained from Archive Team\u2019s Twitter stream grab [Cite_Footnote_4] . The Chinese Weibo corpus comes from Open Weiboscope Data Access 5 (Fu et al., 2013). Both corpora cover the whole year of 2012. We then randomly down-sample each corpus to 100 million messages where each message con-tains at least 10 characters, normalize the text (Han et al., 2012), lemmatize the text (Manning et al., 2014) and use LTP (Che et al., 2010) to perform word segmentation for the Chinese corpus."
  },
  {
    "id": 924,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://weiboscope.jmsc.hku.hk/datazip/",
    "section_title": "3 Experimental Setup",
    "add_info": "5 http://weiboscope.jmsc.hku.hk/datazip/",
    "text": "Entity Linking and Word Embedding Entity linking is a preprocessing step which links vari-ous entity mentions (surface forms) to the identity of corresponding entities. For the Twitter corpus, we use Wikifier (Ratinov et al., 2011; Cheng and Roth, 2013), a widely used entity linker in En-glish. Because no sophisticated tool for Chinese short text is available, we implement our own tool that is greedy for high precision. We train En-glish and Chinese monolingual word embedding respectively using word2vec\u2019s skip-gram method with a window size of [Cite_Footnote_5] (Mikolov et al., 2013b)."
  },
  {
    "id": 925,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.bing.com/translator/api/Dictionary/Lookup?from=en&to=zh-CHS&text=",
    "section_title": "3 Experimental Setup",
    "add_info": "6 http://www.bing.com/translator/api/Dictionary/Lookup?from=en&to=zh-CHS&text=",
    "text": "Bilingual Lexicon Our bilingual lexicon is collected from Microsoft Translator [Cite_Footnote_6] , which trans-lates English words to multiple Chinese words with confidence scores. Note that all named en-tities and slang terms used in the following exper-iments are excluded from this bilingual lexicon."
  },
  {
    "id": 926,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.chinasmack.com/glossary",
    "section_title": "5 Task 2: Finding most similar words for slang across languages 5.1 Ground Truth",
    "add_info": "8 https://www.chinasmack.com/glossary",
    "text": "Slang Terms. We collect the Chinese slang terms from an online Chinese slang glossary [Cite_Footnote_8] consisting of 200 popular slang terms with English expla-nations. For English, we resort to a slang word Truth Sets. For each Chinese slang term, its truth set is a set of words extracted from its English ex-planation. For example, we construct the truth set of the Chinese slang term \u201c\u4e8c\u767e\u4e94\u201d by manually extracting significant words about its slang mean-ings (bold) in the glossary:"
  },
  {
    "id": 927,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://onlineslangdictionary.com/word-list/",
    "section_title": "5 Task 2: Finding most similar words for slang across languages 5.1 Ground Truth",
    "add_info": "9 http://onlineslangdictionary.com/word-list/",
    "text": "Table 5: ACS Sum Results of Slang Translation list from OnlineSlangDictionary [Cite_Footnote_9] with explana-tions and downsample the list to 200 terms."
  },
  {
    "id": 928,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://cc-cedict.org/wiki/",
    "section_title": "5 Task 2: Finding most similar words for slang across languages 5.2 Baseline and Our Methods",
    "add_info": "10 https://cc-cedict.org/wiki/",
    "text": "We propose two types of baseline methods for this task. The first is based on well-known on-line translators, namely Google (Gg), Bing (Bi) and Baidu (Bd). Note that experiments using them are done in August, 2017. Another baseline method for Chinese is CC-CEDICT [Cite_Footnote_10] (CC), an on-line public Chinese-English dictionary, which is constantly updated for popular slang terms."
  },
  {
    "id": 929,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.urbandictionary.com/",
    "section_title": "5 Task 2: Finding most similar words for slang across languages 5.2 Baseline and Our Methods",
    "add_info": "11 http://www.urbandictionary.com/",
    "text": "Considering situations where many slang terms have literal meanings, it may be unfair to re-trieve target terms from such machine translators by solely inputing slang terms without specific contexts. Thus, we utilize example sentences of their slang meanings from some websites (mainly from Urban Dictionary [Cite_Footnote_11] ). The following example shows how we obtain the target translation terms for the slang word \u201cfruitcake\u201d (an insane person):"
  },
  {
    "id": 930,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.englishbaby.com/lessons/4349/slang/fruitcake",
    "section_title": "5 Task 2: Finding most similar words for slang across languages 5.2 Baseline and Our Methods",
    "add_info": "12 http://www.englishbaby.com/lessons/4349/slang/fruitcake",
    "text": "Input sentence: Oh man, you don\u2019t want to date that girl. She\u2019s always drunk and yelling. She is a total fruitcake. [Cite_Footnote_12]"
  },
  {
    "id": 931,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "5 Task 2: Finding most similar words for slang across languages 5.3 Experimental Results",
    "add_info": "13 https://nlp.stanford.edu/projects/glove/",
    "text": "The above equation illustrates such computation, where A and B are the two word sets: A is the truth set and B is a similar list produced by each method. In the previous case of \u201c\u4e8c\u767e\u4e94\u201d (Sec-tion 5.1), A is {foolish, stubborn, rude, impetu-ous} while B can be {imbecile, brainless, scum-bag, imposter}. A i and B j denote the word vector of the i th word in A and j th word in B respec-tively. The embeddings used in ACS computations are pre-trained GloVe word vectors [Cite_Footnote_13] and thus the computation is fair among different methods."
  },
  {
    "id": 932,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm",
    "section_title": "4 Evaluation 4.2 Results",
    "add_info": "6 Scripts for running these tests are available at http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm",
    "text": "We calculated statistical significance for the main results on the development section using bootstrap random sampling. [Cite_Footnote_6] After re-sampling 1000 times, significance was calculated using a paired t-test (999 d.f.). The results indicated that lp-only exceeded the baseline, lp-ngram and lp-syn exceeded lp-only, and the full model exceeded lp-syn, with p < 0.0001 in each case."
  },
  {
    "id": 933,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://www.icst.pku.edu.cn/lcwm/grass",
    "section_title": "References",
    "add_info": null,
    "text": "This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of fac-torization together with different higher-order features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrat-ing heterogeneous factorizations results in intractability in decoding. We pro-pose a principled method to obtain opti-mal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a signifi-cant improvement over the best published result in the literature. Our implementa-tion is available at  http://www.icst.pku.edu.cn/lcwm/grass."
  },
  {
    "id": 934,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://github.com/noutenki/vex",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "VEX is available as free, open-source soft-ware for download at  http://github.com/noutenki/vex and as a web service at http://cosyne.h-its.org/vex."
  },
  {
    "id": 935,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://cosyne.h-its.org/vex",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "VEX is available as free, open-source soft-ware for download at http://github.com/noutenki/vex and as a web service at  http://cosyne.h-its.org/vex."
  },
  {
    "id": 936,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://github.com/wikilinks/neleval",
    "section_title": "4 Implementation",
    "add_info": "5 http://github.com/wikilinks/neleval",
    "text": "VEX consists of three main components. The input component, implemented in Java 8, reads gold and system annotations files, as well as the original documents. Currently, the annotation for-mat read by the official TAC 2014 scorer [Cite_Footnote_5] , as well as a simple JSON input format are supported. All system and gold character offset ranges contained in the input files are converted into HTML spans and inserted into the document text. Since HTML elements are required to conform to a tree struc-ture, any overlap or nesting of spans is handled by breaking up such spans into non-overlapping sub-spans."
  },
  {
    "id": 937,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/jknack/handlebars.java",
    "section_title": "4 Implementation",
    "add_info": "7 https://github.com/jknack/handlebars.java",
    "text": "The output component employs a tem-plate engine [Cite_Footnote_7] to convert the data collected by the processing component into HTML and JavaScript for handling display and user interac-tion in the web browser."
  },
  {
    "id": 938,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/abetusk/euclideanmst.js",
    "section_title": "4 Implementation 4.1 Design Decisions",
    "add_info": "9 https://github.com/abetusk/euclideanmst.js. This library employs Kruskal\u2019s algorithm (Kruskal, 1956) for finding MSTs.",
    "text": "Since the actual positions of mention span el-ements on the user\u2019s screen depend on various user environment factors such as font size and browser window dimensions, the MSTs of dis-played entities are computed using a client-side JavaScript library [Cite_Footnote_9] and are automatically redrawn if the browser window is resized. Drawing of edges is performed via jsPlumb , a highly cus-tomizable library for line drawing in HTML doc-uments."
  },
  {
    "id": 939,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.jsplumb.org",
    "section_title": "4 Implementation 4.1 Design Decisions",
    "add_info": "10 http://www.jsplumb.org",
    "text": "Since the actual positions of mention span el-ements on the user\u2019s screen depend on various user environment factors such as font size and browser window dimensions, the MSTs of dis-played entities are computed using a client-side JavaScript library and are automatically redrawn if the browser window is resized. Drawing of edges is performed via jsPlumb [Cite_Footnote_10] , a highly cus-tomizable library for line drawing in HTML doc-uments."
  },
  {
    "id": 940,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/disfluency-detection/downloads",
    "section_title": "4 Experiments 4.2 Experimental results 4.2.1 Performance of disfluency detection on English Swtichboard corpus",
    "add_info": "2 The toolkit is available at https://code.google.com/p/disfluency-detection/downloads.",
    "text": "The evaluation results of both disfluency detec-tion and parsing accuracy are presented in Table 2. The accuracy of M 3 N directly refers to the re-sults reported in (Qian and Liu, 2013). The re-sults of M 3 N \u2020 come from our experiments with the toolkit [Cite_Footnote_2] released by Qian and Liu (2013) which uses our data set with the same pre-processing. It is comparable between our models and the L2R parsing based joint model presented by Honni-bal and Johnson (2014), as we all conduct experi-ments on the same pre-processed data set. In order to compare parsing accuracy, we use the CRF and M 3 N \u2020 model to pre-process the test set by remov-ing all the detected disfluencies, then evaluate the parsing performance on the processed set. From the table, our BCT model with new disfluency fea-tures achieves the best performance on disfluency detection as well as dependency parsing."
  },
  {
    "id": 941,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://homepages.inf.ed.ac.uk/lzhang10/maxent.html",
    "section_title": "5 Experiments",
    "add_info": "4 http://homepages.inf.ed.ac.uk/lzhang10/maxent.html",
    "text": "All the three systems share with the same target-side parsed, word-aligned training data. The his-togram pruning parameter b is set to 100 and phrase table limit is set to 20 for all the three sys-tems. Moses shares the same feature set with our system except for the dependency features. For the bottom-up string-to-dependency system, we in-cluded both well-formed and ill-formed structures in chart parsing. To control the grammar size, we only extracted \u201ctight\u201d initial phrase pairs (i.e., the boundary words of a phrase must be aligned) as suggested by (Chiang, 2007). For our system, we used the Le Zhang\u2019s maximum entropy modeling toolkit to train the shift-reduce parsing model after extracting 32.6M events from the training data. [Cite_Footnote_4] We set the iteration limit to 100. The accuracy on the training data is 90.18%."
  },
  {
    "id": 942,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cis.upenn.edu/dbikel/software.html",
    "section_title": "5 Experiments",
    "add_info": "6 Available at http://www.cis.upenn.edu/dbikel/software.html",
    "text": "While the main benefit of our joint model is the ability to get a consistent output over both types of annotations, we also found that modeling the parse and named entities jointly resulted in improved per-formance on both. When looking at these numbers, it is important to keep in mind that the sizes of the training and test sets are significantly smaller than the Penn Treebank. The largest of the six datasets, CNN, has about one seventh the amount of training data as the Penn Treebank, and the smallest, MNB, has around 500 sentences from which to train. Parse performance was improved by the joint model for five of the six datasets, by up to 1.36%. Looking at the parsing improvements on a per-label basis, the largest gains came from improved identication of NML consituents, from an F-score of 45.9% to 57.0% (on all the data combined, for a total of 420 NML constituents). This label was added in the new treebank annotation conventions, so as to identify in-ternal left-branching structure inside previously flat NPs. To our surprise, performance on NPs only in-creased by 1%, though over 12,949 constituents, for the largest improvement in absolute terms. The sec-ond largest gain was on PPs, where we improved by 1.7% over 3,775 constituents. We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel\u2019s randomized parsing evaluation com-parator [Cite_Footnote_6] and found that both the precision and recall gains were significant at p \u2264 0.01."
  },
  {
    "id": 943,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.ntu.edu.au/education/langs/ielex/IE-DATA1",
    "section_title": "1 Introduction 1.1 Previous work",
    "add_info": "I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.",
    "text": "The task of reconstructing phylogenetic trees for languages has been studied by several authors. These approaches descend from glottochronology (Swadesh, 1955), which views a language as a col-lection of shared cognates but ignores the structure of those cognates. This information is obtained from manually curated cognate lists such as the data of Dyen et al. (1997)  ."
  },
  {
    "id": 944,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.ntu.edu.au/education/langs/ielex/IE-DATA1",
    "section_title": "4 Experiments 4.1 Corpus",
    "add_info": "I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.",
    "text": "Since we used automatic tools for preparing our corpus rather than careful linguistic analysis, our cognate list is much noiser in terms of the pres-ence of borrowed words and phonemeic transcrip-tion errors compared to the ones used by previous approaches (Swadesh, 1955; Dyen et al., 1997  ). The benefit of our mechanical preprocessing is that more cognate data can easily be made available, allowing us to effectively train richer models. We show in the rest of this section that our phonological model can indeed overcome this noise and recover meaningful patterns from the data. length |w i |+|w j | and d is the Levenshtein distance."
  },
  {
    "id": 945,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://stream.twitter.com",
    "section_title": "Corpora",
    "add_info": "1 http://stream.twitter.com",
    "text": "Traditional tracking datasets are unsuitable to approach tracking at scale as they consist of only a few thousand documents and several hundred topics (Allan, 2002). We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter\u2019s streaming API [Cite_Footnote_1] . The tweets are order by their time-stamps. Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update."
  },
  {
    "id": 946,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://en.wikipedia.org/wiki/Wikipediadatabase",
    "section_title": "Corpora",
    "add_info": "2 http://en.wikipedia.org/wiki/Wikipediadatabase",
    "text": "Traditional tracking datasets are unsuitable to approach tracking at scale as they consist of only a few thousand documents and several hundred topics (Allan, 2002). We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter\u2019s streaming API . The tweets are order by their time-stamps. Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [Cite_Footnote_2] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update."
  },
  {
    "id": 947,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://newsblaster.cs.columbia.edu",
    "section_title": "5 Validating the results on current news",
    "add_info": "4 http://newsblaster.cs.columbia.edu",
    "text": "We tested the classifiers on data different from that provided by DUC, and also tested human consen-sus on the hearer-new/old distinction. For these pur-poses, we downloaded 45 clusters from one day\u2019s output from Newsblaster [Cite_Footnote_4] . We then automatically compiled the list of people mentioned in the ma-chine summaries for these clusters. There were 107 unique people that appeared in the machine sum-maries, out of 1075 people in the input clusters."
  },
  {
    "id": 948,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.icsi.berkeley.edu/~framenet/",
    "section_title": "2 FrameNet",
    "add_info": "1 http://www.icsi.berkeley.edu/~framenet/",
    "text": "This section presents the semantic role paradigm and the role-annotated corpus on which the present study is based. FrameNet [Cite_Footnote_1] is a lexical resource based on Fillmore\u2019s Frame Semantics (Fillmore, 1985). It de-scribes frames, representations of prototypical situa-tions. Each frame provides its set of semantic roles, the entities or concepts pertaining to the prototypi-cal situation. Each frame is further associated with a set of target predicates (nouns, verbs or adjectives), occurrences of which can introduce the frame."
  },
  {
    "id": 949,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://datasets.maluuba.com/Frames",
    "section_title": "4 Experiments and Results 4.3 User Simulator",
    "add_info": "1 https://datasets.maluuba.com/Frames",
    "text": "Training reinforcement learners is challenging be-cause they need an environment to interact with. In the dialogue research community, it is common to use simulated users as shown in Figure 3 for this purpose (Schatzmann et al., 2007; Asri et al., 2016). In this work, we adapted the publicly-available user simulator, developed by Li et al. (2016), to the composite task-completion dialogue setting using the human-human conversation data described in Section 4.1. During training, the simulator provides the agent with an (extrinsic) re-ward signal at the end of the dialogue. A dialogue is considered to be successful only when a travel plan is made successfully, and the information provided by the agent satisfies user\u2019s constraints. At the end of each dialogue, the agent receives a positive reward of 2\u21e4max turn (max turn = 60 in our experiments) for success, or a negative re-ward of max turn for failure. Furthermore, at each turn, the agent receives a reward of [Cite_Footnote_1] so that shorter dialogue sessions are encouraged."
  },
  {
    "id": 950,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://huggingface.co/transformers/",
    "section_title": "2 Constrained Verification 2.3 Methodology Taker datasets following Clark et al. (2020).",
    "add_info": "7 https://huggingface.co/transformers/",
    "text": "We compare the above-proposed curricula (CWA, Skip\u2013fact) against a baseline curriculum (Original) where we initialize the verification models with standard pretrained BERT weights (bert-base-cased). We use huggingface transform-ers (Wolf et al., 2019) in all of our experiments. [Cite_Footnote_7]"
  },
  {
    "id": 951,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://pytorch.org/docs/master/index.html",
    "section_title": "4 Experiments 4.3 Implementation Detail",
    "add_info": "6 http://pytorch.org/docs/master/index.html",
    "text": "We use PyTorch [Cite_Footnote_6] to implement our models."
  },
  {
    "id": 952,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "4 Experiments 4.3 Implementation Detail",
    "add_info": "7 https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "For Req2Seq and AP-Ref2Seq, we set the hid-den size and word embedding size as 256. We ap-ply a dropout rate of 0.5 for the encoder and 0.2 for the decoder. The size of the justification refer-ence l r is set to 5 and the number of fine-grained aspects K in the user persona and item profile is set to 30. We train the model using Adam with learning rate 2e \u22124 and stop training either when it reaches 20 epochs or the perplexity does not im-prove (on the Dev set). For ACMLM, we build our model based on the BERT implementation from HuggingFace. [Cite_Footnote_7] We initialize our decoder using the pre-trained \u2018Bert-base\u2019 model and set the max sequence length to 30. We train the model for 5 epochs using Adam with learning rate 2e \u22125 . For models using beam search, we set the beam size as 10. For models using \u2018top-k\u2019 sampling, we set k to 5. For ACMLM, we use a burn-in step equal to the length of the initial sequence. Our data and code are available online. 8"
  },
  {
    "id": 953,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/YatinChaudhary/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We evaluate the effectiveness of multi-source neural topic modeling in multi-view embedding spaces using 7 (5 low-resource and 2 high-resource) target and 5 (high-resource) source corpora from news and medical domains, consisting of short-text, long-text, small and large document col-lections. We have shown state-of-the-art re-sults with significant gains quantified by gener-alization (perplexity), interpretability (topic co-herence) and text retrieval. The code is avail-able at  https://github.com/YatinChaudhary/ Multi-view-Multi-source-Topic-Modeling . pretrained word embeddings from a WordPool at each autoregressive step i. Double circle \u2192 multinomial (soft-max) unit (Larochelle and Lauly, 2012). (Right) DocNADE (GVT+MST): Multi-source transfer learning in NTM by introducing pretrained (latent) topic embeddings from a TopicPool, illustrating topic alignments between source and target corpora. Each outgoing row from Z k \u2208R H\u00d7K signify a topic embedding of corresponding kth source corpus, DC k . Here, NTM refers to a DocNADE (Larochelle and Lauly, 2012) based Neural Topic Model."
  },
  {
    "id": 954,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/Franck-Dernoncourt/pubmed-rct",
    "section_title": "A Data Description",
    "add_info": null,
    "text": "To prepare knowledge base of word embed-ings (local semantics) and latent topics (global semantics) features, we use the following six datasets in the source S: (1) 20NS: 20News-Groups corpus, a collection of news stories from nltk.corpus . (2) TMN: The Tag My News (TMN) news dataset. (3) R21578: Reuters corpus, a collection of new stories from nltk. corpus . (4) AGnews: AGnews data sellection. PubMed: Medical abstracts of randomized con-trolled trials. Source:  https://github.com/Franck-Dernoncourt/pubmed-rct."
  },
  {
    "id": 955,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/clarkkev/deep-coref",
    "section_title": "1 Introduction",
    "add_info": "1 Code and trained models are available at https://github.com/clarkkev/deep-coref.",
    "text": "Our system uses little manual feature engineer-ing, which means it is easily extended to multiple languages. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset. The cluster-ranking model signifi-cantly outperforms a mention-ranking model that does not use entity-level information. We also show that using an easy-first strategy improves the performance of the cluster-ranking model. Our fi-nal system achieves CoNLL F 1 scores of 65.29 for English and 63.66 for Chinese, substantially out-performing other state-of-the-art systems. [Cite_Footnote_1]"
  },
  {
    "id": 956,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.statmt.org/moses/giza/GIZA++.html",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.statmt.org/moses/giza/GIZA++.html",
    "text": "In statistical machine translation (SMT), word alignment plays an essential role in obtaining phrase tables (Och and Ney, 2004; Koehn et al., 2003) or syntactic transformation rules (Chiang, 2007; Shen et al., 2008). IBM models (Brown et al., 1993), which are based on word sequences, have been widely used for obtaining word align-ments because they are fast and their implementa-tion is available as GIZA++. [Cite_Footnote_1]"
  },
  {
    "id": 957,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://jasonriesa.github.io/nile/",
    "section_title": "1 Introduction",
    "add_info": "2 http://jasonriesa.github.io/nile/",
    "text": "Recently, a hierarchical alignment model (whose implementation is known as Nile [Cite_Footnote_2] ) (Riesa et al., 2011), which performs better than IBM models, has been proposed. In the hierarchi-cal alignment model, both source and target con-stituency trees are used for incorporating syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees. It achieved significantly better results than the IBM Model4 in Arabic-English and Chinese-English word alignment tasks, even though the model was trained on only 2,280 and 1,102 par-allel sentences as gold standard alignments. How-ever, their models rely only on 1-best source and target side parse trees, which are not necessarily good for word alignment tasks."
  },
  {
    "id": 958,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/hitochan777/mt-tools/releases/tag/1.0.1",
    "section_title": "3 Experiments 3.1 Experimental Settings",
    "add_info": "3 The conversion program is available at https://github.com/hitochan777/mt-tools/releases/tag/1.0.1",
    "text": "We conducted alignment experiments on the Japanese-English language pair. For dependency parsers, we used KNP (Kawahara and Kurohashi, 2006) for Japanese and Berkeley Parser (Petrov and Klein, 2007) for English. We converted con-stituent parse trees obtained by Berkeley Parser to dependency parse trees using rules. [Cite_Footnote_3] We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200. The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile , the Bayesian subtree alignment model (Nakazawa model) (Nakazawa and Kurohashi, 2011) and IBM Model4. We used Nile without automatically ex-tracted rule features and constellation features to make a fair comparison with our model."
  },
  {
    "id": 959,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://lotus.kuee.kyoto-u.ac.jp/ASPEC/",
    "section_title": "3 Experiments 3.1 Experimental Settings",
    "add_info": "4 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/",
    "text": "We conducted alignment experiments on the Japanese-English language pair. For dependency parsers, we used KNP (Kawahara and Kurohashi, 2006) for Japanese and Berkeley Parser (Petrov and Klein, 2007) for English. We converted con-stituent parse trees obtained by Berkeley Parser to dependency parse trees using rules. We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. [Cite_Footnote_4] Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200. The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile , the Bayesian subtree alignment model (Nakazawa model) (Nakazawa and Kurohashi, 2011) and IBM Model4. We used Nile without automatically ex-tracted rule features and constellation features to make a fair comparison with our model."
  },
  {
    "id": 960,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/xuuuluuu/SynLSTM-for-NER",
    "section_title": "References",
    "add_info": "1 We make our code publicly available at https://github.com/xuuuluuu/SynLSTM-for-NER.",
    "text": "It has been shown that named entity recogni-tion (NER) could benefit from incorporating the long-distance structured information cap-tured by dependency trees. We believe this is because both types of features \u2013 the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural net-works such as graph convolutional networks (GCNs) for building improved NER models, where the exact interaction mechanism be-tween the two different types of features is not very clear, and the performance gain does not appear to be significant. In this work, we pro-pose a simple and robust solution to incorpo-rate both types of features with our Synergized-LSTM (Syn-LSTM), which clearly captures how the two types of features interact. We con-duct extensive experiments on several standard datasets across four languages. The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. Our fur-ther analysis demonstrates that our model can capture longer dependencies compared with strong baselines. [Cite_Footnote_1]"
  },
  {
    "id": 961,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/hanxiao/bert-as-service",
    "section_title": "4 Experiments",
    "add_info": "Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.",
    "text": "Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018)  to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data. We use the cased version of BERT base model for the experiments on the other three datasets. For the character embedding, we ran-domly initialize the character embeddings and set the dimension as 30, and set the hidden size of character-level BiLSTM as 50. The hidden size of GCN and Syn-LSTM is set as 200, the number of GCN layer is 2. We adopt stochastic gradient de-scent (SGD) to optimize our model with batch size 100, L2 regularization 10 \u22128 , initial learning rate lr 0.2 and the learning rate is decayed with respect to the number of epoch. We select the best model based on the performance on the dev set and apply it to the test set. We use the bootstrapping t-test to compare the results."
  },
  {
    "id": 962,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/hanxiao/bert-as-service",
    "section_title": "References",
    "add_info": "Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.",
    "text": "For hyper-parameter, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to ini-tialize the word embeddings for Catalan, Spanish, and Chinese. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018)  to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we select the 18 th layer of the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data. We use the the 9 th layer of cased version of BERT base model for the experiments on the rest three datasets. For the character embedding, we randomly initialize the character embeddings and set the dimension as 30, and set the hidden size of character-level BiLSTM as 50. The hidden size of GCN and Syn-LSTM is set as 200. Note that we only use one layer of bi-directional Syn-LSTM for our experi-ments. Dropout is set to 0.5 for input embeddings and hidden states. We adopt stochastic gradient descent (SGD) to optimize our model with batch size 100, L2 regularization 10 \u22128 , learning rate 0.2 and the learning rate is decayed with respect to the number of epoch 9 . indicates the value of gate m t , the y-axis denotes the number of cells. sentence length. y-axis:F 1 score (%). Note that DGLSTM-CRF +ELMO have better performance com-pared to DGLSTM-CRF +BERT based on the results in the main paper."
  },
  {
    "id": 963,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/h-shahidi/",
    "section_title": "References",
    "add_info": null,
    "text": "A number of researchers have recently ques-tioned the necessity of increasingly complex neural network (NN) architectures. In partic-ular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and un-structured data. We consider neural table-to-text generation and neural question gener-ation (NQG) tasks for text generation from structured and unstructured data, respectively. Table-to-text generation aims to generate a de-scription based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models. Experimen-tal results demonstrate that a basic attention-based seq2seq model trained with the expo-nential moving average technique achieves the state of the art in both tasks. Code is avail-able at  https://github.com/h-shahidi/ 2birds-gen ."
  },
  {
    "id": 964,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://w3.msi.vxu.se/\u02dcnivre/research/Penn2Malt.html",
    "section_title": "3 Experiments 3.1 Data and Tools",
    "add_info": "3 http://w3.msi.vxu.se/\u02dcnivre/research/Penn2Malt.html",
    "text": "In the first two experiments, we used the Wal-l Street Journal (WSJ) and Brown (B) portion-s of the English Penn TreeBank (Marcus et al., 1993). In the first experiment denoted by \u201cWSJ-to-B\u201d, WSJ corpus is used as the source domain and Brown corpus as the target domain. In the second experiment, we use the reverse order of the two corpora and denote it by \u201cB-to-WSJ\u201d. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool [Cite_Footnote_3] with the stan-dard head rules (Yamada and Matsumoto, 2003)."
  },
  {
    "id": 965,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz",
    "section_title": "3 Experiments 3.1 Data and Tools",
    "add_info": "4 Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz",
    "text": "In the third experiment denoted by \u2019\u201cWSJ-to- G\u201d, we used WSJ corpus as the source domain and Genia corpus (G) [Cite_Footnote_4] as the target domain. Following Plank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007). The Genia corpus is converted to CoNLL format with LTH converter, too. We randomly selected about 1000 sentences from the training portion of Genia data and use them as the labeled data of the target domain, and the rest of training data of Ge-nia as the unlabeled data of the target domain. Ta-ble 1 shows the number of sentences of each data set used in the experiments."
  },
  {
    "id": 966,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://sourceforge.net/projects/maxparser/",
    "section_title": "3 Experiments 3.2 Comparison Systems",
    "add_info": null,
    "text": "Self-Training: Following Reichart and Rap-poport (2007), we train a parser with the union of the source and target labeled data, parse the unlabeled data in the target domain, 5  http://sourceforge.net/projects/maxparser/ add the entire auto-parsed trees to the man-ually labeled data in a single step without checking their parsing quality, and retrain the parser."
  },
  {
    "id": 967,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/nlpsoc/reliability_bias",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/nlpsoc/reliability_bias.",
    "text": "Various measures have been proposed to quan-tify human-like social biases in word embed-dings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reli-ability, concerning the extent to which a mea-sure produces consistent results. In this pa-per, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and in-ternal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures\u2019 reliability scores. Our findings inform better design of word embedding gender bias measures. More-over, we urge researchers to be more critical about the application of such measures. [Cite_Footnote_1]"
  },
  {
    "id": 968,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://archive.org/download/2015_reddit_comments_corpus/reddit_data/2014/",
    "section_title": "5 Experiments 5.1 Experimental Setup",
    "add_info": "5 We use all posts and replies from 2014, retrieved from https://archive.org/download/2015_reddit_comments_corpus/reddit_data/2014/.",
    "text": "Training Embeddings We select three corpora with different characteristics to train word embed-dings. Two are from subReddits: r/AskScience (\u223c 158 million tokens) and r/AskHistorians (\u223c 137 million tokens, also used by Antoniak and Mimno 2018) [Cite_Footnote_5] . The third is the training set of WikiText-103 (\u223c 527 million tokens, Merity et al., 2016), consisting of high-quality Wikipedia articles."
  },
  {
    "id": 969,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/stanfordnlp/GloVe",
    "section_title": "5 Experiments 5.1 Experimental Setup",
    "add_info": "6 For SGNS, we use Gensim 3.8.3 (R\u030cehu\u030ar\u030cek and Sojka, 2010) with a window size of 5, a minimum word count of 5 and 5 iterations. For GloVe, we use the official implemen-tation https://github.com/stanfordnlp/GloVe, with a window size of 15, a minimum word count of 5 and 15 iterations. Because we do not fine-tune hyper-parameters, our results do not necessarily indicate which algorithm itself is of better reliability. To investigate the potential impact of this decision, we also include an explorative study on the influence of hyper-parameters in Appendix E.",
    "text": "We use two popular word embedding algo-rithms: Skip-Gram with Negative Sampling (SGNS; Mikolov et al. 2013) and GloVe (Penning-ton et al., 2014). For both algorithms, we set the number of embedding dimensions to 300. For all other hyper-parameters, we use the default values of previous implementations. [Cite_Footnote_6] For each corpus-algorithm pair, we train k = 32 word embedding models using different random seeds."
  },
  {
    "id": 970,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://github.com/first20hours/google-10000-english",
    "section_title": "5 Experiments 5.1 Experimental Setup",
    "add_info": "7 https://github.com/first20hours/google-10000-english.",
    "text": "Target Word Lists For the assessment of test-retest reliability and inter-rater consistency, we in-clude three word lists used in previous word embed-ding bias studies: 1) 320 occupation words from Bolukbasi et al. (2016) (OCC16), 2) 76 occupation words from Garg et al. (2018) (OCC18) and 3) 230 adjectives from Garg et al. (2018) (ADJ). How-ever, these three lists are very specific (i.e. only concerning occupation words and adjectives) and thus unlikely applicable to other (future) research where different biases are of interest and different target words might be used (e.g. measuring gen-der biases of a whole corpus). Therefore, we also consider two additional, larger target word lists: 4) the top 10,000 most frequent words of Google\u2019s trillion word corpus (Google10K) [Cite_Footnote_7] and 5) the full vocabulary of each corpus (Full)."
  },
  {
    "id": 971,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/explosion/spaCy",
    "section_title": "B Environmental Setup",
    "add_info": "11 https://github.com/explosion/spaCy The word embeddings are trained with GloVe on r/AskScience. The word embeddings are trained with GloVe on WikiText-103. The word embeddings are trained with SGNS on r/AskHistorians. SGNS word embeddings trained with three iterations or 100 dimensions on r/AskHistorians. dings trained with three iterations or 100 dimensions on r/AskHistorians.",
    "text": "Data Preprocessing For Reddit data (i.e. r/AskScience and r/AskHistorians), we lower-cased, removed redundant spaces/urls, and used the Spacy [Cite_Footnote_11] library to tokenize each sentence. For training GloVe embeddings, we substi-tuted \u201c\u201d symbols in WikiText-103 with \u201c\u201d symbols."
  },
  {
    "id": 972,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://code.google.com/p/universal-tagger/",
    "section_title": "6 Conclusion",
    "add_info": "4 https://code.google.com/p/universal-tagger/",
    "text": "We have proposed a method for unsupervised POS tagging that performs on par with the current state-of-the-art (Das and Petrov, 2011), but is substan-tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM). The complexity of our algorithm is O(nlogn) compared to O(n 2 ) for that of Das and Petrov (2011) where n is the size of training data. We made our code are available for download. [Cite_Footnote_4]"
  },
  {
    "id": 973,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.delph-in.net/matrix/customize/matrix.cgi",
    "section_title": "2 Background 2.1 The LinGO Grammar Matrix",
    "add_info": "1 http://www.delph-in.net/matrix/customize/matrix.cgi",
    "text": "The Grammar Matrix consists of a cross-linguistic core type hierarchy and a collection of phenomenon-specific libraries. The core type hierar-chy defines the basic feature geometry, the ways that heads combine with arguments and adjuncts, linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Min-imal Recursion Semantics (Copestake et al., 2005; Flickinger and Bender, 2003). The libraries provide collections of analyses for cross-linguistically vari-able phenomena. The current libraries include anal-yses of major constituent word order (SOV, SVO, etc), sentential negation, coordination, and yes-no question formation. The Matrix is accessed through a web-based configuration system [Cite_Footnote_1] which elicits ty-pological information from the user-linguist through a questionnaire and then outputs a grammar consist-ing of the Matrix core plus selected types and con-straints from the libraries according to the specifica-tions in the questionnaire."
  },
  {
    "id": 974,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://homepage.fudan.edu.cn/zhengxq/deeplearning/",
    "section_title": "1 Introduction and Motivation",
    "add_info": "1 The source code is available at http://homepage.fudan.edu.cn/zhengxq/deeplearning/",
    "text": "The proposed parser first encodes each word in a sentence by distributed embeddings using a con-volutional neural network and constructs an initial parse graph by head-modifier predictions with a maximum directed spanning tree algorithm based on the first-order features (i.e. the score is fac-tored over the arcs in a graph). Once an initial parse graph is built, the high-order features (such as grandparent, sibling, and uncle) can be defined, and used to refine the structure of the parse tree in an iterative way. Theoretically, the refinement will continue until no change is made in the iteration. But experimental results demonstrated that pretty good performance can be achieved with no more than twice updates because many dependencies are determined by independent arc prediction and a few head-modifier pairs need to be re-estimated after one update (i.e. only a few changes above and beyond the dominant first-order scores). We call this proposed model an incremental neural de-pendency parsing (INDP) [Cite_Footnote_1] ."
  },
  {
    "id": 975,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://code.google.com/p/word2vec/",
    "section_title": "3 Experiments 3.2 Training Strategy",
    "add_info": "3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep",
    "text": "Previous work demonstrated that the performance can be improved by using word embeddings learned from large-scale unlabeled data in many NLP tasks both in English (Collobert et al., 2011; Socher et al., 2011) and Chinese (Zheng et al., 2013). Unsupervised pretraining guides the learn-ing towards basins of attraction of minima that support better generalization (Erhan et al., 2010). We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [Cite_Footnote_3] proposed in (Mikolov et al., 2013)."
  },
  {
    "id": 976,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.gurobi.com/",
    "section_title": "6 Empirical Evaluation 6.1 Experimental setup",
    "add_info": "3 http://www.gurobi.com/",
    "text": "We used 150 processes (435 questions) for train-ing and 50 processes (150 questions) as the test set. For development, we randomly split the train-ing set 10 times (80%/20%), and tuned hyper-parameters by maximizing average accuracy on question answering. We preprocessed the para-graphs with the Stanford CoreNLP pipeline ver-sion 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package [Cite_Footnote_3] for infer-ence."
  },
  {
    "id": 977,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/e-bug/nmt-difficulty",
    "section_title": "References",
    "add_info": null,
    "text": "The performance of neural machine transla-tion systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more dif-ficult to model. In this paper, we propose cross-mutual information (XMI): an asymmet-ric information-theoretic metric of machine translation difficulty that exploits the proba-bilistic nature of most neural machine trans-lation models. XMI allows us to better eval-uate the difficulty of translating text into the target language while controlling for the dif-ficulty of the target-side generation compo-nent independent of the translation task. We then present the first systematic and con-trolled study of cross-lingual translation dif-ficulties using modern neural translation sys-tems. Code for replicating our experiments is available online at  https://github.com/e-bug/nmt-difficulty."
  },
  {
    "id": 978,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/google-research-datasets/timedial",
    "section_title": "References",
    "add_info": null,
    "text": "Everyday conversations require understanding everyday events, which in turn, requires un-derstanding temporal commonsense concepts interwoven with those events. Despite re-cent progress with massive pre-trained lan-guage models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in di-alogs remains largely under-explored. In this paper, we present the first study to investi-gate pre-trained LMs for their temporal rea-soning capabilities in dialogs by introducing a new task and a crowd-sourced English chal-lenge set, T IME D IAL . We formulate T IME - D IAL as a multiple choice cloze task with over 1.1K carefully curated dialogs. Empirical re-sults demonstrate that even the best perform-ing models struggle on this task compared to humans, with 23 absolute points of gap in ac-curacy. Furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling tem-poral concepts in text and robust contextual reasoning about them. The dataset is pub-licly available at:  https://github.com/google-research-datasets/timedial."
  },
  {
    "id": 979,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://nlp.stanford.edu/software/sutime.shtml",
    "section_title": "3 Dataset: T IME D IAL 3.1 Data Collection",
    "add_info": "1 https://nlp.stanford.edu/software/sutime.shtml",
    "text": "Temporal expression identification. Here, we select dialogs that are rich with temporal informa-tion, in order to focus on complex temporal rea-soning that arises in natural dialogs. Temporal expressions are automatically identified with SU-Time (Chang and Manning, 2012), an off-the-shelf temporal expression detector. [Cite_Footnote_1] We keep only the dialogs with more than 3 temporal expressions and at least one expression that contains numerals like \u201ctwo weeks\u201d (as opposed to non-numeric spans, like \u201csummer\u201d, \u201cright now\u201d, and \u201clater\u201d). In our initial experiment, we observe that language models can often correctly predict these non-numerical tempo-ral phrases."
  },
  {
    "id": 980,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://cloud.google.com/tpu",
    "section_title": "5 Experiments and Analyses",
    "add_info": "5 https://cloud.google.com/tpu",
    "text": "Using the proposed T IME D IAL challenge set, we next conduct extensive experiments and analyses on the different model variants and context settings. We use either 4x4 or 8x8 Cloud TPUs V3 pod slices [Cite_Footnote_5] for fine-tuning and one V100 GPU for infer-ence. We provide more details of the experiment configurations in the appendix."
  },
  {
    "id": 981,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/sunlab-osu/covid-faq",
    "section_title": "Appendix B Aggregation Schemes",
    "add_info": null,
    "text": "We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval. Simi-lar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains \u223c16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we in-troduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains \u223c32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ re-trieval models built on top of BM25 and BERT, among which the best model achieves 48.8 under P@5, indicating a great challenge pre-sented by COUGH and encouraging future re-search for further improvement. Our COUGH dataset is available at  https://github.com/sunlab-osu/covid-faq."
  },
  {
    "id": 982,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.nltk.org/",
    "section_title": "Appendix C Implementation Details",
    "add_info": "14 https://www.nltk.org/",
    "text": "We first preprocess user query and FAQ items with nltk porter stemmer 5 [Cite_Footnote_14] . For baselines including BM25 and Sentence-BERT , we take the stan-dard off-the-shelf version. More specifically, we keep the default k1 as 2 and b as 0.75 for BM25 over Q-q, Q-a and Q-q+a settings. When deploy-ing synthetic query generation model (i.e., GPT2), hyper-parameters are set as instructed by Mass et al. (2020) (see their Section 3.4). We adopt the in-batch negatives training strategy to fine-tune both Sentence-BERT and cross-encoder BERT. For both BERT models, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5 and fine-tune up to 10 epochs. We set the batch sizes as 24 and 4 for Sentence-BERT and cross-encoder BERT, respectively. All experiments are conducted using one single GeForce GTX 2080 Ti 12 GB GPU (with significant CPU resources)."
  },
  {
    "id": 983,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://pypi.org/project/rank-bm25/",
    "section_title": "Appendix C Implementation Details",
    "add_info": "15 https://pypi.org/project/rank-bm25/",
    "text": "We first preprocess user query and FAQ items with nltk porter stemmer 5 . For baselines including BM25 [Cite_Footnote_15] and Sentence-BERT , we take the stan-dard off-the-shelf version. More specifically, we keep the default k1 as 2 and b as 0.75 for BM25 over Q-q, Q-a and Q-q+a settings. When deploy-ing synthetic query generation model (i.e., GPT2), hyper-parameters are set as instructed by Mass et al. (2020) (see their Section 3.4). We adopt the in-batch negatives training strategy to fine-tune both Sentence-BERT and cross-encoder BERT. For both BERT models, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5 and fine-tune up to 10 epochs. We set the batch sizes as 24 and 4 for Sentence-BERT and cross-encoder BERT, respectively. All experiments are conducted using one single GeForce GTX 2080 Ti 12 GB GPU (with significant CPU resources)."
  },
  {
    "id": 984,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/UKPLab/sentence-transformers",
    "section_title": "Appendix C Implementation Details",
    "add_info": "16 https://github.com/UKPLab/sentence-transformers and we use distilbert-base-nli-stsb-quora-ranking model card.",
    "text": "We first preprocess user query and FAQ items with nltk porter stemmer 5 . For baselines including BM25 and Sentence-BERT [Cite_Footnote_16] , we take the stan-dard off-the-shelf version. More specifically, we keep the default k1 as 2 and b as 0.75 for BM25 over Q-q, Q-a and Q-q+a settings. When deploy-ing synthetic query generation model (i.e., GPT2), hyper-parameters are set as instructed by Mass et al. (2020) (see their Section 3.4). We adopt the in-batch negatives training strategy to fine-tune both Sentence-BERT and cross-encoder BERT. For both BERT models, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5 and fine-tune up to 10 epochs. We set the batch sizes as 24 and 4 for Sentence-BERT and cross-encoder BERT, respectively. All experiments are conducted using one single GeForce GTX 2080 Ti 12 GB GPU (with significant CPU resources)."
  },
  {
    "id": 985,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://twitter.com",
    "section_title": "4 Empirical Evaluation 4.1 Datasets",
    "add_info": "2 https://twitter.com",
    "text": "\u2022 Tweets - Twitter [Cite_Footnote_2] is a microblogging plat-form which allows users to post statuses of less than 140 characters. We use two collections for sarcasm detection on tweets. More specifically, we use the dataset ob-tained from (1) (Pta\u0301c\u030cek et al., 2014) in which tweets are trained via hashtag based semi-supervised learning, i.e., hashtags such as #not, #sarcasm and #irony are marked as sar-castic tweets and (2) (Riloff et al., 2013) in which Tweets are hand annotated and manu-ally checked for sarcasm. For both datasets, we retrieve. Tweets using the Twitter API us-ing the provided tweet IDs."
  },
  {
    "id": 986,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://nlds.soe.ucsc.edu/sarcasm1",
    "section_title": "4 Empirical Evaluation 4.1 Datasets",
    "add_info": "4 https://nlds.soe.ucsc.edu/sarcasm1",
    "text": "\u2022 Debates - We use two datasets [Cite_Footnote_4] from the In-ternet Argument Corpus (IAC) (Lukin and Walker, 2017) which have been hand anno-tated for sarcasm. This dataset, unlike the first two, is mainly concerned with long text and provides a diverse comparison from the other datasets. The IAC corpus was designed for research on political debates on online fo-rums. We use the V1 and V2 versions of the sarcasm corpus which are denoted as IAC-V1 and IAC-V2 respectively."
  },
  {
    "id": 987,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/chrisjbryant/errant",
    "section_title": "3 Erroneous Span Detection",
    "add_info": "1 Alignment can be solved by dynamic programming like Levenshtein distance. We here use ERRANT (https://github.com/chrisjbryant/errant) for alignment.",
    "text": "To identify incorrect spans, we use a binary se-quence tagging model in which tag 0 means the token is in a correct span; while tag 1 means the to-ken is in a grammatically incorrect span that needs to be edited, as shown in Figure 1(a). In order to train the tagging model, we align [Cite_Footnote_1] tokens across the source and target sentence in training data. With token alignment, we can identify the text spans that are edited and thus can annotate the edited text spans in the original sentences as erroneous spans."
  },
  {
    "id": 988,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/nusnlp/m2scorer",
    "section_title": "5 Experiments 5.1 Experimental Setting",
    "add_info": "2 https://github.com/nusnlp/m2scorer",
    "text": "Following recent work in English GEC, we conduct experiments in the same setting with the restricted track of the BEA-2019 GEC shared task (Bryant et al., 2019), using FCE (Yannakoudakis et al., 2011), Lang-8 Corpus of Learner English (Mizu-moto et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Granger, 1998; Bryant et al., 2019) as training data. We use CoNLL-2013 test set as the dev set to choose the best-performing models, and evaluate on the well-known GEC benchmark datasets: CoNLL-2014 (Ng et al., 2014) and BEA-2019 test set with the official evaluation scripts (m2scorer [Cite_Footnote_2] for CoNLL-14, ERRANT for BEA-19). As previous work (Grundkiewicz et al., 2019) trained with synthetic data, we synthesize"
  },
  {
    "id": 989,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://en.wikipedia.org/",
    "section_title": "References",
    "add_info": "5 https://en.wikipedia.org/",
    "text": "We thank Xin Sun and Xiangxin Zhou for their help with experiments. We thank the anonymous reviewers for their valuable comments. The corre-sponding author of this paper is Tao Ge. A Experiment Details Table 5 describes the details of datasets used for English GEC. Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia [Cite_Footnote_5] , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task. For sentence corruption, we follow Edunov et al. (2018) to randomly insert, delete, replace and swap adjacent tokens in a sentence."
  },
  {
    "id": 990,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://data.statmt.org/news-crawl/en/",
    "section_title": "References",
    "add_info": "6 http://data.statmt.org/news-crawl/en/",
    "text": "We thank Xin Sun and Xiangxin Zhou for their help with experiments. We thank the anonymous reviewers for their valuable comments. The corre-sponding author of this paper is Tao Ge. A Experiment Details Table 5 describes the details of datasets used for English GEC. Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia , English Gigaword (Parker et al., 2011) and Newscrawl [Cite_Footnote_6] as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task. For sentence corruption, we follow Edunov et al. (2018) to randomly insert, delete, replace and swap adjacent tokens in a sentence."
  },
  {
    "id": 991,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://mpqa.cs.pitt.edu/corpora/",
    "section_title": "3 Task Definition 3.1 Gold Standard Corpus: MPQA 3.0",
    "add_info": "4 Available at http://mpqa.cs.pitt.edu/corpora/",
    "text": "MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015). [Cite_Footnote_4] It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agen-cies, and covers a wide range of topics."
  },
  {
    "id": 992,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://mpqa.cs.pitt.edu/lexicons/effectlexicon/",
    "section_title": "4 PSL for Sentiment Analysis 4.4 PSL Augmented with +/-Effect Events (PSL3)",
    "add_info": "7 Available at: http://mpqa.cs.pitt.edu/lexicons/effectlexicon/",
    "text": "+E FFECT (x) and -E FFECT (x): We use the +/-effect sense-level lexicon (Choi and Wiebe, 2014) [Cite_Footnote_7] to extract the +/-effect events in each sen-tence. The score of +E FFECT (x) is the fraction of that word\u2019s senses that are +effect senses accord-ing to the lexicon, and the score of -E FFECT (x) is the fraction of that word\u2019s senses that are -effect senses according to the lexicon. If a word does not appear in the lexicon, we do not treat it as a +/-effect event, and thus assign 0 to both +E FFECT (x) and -E FFECT (x)."
  },
  {
    "id": 993,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/Embedding/Chinese-Word-Vectors",
    "section_title": "5 Experiments 5.1 Dataset and Experiment Settings",
    "add_info": "6 We use the merge_sgns_bigram_char300.txt from https://github.com/Embedding/Chinese-Word-Vectors.",
    "text": "Implementation Details. In the experiment, we use the pre-trained Chinese word vectors with 300 dimensions to initialize the Chinese word embed-dings (Li et al., 2018). [Cite_Footnote_6] We use the Stanford CoreNLP to extract POS and NE features. The MiniSom is employed for constructing the neural clustering model, and we choose an 11 \u00d7 11 square map with a sigma of 4, an initial learning rate of 0.5, the Euclidean distance function to activate the map and the Gaussian function to weigh the neigh-borhood of nodes in the map. The representations of POS, NE and cluster index features are all ran-domly initialized as 32-dimensional vectors in the training stage. The depth of the LSTM layer is set to 2, while the hidden size of Bi-LSTM in both the baseline model and the extra attention-based model is 128, and the size of the dense layer is set to 64. Besides, to avoid overfitting, we use dropout before the Bi-LSTM layer and the dense layer with a dropout rate of 0.5. We set the batch size to 64 and use the optimization algorithm Adam (Kingma and Ba, 2015) with default parameters as an initial learning rate of 0.001. Our models are all trained on a single GPU and the results are reported on the test set."
  },
  {
    "id": 994,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/stanfordnlp/CoreNLP",
    "section_title": "5 Experiments 5.1 Dataset and Experiment Settings",
    "add_info": "7 https://github.com/stanfordnlp/CoreNLP",
    "text": "Implementation Details. In the experiment, we use the pre-trained Chinese word vectors with 300 dimensions to initialize the Chinese word embed-dings (Li et al., 2018). We use the Stanford CoreNLP to extract POS and NE features. [Cite_Footnote_7] The MiniSom is employed for constructing the neural clustering model, and we choose an 11 \u00d7 11 square map with a sigma of 4, an initial learning rate of 0.5, the Euclidean distance function to activate the map and the Gaussian function to weigh the neigh-borhood of nodes in the map. The representations of POS, NE and cluster index features are all ran-domly initialized as 32-dimensional vectors in the training stage. The depth of the LSTM layer is set to 2, while the hidden size of Bi-LSTM in both the baseline model and the extra attention-based model is 128, and the size of the dense layer is set to 64. Besides, to avoid overfitting, we use dropout before the Bi-LSTM layer and the dense layer with a dropout rate of 0.5. We set the batch size to 64 and use the optimization algorithm Adam (Kingma and Ba, 2015) with default parameters as an initial learning rate of 0.001. Our models are all trained on a single GPU and the results are reported on the test set."
  },
  {
    "id": 995,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/JustGlowing/minisom",
    "section_title": "5 Experiments 5.1 Dataset and Experiment Settings",
    "add_info": "8 https://github.com/JustGlowing/minisom",
    "text": "Implementation Details. In the experiment, we use the pre-trained Chinese word vectors with 300 dimensions to initialize the Chinese word embed-dings (Li et al., 2018). We use the Stanford CoreNLP to extract POS and NE features. The MiniSom [Cite_Footnote_8] is employed for constructing the neural clustering model, and we choose an 11 \u00d7 11 square map with a sigma of 4, an initial learning rate of 0.5, the Euclidean distance function to activate the map and the Gaussian function to weigh the neigh-borhood of nodes in the map. The representations of POS, NE and cluster index features are all ran-domly initialized as 32-dimensional vectors in the training stage. The depth of the LSTM layer is set to 2, while the hidden size of Bi-LSTM in both the baseline model and the extra attention-based model is 128, and the size of the dense layer is set to 64. Besides, to avoid overfitting, we use dropout before the Bi-LSTM layer and the dense layer with a dropout rate of 0.5. We set the batch size to 64 and use the optimization algorithm Adam (Kingma and Ba, 2015) with default parameters as an initial learning rate of 0.001. Our models are all trained on a single GPU and the results are reported on the test set."
  },
  {
    "id": 996,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.wikipedia.com",
    "section_title": "3 Methodology 3.3 Pre-trained Knowledge Encoder",
    "add_info": "M. Remy. 2002. Wikipedia: The free encyclope-dia200214wikipedia: The free encyclopedia. 2001 \u2013 updated daily. gratis http://www.wikipedia.com.",
    "text": "We want to incorporate implicit external knowl-edge as well as math-aware knowledge which can be learned from the training set in our model. Lan-guage models, and especially transformer-based language models, have shown to contain com-monsense and factual knowledge (Petroni et al., 2019; Jiang et al., 2019). We adopt this direc-tion in our model and build an encoder, pre-trained with Roberta (Liu et al., 2019c), which has been pre-trained on the huge language corpora (e.g., BooksCorpus (Zhu et al., 2015), Wikipedia (Remy, 2002)  ) to capture implicit knowledge. We tokenize a description Q using WordPiece (Wu et al., 2016) as in BERT (Devlin et al., 2019a), giving us a se-quence of |Q| tokens and embed them with the pre-trained Roberta embeddings and append Roberta\u2019s positional encoding, giving us a sequence of d-dimensional token representation x Q1 , ..., x Q|Q| . We feed these into the transformer-based pre-trained knowledge encoder, fine-tuning the representation during training. We mean-pool the output of all transformer steps to get our combined implicit knowledge representation Y p ."
  },
  {
    "id": 997,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/viking-sudo-rm/rr-experiments",
    "section_title": "5 Experiments",
    "add_info": "11 https://github.com/viking-sudo-rm/rr-experiments",
    "text": "In Subsection 4.3, we showed that different satu-rated RNNs vary in their ability to recognize a n b n and a n b n \u03a3 \u2217 . We now test empirically whether these predictions carry over to the learnable capac-ity of unsaturated RNNs. [Cite_Footnote_11] We compare the QRNN and LSTM when coupled with a linear decoder D 1 . We also train a 2-layer QRNN (\u201cQRNN2\u201d) and a 1-layer QRNN with a D 2 decoder (\u201cQRNN+\u201d)."
  },
  {
    "id": 998,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/viking-sudo-rm/",
    "section_title": "0 < [h t ] 4 \u2228 [h t ] 1 + [h t ] 2 + [h t ] 3 \u2264 0 . (76) E Experimental Details",
    "add_info": null,
    "text": "Models were trained on strings up to length 64, and, at each index t, were asked to classify whether or not the prefix up to t was a valid string in the language. Models were then tested on indepen-dent datasets of lengths 64, 128, 256, 512, 1024, and 2048. The training dataset contained 100000 strings, and the validation and test datasets con-tained 10000. We discuss task-specific schemes for sampling strings in the next paragraph. All models were trained for a maximum of 100 epochs, with early stopping after 10 epochs based on the valida-tion cross entropy loss. We used default hyperpa-rameters provided by the open-source AllenNLP framework (Gardner et al., 2018). The code is avail-able at  https://github.com/viking-sudo-rm/ rr-experiments ."
  },
  {
    "id": 999,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/WadeYin9712/SentiBERT",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Results on phrase-level sentiment classification on Stanford Sentiment Treebank (SST) (Socher et al., 2013) indicate that SentiBERT improves significantly over recursive networks and the base-composition module based on an attention mechanism; Module III is a predictor for phrase-level sentiment. The semantic composition module is a two layer attention-based network (see Section 3.1) The first layer (Attention to Tokens) generates representation for each phrase based on the token it covers and the second layer (Attention to Children) refines the phrase representation obtained from the first layer based on its children. line BERT model. As phrase-level sentiment labels are expensive to obtain, we further explore if the compositional sentiment semantics learned from one task can be transferred to others. In particular, we find that SentiBERT trained on SST can be transferred well to other related tasks such as twit-ter sentiment analysis (Rosenthal et al., 2017) and emotion intensity classification (Mohammad et al., 2018) and contextual emotion detection (Chatter-jee et al., 2019). Furthermore, we conduct com-prehensive quantitative and qualitative analyses to evaluate the effectiveness of SentiBERT under various situations and to demonstrate the seman-tic compositionality captured by the model. The source code is available at  https://github.com/WadeYin9712/SentiBERT."
  },
  {
    "id": 1000,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface",
    "section_title": "4 Experiments 4.1 Experimental Settings",
    "add_info": "1 https://github.com/huggingface",
    "text": "We build SentiBERT on the HuggingFace li-brary [Cite_Footnote_1] and initialize the model parameters using pre-trained BERT-base and RoBERTa-base models whose maximum length is 128, layer number is 12, and embedding dimension is 768. For the train-ing on SST-phrase, the learning rate is 2 \u00d7 10 \u22125 , batch size is 32 and the number of training epochs is 3. For masking mechanism, to put emphasis on modeling sentiments, the probability of masking opinion words which can be retrieved from Senti-WordNet (Baccianella et al., 2010) is set to 20%, and for the other words, the probability is 15%. For fine-tuning on downstream tasks, the learning rate is {1\u00d710 \u22125 \u22121\u00d710 \u22124 }, batch size is {16, 32} and the number of training epochs is 1\u22125. We use Stan-ford CoreNLP API (Manning et al., 2014) to obtain binary constituency trees for the sentences of these tasks to keep consistent with the settings on SST-phrase. Note that when fine-tuning on sentence-level sentiment and emotion classification tasks, the objective is to correctly label the root of tree, instead of targeting at the [CLS] token representa-tion as in the original BERT."
  },
  {
    "id": 1001,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.mdbg.net/chindict/chindict.php?page=cedict",
    "section_title": "4 Emotion Detection via Bilingual and Sentiment Information 4.1 Bilingual Information",
    "add_info": "1 MDBG CC-CEDICT is adopted as the bilingual lexicon: http://www.mdbg.net/chindict/chindict.php?page=cedict",
    "text": "For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon [Cite_Footnote_1] , and is the language model, and p \u03b8 LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary and PMI based word correlation are used to enhance the language mod-el for machine translation. p \u03b8 SYN (c) denotes the synonym similarity between translated words and the contexts. This is necessary since the sense of translated words and the contexts are expected to be similar; and p \u03b8 PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine (Tur-ney, 2002). This is to ensure that the translated words are highly associated with the contexts."
  },
  {
    "id": 1002,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.ltp-cloud.com/",
    "section_title": "4 Emotion Detection via Bilingual and Sentiment Information 4.1 Bilingual Information",
    "add_info": "3 TongYiCiLin is adopted as the Chinese synonym dictio-nary: http://www.ltp-cloud.com/",
    "text": "For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon , and is the language model, and p \u03b8 LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary [Cite_Footnote_3] and PMI based word correlation are used to enhance the language mod-el for machine translation. p \u03b8 SYN (c) denotes the synonym similarity between translated words and the contexts. This is necessary since the sense of translated words and the contexts are expected to be similar; and p \u03b8 PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine (Tur-ney, 2002). This is to ensure that the translated words are highly associated with the contexts."
  },
  {
    "id": 1003,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.bing.com/",
    "section_title": "4 Emotion Detection via Bilingual and Sentiment Information 4.1 Bilingual Information",
    "add_info": "4 We use BING.com as the search engine for PMI: http://www.bing.com/",
    "text": "For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon , and is the language model, and p \u03b8 LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary and PMI based word correlation are used to enhance the language mod-el for machine translation. p \u03b8 SYN (c) denotes the synonym similarity between translated words and the contexts. This is necessary since the sense of translated words and the contexts are expected to be similar; and p \u03b8 PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine [Cite_Footnote_4] (Tur-ney, 2002). This is to ensure that the translated words are highly associated with the contexts."
  },
  {
    "id": 1004,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://ir.dlut.edu.cn/EmotionOntologyDownload.aspx",
    "section_title": "4 Emotion Detection via Bilingual and Sentiment Information 4.2 Sentimental Information",
    "add_info": "5 DUTIR Sentiment Lexicon is adopt-ed as the Chinese sentiment lexicon: http://ir.dlut.edu.cn/EmotionOntologyDownload.aspx",
    "text": "In this paper, both Chinese [Cite_Footnote_5] and English sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002)."
  },
  {
    "id": 1005,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://mpqa.cs.pitt.edu/lexicons/subjlexicon/",
    "section_title": "4 Emotion Detection via Bilingual and Sentiment Information 4.2 Sentimental Information",
    "add_info": "6 English sentiment lexicon is uti-lized from MPQA Subjectivity Lexicon: http://mpqa.cs.pitt.edu/lexicons/subjlexicon/",
    "text": "In this paper, both Chinese and English [Cite_Footnote_6] sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002)."
  },
  {
    "id": 1006,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/xpqiu/fnlp/",
    "section_title": "5 Experiments 5.1 Experimental Settings",
    "add_info": "7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/",
    "text": "As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [Cite_Footnote_7] is used for Chinese word segmentation."
  },
  {
    "id": 1007,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "5 Experiments 5.2 Experimental Results",
    "add_info": "8 ME algorithm is implemented with the MALLET Toolkit, http://mallet.cs.umass.edu",
    "text": "Our first group of experiments is to investigate whether our proposed label propagation model with both bilingual and sentimental information can improve emotion detection in code-switching texts. Figure 3 shows the experimental results of different models, where ME is the basic Maximum Entropy (ME) classification model [Cite_Footnote_8] in which all Chinese and English words of each post function as a feature, ME-CN and ME-EN in which only the Chinese or English text of each post function as features, and BLP-BS, our proposed LP-based ap-proach which incorporates both bilingual and sen-timental information. We adopt F1-Measure (F1.) to measure the performance of each model in the respective emotions."
  },
  {
    "id": 1008,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/cnap/grammaticality-metrics",
    "section_title": "6 Summary",
    "add_info": "9 https://github.com/cnap/grammaticality-metrics",
    "text": "To facilitate GEC evaluation, we have set up an online platform for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available. [Cite_Footnote_9]"
  },
  {
    "id": 1009,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/iceboal/word-representations-bptf",
    "section_title": "3 Experiments 3.1 Evaluation settings 3.1.2 Resource for training",
    "add_info": "1 https://github.com/iceboal/word-representations-bptf",
    "text": "For supervised dataset, we used synonym and antonym pairs in two thesauri: WordNet (Miller, 1995) and Roget (Kipfer, 2009). These pairs were provided by Zhang et al. (2014) [Cite_Footnote_1] . There were 52,760 entries (words), each of which had 11.7 synonyms on average, and 21,319 entries, each of which had 6.5 antonyms on average."
  },
  {
    "id": 1010,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.iptc.org",
    "section_title": "5 Dynamic Hierarchies",
    "add_info": "2 International Press Telecommunications Council, http://www.iptc.org",
    "text": "We suggest the we adopt text categorization on top of topic detection and tracking, similar to Figure 3. There has been good results in text categorization (see, e.g., (Yang and Liu, 1999; Sebastiani, 2002)) The pre-defined categories would form the static hierarchy \u2013 the IPTC Subject Reference System [Cite_Footnote_2] , for example \u2013 on top of all event-based information organization, and the mod-els for the categories could be built on the basis of the test set."
  },
  {
    "id": 1011,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "https://keras.io/",
    "section_title": "3 Models 3.2 Neural model architecture",
    "add_info": "1 https://keras.io/",
    "text": "Additionally, we also use word embeddings from pre-trained word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models, represent-ing each utterance summing all of the vectors for the component words. Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras [Cite_Footnote_1] . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 2 . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014). With the word2vec and GloVe embeddings, we use a convolutional neural network (CNN) model with global max pooling, trained for 20 epochs with a batch size of 128."
  },
  {
    "id": 1012,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "3 Models 3.2 Neural model architecture",
    "add_info": "2 https://code.google.com/archive/p/word2vec/",
    "text": "Additionally, we also use word embeddings from pre-trained word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models, represent-ing each utterance summing all of the vectors for the component words. Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [Cite_Footnote_2] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014). With the word2vec and GloVe embeddings, we use a convolutional neural network (CNN) model with global max pooling, trained for 20 epochs with a batch size of 128."
  },
  {
    "id": 1013,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2002T07",
    "section_title": "1 Introduction",
    "add_info": "1 https://catalog.ldc.upenn.edu/LDC2002T07",
    "text": "RST is a descriptive framework that has been widely used in the analysis of discourse organiza-tion of written texts (Taboada and Mann, 2006b), and has been applied to various natural lan-guage processing tasks, including language gen-eration, text summarization, and machine trans-lation (Taboada and Mann, 2006a). In particu-lar, the availability of RST annotations on a se-lection of 385 Wall Street Journal articles from the Penn Treebank [Cite_Footnote_1] (Carlson et al., 2001) has facilitated RST-based discourse analysis of writ-ten texts, since it provides a standard benchmark for comparing the performance of different tech-niques for document-level discourse parsing (Joty et al., 2013; Feng and Hirst, 2014)."
  },
  {
    "id": 1014,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.isi.edu/licensed-sw/RSTTool/index.html",
    "section_title": "3 Annotation 3.1 Guidelines",
    "add_info": "2 Downloaded from http://www.isi.edu/licensed-sw/RSTTool/index.html",
    "text": "The discourse annotation tool used in the RST Discourse Treebank 2 was also adopted for this study. Using this tool, annotators incrementally build hierarchical discourse trees in which the leaves are the EDUs and the internal nodes cor-respond to contiguous spans of text. When the an-notators assign the rhetorical relation for a node of the tree, they provide the relation\u2019s label (drawn from the pre-defined set of relations in the anno-tation guidelines) and also indicate whether the spans that comprise the relation are nuclei or satel-lites. Figure 1 shows an example of an annotated RST tree for a response with a proficiency score of 1. This response includes three disfluencies (EDUs 3, 6, and 9), which are satellites of the corresponding repair nuclei. In addition, the re-sponse also includes an awkward Comment-Topic relation between EDU 2 and the node combin-ing EDUs 3-11, indicated by awkward-Comment-Topic-2; in this multinuclear relation, the annota-tor judged that the second branch of the relation was awkward, which is indicated by the [Cite_Footnote_2] that was appended to the relation label."
  },
  {
    "id": 1015,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://toeflpractice.ets.org/",
    "section_title": "3 Annotation 3.2 Pilot Annotation",
    "add_info": "3 https://toeflpractice.ets.org/",
    "text": "The manual annotations were provided by two ex-perts with prior experience in various types of data annotation on both text and speech. First, a pi-lot annotation was conducted to train and cali-brate the annotators based on 48 training samples drawn from the TOEFL R Practice Online (TPO) product [Cite_Footnote_3] , which offers practice tests simulating the TOEFL iBT testing experience with authentic test questions. The training samples were selected from a TPO test form with 6 test questions and were balanced according to test question and pro-ficiency score, i.e., 2 responses from each score level for each question."
  },
  {
    "id": 1016,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/dbamman/litbank",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We present in this work a new dataset of en-tity annotations for a wide sample of 210,532 to-kens from 100 literary texts to help address these issues and help advance computational work on literature. These annotations follow the guide-lines set forth by the ACE 2005 entity tagging task (LDC, 2005) in labeling all nominal entities (named and common alike), including those with nested structure. In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at  https://github.com/dbamman/litbank."
  },
  {
    "id": 1017,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/dbamman/",
    "section_title": "5 Conclusion",
    "add_info": null,
    "text": "All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at:  https://github.com/dbamman/ litbank; code to support this work can be found at: https://github.com/dbamman/NAACL2019-literary-entities."
  },
  {
    "id": 1018,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/dbamman/NAACL2019-literary-entities",
    "section_title": "5 Conclusion",
    "add_info": null,
    "text": "All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: https://github.com/dbamman/ litbank; code to support this work can be found at:  https://github.com/dbamman/NAACL2019-literary-entities."
  },
  {
    "id": 1019,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.newsreader-project.eu/results/data/the-ecb-corpus/",
    "section_title": "References",
    "add_info": null,
    "text": "In this paper, we propose a new model for cross-document coreference resolution that extends the efficient sequential prediction paradigm to multiple documents. The sequential prediction is combined with incremental candidate composition that allows the model to use the history of past coreference de-cisions at every step. Our model achieves compet-itive results for both entity and event coreference and our analysis provides strong evidence of the efficacy of both sequential models and higher-order inference in cross-document settings. In future, we intend to adapt this model to coreference across document streams and investigate alternatives to greedy prediction (e.g., beam search). A Implementation details The dataset is available here:  http://www.newsreader-project.eu/results/data/the-ecb-corpus/."
  },
  {
    "id": 1020,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.seas.upenn.edu/\u223cpdtb/tools.shtml#annotator",
    "section_title": "4 Annotation experiment 4.1 Set-up",
    "add_info": "3 http://www.seas.upenn.edu/\u223cpdtb/tools.shtml#annotator",
    "text": "The agreement statistics come from annotation con-ducted by two annotators in training so far. The data set consists of 98 files taken from the Chinese Tree-bank (Xue et al., 2005). The source of these files is Xinhua newswire. The annotation is carried out on the PDTB annotation tool [Cite_Footnote_3] ."
  },
  {
    "id": 1021,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/DeepLearnXMU/NSEG",
    "section_title": "3 Experiments 3.1 Datasets",
    "add_info": "1 https://github.com/DeepLearnXMU/NSEG",
    "text": "The experiments are conducted on six public datasets in different domains: NIPS abstract, AAN abstract, NSF abstract, arXiv abstract: These datasets contain ab-stracts of research papers. NIPS abstract [Cite_Footnote_1] is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018). AAN abstract (Logeswaran et al., 2018) is col-lected from ACL Anthology Network corpus. ACL papers published up to year 2010 for train-ing, year 2011 for validation and 2012-2013 for testing. NSF abstract (Logeswaran et al., 2018) is from NSF Research Award abstract dataset, where abstracts in years 1990-1999/2000/2001-2003 for training/validation/testing. ArXiv abstract (Gong et al., 2016; Chen et al., 2016) is from arXiv web-site 2 . The validation and test sets of this dataset are the first and last 10% abstracts from the shuf-fled data, and the remaining data are for training."
  },
  {
    "id": 1022,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/FudanNLP/NeuralSentenceOrdering",
    "section_title": "3 Experiments 3.1 Datasets",
    "add_info": "2 https://github.com/FudanNLP/NeuralSentenceOrdering",
    "text": "The experiments are conducted on six public datasets in different domains: NIPS abstract, AAN abstract, NSF abstract, arXiv abstract: These datasets contain ab-stracts of research papers. NIPS abstract 1 is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018). AAN abstract (Logeswaran et al., 2018) is col-lected from ACL Anthology Network corpus. ACL papers published up to year 2010 for train-ing, year 2011 for validation and 2012-2013 for testing. NSF abstract (Logeswaran et al., 2018) is from NSF Research Award abstract dataset, where abstracts in years 1990-1999/2000/2001-2003 for training/validation/testing. ArXiv abstract (Gong et al., 2016; Chen et al., 2016) is from arXiv web-site [Cite_Footnote_2] . The validation and test sets of this dataset are the first and last 10% abstracts from the shuf-fled data, and the remaining data are for training."
  },
  {
    "id": 1023,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://visionandlanguage.net/VIST/dataset.html",
    "section_title": "3 Experiments 3.1 Datasets",
    "add_info": "3 http://visionandlanguage.net/VIST/dataset.html",
    "text": "SIND, ROCStory: SIND is a visual storytelling dataset [Cite_Footnote_3] (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset (Wang and Wan, 2019; Mostafazadeh et al., 2016). It is randomly split by 8:1:1 for the training/validation/test sets. Both of two datasets consist of 5 sentences in each story text."
  },
  {
    "id": 1024,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/sodawater/SentenceOrdering",
    "section_title": "3 Experiments 3.1 Datasets",
    "add_info": "4 https://github.com/sodawater/SentenceOrdering",
    "text": "SIND, ROCStory: SIND is a visual storytelling dataset (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset [Cite_Footnote_4] (Wang and Wan, 2019; Mostafazadeh et al., 2016). It is randomly split by 8:1:1 for the training/validation/test sets. Both of two datasets consist of 5 sentences in each story text."
  },
  {
    "id": 1025,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/DeepLearnXMU/NSEG",
    "section_title": "3 Experiments 3.3 Experimental Setup",
    "add_info": "5 Code for metrics: https://github.com/DeepLearnXMU/NSEG",
    "text": "We adopt the BERT BASE in the experiment and fine-tune it on each dataset. The paragraph en-coder has 2 self-attention layers with 8 heads. The hidden size is 768 and beam size is 16. Adam is employed as the optimizer. To search for the op-timal hyper-parameters, we adopt the grid search strategy for learning rate from {2e-5, 5e-5}, batch size from {8, 16, 32}, the number of epochs from {5, 10, 20}, and the coefficient \u03b1 in the loss func-tion from {0.2, 0.4, 0.6, 0.8, 1.0}. The model with the best performance on the validation set is se-lected for each setting. The recommended hyper-parameter configuration of the model on each dataset are presented in Table 3. To diminish the effects of randomness in training, the results of our model are averaged with [Cite_Footnote_5] random initializa-tions. For data preprocessing, we use the tok-enizer 6 from BERT to preprocess the sentences. The experiments are conducted on GeForce GTX 1080Ti GPU with PyTorch framework ."
  },
  {
    "id": 1026,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "3 Experiments 3.3 Experimental Setup",
    "add_info": "6 https://github.com/google-research/bert",
    "text": "We adopt the BERT BASE in the experiment and fine-tune it on each dataset. The paragraph en-coder has 2 self-attention layers with 8 heads. The hidden size is 768 and beam size is 16. Adam is employed as the optimizer. To search for the op-timal hyper-parameters, we adopt the grid search strategy for learning rate from {2e-5, 5e-5}, batch size from {8, 16, 32}, the number of epochs from {5, 10, 20}, and the coefficient \u03b1 in the loss func-tion from {0.2, 0.4, 0.6, 0.8, 1.0}. The model with the best performance on the validation set is se-lected for each setting. The recommended hyper-parameter configuration of the model on each dataset are presented in Table 3. To diminish the effects of randomness in training, the results of our model are averaged with 5 random initializa-tions. For data preprocessing, we use the tok-enizer [Cite_Footnote_6] from BERT to preprocess the sentences. The experiments are conducted on GeForce GTX 1080Ti GPU with PyTorch framework ."
  },
  {
    "id": 1027,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/shrimai/Topological-Sort-for-Sentence-Ordering",
    "section_title": "A Appendix A.2 Two other metrics used in the Analysis",
    "add_info": "10 Codes for metric: https://github.com/shrimai/Topological-Sort-for-Sentence-Ordering",
    "text": "Longest Common Subsequence (LCS): It cal-culates the percentage of longest correct sub-sequence between the predicted order and the gold order (Gong et al., 2016). The consecutiveness is not necessary for it [Cite_Footnote_10] ."
  },
  {
    "id": 1028,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/DeepLearnXMU/NSEG",
    "section_title": "A Appendix A.2 Two other metrics used in the Analysis",
    "add_info": "11 Code for metric: https://github.com/DeepLearnXMU/NSEG",
    "text": "Rouge-S: This metric (Chen et al., 2016; Gong et al., 2016) measures the fraction of pairs of sen-tences whose predicted relative order is the same as the ground truth order [Cite_Footnote_11] . It allows for any arbi-trary gaps between two sentences as long as their relative order is correctly identified."
  },
  {
    "id": 1029,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/fenchri/dsre-vae",
    "section_title": "References",
    "add_info": "1 Source code is available at https://github.com/fenchri/dsre-vae",
    "text": "We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extrac-tion by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the la-tent space of sentences via a Variational Au-toencoder ( VAE ) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indi-cate that multi-task learning results in perfor-mance benefits. Additional exploration of em-ploying Knowledge Base priors into the VAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results [Cite_Footnote_1] ."
  },
  {
    "id": 1030,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://developers.google.com/freebase",
    "section_title": "3 Experimental Settings 3.1 Datasets",
    "add_info": "2 https://developers.google.com/freebase 3 https://www.wikidata.org/",
    "text": "For the choice of the Knowledge Base, we use a subset of Freebase [Cite_Footnote_2] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples."
  },
  {
    "id": 1031,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.wikidata.org/",
    "section_title": "3 Experimental Settings 3.1 Datasets",
    "add_info": "2 https://developers.google.com/freebase 3 https://www.wikidata.org/",
    "text": "For the choice of the Knowledge Base, we use a subset of Freebase [Cite_Footnote_2] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples."
  },
  {
    "id": 1032,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://deepgraphlearning.github.io/project/wikidata5m",
    "section_title": "3 Experimental Settings 3.1 Datasets",
    "add_info": "4 https://deepgraphlearning.github.io/project/wikidata5m",
    "text": "For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [Cite_Footnote_4] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning."
  },
  {
    "id": 1033,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/saffsd/langid.py",
    "section_title": "6 Data and Evaluation",
    "add_info": "1 https://github.com/saffsd/langid.py",
    "text": "The training data consist of two corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the Lang-8 Learner Corpora v2 (Mizumoto et al., 2011). We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py [Cite_Footnote_1] (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source-target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2. A sentence pair where the source or target sentence has more than 80 words is also removed from both NUCLE and Lang-8. The statistics of the data after pre-processing are shown in Table 1."
  },
  {
    "id": 1034,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://deeplearning.net/software/theano",
    "section_title": "7 Experiments and Results 7.2 NNJM Adaptation",
    "add_info": "2 http://deeplearning.net/software/theano",
    "text": "We implement NNJM in Python using the deep learning library Theano [Cite_Footnote_2] (Bergstra et al., 2010) in order to use the massively parallel processing power of GPUs for training. We first train an NNJM (NNJM B ASELINE ) with complete training data for 10 epochs. The source context window size is set to 5 and the target context window size is set to 4, mak-ing it a (5+5)-gram joint model. Training is done using stochastic gradient descent with a mini-batch of GEC systems. All results are averaged over 5 runs of tuning size of 128 and learning rate of 0.1. To speed up training and decoding, a single hidden layer neural network is used with an input embedding dimen-sion of 192 and 512 hidden units. We use a self-normalization coefficient of 0.1. We pick 16,000 and 32,000 most frequent words on the source and tar-get sides as our source context vocabulary and target context vocabulary, respectively. The output vocab-ulary is set to be the same as the target vocabulary. The vocabulary is selected from the complete train-ing data, and not based on the L1-specific in-domain data. We add the self-normalized NNJM as a fea-ture to our baseline GEC system, S C ONCAT to build a stronger baseline. This is referred to as S C ONCAT + NNJM B ASELINE in Table 4."
  },
  {
    "id": 1035,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://psy.takelab.fer.hr/datasets/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "6 https://psy.takelab.fer.hr/datasets/",
    "text": "We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurkovic\u0301 et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora [Cite_Footnote_6] is another dataset collected from Reddit, where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset."
  },
  {
    "id": 1036,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.reddit.com/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "7 https://www.reddit.com/",
    "text": "We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurkovic\u0301 et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora is another dataset collected from Reddit, [Cite_Footnote_7] where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset."
  },
  {
    "id": 1037,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://pytorch.org/",
    "section_title": "4 Experiments 4.3 Training Details",
    "add_info": "8 https://pytorch.org/",
    "text": "We implement our TrigNet in Pytorch [Cite_Footnote_8] and train it on four NVIDIA RTX 2080Ti GPUs. Adam (Kingma and Ba, 2014) is utilized as the optimizer, with the learning rate of BERT set to 2e-5 and of other components set to 1e-3. We set the maxi-mum number of posts, r, to 50 and the maximum length of each post, s, to 70, considering the limit of available computational resources. After tuning on the validation dataset, we set the dropout rate to 0.2 and the mini-batch size to 32. The maximum number of nodes, r + m + n, is set to 500 for Kag-gle and 970 for Pandora, which cover 98.95% and 97.07% of the samples, respectively. Moreover, the two hyperparameters, the numbers of flow GAT layers L and heads K, are searched in {1, 2, 3} and {1, 2, 4, 6, 8, 12, 16, 24}, respectively, and the best choices are L = 1 and K = 12. The reasons for L = 1 are likely twofold. First, our flow GAT can already realize the interactions between nodes when L = 1, whereas the vanilla GAT needs to stack 4 layers. Second, after trying L = 2 and L = 3, we find that they lead to slight performance drops compared to that of L = 1."
  },
  {
    "id": 1038,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf",
    "section_title": "2 Related Work 2.1 Personality Detection",
    "add_info": "Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).",
    "text": "Traditional studies on this problem generally re-sort to feature-engineering methods, which first ex-tracts various psychological categories via LIWC (Tausczik and Pennebaker, 2010) or statistical fea-tures by the bag-of-words model (Zhang et al., 2010). These features are then fed into a classi-fier such as SVM (Cui and Qi, 2017)  and XGBoost (Tadesse et al., 2018) to predict the personality traits. Despite interpretable features that can be expected, feature engineering has such limitations as it relies heavily on manually designed features."
  },
  {
    "id": 1039,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.h-its.org/nlp/download",
    "section_title": "3 Corpus Creation 3.3 Gold Standard",
    "add_info": null,
    "text": "Our final gold standard corpus consists of 50 texts from the WSJ portion of the OntoNotes corpus- The corpus will be made publically available as OntoNotes annotation layer via  http://www.h-its.org/nlp/download."
  },
  {
    "id": 1040,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://gitlab.com/sutdnlp/statnlp-core",
    "section_title": "4 Experiments",
    "add_info": "12 https://gitlab.com/sutdnlp/statnlp-core",
    "text": "Hyperparameters We set the maximum depth c of the semantic tree to 20, following Lu (2015). The L 2 regularization coefficient is tuned from 0.01 to 0.05 using 5-fold cross-validation on the training set. The Polyglot (Al-Rfou et al., 2013) multilingual word embeddings (with 64 dimen-sions) are used for all languages. We use L-BFGS (Liu and Nocedal, 1989) to optimize the D EP HT model until convergence and stochas-tic gradient descent (SGD) with a learning rate of 0.05 to optimize the neural D EP HT model. We implemented our neural component with the Torch7 library (Collobert et al., 2011). Our com-plete implementation is based on the StatNLP [Cite_Footnote_12] structured prediction framework (Lu, 2017)."
  },
  {
    "id": 1041,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/facebookresearch/EGG",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "EGG is implemented in PyTorch (Paszke et al., 2017) and it is licensed under the MIT license. EGG can be installed from  https://github.com/facebookresearch/EGG."
  },
  {
    "id": 1042,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://www.tensorflow.org/tensorboard",
    "section_title": "1 Introduction",
    "add_info": "2 https://www.tensorflow.org/tensorboard",
    "text": "Notable features of EGG include: (a) Prim-itives for implementing single-symbol or variable-length communication (with vanilla RNNs (Elman, 1990), GRUs (Cho et al., 2014), LSTMs (Hochreiter and Schmidhuber, 1997)); (b) Training with optimization of the com-munication channel through REINFORCE or Gumbel-Softmax relaxation via a common interface; (c) Simplified configuration of the general components, such as check-pointing, optimization, Tensorboard support, [Cite_Footnote_2] etc.; (d) A simple CUDA-aware command-line tool for hyperparameter grid-search."
  },
  {
    "id": 1043,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://colab.research.google.com/github/facebookresearch/EGG/blob/master/egg/zoo/language_bottleneck/",
    "section_title": "5 Some pre-implemented games",
    "add_info": "5 A small illustration can be run in Google Colab-oratory: https://colab.research.google.com/github/facebookresearch/EGG/blob/master/egg/zoo/language_bottleneck/mnist-style-transfer-via-bottleneck.ipynb.",
    "text": "EGG contains implementations of several games. They (a) illustrate how EGG can be used to ex-plore interesting research questions, (b) provide reference usage patterns and building blocks, (c) serve as means to ensure reproducibility of stud-ies reported in the literature. For example, EGG incorporates an implementation of the signaling game of Lazaridou et al. (2016) and Bouchacourt and Baroni (2018). It contains code that was re-cently used to study the communicative efficiency of artificial LSTM-based agents (Chaabouni et al., 2019a) and the information-minimization proper-ties of emergent discrete codes (Kharitonov et al., 2019). [Cite_Footnote_5] Finally, EGG provides a pre-implemented game that allows to train agents entirely via the command line and external input/output files, without having to write a single line of Python code. We hope this will lower the learning curve for those who want to experiment with language emergence without previous coding experience."
  },
  {
    "id": 1044,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "https://seekingalpha.com/",
    "section_title": "3 Data and pre-processing",
    "add_info": "2 In Appendix A in supplemental material we provide the stock tickers for the calls in our data; the corpus can be re-assembled from multiple sources, such as https://seekingalpha.com/.",
    "text": "Our data [Cite_Footnote_2] consists of transcripts of 12,285 earn-ings calls held between January 1, 2010 and De-cember 31, 2017. In order to control for analyst coverage effects (larger companies with a greater market share will typically be covered by more an-alysts), we include only calls from S&P 500 com-panies. We split the data by year into training, val-idation and testing sets (see Table 1)."
  },
  {
    "id": 1045,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0",
    "section_title": "4 Pragmatic correlations with analysts\u2019 pre-call judgments 4.1 Pragmatic lexical features",
    "add_info": "6 Version 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0. pdf Section 2.6.",
    "text": "Named entity counts and concreteness ratio. For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [Cite_Footnote_6] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn."
  },
  {
    "id": 1046,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://pytorch.org/",
    "section_title": "5 Predicting changes in analysts\u2019 post-call forecasts 5.3 Models 5.3.3 Turn-by-turn models",
    "add_info": "20 https://pytorch.org/",
    "text": "Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [Cite_Footnote_20] and optimized with Adam (Kingma and Ba, 2014) ."
  },
  {
    "id": 1047,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.msci.com/gics",
    "section_title": "5 Predicting changes in analysts\u2019 post-call forecasts 5.4 Results.",
    "add_info": "21 See https://www.msci.com/gics. There are 11 broad industry sectors.",
    "text": "Breakdown of results by industry. We ana-lyze errors on the validation data by segmenting earnings calls by each company\u2019s Global Industry Classification Standard (GICS) sector [Cite_Footnote_21] . See Fig-ure 2 for the breakdown results. Notably, the bag-of-words model performs almost 2.5 times worse on earnings calls from the Materials sector versus the Utilities and Telecommunication Services sec-tors. This suggests industry-specific models may be important in future work."
  },
  {
    "id": 1048,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://code.google.com/p/mate-tools/",
    "section_title": "4 Experiments",
    "add_info": "1 http://code.google.com/p/mate-tools/",
    "text": "We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity [Cite_Footnote_1] or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels."
  },
  {
    "id": 1049,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://verbs.colorado.edu/jdpacorpus/",
    "section_title": "4 Experiments",
    "add_info": "2 Available from http://verbs.colorado.edu/jdpacorpus/ \u2013 we ignore cars batch 009 where no arguments of comparative predicates are annotated.",
    "text": "We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity or entity [Cite_Footnote_2] depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels."
  },
  {
    "id": 1050,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.uic.edu/\u02dcliub/FBS/data.tar.gz",
    "section_title": "4 Experiments",
    "add_info": "3 Available from http://www.cs.uic.edu/\u02dcliub/FBS/data.tar.gz \u2013 although the original paper works on some unknown subset of this data, so our results are not directly",
    "text": "We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) [Cite_Footnote_3] . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels."
  },
  {
    "id": 1051,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/corenlp.shtml",
    "section_title": "4 Experiments",
    "add_info": "4 http://nlp.stanford.edu/software/corenlp.shtml",
    "text": "We do sentence segmentation and tokenization with the Stanford Core NLP [Cite_Footnote_4] . Annotations are mapped to the extracted tokens. We ignore anno-tations that do not correspond to complete tokens. In the JDPA corpus, if an annotated argument is out-side the current sentence, we follow the coreference chain to find a coreferent annotation in the same sen-tence. If this is not successful, the argument is ig-nored. We extract all sentences where we found at least one comparative predicate as our dataset."
  },
  {
    "id": 1052,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://odur.let.rug.nl/\u223cvannoord/TextCat/",
    "section_title": "4 Building Noun Similarity Lists 4.1 Web Corpus",
    "add_info": "4 http://odur.let.rug.nl/\u223cvannoord/TextCat/",
    "text": "We set up a spider to download roughly 70 million web pages from the Internet. Initially, we use the links from Open Directory project as seed links for our spider. Each webpage is stripped of HTML tags, tokenized, and sentence segmented. Each docu-ment is language identified by the software TextCat [Cite_Footnote_4] which implements the paper by Cavnar and Trenkle (1994). We retain only English documents. The web contains a lot of duplicate or near-duplicate docu-ments. Eliminating them is critical for obtaining bet-ter representation statistics from our collection. The problem of identifying near duplicate documents in linear time is not trivial. We eliminate duplicate and near duplicate documents by using the algorithm de-scribed by Kolcz et al. (2004). This process of dupli-cate elimination is carried out in linear time and in-volves the creation of signatures for each document. Signatures are designed so that duplicate and near duplicate documents have the same signature. This algorithm is remarkably fast and has high accuracy. This entire process of removing non English docu-ments and duplicate (and near-duplicate) documents reduces our document set from 70 million web pages to roughly 31 million web pages. This represents roughly 138GB of uncompressed text."
  },
  {
    "id": 1053,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://w3.usf.edu/FreeAssociation/",
    "section_title": "3 The SimVerb-3500 Data Set 3.2 Choice of Verb Pairs and Coverage",
    "add_info": "3 http://w3.usf.edu/FreeAssociation/",
    "text": "To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set [Cite_Footnote_3] (Nelson et al., 2004), and (2) the VerbNet verb lexicon (Kipper et al., 2004; Kipper et al., 2008)."
  },
  {
    "id": 1054,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://verbs.colorado.edu/verb-index/",
    "section_title": "3 The SimVerb-3500 Data Set 3.2 Choice of Verb Pairs and Coverage",
    "add_info": "4 http://verbs.colorado.edu/verb-index/",
    "text": "To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set (Nelson et al., 2004), and (2) the VerbNet verb lexicon [Cite_Footnote_4] (Kipper et al., 2004; Kipper et al., 2008)."
  },
  {
    "id": 1055,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://prolific.ac/",
    "section_title": "4 Word Pair Scoring",
    "add_info": "11 https://prolific.ac/ (We chose PA for logistic reasons.)",
    "text": "We employ the Prolific Academic (PA) crowdsourc-ing platform, [Cite_Footnote_11] an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower."
  },
  {
    "id": 1056,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.umiacs.umd.edu/\u02dcjbg/static/downloads_and_media.html",
    "section_title": "4 Experiments 4.3 Sentiment Prediction",
    "add_info": "6 We followed Pang and Lee\u2019s method for creating a nu-merical score between 0 and 1 from a star rating. We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not as-sume integer values). The German movie review corpus is available at http://www.umiacs.umd.edu/\u02dcjbg/static/downloads_and_media.html",
    "text": "We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. [Cite_Footnote_6]"
  },
  {
    "id": 1057,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.mdbg.net/chindict/",
    "section_title": "3 Bridges Across Languages",
    "add_info": "Paul Denisowski. 1997. CEDICT. http://www.mdbg.net/chindict/.",
    "text": "Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997)  , and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b)."
  },
  {
    "id": 1058,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://chdw.de",
    "section_title": "3 Bridges Across Languages",
    "add_info": "Jan Hefti. 2005. HanDeDict. http://chdw.de.",
    "text": "Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005)  . As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b)."
  },
  {
    "id": 1059,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.statmt.org/europarl/",
    "section_title": "4 Experiments 4.1 Matching on Multilingual Topics",
    "add_info": "Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit. http://www.statmt.org/europarl/.",
    "text": "We took the 1996 documents from the Europarl cor-pus (Koehn, 2005)  using three bridges: GermaNet, dictionary, and the uninformative flat matching. The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework (as there is no associated response variable for Eu-roparl documents); this experiment is to demonstrate the effectiveness of the multilingual aspect of the model. To test whether the topics learned by the model are consistent across languages, we represent each document using the probability distribution \u03b8 d over topic assignments. Each \u03b8 d is a vector of length K and is a language-independent representation of the document."
  },
  {
    "id": 1060,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://snowball.tartarus.org/credits.php",
    "section_title": "3 Bridges Across Languages",
    "add_info": "Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php.",
    "text": "WordNet We took the alignment of GermaNet to WordNet 1.6 (Kunze and Lemnitzer, 2002) and re-moved all synsets that were had no mapped German words. Any German synsets that did not have English translations had their words mapped to the lowest extant English hypernym (e.g. \u201cbeinbruch,\u201d a bro-ken leg, was mapped to \u201cfracture\u201d). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970)  . An example of the paths for the German word \u201cwunsch\u201d (wish, request) is shown in Figure 2(a)."
  },
  {
    "id": 1061,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://snowball.tartarus.org/credits.php",
    "section_title": "4 Experiments 4.1 Matching on Multilingual Topics",
    "add_info": "Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php.",
    "text": "4 For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970)  , and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content words were excluded."
  },
  {
    "id": 1062,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www-user.tu-chemnitz.de/fri/ding/",
    "section_title": "3 Bridges Across Languages",
    "add_info": "Frank Richter. 2008. Dictionary nice grep. http://www-user.tu-chemnitz.de/fri/ding/.",
    "text": "Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008)  a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b)."
  },
  {
    "id": 1063,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.filmrezension.de",
    "section_title": "4 Experiments 4.3 Sentiment Prediction",
    "add_info": "Tobias Vetter, Manfred Sauer, and Philipp Wallutat. 2000. Filmrezension.de: Online-magazin fu\u0308r filmkritik. http://www.filmrezension.de.",
    "text": "We gathered 330 film reviews from a German film review site (Vetter et al., 2000)  and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. 6"
  },
  {
    "id": 1064,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.nlm.nih.gov/research/umls/",
    "section_title": "1 Introduction",
    "add_info": "Url1. 2013. Umls: Unified medical language system (http://www.nlm.nih.gov/research/umls/) (ac-cessed july 1, 2013).",
    "text": "Clinical narratives, unlike newswire data, provide a domain with significant knowledge that can be ex-ploited systematically to improve the accuracy of the prediction task. Knowledge in this domain can be thought of as belonging to two categories: (1) Background Knowledge captured in medical ontolo-gies like UMLS (Url1, 2013)  , MeSH and SNOMED CT and (2) Discourse Knowledge driven by the fact that the narratives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the lat-ter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference for-mulation."
  },
  {
    "id": 1065,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://metamap.nlm.nih.gov/",
    "section_title": "2 Identifying Medical Concepts",
    "add_info": "Url2. 2013. Metamap (http://metamap.nlm.nih.gov/) (accessed july 1, 2013).",
    "text": "Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013  ), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) of words in a window of size 3. We also use conjunctions of the features."
  },
  {
    "id": 1066,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cogcomp.cs.illinois.edu/page/softwareview/",
    "section_title": "2 Identifying Medical Concepts",
    "add_info": "Url3. 2013. Illinois part-of-speech tagger (http://cogcomp.cs.illinois.edu/page/softwareview/ pos) (accessed july 1, 2013).",
    "text": "Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013)  of words in a window of size 3. We also use conjunctions of the features."
  },
  {
    "id": 1067,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.nlm.nih.gov/mesh/meshhome.html",
    "section_title": "2 Identifying Medical Concepts",
    "add_info": "Url4. 2013. Mesh: Medical subject headings (http://www.nlm.nih.gov/mesh/meshhome.html) (ac-cessed july 1, 2013).",
    "text": "After finding concept boundaries, we determine the probability distribution for each concept over 4 possible types (TEST, TRE, PROB or NULL). These probability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011). Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013)  and SNOMED CT (Url5, 2013), MeSH and SNOMED CT descriptors."
  },
  {
    "id": 1068,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.ihtsdo.org/snomed-ct/",
    "section_title": "2 Identifying Medical Concepts",
    "add_info": "Url5. 2013. Snomed ct: Snomed clinical terms (http://www.ihtsdo.org/snomed-ct/) (accessed july 1, 2013).",
    "text": "After finding concept boundaries, we determine the probability distribution for each concept over 4 possible types (TEST, TRE, PROB or NULL). These probability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011). Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) and SNOMED CT (Url5, 2013)  , MeSH and SNOMED CT descriptors."
  },
  {
    "id": 1069,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.gurobi.com/",
    "section_title": "3 Modeling Global Inference 3.2 Final Optimization Problem - An IQP",
    "add_info": "Url6. 2013. Gurobi optimization toolkit (http://www.gurobi.com/) (accessed july 1, 2013).",
    "text": "After incorporating all the constraints mentioned above, the final optimization problem (an IQP) is shown in Figure 2. We used Gurobi toolkit (Url6, 2013)  to solve such IQPs. In our case, it solves 76 IQPs per second on a quad-core server with In-tel Xeon X5650 @ 2.67 GHz processors and 50 GB RAM."
  },
  {
    "id": 1070,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "https://www.ldc.upenn.edu/collaborations/past-projects/ace",
    "section_title": "4 Research Plan",
    "add_info": "1 https://www.ldc.upenn.edu/collaborations/past-projects/ace",
    "text": "Besides employing active learning to create spe-cific data set with nested, overlapping, discontinu-ous entity mentions, we notice that there are some off-the-shelf corpora in biomedical domain that we can use to evaluate our proposed methods, al-though, to our knowledge, none of these data sets contains all three kinds of complex mentions, e.g., GENIA (Kim et al., 2003) only contains nested en-tity mentions, and CADEC (Karimi et al., 2015) and SemEval2014 (Pradhan et al., 2014) contain overlapping and discontinuous mentions. In ad-dition, ACE [Cite_Footnote_1] and NNE (Ringland, 2016) are newswire corpora with nested entity mentions."
  },
  {
    "id": 1071,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.dh.uni-leipzig.de/wo/projects/open-greek-and-latin-project/",
    "section_title": "3 Generating PHI-ML",
    "add_info": "Gregory Ralph Crane, Monica Berti, Annette Ge\u00dfner, Matthew Munson, and Tabea Selle. 2014. Open greek and latin project. http://www.dh.uni-leipzig.de/wo/projects/open-greek-and-latin-project/. Ac-cessed on 2019-04-24.",
    "text": "Due to availability of digitised epigraphic corpora, P YTHIA has been trained on ancient Greek (hence-forth, \"AG\") inscriptions, written in the ancient Greek language between 7 th century BCE and 5 th century CE. We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014)  for ancient literary texts."
  },
  {
    "id": 1072,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions",
    "section_title": "3 Generating PHI-ML",
    "add_info": "Laura Gawlinski. 2017. Review: Packard hu-manities institute\u2019s searchable greek inscrip-tions. https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions. Accessed on 2019-08-26.",
    "text": "Due to availability of digitised epigraphic corpora, P YTHIA has been trained on ancient Greek (hence-forth, \"AG\") inscriptions, written in the ancient Greek language between 7 th century BCE and 5 th century CE. We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017  ) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) for ancient literary texts."
  },
  {
    "id": 1073,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://people.cs.kuleuven.be/\u223civan.vulic/software/",
    "section_title": "4 Experimental Setup",
    "add_info": "1 Available at http://people.cs.kuleuven.be/\u223civan.vulic/software/",
    "text": "Test Data. We have constructed test datasets in Spanish (ES), Italian (IT) and Dutch (NL), where the aim is to find their correct translation in En-glish (EN) given the sentential context. We have selected 15 polysemous nouns (see tab. 2 for the list of nouns along with their possible transla-tions) in each of the 3 languages, and have man-ually extracted 24 sentences (not present in the training data) for each noun that capture different meanings of the noun from Wikipedia. In order to construct datasets that are balanced across dif-ferent possible translations of a noun, in case of q different translation candidates in T for some word w S1 , the dataset contains exactly 24/q sen-tences for each translation from T . In total, we have designed 360 sentences for each language pair (ES/IT/NL-EN), 1080 sentences in total. [Cite_Footnote_1] . We have used 5 extra nouns with 20 sentences each as a development set to tune the parameters of our models. As a by-product, we have built an initial repository of ES/IT/NL ambiguous words. Tab. 1 presents a small sample from the IT evaluation dataset, and illustrates the task of suggesting word translations in context."
  },
  {
    "id": 1074,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.naist.jp/hiki/ChaSen/",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "1 http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)",
    "text": "We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen [Cite_Footnote_1] 2.3.3 and then labeled the NEs. Unreadable to-kens such as parentheses were removed in to-kenization. After tokenization, the text cor-pus had 264,388 words of 60 part-of-speech types. Since three different kinds of charac-ters are used in Japanese, the character types used as features included: single-kanji (words written in a single Chinese charac-ter), all-kanji (longer words written in Chi-nese characters), hiragana (words written in hiragana Japanese phonograms), katakana (words written in katakana Japanese phono-grams), number, single-capital (words with a single capitalized letter), all-capital, capitalized (only the first letter is capital-ized), roman (other roman character words), and others (all other words). We used all the fea-tures that appeared in each training set (no feature selection was performed). The chunking states in-cluded in the NE classes were: BEGIN (beginning of a NE), MIDDLE (middle of a NE), END (ending of a NE), and SINGLE (a single-word NE). There were 33 NE classes (eight categories * four chunk-ing states + OTHER), and therefore we trained 33 SVMs to distinguish words of a class from words of other classes. For NER, we used an SVM-based chunk annotator YamCha 2 0.33 with a quadratic kernel (1 + x~ \u00b7 y~) and a soft margin parameter of SVMs C=0.1 for training and applied sigmoid function s n (x) with \u03b2 n =1.0 and Viterbi search to the SVMs\u2019 outputs. These parameters were exper-imentally chosen using the test set."
  },
  {
    "id": 1075,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.chasen.org/\u02dctaku/software/yamcha/",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "2 http://www.chasen.org/\u02dctaku/software/yamcha/",
    "text": "We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen 2.3.3 and then labeled the NEs. Unreadable to-kens such as parentheses were removed in to-kenization. After tokenization, the text cor-pus had 264,388 words of 60 part-of-speech types. Since three different kinds of charac-ters are used in Japanese, the character types used as features included: single-kanji (words written in a single Chinese charac-ter), all-kanji (longer words written in Chi-nese characters), hiragana (words written in hiragana Japanese phonograms), katakana (words written in katakana Japanese phono-grams), number, single-capital (words with a single capitalized letter), all-capital, capitalized (only the first letter is capital-ized), roman (other roman character words), and others (all other words). We used all the fea-tures that appeared in each training set (no feature selection was performed). The chunking states in-cluded in the NE classes were: BEGIN (beginning of a NE), MIDDLE (middle of a NE), END (ending of a NE), and SINGLE (a single-word NE). There were 33 NE classes (eight categories * four chunk-ing states + OTHER), and therefore we trained 33 SVMs to distinguish words of a class from words of other classes. For NER, we used an SVM-based chunk annotator YamCha 2 0.33 with a quadratic kernel (1 + x~ \u00b7 y~) [Cite_Footnote_2] and a soft margin parameter of SVMs C=0.1 for training and applied sigmoid function s n (x) with \u03b2 n =1.0 and Viterbi search to the SVMs\u2019 outputs. These parameters were exper-imentally chosen using the test set."
  },
  {
    "id": 1076,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://archive.ics.uci.edu/ml",
    "section_title": "3 Learning Classifiers from Language 3.1 Mapping language to constraints 3.1.1 Semantic Parser components",
    "add_info": "M. Lichman. 2013. UCI machine learning repository. http://archive.ics.uci.edu/ml.",
    "text": "Since the constraint type is determined by syntactic and dependency parse features, this component does not need to be retrained for new domains. In this work, we trained this classifier based on a manually annotated set of 80 sentences describing classes in the small UCI Zoo dataset (Lichman, 2013)  , and used this model for all experiments."
  },
  {
    "id": 1077,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN",
    "section_title": "5 Experimental Settings",
    "add_info": "Sadao Kurohashi and Daisuke Kawahara, 2009a. Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.",
    "text": "We used JUMAN (Kurohashi and Kawahara, 2009a)  for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an \u201cobligatory case\u201d or an \u201cadjacent case\u201d, we regarded dependency relations flagged by KNP as obligatory in the sentence com-pression. KNP utilizes Kyoto University\u2019s case frames (Kawahara and Kurohashi, 2006) as the re-source for detecting obligatory or adjacent cases."
  },
  {
    "id": 1078,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP",
    "section_title": "5 Experimental Settings",
    "add_info": "Sadao Kurohashi and Daisuke Kawahara, 2009b. KN parser (Kurohashi-Nagao parser) 3.0 Users Man-ual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.",
    "text": "We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b)  . Since KNP internally has a flag that indicates either an \u201cobligatory case\u201d or an \u201cadjacent case\u201d, we regarded dependency relations flagged by KNP as obligatory in the sentence com-pression. KNP utilizes Kyoto University\u2019s case frames (Kawahara and Kurohashi, 2006) as the re-source for detecting obligatory or adjacent cases."
  },
  {
    "id": 1079,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.cnts.ua.ac.be/conll2000/chunking/output.html",
    "section_title": "6 Experiments 6.2 Baselines",
    "add_info": "3 http://www.cnts.ua.ac.be/conll2000/chunking/output.html",
    "text": "To apply these methods except for Target, we treat each of the eight domains in turn as the test domain, with one of remaining seven domain as the source domain. As in general domain adap-tation setting, we assume that the source domain has a sufficient amount of labeled data but the tar-get domain has an insufficient amount of labeled data. Specifically, For each test or target domain, we only use 10% of the training examples to sim-ulate data scarcity. In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script [Cite_Footnote_3]"
  },
  {
    "id": 1080,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/deansong/contextLSTMCNN",
    "section_title": "angus.roberts@kcl.ac.uk",
    "add_info": "1 The code is publicly available at https://github.com/deansong/contextLSTMCNN",
    "text": "In many cases, however, the context available is of significant size. We therefore introduce a new method, Context-LSTM-CNN [Cite_Footnote_1] , which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method (Zhang et al., 2015), and an architecture that combines an LSTM and CNN for the focus sentence. The method consis-tently improves over results obtained from either LSTM alone, CNN alone, or these two combined, with little increase in training time."
  },
  {
    "id": 1081,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://sail.usc.edu/iemocap/iemocap_release.htm",
    "section_title": "4 Experiments",
    "add_info": "2 http://sail.usc.edu/iemocap/iemocap_release.htm",
    "text": "Interactive Emotional Dyadic Motion Cap-ture Database (Busso et al., 2008) [Cite_Footnote_2] (IEMO-CAP). Originally created for the analysis of hu-man emotions based on speech and video, a tran-script of the speech component is available for NLP research. Each sentence in the dialogue is annotated with one of 10 types of emotion. There is a class imbalance in the labelled data, and so we follow the approach of (Chernykh et al., 2017), and only use sentences classified with one of four labels (\u2018Anger\u2019, \u2018Excitement\u2019, \u2018Neutral\u2019 and \u2018Sad-ness\u2019). For this dataset, instead of using left and right contexts, we assign all sentences from one person to one context and all sentences from the other person to the other context. While only the sentences with the four classes of interest are used for classification, all sentences of the dialog are used as the context. This results in a set of 4936 la-belled sentences with average sentence length 14, and average document length is 986."
  },
  {
    "id": 1082,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://sites.google.com/site/adecorpus/home/document",
    "section_title": "4 Experiments",
    "add_info": "3 https://sites.google.com/site/adecorpus/home/document",
    "text": "Drug-related Adverse Effects (Gurulingappa et al., 2012) [Cite_Footnote_3] (ADE). This dataset contains sen-tences sampled from the abstracts of medical case reports. For each sentence, the annotation indi-cates whether adverse effects of a drug are be-ing described (\u2018Positive\u2019) or not (\u2018Negative\u2019). The original release of the data does not contain the document context, which we reconstructed from PubMed . Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129."
  },
  {
    "id": 1083,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.ncbi.nlm.nih.gov/pubmed/",
    "section_title": "4 Experiments",
    "add_info": "4 https://www.ncbi.nlm.nih.gov/pubmed/",
    "text": "Drug-related Adverse Effects (Gurulingappa et al., 2012) (ADE). This dataset contains sen-tences sampled from the abstracts of medical case reports. For each sentence, the annotation indi-cates whether adverse effects of a drug are be-ing described (\u2018Positive\u2019) or not (\u2018Negative\u2019). The original release of the data does not contain the document context, which we reconstructed from PubMed [Cite_Footnote_4] . Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129."
  },
  {
    "id": 1084,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/attardi/wikiextractor",
    "section_title": "3 Experimental settings",
    "add_info": "4 https://github.com/attardi/wikiextractor",
    "text": "More concretely, we use WikiExtractor [Cite_Footnote_4] to ex-tract plain text from Wikipedia dumps, and pre-process the resulting corpus using standard Moses tools (Koehn et al., 2007) by applying sentence splitting, punctuation normalization, tokenization with aggressive hyphen splitting, and lowercasing. We then train word embeddings for each language using the skip-gram implementation of fastText (Bojanowski et al., 2017) with default hyperpa-rameters, restricting the vocabulary to the 200,000 most frequent tokens. The official embeddings in the MUSE dataset were trained using these exact same settings, so our embeddings only differ in the Wikipedia dump used to extract the training cor-pus and the pre-processing applied to it, which is not documented in the original dataset."
  },
  {
    "id": 1085,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/artetxem/monoses",
    "section_title": "6 Conclusions and future work",
    "add_info": null,
    "text": "We propose a new approach to BLI which, instead of directly inducing bilingual dictionaries from cross-lingual embedding mappings, uses them to build an unsupervised machine translation system, which is then used to generate a synthetic paral-lel corpus from which to extract bilingual lexica. Our approach does not require any additional re-source besides the monolingual corpora used to train the embeddings, and outperforms traditional retrieval techniques by a substantial margin. We thus conclude that, contrary to recent trend, future work in BLI should not focus exclusively in direct retrieval approaches, nor should BLI be the only evaluation task for cross-lingual embeddings. Our code is available at  https://github.com/artetxem/monoses."
  },
  {
    "id": 1086,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://toolbox.google.com/factcheck/explorer",
    "section_title": "2 Related Work",
    "add_info": "9 http://toolbox.google.com/factcheck/explorer",
    "text": "In an industrial setting, Google has developed Fact Check Explorer, [Cite_Footnote_9] which is an exploration tool that allows users to search a number of fact-checking websites (those that use ClaimReview from schema.org ) for the mentions of a topic, a person, etc. However, the tool cannot handle a complex claim, as it runs Google search, which is not optimized for semantic matching of long claims. While this might change in the future, as there have been reports that Google has started using BERT in its search, at the time of writing, the tool could not handle a long claim as an input."
  },
  {
    "id": 1087,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://schema.org/ClaimReview",
    "section_title": "2 Related Work",
    "add_info": "10 http://schema.org/ClaimReview",
    "text": "In an industrial setting, Google has developed Fact Check Explorer, which is an exploration tool that allows users to search a number of fact-checking websites (those that use ClaimReview from schema.org [Cite_Footnote_10] ) for the mentions of a topic, a person, etc. However, the tool cannot handle a complex claim, as it runs Google search, which is not optimized for semantic matching of long claims. While this might change in the future, as there have been reports that Google has started using BERT in its search, at the time of writing, the tool could not handle a long claim as an input."
  },
  {
    "id": 1088,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.snopes.com/fact-check-ratings/",
    "section_title": "4 Datasets 4.2 Snopes Dataset",
    "add_info": "13 http://www.snopes.com/fact-check-ratings/",
    "text": "We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [Cite_Footnote_13] )."
  },
  {
    "id": 1089,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://svmlight.joachims.org/svmstruct.html",
    "section_title": "3 Problem Formulation 3.3 The Large Margin Model",
    "add_info": "1 http://svmlight.joachims.org/svmstruct.html",
    "text": "Figure 3: Induced synchronous grammar from a sen-tence pair using a strategy that extracts general rules. grammatical or are poor compressions. The train-ing procedure learns weights such that the model can discriminate between these trees and predict a good target tree. For this we develop a discriminative training process which learns a weighted tree-to-tree transducer. Our model is based on Tsochantaridis et al.\u2019s (2005) framework for learning Support Vector Machines (SVMs) with structured output spaces, us-ing the SVM struct implementation. [Cite_Footnote_1] We briefly sum-marise the approach below; for a more detailed de-scription we refer the interested reader to Tsochan-taridis et al. (2005)."
  },
  {
    "id": 1090,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://homepages.inf.ed.ac.uk/s0460084/data/",
    "section_title": "4 Evaluation Set-up",
    "add_info": "4 The corpus can be downloaded from http://homepages.inf.ed.ac.uk/s0460084/data/.",
    "text": "Corpora We evaluated our system on two dif-ferent corpora. The first is the compression cor-pus of Knight and Marcu (2002) derived automati-cally from the document-abstract pairs of the Ziff-Davis corpus. Previous compression work has al-most exclusively used this corpus. Our experiments follow Knight and Marcu\u2019s partition of training, test, and development sets (1,002/36/12 instances). We also present results on Clarke and Lapata\u2019s (2006a) Broadcast News corpus. [Cite_Footnote_4] This corpus was created manually (annotators were asked to produce com-pressions for 50 Broadcast news stories) and poses more of a challenge than Ziff-Davis. Being a speech corpus, it often contains incomplete and ungram-matical utterances and speech artefacts such as dis-fluencies, false starts and hesitations. Furthermore, spoken utterances have varying lengths, some are very wordy whereas others cannot be reduced any further. Thus a hypothetical compression system trained on this domain should be able to leave some sentences uncompressed. Again we used Clarke and Lapata\u2019s training, test, and development set split (882/410/78 instances)."
  },
  {
    "id": 1091,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://www.research.ibm.com/haifa/dept/vst/mlta_data.shtml",
    "section_title": "1 Introduction",
    "add_info": "1 https://www.research.ibm.com/haifa/dept/vst/mlta_data.shtml",
    "text": "We report various consistency measures that indicate the validity of TR9856. In addition, we report baseline results over TR9856 for sev-eral methods, commonly used to assess term\u2013 relatedness. Furthermore, we demonstrate how the new data can be exploited to train an ensemble\u2013 based method, that relies on these methods as un-derlying features. We believe that the new TR9856 benchmark, which is freely available for research purposes, [Cite_Footnote_1] along with the reported results, will contribute to the development of novel term relat-edness methods."
  },
  {
    "id": 1092,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://idebate.org/debatabase",
    "section_title": "3 Dataset generation methodology 3.1 Defining topics and articles of interest",
    "add_info": "2 http://idebate.org/debatabase",
    "text": "We start by observing that framing the related-ness question within a pre-specified context may simplify the task for humans and machines alike, in particular since the correct sense of ambigu-ous terms can be identified. Correspondingly, we focus on 47 topics selected from Debatabase [Cite_Footnote_2] . For each topic, 5 human annotators searched Wikipedia for relevant articles as done in (Aharoni et al., 2014). All articles returned by the annota-tors \u2013 an average of 21 articles per topic \u2013 were considered in the following steps. The expectation was that articles associated with a particular topic will be enriched with terms related to that topic, hence with terms related to one another."
  },
  {
    "id": 1093,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/PyTorchLightning/pytorch-lightning,3",
    "section_title": "C Hyperparameters",
    "add_info": "et al. Falcon, WA. 2019. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning,3.",
    "text": "Training details All of our experiments were done on a single Nvidia GeForceRTX 2080 Ti. We base our implementation on PyTorch (Paszke et al., 2017) and also use PyTorch Lightning (Fal-con, 2019)  and Huggingface (Wolf et al., 2019). The gates and the experts in our MoE model were a single layer MLP. For the experts, we set the in-put size set to be the same as output size. Table 7 shows the parameters shared by all the methods, and 8 shows the hyperparameters applicable to GCN encoder."
  },
  {
    "id": 1094,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/yym6472/OVSlotTagging",
    "section_title": "1 Introduction",
    "add_info": "1 Our code is available at https://github.com/yym6472/OVSlotTagging",
    "text": "In this paper, we propose a robust adversarial slot filling approach that explicitly decouples local se-mantic representation inherent in open-vocabulary slot words from the global context. Our approach aims to focus more on the holistic semantics at the level of the whole sentence, not only the vicin-ity of the local context within open-vocabulary slots. Specifically, our approach generates model-agnostic adversarial worst-case perturbations to the inputs in the direction that significantly increases the model\u2019s loss. Our main contributions are three-fold: (1) We dive into the issues of open-vocabulary slots in slot filling task and propose a novel adver-sarial semantic decoupling method which distin-guishes local semantics from the global context. (2) Our method can be easily applied to all the pre-vious slot filling neural-based models. (3) Experi-ments show that our proposed method consistently outperforms various SOTA baselines, especially in open-vocabulary slot f1. [Cite_Footnote_1]"
  },
  {
    "id": 1095,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "https://groups.csail.mit.edu/sls/downloads/restaurant/",
    "section_title": "3 Experiment 3.1 Setup",
    "add_info": "4 https://groups.csail.mit.edu/sls/downloads/restaurant/",
    "text": "Datasets To evaluate our approach, we conduct experiments on two public benchmark datasets, Snips (Coucke et al., 2018) and MIT-restaurant (MR) [Cite_Footnote_4] . Snips contains user utterances from vari-ous domains resulting in relatively extensive open-vocabulary slots, such as album and movie name. MR is a single-domain dataset associated with restaurant reservations, which contains open-vocabulary slots, such as restaurant name and amenity. Table 1 shows the full statistics and Ta-ble 2 shows all the open-vocabulary slots of Snips and MR datasets. Note that we identify the open-vocabulary slots according to the diversity of dif-ferent slot values as well as the average length of slot values."
  },
  {
    "id": 1096,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://glaros.dtc.umn.edu/gkhome/views/cluto",
    "section_title": "2 Related Work 3.2 NE-based Clustering for LM Adaptation",
    "add_info": "1 Available at http://glaros.dtc.umn.edu/gkhome/views/cluto",
    "text": "Clustering is a simple unsupervised topic analysis method. We use NEs to construct feature vectors for the documents, rather than considering all the words as in most previous work. We use the CLUTO [Cite_Footnote_1] toolkit to perform clustering. It finds a predefined number of clusters based on a specific criterion, for which we chose the following func-tion: where K is the desired number of clusters, S i is the set of documents belonging to the i th cluster, v and u represent two documents, and sim(v, u) is the similarity between them. We use the cosine dis-tance to measure the similarity between two docu-ments: where vr and ur are the feature vectors represent-ing the two documents respectively, in our experi-ments composed of NEs. For clustering, the elements in every feature vector are scaled based on their term frequency and inverse document fre-quency, a concept widely used in information re-trieval."
  },
  {
    "id": 1097,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf",
    "section_title": "2 Related Work 4.3 Clustering vs. LDA Based LM Adaptation",
    "add_info": "2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf",
    "text": "We suspect that using only named entities may not provide enough information about the \u2018topics\u2019 of the documents, therefore we investigate expanding the feature vectors with other words. Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [Cite_Footnote_2] . This is similar to the removal of stop words widely used in information retrieval."
  },
  {
    "id": 1098,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "https://github.com/TemporalKGTeam/TANGO",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "1 Code and datasets are available at https://github.com/TemporalKGTeam/TANGO. filtered MRR (%) on ICEWS05-15 with regard to dif-ferent \u2206t. preprint arXiv:2012.08492.",
    "text": "We evaluate our model by performing future link prediction on five tKG datasets [Cite_Footnote_1] . We compare TANGO\u2019s performance with several existing meth-ods and evaluate its potential with inductive link prediction and long horizontal link forecasting. Be-sides, an ablation study is conducted to show the effectiveness of our graph transition layer. 4.1.1 Datasets We use five benchmark datasets to evaluate TANGO: 1) ICEWS14 (Trivedi et al., 2017) 2) ICEWS18 (Boschee et al., 2015) 3) ICEWS05-15 (Garc\u00eda-Dur\u00e1n et al., 2018) 4) YAGO (Mahdisoltani et al., 2013) 5) WIKI (Leblay and Chekol, 2018). Integrated Crisis Early Warning System (ICEWS) (Boschee et al., 2015) is a dataset consisting of timestamped political events, e.g., (Barack Obama, visit, India, 2015-01-25). Specifically, ICEWS14 contains events occurring in 2014, while ICEWS18 contains events from January 1, 2018, to Octo-ber 31, 2018. ICEWS05-15 is a long-term dataset that contains the events between 2005 and 2015. WIKI and YAGO are two subsets extracted from Wikipedia and YAGO3 (Mahdisoltani et al., 2013), respectively. The details of each dataset and the dataset split strategy are provided in Appendix D."
  },
  {
    "id": 1099,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/ibalazevic/TuckER",
    "section_title": "C Implementation Details",
    "add_info": "2 https://github.com/ibalazevic/TuckER",
    "text": "We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER [Cite_Footnote_2] , COMPGCN , and RE-Net . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE 5 , TNTComplEx 6 , and CyGNet 7 . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512."
  },
  {
    "id": 1100,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/malllabiisc/CompGCN",
    "section_title": "C Implementation Details",
    "add_info": "3 https://github.com/malllabiisc/CompGCN",
    "text": "We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER , COMPGCN [Cite_Footnote_3] , and RE-Net . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE 5 , TNTComplEx 6 , and CyGNet 7 . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512."
  },
  {
    "id": 1101,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/INK-USC/RE-Net",
    "section_title": "C Implementation Details",
    "add_info": "4 https://github.com/INK-USC/RE-Net",
    "text": "We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER , COMPGCN , and RE-Net [Cite_Footnote_4] . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE 5 , TNTComplEx 6 , and CyGNet 7 . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512."
  },
  {
    "id": 1102,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/BorealisAI/de-simple",
    "section_title": "C Implementation Details",
    "add_info": "5 https://github.com/BorealisAI/de-simple",
    "text": "We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER 2 , COMPGCN 3 , and RE-Net 4 . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE [Cite_Footnote_5] , TNTComplEx , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512."
  },
  {
    "id": 1103,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/facebookresearch/tkbc",
    "section_title": "C Implementation Details",
    "add_info": "6 https://github.com/facebookresearch/tkbc",
    "text": "We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER 2 , COMPGCN 3 , and RE-Net 4 . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx [Cite_Footnote_6] , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512."
  },
  {
    "id": 1104,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/CunchaoZ/CyGNet",
    "section_title": "C Implementation Details",
    "add_info": "7 https://github.com/CunchaoZ/CyGNet",
    "text": "We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER 2 , COMPGCN 3 , and RE-Net 4 . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx , and CyGNet [Cite_Footnote_7] . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512."
  },
  {
    "id": 1105,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://cloud.google.com/bigquery",
    "section_title": "2 The Humor Dataset 2.2 Collecting Headlines",
    "add_info": "2 https://cloud.google.com/bigquery",
    "text": "We obtain all Reddit posts from the popular sub-reddits r/worldnews and r/politics from January 2017 to May 2018 using Google Big-Query [Cite_Footnote_2] . Each of these posts is a headline from a news source. We remove duplicate headlines and headlines that have fewer than 4 words or more than 20 words. Finally, we keep only the headlines from the 25 English news sources that contribute the most headlines in the Reddit data, resulting in a total of 287,076 news headlines."
  },
  {
    "id": 1106,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/",
    "section_title": "3 Experiments and results 3.1 Basic SVM",
    "add_info": "6 All our SVM experiments were performed us-ing the libsvm implementation downloadable from http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/",
    "text": "First we implemented the standard linear SVM [Cite_Footnote_6] on this problem with only word-based features (uni-grams and bigrams) as the input. Quite surprisingly, the model achieves an F1 score of only 79.44% as shown in entry 1 of table 5. On inspection, we no-ticed that the SVM assigns high weights to many spurious features owing to their strong correlation with the class."
  },
  {
    "id": 1107,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/csong27/collision-bert",
    "section_title": "References",
    "add_info": null,
    "text": "We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts\u2014including paraphrase identification, document retrieval, response suggestion, and extractive summa-rization\u2014are vulnerable to semantic colli-sions. For example, given a target query, insert-ing a crafted collision into an irrelevant doc-ument can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at  https://github.com/csong27/collision-bert."
  },
  {
    "id": 1108,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/castorini/birch",
    "section_title": "5 Experiments 5.1 Tasks and Models",
    "add_info": "3 https://github.com/castorini/birch",
    "text": "Our target model is Birch (Yilmaz et al., 2019a,b). Birch retrieves 1,000 candidate docu-ments using the BM25 and RM3 baseline (Abdul-jaleel et al., 2004) and re-ranks them using the similarity scores from a fine-tuned BERT model. Given a query x q and a document x d , the BERT model assigns similarity scores S(x q , x i ) for each sentence x i in x d . The final score used by Birch for re-reranking is: \u03b3\u00b7S BM25 +(1\u2212\u03b3)\u00b7P i \u03ba i \u00b7S(x q , x i ) where S BM25 is the baseline BM25 score and \u03b3, \u03ba i are weight coefficients. We use the published mod-els [Cite_Footnote_3] and coefficient values for evaluation."
  },
  {
    "id": 1109,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://parl.ai/docs/zoo.html",
    "section_title": "5 Experiments 5.1 Tasks and Models",
    "add_info": "4 https://parl.ai/docs/zoo.html",
    "text": "We use transformer-based Bi- and Poly-encoders that achieved state-of-the-art results on this dataset (Humeau et al., 2020). Bi-encoders com-pute a similarity score for the dialogue context x a and each possible next utterance x b as S(x a , x b ) = f pool (x a ) > f pool (x b ) where f pool (x) \u2208 R h is the pooling-over-time representation from transform-ers. Poly-encoders extend Bi-encoders compute S(x a , x b ) = P Ti=1 \u03b1 i \u00b7 f(x a ) i> f pool (x b ) where \u03b1 i is the weight from attention and f(x a ) i is the ith token\u2019s contextualized representation. We use the published models [Cite_Footnote_4] for evaluation."
  },
  {
    "id": 1110,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/nlpyang/PreSumm",
    "section_title": "5 Experiments 5.1 Tasks and Models",
    "add_info": "5 https://github.com/nlpyang/PreSumm",
    "text": "Our target model is PreSumm (Liu and Lapata, 2019). Given a text x d , PreSumm first obtains a vector representation \u03c6 i \u2208 R h for each sentence x i using BERT, and scores each sentence x i in the text as S(x d , x i ) = sigmoid(u > f(\u03c6 1 , . . . , \u03c6 T ) i ) where u is a weight vector, f is a sentence-level transformer, and f(\u00b7) i is the ith sentence\u2019s contex-tualized representation. Our objective is to insert a collision c into x d such that the rank of S(x d , c) among all sentences is high. We use the published models [Cite_Footnote_5] for evaluation."
  },
  {
    "id": 1111,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://opennlp.sourceforge.net/",
    "section_title": "3 Description of SRES 3.2 Instance Extractor",
    "add_info": null,
    "text": "The Instance Extractor applies the patterns gener-ated by the Pattern Learner to the text corpus. In order to be able to match the slots of the patterns, the Instance Extractor utilizes an external shallow parser from the OpenNLP package (  http://opennlp.sourceforge.net/), which is able to find all proper and common noun phrases in a sen-tence. These phrases are matched to the slots of the patterns. In other respects, the pattern matching and extraction process is straightforward."
  },
  {
    "id": 1112,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://pypi.org/project/newspaper3k/",
    "section_title": "from Common Crawl 2 and apply it to create the",
    "add_info": "3 https://pypi.org/project/newspaper3k/(cf. Pustejovsky et al. (2003)) and mentions using",
    "text": "2. We download these web pages, remove excess markup and detect publication dates with the newspaper3k framework [Cite_Footnote_3] ."
  },
  {
    "id": 1113,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/ns-moosavi/coval",
    "section_title": "A.4 Full Experiment Results",
    "add_info": null,
    "text": "We measure coreference resolution performance with the MUC (Vilain et al., 1995), CEAF e (Luo, 2005), B 3 (Bagga and Baldwin, 1998), CoNLL F1 (Pradhan et al., 2012) and LEA (Moosavi and Strube, 2016) metrics. We use the scorer implementation from  https://github.com/ns-moosavi/coval. Tables 12 to 16 report the full P/R/F1 scores of the coreference resolu-tion experiments reported in Section 4.1 for each of these metrics respectively. Table 17 reports the best training epochs, clustering thresholds \u03c4 and development set LEA F1 scores of each model and independent trial."
  },
  {
    "id": 1114,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://ml.nec-labs.com/senna/",
    "section_title": "2 Approach 2.2 Using Word Representations to Identify Event Roles",
    "add_info": "2 Code and resources can be found at http://ml.nec-labs.com/senna/",
    "text": "After having generated for each word their vec-tor representation, we use them as features for the annotated data to classify event roles. However, event role fillers are not generally single words but noun phrases that can be, in some cases, identi-fied as named entities. For identifying the event roles, we therefore apply a two-step strategy. First, we extract the noun chunks using SENNA [Cite_Footnote_2] parser (Collobert et al., 2011; Collobert, 2011) and we build a representation for these chunks defined as the maximum, per column, of the vector represen-tations of the words it contains. Second, we use a statistical classifier to recognize the slot fillers, using this representation as features. We chose the extra-trees ensemble classifier (Geurts et al., 2006), which is a meta estimator that fits a num-ber of randomized decision trees (extra-trees) on various sub-samples of the data set and use averag-ing to improve the predictive accuracy and control over-fitting."
  },
  {
    "id": 1115,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://metaoptimize.com/projects/wordreprs",
    "section_title": "3 Experiments and Results 3.2 Experiments",
    "add_info": "3 C&W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embed-dings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/projects/wordreprs induced from the Reuters-RCV1",
    "text": "We used the extra-trees ensemble classifier im-plemented in (Pedregosa et al., 2011), with hyper-parameters optimized on the validation data: for-est of 500 trees and the maximum number of features\u221ato consider when looking for the best split is number features. We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representa-tions (DRVR-50) to more generic word embed-dings (C&W50, HLBL-50) [Cite_Footnote_3] and finally to another word representation construction on the domain-specific data (W2V-50) 4 ."
  },
  {
    "id": 1116,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "3 Experiments and Results 3.2 Experiments",
    "add_info": "4 W2V-50 are the embeddings induced from the MUC4 data set using the negative sampling training algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), available at https://code.google.com/p/word2vec/",
    "text": "We used the extra-trees ensemble classifier im-plemented in (Pedregosa et al., 2011), with hyper-parameters optimized on the validation data: for-est of 500 trees and the maximum number of features\u221ato consider when looking for the best split is number features. We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representa-tions (DRVR-50) to more generic word embed-dings (C&W50, HLBL-50) 3 and finally to another word representation construction on the domain-specific data (W2V-50) [Cite_Footnote_4] ."
  },
  {
    "id": 1117,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://cond.org/schbase.html",
    "section_title": "References",
    "add_info": null,
    "text": "Extracted keyphrases can enhance numer-ous applications ranging from search to tracking the evolution of scientific dis-course. We present SCHB ASE , a hier-archical database of keyphrases extracted from large collections of scientific liter-ature. SCHB ASE relies on a tendency of scientists to generate new abbrevia-tions that \u201cextend\u201d existing forms as a form of signaling novelty. We demon-strate how these keyphrases/concepts can be extracted, and their viability as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful \u201cphylogenetic\u201d structure and describe key features of this structure. The com-plete SCHB ASE dataset is available at:  http://cond.org/schbase.html."
  },
  {
    "id": 1118,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://cond.org/schbase.html",
    "section_title": "1 Introduction",
    "add_info": "1 Full database available at: http://cond.org/schbase.html",
    "text": "In this paper we introduce SCHB ASE , a hi-erarchical database of keyphrases. We demon-strate how we can simply, but effectively, extract keyphrases by mining abbreviations from scien-tific literature and composing those keyphrases into semantically-meaningful hierarchies. We fur-ther show that abbreviations are a viable mech-anism for building a domain-specific keyphrase database by comparing our extracted keyphrases to a number of author-defined and automatically-created keyphrase corpora. Finally, we illustrate how authors build upon each others\u2019 terminology over time to create new keyphrases. [Cite_Footnote_1]"
  },
  {
    "id": 1119,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://academic.research.microsoft.com",
    "section_title": "4 Overlap with Keyphrase Corpora 4.2 Microsoft Academic (MSRAC ORPUS )",
    "add_info": "Microsoft. 2015. Microsoft academic search. http://academic.research.microsoft.com. Accessed: 2015-2-26.",
    "text": "Our second keyphrase dataset comes from the Mi-crosoft Academic (MSRA) search corpus (Mi-crosoft, 2015)  . While particularly focused on"
  },
  {
    "id": 1120,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://snowball.tartarus.org/texts/introduction.html",
    "section_title": "3 Keyphrases and Hierarchies 3.1 Abbreviation Extraction 3.1.1 The A BBREV C ORPUS",
    "add_info": "Martin F. Porter. 2001. Snowball: A language for stemming algorithms. http://snowball.tartarus.org/texts/introduction.html. Accessed: 2015-2-26.",
    "text": "In addition to the filtering rules described above, we manually constructed a set of fil-ter terms to remove publication venues, agen-cies, and other institutions: \u2018university\u2019, \u2018confer-ence\u2019, \u2018symposium\u2019, \u2018journal\u2019, \u2018foundation\u2019, \u2018con-sortium\u2019, \u2018agency\u2019, \u2018institute\u2019 and \u2018school\u2019 are dis-carded. We further normalize our keyphrases by lowercasing, removing hyphens, and using the Snowball stemmer (Porter, 2001)  to merge plu-ral variants. After stemming and normalizing, we found a total of 155,957 unique abbreviation ex-pansions. Among these, 48,890 expansions occur more than once, 25,107 expansions thrice or more and 16,916 expansions four or more times. We re-fer to this collection as the A BBREV C ORPUS ."
  },
  {
    "id": 1121,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/shivashankarrs/classimb_fairness",
    "section_title": "References",
    "add_info": "1 Code available at: https://github.com/shivashankarrs/classimb_fairness",
    "text": "Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between re-search on class-imbalanced learning and mit-igating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning meth-ods for tweet sentiment and occupation clas-sification, and extend a margin-loss based ap-proach with methods to enforce fairness. We empirically show through controlled experi-ments that the proposed approaches help miti-gate both class imbalance and demographic bi-ases. [Cite_Footnote_1]"
  },
  {
    "id": 1122,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/lemmonation/jm-nat",
    "section_title": "5 Experiments 5.1.5 Training and Inference",
    "add_info": null,
    "text": "As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 \u2020 404 \u2020 ms 1.50\u00d725.48 29.32 30.19 25.94 30.42 \u2217 62 \u2217 ms 9.79\u00d729.90 32.53 33.23 27.03 31.71 \u2217 161 \u2217 ms 3.77\u00d730.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5\u00d7 27.69 106 ms 5.73\u00d732.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at  https://github.com/lemmonation/jm-nat."
  },
  {
    "id": 1123,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/jcyk/AMR-parser",
    "section_title": "5 Experiments 5.1 Setup",
    "add_info": "5 Our code can be found at https://github.com/jcyk/AMR-parser.",
    "text": "We use Stanford CoreNLP (Manning et al., 2014) for text preprocessing, including tokeniza-tion, lemmatization, part-of-speech, and named-entity tagging. The input for sentence encoder consists of the randomly initialized lemma, part-of-speech tag, and named-entity tag embeddings, as well as the output from a learnable CNN with character embeddings as inputs. The graph en-coder uses randomly initialized concept embed-dings and another char-level CNN. Model hyper-parameters are chosen by experiments on the de-velopment set. The details of the hyper-parameter settings are provided in the Appendix. During testing, we use a beam size of 8 for generating graphs. [Cite_Footnote_5]"
  },
  {
    "id": 1124,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/DeepLearnXMU/CG-RL",
    "section_title": "References",
    "add_info": null,
    "text": "Due to the great potential in facilitating soft-ware development, code generation has at-tracted increasing attention recently. Gener-ally, dominant models are Seq2Tree models, which convert the input natural language de-scription into a sequence of tree-construction actions corresponding to the pre-order traver-sal of an Abstract Syntax Tree (AST). How-ever, such a traversal order may not be suit-able for handling all multi-branch nodes. In this paper, we propose to equip the Seq2Tree model with a context-based Branch Selector, which is able to dynamically determine opti-mal expansion orders of branches for multi-branch nodes. Particularly, since the selec-tion of expansion orders is a non-differentiable multi-step operation, we optimize the selector through reinforcement learning, and formulate the reward function as the difference of model losses obtained through different expansion or-ders. Experimental results and in-depth analy-sis on several commonly-used datasets demon-strate the effectiveness and generality of our ap-proach. We have released our code at  https://github.com/DeepLearnXMU/CG-RL ."
  },
  {
    "id": 1125,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.cis.hut.fi/ahonkela/dippa/dippa.html",
    "section_title": "2 Bayesian Model of Summaries",
    "add_info": "3 http://www.cis.hut.fi/ahonkela/dippa/dippa.html",
    "text": "The expectation and variance of Dirichlet(\u03b8|v) are given as follows. [Cite_Footnote_3] where v 0 = i v i . Therefore the variance of a scaled Dirichlet is:"
  },
  {
    "id": 1126,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/LayneIns/Unsupervised_dialo_disentanglement",
    "section_title": "1 Introduction",
    "add_info": "3 Code will be publicly available at https://github.com/LayneIns/Unsupervised_dialo_disentanglement",
    "text": "We conduct experiments on the large public Movie Dialogue Dataset (Liu et al., 2020). Ex-perimental results demonstrate that our proposed method outperforms strong baselines based on BERT (Devlin et al., 2019) in two-step settings, and achieves competitive results compared to those of the state-of-the-art supervised end-to-end methods. Moreover, we apply the disentangled conversations predicted by our method to the downstream task of multi-party response selection and get significant improvements compared to a baseline system. [Cite_Footnote_3] In summary, our main contributions are three-fold:"
  },
  {
    "id": 1127,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2005T23",
    "section_title": "3 Experiments 3.1 Experimental Setting",
    "add_info": "1 https://catalog.ldc.upenn.edu/LDC2005T23",
    "text": "To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0. [Cite_Footnote_1] We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid. The test set includes 72 files, which are chtb 001.fid to chtb 040.fid, and chtb 900.fid to chtb 931.fid. We use another annotated corpus with distinct se-mantic role labels and annotation schema, which is designed by ourselves for other projects, as hetero-geneous resource. This labeled dataset has 17,308 annotated sentences, and the semantic roles con-cerned are like \u201cagent\u201d and \u201cpatient\u201d, resulting in 21 kinds of types, which are all distinct from the semantic roles defined in CPB. We use the devel-opment set of CPB for model selection, and the hyper parameter setting of our model is reported in Table 1."
  },
  {
    "id": 1128,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "3 Experiments 3.2 Chinese SRL Performance",
    "add_info": "3 https://code.google.com/p/word2vec/",
    "text": "Further, we conduct experiments with the intro-duction of heterogenous resource. Previous work found that the performance can be improved by pre-training the word embeddings on large unla-beled data and using the obtained embeddings to make initialization. With the result in Table 2, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec [Cite_Footnote_3] on Chinese Gigaword Corpus for word pre-training). However, as shown in Table 2, com-pared to standard pre-training, the influence of het-erogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improve-ment. The information provided by standard pre-training with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident."
  },
  {
    "id": 1129,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/segmenter.shtml",
    "section_title": "2 System Architecture 2.2 MSU Based Tagging 2.2.1 Minimum Semantic Unit",
    "add_info": "3 http://nlp.stanford.edu/software/segmenter.shtml",
    "text": "For a given full form, we first segment it us-ing a standard word segmenter to get a coarse-grained segmentation result. Here we use the Stan-ford Chinese Word Segmenter [Cite_Footnote_3] . Then we use the MSU set to segment each word using the strategy of \u201cMaximum Forward Matching\u201d to get the fine-grained MSU segmentation result."
  },
  {
    "id": 1130,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/tagger.shtml",
    "section_title": "2 System Architecture 2.2 MSU Based Tagging 2.2.3 Feature templates",
    "add_info": "6 http://nlp.stanford.edu/software/tagger.shtml",
    "text": "Templates 1, 2 and 3 express word uni-grams and bi-grams. In MSU-based tagging, we can uti-lize the POS information, which we get from the Stanford Chinese POS Tagger [Cite_Footnote_6] . In template 4, the type of word refers to whether it is a number, an English word or a Chinese word. Because the ba-sic tagging unit is MSU, which carries word infor-mation, we can use many features that are infeasi-ble in character-based tagging."
  },
  {
    "id": 1131,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://crfpp.sourceforge.net/",
    "section_title": "3 Experiments 3.1 Data and Evaluation Metric",
    "add_info": "7 http://crfpp.sourceforge.net/",
    "text": "CRF++ [Cite_Footnote_7] , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data."
  },
  {
    "id": 1132,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://lpsolve.sourceforge.net/5.5/",
    "section_title": "3 Experiments 3.1 Data and Evaluation Metric",
    "add_info": "8 http://lpsolve.sourceforge.net/5.5/",
    "text": "CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve [Cite_Footnote_8] , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data."
  },
  {
    "id": 1133,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://www.cs.cmu.edu/\u02dcyww/data/WeiboTreebank.zip",
    "section_title": "1 Introduction",
    "add_info": "3 http://www.cs.cmu.edu/\u02dcyww/data/WeiboTreebank.zip",
    "text": "\u2022 We present a freely available Chinese Weibo dependency treebank [Cite_Footnote_3] , manually annotated with more than 18,000 tokens;"
  },
  {
    "id": 1134,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://groups.csail.mit.edu/rbg/code/dependency/",
    "section_title": "References",
    "add_info": "1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/",
    "text": "We present an approach to grammar induc-tion that utilizes syntactic universals to im-prove dependency parsing across a range of languages. Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies be-tween pairs of syntactic categories that com-monly occur across languages. During infer-ence of the probabilistic model, we use pos-terior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also auto-matically refine the syntactic categories given in our coarsely tagged input. Across six lan-guages our approach outperforms state-of-the-art unsupervised methods by a significant mar-gin. [Cite_Footnote_1]"
  },
  {
    "id": 1135,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/eladsegal/tag-based-multi-span-extraction",
    "section_title": "6 Conclusion",
    "add_info": null,
    "text": "In this work, we cast the task of answering multi-span questions as a sequence tagging prob-lem, and present a simple corresponding multi-span architecture. We show that replacing the standard single-span architecture with our multi-span architecture dramatically improves results on multi-span questions, without harming per-formance on single-span questions, leading to state-of-the-art results on Q UOREF . In addition, integrating our multi-span architecture into ex-isting models further improves performance on DROP, as is evident from the leading models on DROP\u2019s leaderboard. Our code can be downloaded from  https://github.com/eladsegal/tag-based-multi-span-extraction."
  },
  {
    "id": 1136,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md#optimizers",
    "section_title": "7:453\u2013466.",
    "add_info": "4 https://github.com/huggingface/transformers/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md#optimizers",
    "text": "This research was partially supported by The Israel Science Foundation grants 942/16 and 1186/18, The Yandex Initiative for Machine Learning and the European Research Council (ERC) under the European Union Horizons 2020 research and inno-vation programme (grant ERC DELPHI 802800). Appendix for \u201cA Simple and Effective Model for Answering Multi-span Questions\u201d A Experimental Setup We experiment with model variations that use either SSE, TASE, or their combination as a multi-head model. For DROP, we additionally use arithmetic and count heads based on (Dua et al., 2019; Kin-ley and Lin, 2019). Our model is implemented with PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2017). For f in Eq. (1) we use a 2-layer feed-forward network with ReLU activations and |S| outputs. We use the Hugging Face im-plementation of RoBERTa LARGE (Wolf et al., 2019; Liu et al., 2019) as the encoder in our model. 5% of DROP and 30% of Q UOREF are inputs with over 512 tokens. Due to RoBERTa LARGE \u2019s limita-tion of 512 positional embeddings, we truncate inputs by removing over-flowing tokens from the passage, both at train and test time. We discard 3.87% of the training examples of DROP and 5.05% of the training example of Q UOREF , which are cases when the answer cannot be outputted by the model (due to a dataset error, or trunca-tion of the correct answer span). For training, the BertAdam [Cite_Footnote_4] optimizer is used with default parame-ters and learning rates of either 5 \u00d7 10 \u22126 or 10 \u22125 . Hyperparameter search was not performed. We train on a single NVIDIA Titan XP with a batch size of 2 and gradient accumulation of 6, result-ing in an effective batch size of 12, for 20 epochs with an early-stopping patience of 10. The average runtime per epoch is 3.5 hours. Evaluation was performed with the official evaluation scripts of DROP and Q UOREF . Our full implementation can be found at https://github.com/eladsegal/tag-based-multi-span-extraction."
  },
  {
    "id": 1137,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/huggingface/neuralcoref",
    "section_title": "4 Experiments",
    "add_info": "4 https://github.com/huggingface/neuralcoref",
    "text": "Training Multee: For OpenBookQA we use cross entropy loss for labels corresponding to [Cite_Footnote_4] an-swer choices. For MultiRC, we use binary cross entropy loss for each answer-choice separately since in MultiRC each question can have more than one correct answer choice. The entailment components are pre-trained on sentence-level en-tailment tasks and then fine-tuned as part of end-to-end QA training. The MultiRC dataset includes sentence-level relevance labels. We supervise the Sentence Relevance module with a binary cross entropy loss for predicting these relevance labels when available. We used PyTorch (Paszke et al., 2017) and AllenNLP to implement our models and ran them on Beaker 7 . For pre-training we use the same hyper-parameters of ESIM(Chen et al., 2017) as available in implementation of AllenNLP (Gardner et al., 2017) and fine-tune the model pa-rameters. We do not perform any hyper-parameter tuning for any of our models. We fine-tune all lay-ers in ESIM except for the embedding layer."
  },
  {
    "id": 1138,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/allenai/OpenBookQA",
    "section_title": "4 Experiments",
    "add_info": "6 https://github.com/allenai/OpenBookQA",
    "text": "Datasets: We evaluate Multee on two datasets, OpenBookQA (Mihaylov et al., 2018) and Mul-tiRC (Khashabi et al., 2018), both of which are specifically designed to test reasoning over multiple sentences. MultiRC is paragraph-based multiple-choice QA dataset derived from varying topics where the questions are answerable based on information from the paragraph. In MultiRC, each question can have more than one correct an-swer choice, and so it can be viewed as a bi-nary classification task (one prediction per an-swer choice), with 4,848 / 4,583 examples in Dev/Test sets. OpenBookQA, on the other hand, has multiple-choice science questions with exactly one correct answer choice and no associated para-graph. As a result, this dataset requires the rele-vant facts to be retrieved from auxiliary resources including the open book of facts released with the paper and other sources such as WordNet (Miller, 1995) and ConceptNet (Speer and Havasi, 2012). It contains 500 questions in the Dev and Test sets. Preprocessing: For each question and answer choice, we create an answer hypothesis statement using a modified version of the script used in Sc-iTail (Khot et al., 2018) construction. We wrote a handful of rules to better convert the question and answer to a hypothesis. We also mark the span of answer in the hypothesis with special begin and end tokens, @@@answer and answer@@@ re-spectively . For MultiRC, we also apply an off-the-shelf coreference resolution model 4 and re-place the mentions when they resolve to pronouns occurring in a different sentence . For Open-BookQA, we use the exact same retrieval as re-leased by the authors of OpenBookQA [Cite_Footnote_6] and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query."
  },
  {
    "id": 1139,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://beaker.org/",
    "section_title": "4 Experiments",
    "add_info": "7 https://beaker.org/",
    "text": "Training Multee: For OpenBookQA we use cross entropy loss for labels corresponding to 4 an-swer choices. For MultiRC, we use binary cross entropy loss for each answer-choice separately since in MultiRC each question can have more than one correct answer choice. The entailment components are pre-trained on sentence-level en-tailment tasks and then fine-tuned as part of end-to-end QA training. The MultiRC dataset includes sentence-level relevance labels. We supervise the Sentence Relevance module with a binary cross entropy loss for predicting these relevance labels when available. We used PyTorch (Paszke et al., 2017) and AllenNLP to implement our models and ran them on Beaker [Cite_Footnote_7] . For pre-training we use the same hyper-parameters of ESIM(Chen et al., 2017) as available in implementation of AllenNLP (Gardner et al., 2017) and fine-tune the model pa-rameters. We do not perform any hyper-parameter tuning for any of our models. We fine-tune all lay-ers in ESIM except for the embedding layer."
  },
  {
    "id": 1140,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "3 Experiments 3.2 Setup",
    "add_info": "2 https://spacy.io/",
    "text": "We use spaCy [Cite_Footnote_2] to get the meta dependency paths of sentences in a document. Following Yao et al. (2019) and Wang et al. (2019), we use the GloVe (Pennington et al., 2014) embedding with BiLSTM, and Uncased BERT-Base (Devlin et al., 2019) as the context encoder. All hyper-parameters are tuned based on the development set. We list some of the important hyper-parameters in Table 1."
  },
  {
    "id": 1141,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.vivino.com",
    "section_title": "1 Introduction",
    "add_info": "2 Vivino: http://www.vivino.com",
    "text": "In the long run, as we are training automatic systems to predict wine properties, we could use such systems for automatic metadata prediction and error correction in wine review databases. These systems are also a first step towards a recommender system for wines based on review content and flavor descriptions. Current recom-mender systems such as the mobile apps Vivino [Cite_Footnote_2] or Delectable work with metadata and user-based filtering, i.e. the principle of \u2018other users also bought . . .\u2019. So there is potential here for content-based recommender systems to be developed."
  },
  {
    "id": 1142,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.delectable.com",
    "section_title": "1 Introduction",
    "add_info": "3 Delectable: http://www.delectable.com",
    "text": "In the long run, as we are training automatic systems to predict wine properties, we could use such systems for automatic metadata prediction and error correction in wine review databases. These systems are also a first step towards a recommender system for wines based on review content and flavor descriptions. Current recom-mender systems such as the mobile apps Vivino or Delectable [Cite_Footnote_3] work with metadata and user-based filtering, i.e. the principle of \u2018other users also bought . . .\u2019. So there is potential here for content-based recommender systems to be developed."
  },
  {
    "id": 1143,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.wiktionary.org/",
    "section_title": "2 A Collection of BV\u2013PV Analogies",
    "add_info": "2 https://www.wiktionary.org/",
    "text": "BV 1 : P V 1 :: BV 2 : P V 2 such as klappern:abklappern::klopfen:abklopfen. We aimed for \u2248200 analogies per particle type, focusing on the four highly frequent particle types ab, an, auf, aus. The target selection was re-stricted to PV 1 /PV 2 combinations with identical particles, and where the two PVs were deemed (near-)synonyms according to the German stan-dard dictionary DUDEN or the German Wik-tionary [Cite_Footnote_2] , as we were interested in BV\u2013PV analo-gies with semantically highly similar PVs."
  },
  {
    "id": 1144,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/Babylonpartners/fastText_multilingual",
    "section_title": "3 Representations of BV\u2013PV Analogies 3.3 Affect Models",
    "add_info": "4 https://github.com/Babylonpartners/fastText_multilingual",
    "text": "We enriched the basic similarity model by inte-grating affective information from human-created lexicons. Since affective datasets are typically small-scale and mostly exist for English, we ap-plied a cross-lingual approach (Smith et al., 2017) to learn a linear transformation that aligns mono-lingual vectors from two languages in a single vec-tor space. We took off-the-shelf word represen-tations [Cite_Footnote_4] for German and English that live in the same semantic space, learned a regression model based on the English data, and applied it to the German data by relying on findings from Ko\u0308per and Schulte im Walde (2017), who showed that a feed-forward neural network obtained a high cor-relation with human-annotated ratings."
  },
  {
    "id": 1145,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/raylin1000/drop-bert",
    "section_title": "2 A Description of MOCHA 2.2 Collecting Candidates",
    "add_info": "2 https://github.com/raylin1000/drop-bert",
    "text": "Candidates on all four generative datasets are gen-erated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for Nar-rativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each ques-tion has two references. We treat the second ref-erence as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT [Cite_Footnote_2] models for DROP."
  },
  {
    "id": 1146,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/salaniz/pycocoevalcap",
    "section_title": "B Details on Baselines",
    "add_info": "4 https://github.com/salaniz/pycocoevalcap",
    "text": "We use implementations of BLEU, METEOR, and ROUGE using Microsoft MS COCO evaluation scripts [Cite_Footnote_4] . We removed question marks, periods, and exclamation marks from references and candi-dates when evaluating with BLEU, METEOR, and ROUGE."
  },
  {
    "id": 1147,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cl.cam.ac.uk/\u02dcah433/mcrae-quantified-majority.txt",
    "section_title": "1 Introduction",
    "add_info": "1 Data available at http://www.cl.cam.ac.uk/\u02dcah433/mcrae-quantified-majority.txt",
    "text": "In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers. To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vec-tors \u2013 giving us a common representation across for-malisms. Using a publicly available dataset of feature norms annotated with quantifiers [Cite_Footnote_1] (Herbelot and Vec-chi, 2015), we show that human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data."
  },
  {
    "id": 1148,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cl.cam.ac.uk/\u02dcah433/material/herbelot_iwcs13_data.txt",
    "section_title": "3 Annotated datasets 3.2 Additional animal data",
    "add_info": "3 Data available at http://www.cl.cam.ac.uk/\u02dcah433/material/herbelot_iwcs13_data.txt.",
    "text": "QMR gives us an average of 11 features per con-cept. This results in fairly sparse vectors in the model-theoretic semantic space (see \u00a74). In order to remedy data sparsity, we consider the use of additional data in the form of the animal dataset from Herbelot (2013) (henceforth AD). AD [Cite_Footnote_3] is a set of 72 animal concepts with quantification annotations along 54 features. The main differences between QMR and AD are as follows:"
  },
  {
    "id": 1149,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://wacky.sslmit.unibo.it",
    "section_title": "4 Semantic spaces 4.1 The distributional semantic space",
    "add_info": "4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk",
    "text": "We consider two distributional semantic space archi-tectures which have each shown to have considerable success in a number of semantic tasks. First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [Cite_Footnote_4] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimen-sions to 300 through Singular Value Decomposition."
  },
  {
    "id": 1150,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.natcorp.ox.ac.uk",
    "section_title": "4 Semantic spaces 4.1 The distributional semantic space",
    "add_info": "4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk",
    "text": "We consider two distributional semantic space archi-tectures which have each shown to have considerable success in a number of semantic tasks. First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [Cite_Footnote_4] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimen-sions to 300 through Singular Value Decomposition."
  },
  {
    "id": 1151,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://code.google.com/p/word2vec",
    "section_title": "4 Semantic spaces 4.1 The distributional semantic space",
    "add_info": "6 https://code.google.com/p/word2vec",
    "text": "Next we consider the context-predicting vectors (DS Mikolov ) available as part of the word2vec [Cite_Footnote_6] project (Mikolov et al., 2013a). We use the publicly avail-able vectors which were trained on a Google News dataset of circa 100 billion tokens. Baroni et al. (2014b) showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this op-tion as a valid distributional representation of a word\u2019s semantics."
  },
  {
    "id": 1152,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/webis-de/acl20-editorials-style-persuasive-effect",
    "section_title": "1 Introduction",
    "add_info": "1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/acl20-editorials-style-persuasive-effect",
    "text": "This paper analyzes the persuasive effect of style in news editorial argumentation on readers with dif-ferent political ideologies (conservative vs. liberal). We model style with widely-used features captur-ing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NY-Times editorial corpus of El Baff et al. (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented clas-sifiers for persuasive effect (Section 5). [Cite_Footnote_1]"
  },
  {
    "id": 1153,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://home.uchicago.edu/~cbs2/banglainstruction.html",
    "section_title": "3 Resource Preparation",
    "add_info": "1 http://home.uchicago.edu/~cbs2/banglainstruction.html",
    "text": "These lists have been converted to Bengali us-ing English to Bengali bilingual dictionary [Cite_Footnote_1] . These six lists have been termed as Emotion lists. A Bengali SentiWordNet is being developed by replacing each word entry in the synonymous set of the English SentiWordNet (Esuli et al., 2006) by its equivalent Bengali meaning using the same English to Bengali bilingual dictionary."
  },
  {
    "id": 1154,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf",
    "section_title": "4 Word Level Emotion Classification 4.1 Feature Selection and Training",
    "add_info": "2 http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf",
    "text": "\uf0b7 POS information: We are interested with the verb, noun, adjective and adverb words as these are emotion informative constitu-ents. For this feature, total 1300 sentences has been passed through a Bengali part of speech tagger (Ekbal et al. 2008) based on Support Vector Machine (SVM) tech-nique. The POS tagger was developed with a tagset of 26 POS tags [Cite_Footnote_2] , defined for the Indian languages. The POS tagger has demonstrated an overall accuracy of ap-proximately 90%."
  },
  {
    "id": 1155,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/UKPLab/emnlp2017-cmapsum-corpus",
    "section_title": "References",
    "add_info": "1 Available at https://github.com/UKPLab/emnlp2017-cmapsum-corpus",
    "text": "Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created cor-pus of concept maps that summarize het-erogeneous collections of web documents on educational topics. It was created us-ing a novel crowdsourcing approach that allows us to efficiently determine impor-tant elements in large document collec-tions. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [Cite_Footnote_1]"
  },
  {
    "id": 1156,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/knowitall/openie",
    "section_title": "5 Corpus Creation 5.2 Proposition Extraction",
    "add_info": "7 https://github.com/knowitall/openie",
    "text": "While the relation phrase is similar to a relation in a concept map, many arguments in these tuples represent useful concepts. We used Open IE 4 [Cite_Footnote_7] , a state-of-the-art system (Stanovsky and Dagan, 2016) to process all sentences. After removing du-plicates, we obtained 4137 tuples per topic."
  },
  {
    "id": 1157,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/kermitt2/grobid",
    "section_title": "4 Methodology 4.3 Preprocessing and Feature Extraction",
    "add_info": "2 https://github.com/kermitt2/grobid",
    "text": "We extracted the text from PDFs using Grobid. [Cite_Footnote_2] Several preprocessing steps were necessary before using the articles\u2019 text as features in our model."
  },
  {
    "id": 1158,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/NP-NET-research/Recursive-Semi-Markov-Model",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Our code is released at  https://github.com/NP-NET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019)."
  },
  {
    "id": 1159,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://nlp.cs.nyu.edu/evalb",
    "section_title": "5 Experiments 5.1 Experimental Setup",
    "add_info": "1 https://nlp.cs.nyu.edu/evalb",
    "text": "We evaluate the proposed framework in both En-glish and Chinese, on the datasets of PTB (WSJ sections (Marcus et al., 1993)) and CTB 5.1 (Xue et al., 2005), respectively. For Chinese, we evaluate both the single task of constituent parsing and the joint task with word segmentation and POS tagging. We follow the standard split of the datasets (Kitaev et al., 2019). In the single task for Chinese, some previous work utilize the Stanford tagger (Toutano-va et al., 2003) to generate the POS tags as input, which leads to a fixed error propagation. In this paper, POS tags are removed and not used as input features in both training and testing in CTB 5.1, following the previous work in (Zhang et al., 2020). Standard precision, recall and F1-measure are em-ployed as evaluation metrics, where the EVALB [Cite_Footnote_1] tool is employed in the single task. The hyper-parameters in the implementation are shown in Ta-ble. 1. Most of them are set following the Berkeley parser (Kitaev and Klein, 2018). When choosing the pre-train models (Wolf et al., 2020), \u201cbert-large-cased\u201d is utilized for English with a single RTX 3090, \u201cbert-base-chinese\u201d is utilized for Chinese with a single RTX 1080TI."
  },
  {
    "id": 1160,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://github.com/ajaech/querycompletion",
    "section_title": "4 Experiments 4.1 Implementation Details",
    "add_info": "2 Code at http://github.com/ajaech/querycompletion",
    "text": "The vocabulary consists of 79 characters including special start and stop tokens. Models were trained for six epochs. The Adam optimizer is used dur-ing training with a learning rate of 10 \u22123 (Kingma and Ba, 2014). When updating the user embed-dings during evaluation, we found that it is easier to use an optimizer without momentum. We use Adadelta (Zeiler, 2012) and tune the online learn-ing rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task. [Cite_Footnote_2]"
  },
  {
    "id": 1161,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://goo.gl/language/wiki-split",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "\u2022 Public release of the English WikiSplit dataset, containing one million rewrites:  http://goo.gl/language/wiki-split"
  },
  {
    "id": 1162,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://github.com/shashiongithub/Split-and-Rephrase",
    "section_title": "2 The WikiSplit Corpus 2.3 Comparison to WebSplit",
    "add_info": "3 We use WebSplit v1.0 throughout, which is the scaled-up re-release by Narayan et al. (2017) at http://github.com/shashiongithub/Split-and-Rephrase, commit a9a288c. Preliminary experiments showed the same trends on the smaller v0.1 corpus, as resplit by Aharoni and Goldberg (2018).",
    "text": "This is to be expected given that WebSplit\u2019s small vocabulary of 7k words must account for the 344k tokens that make up the distinct complex sen-tences themselves. [Cite_Footnote_3]"
  },
  {
    "id": 1163,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ccsasuke/man",
    "section_title": "References",
    "add_info": "1 The source code of MAN is available at https://github.com/ccsasuke/man.",
    "text": "Many text classification tasks are known to be highly domain-dependent. Unfortunately, the availability of training data can vary drasti-cally across domains. Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multino-mial adversarial network [Cite_Footnote_1] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theo-retical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multi-ple probability distributions. MANs are thus a theoretically sound generalization of tradi-tional adversarial networks that discriminate over two distributions. More specifically, for the MDTC task, MAN learns features that are invariant across multiple domains by resort-ing to its ability to reduce the divergence among the feature distributions of each do-main. We present experimental results show-ing that MANs significantly outperform the prior art on the MDTC task. We also show that MANs achieve state-of-the-art performance for domains with no labeled data."
  },
  {
    "id": 1164,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/salesforce/cove",
    "section_title": "2 Proposed model: SAN",
    "add_info": "1 https://github.com/salesforce/cove",
    "text": "Contextual Encoding Layer. Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings. We concatenate a pre-trained 600-dimensional CoVe vectors [Cite_Footnote_1] (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the fi-nal input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer. To reduce the parameter size, we use a maxout layer (Goodfellow et al., 2013) at each BiLSTM layer to shrink its dimension. By a concatena-tion of the outputs of two BiLSTM layers, we obtain H q \u2208 R 2d\u00d7m as representation of Q and H p \u2208 R 2d\u00d7n as representation of P, where d is the hidden size of the BiLSTM."
  },
  {
    "id": 1165,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io",
    "section_title": "3 Experiment Setup",
    "add_info": "2 https://spacy.io",
    "text": "Implementation details: The spaCy tool [Cite_Footnote_2] is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encod-ing. The mini-batch size is set to 32 and Adamax (Kingma and Ba, 2014) is used as our optimizer. The learning rate is set to 0.002 at first and de-creased by half after every 10 epochs. We set the dropout rate for all the hidden units of LSTM, and the answer module output layer to 0.4. To prevent degenerate output, we ensure that at least one step in the answer module is active during training."
  },
  {
    "id": 1166,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://aclweb.org/anthology/D16-1264",
    "section_title": "1 Introduction",
    "add_info": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u20132392. https://aclweb.org/anthology/D16-1264.",
    "text": "Machine reading comprehension (MRC) is a chal-lenging task: the goal is to have machines read a text passage and then answer any question about the passage. This task is an useful benchmark to demonstrate natural language understanding, and also has important applications in e.g. conversa-tional agents and customer service support. It has been hypothesized that difficult MRC problems re-quire some form of multi-step synthesis and rea-soning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016)  illustrates the need for synthesis of information across sentences and multiple steps of reasoning:"
  },
  {
    "id": 1167,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://aclweb.org/anthology/D16-1264",
    "section_title": "3 Experiment Setup",
    "add_info": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u20132392. https://aclweb.org/anthology/D16-1264.",
    "text": "Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016)  . This contains about 23K passages and 100K questions. The passages come from approx-imately 500 Wikipedia articles and the questions and answers are obtained by crowdsourcing. The crowdsourced workers are asked to read a passage (a paragraph), come up with questions, then mark the answer span. All results are on the official de-velopment set, unless otherwise noted."
  },
  {
    "id": 1168,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://aclweb.org/anthology/D16-1264",
    "section_title": "6 Related Work",
    "add_info": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u20132392. https://aclweb.org/anthology/D16-1264.",
    "text": "The recent big progress on MRC is largely due to the availability of the large-scale datasets (Ra-jpurkar et al., 2016  ; Nguyen et al., 2016; Richard-son et al., 2013; Hill et al., 2016), since it is possi-ble to train large end-to-end neural network mod-els. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model first maps the symbolic rep-resentation of the documents and questions into a neural space, then search answers on top of it. We categorize these models into two groups based on the difference of the answer module: single-step and multi-step reasoning. The key difference between the two is what strategies are applied to search the final answers in the neural space."
  },
  {
    "id": 1169,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://code.google.com/p/efficient-java-matrix-library/",
    "section_title": "7 Experimental Setup",
    "add_info": "Peter Abeles. 2014. Efficient java matrix library. https://code.google.com/p/efficient-java-matrix-library/.",
    "text": "Parameters and Solver In our experiments we set k in our beam search algorithm (Section 5) to 200, and l to 20. We run the L-BFGS computation for 50 iterations. We regularize our learning objec-tive using the L 2 -norm and a \u03bb value of 0.1. The set of mathematical relations supported by our im-plementation is {+, \u2212, \u00d7, /}.Our implementation uses the Gaussian Elimination function in the Effi-cient Java Matrix Library (EJML) (Abeles, 2014)  to generate answers given a set of equations."
  },
  {
    "id": 1170,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/facebookresearch/PyTorch-BigGraph",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "4 https://github.com/facebookresearch/PyTorch-BigGraph",
    "text": "KnowBert-Wiki The entity linker in KnowBert-Wiki borrows both the entity can-didate selectors and embeddings from Ganea and Hofmann (2017). The candidate selectors and priors are a combination of CrossWikis, a large, precomputed dictionary that combines statistics from Wikipedia and a web corpus (Spitkovsky and Chang, 2012), and the YAGO dictionary (Hoffart et al., 2011). The entity embeddings use a skip-gram like objective (Mikolov et al., 2013b) to learn 300-dimensional embeddings of Wikipedia page titles directly from Wikipedia descriptions without using any explicit graph structure between nodes. As such, nodes in the KB are Wikipedia page titles, e.g., Prince (musician). Ganea and Hofmann (2017) provide pretrained embed-dings for a subset of approximately 470K entities. Early experiments with embeddings derived from Wikidata relations [Cite_Footnote_4] did not improve results."
  },
  {
    "id": 1171,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "5 https://spacy.io/",
    "text": "KnowBert-WordNet Our WordNet KB com-bines synset metadata, lemma metadata and the re-lational graph. To construct the graph, we first ex-tracted all synsets, lemmas, and their relationships from WordNet 3.0 using the nltk interface. After disregarding certain symmetric relationships (e.g., we kept the hypernym relationship, but removed the inverse hyponym relationship) we were left with 28 synset-synset and lemma-lemma relation-ships. From these, we constructed a graph where each node is either a synset or lemma, and intro-duced the special lemma in synset relation-ship to link synsets and lemmas. The candidate se-lector uses a rule-based lemmatizer without part-of-speech (POS) information. [Cite_Footnote_5]"
  },
  {
    "id": 1172,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/thunlp/ERNIE",
    "section_title": "4 Experiments 4.3 Downstream Tasks",
    "add_info": "7 Data obtained from https://github.com/thunlp/ERNIE",
    "text": "Entity typing We also evaluated KnowBert-W+W using the entity typing dataset from Choi et al. (2018). To directly compare to ERNIE, we adopted the evaluation protocol in Zhang et al. (2019) which considers the nine general entity types. [Cite_Footnote_7] Our model marks the location of a target span with the special [E] and [/E] tokens and uses the representation of the [E] token to predict the type. As shown in Table 7, KnowBert-W+W shows an improvement of 0.6% F 1 over ERNIE and 2.5% over BERT BASE ."
  },
  {
    "id": 1173,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://lingpy.org",
    "section_title": "3 Methods 3.5 Implementation",
    "add_info": null,
    "text": "The code was implemented in Python, as part of the LingPy library (Version 2.5, List and Forkel (2016),  http://lingpy.org). The Igraph soft-ware package (Cs\u00e1rdi and Nepusz 2006) is needed to apply the Infomap algorithm."
  },
  {
    "id": 1174,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://sequencecomparison.github.io",
    "section_title": "3 Methods 3.1 Sequence Similarity",
    "add_info": "Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.",
    "text": "There are various ways to determine the similar-ity or distance between words and morphemes. A general distinction can be made between language-independent and language-specific ap-proaches. The former determine the word simi-larity independently of the languages to which the words belong. As a result, the scores only depend on the substantial and structural differences be-tween words. Examples for language-independent similarity measures are SCA distances, as pro-duced by the Sound-Class-Based Phonetic Align-ment algorithm (List 2012b), or PMI similarities as produced by the Weighted String Alignment algorithm (J\u00e4ger 2013). Language-specific ap-proaches, on the other hand, are based on pre-viously identified recurring correspondences be-tween the languages from which the words are taken (List 2014b: 48-50) and may differ across languages. An example for language-specific similarity measures is the LexStat algorithm, first proposed in List (2012a) and later refined in List (2014b)  . As a general rule, language-specific ap-proaches outperform language-independent ones, provided the sample size is large enough (List 2014a)."
  },
  {
    "id": 1175,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://sequencecomparison.github.io",
    "section_title": "3 Methods 3.1 Sequence Similarity",
    "add_info": "Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.",
    "text": "Two similarity measures are used in this pa-per, one language-independent, and one language-specific one. The above-mentioned SCA method for phonetic alignments (List 2012b, 2014b) re-duces the phonetic space of sound sequences to 28 sound classes. Based on a scoring function which defines transition scores between the sound classes, phonetic sequences are aligned and simi-larity and distance scores can be determined. The LexStat approach List (2012a, 2014b) also uses sound classes, but instead of using a pre-defined scoring function, transition scores between sound classes are determined with help of a permutation test. In this test, words drawn from a random-ized sample are repeatedly aligned with each other in order to create a distribution of sound transi-tions for unrelated languages. This distribution is then compared with the actual distribution re-trieved from aligned words in the word list, and a language-specific scoring function is created List (2014b)  . SCA is very fast in computation, but Lex-Stat has a much higher accuracy. Both approaches are freely available as part of the LingPy software package (List and Forkel 2016)."
  },
  {
    "id": 1176,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://sequencecomparison.github.io",
    "section_title": "3 Methods 3.4 Analyses and Evaluation",
    "add_info": "Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.",
    "text": "With SCA and LexStat, two classical meth-ods for cognate detection were tested List (2014b)  , and their underlying models for phonetic similar-ity (see Sec. 3.1) were used as basis for the par-tial cognate detection algorithm. All in all, this yielded four different methods: LexStat, LexStat-Partial, SCA, and SCA-Partial. Since our new algorithms yield partial cognates, while LexStat and SCA yield ``complete\" cognates, it is not pos-sible to compare them directly. In order to al-low for a direct comparison, partial cognate sets were converted into ``complete\" cognate sets us-ing the above-mentioned strict coding approach proposed by Ben Hamed and Wang (2006): only those words in which all morphemes are cognate were assigned to the cognate same set. With a total of three different clustering algorithms (UPGMA, Markov Clustering, and Infomap), we thus carried out twelve tests on complete cognacy (three for each of our four approaches), and six additional tests on pure partial cognate detection, in which we compared the suitability of SCA and LexStat as string similarity measures. on all datasets. The table shows for each of the 18 different methods the threshold (T) for which the best B-Cubed F-Score was determined, as well as the B-Cubed precision (P), recall (R), and F-score (FS). The best result in each block is shaded in gray."
  },
  {
    "id": 1177,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://lingpy.org",
    "section_title": "3 Methods 3.5 Implementation",
    "add_info": "Johann-Mattis List and Robert Forkel. 2016. LingPy. A Python library for historical linguis-tics. Max Planck Institute for the Science of Hu-man History, Jena. Version 2.5. URL: http: //lingpy.org. With contributions by Steven Moran, Peter Bouda, Johannes Dellert, Taraka Rama, Frank Nagel, and Simon Greenhill.",
    "text": "The code was implemented in Python, as part of the LingPy library (Version 2.5, List and Forkel (2016)  , http://lingpy.org). The Igraph soft-ware package (Cs\u00e1rdi and Nepusz 2006) is needed to apply the Infomap algorithm."
  },
  {
    "id": 1178,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://amr.isi.edu/download.html",
    "section_title": "1 Introduction",
    "add_info": "1 AMR datasets from the LDC can be found at https://amr.isi.edu/download.html",
    "text": "Annotating natural language with AMR is a complex task and training datasets only exist for English [Cite_Footnote_1] , so previous work on AMR-to-text gen-eration has overwhelmingly focused on English. We create training data for multilingual AMR-to- Text models, by taking the EUROPARL multilin-gual corpus and automatically annotating the En-glish data with AMRs using the jamr semantic parser. We then use the English AMRs as the in-put for all generation tasks. To improve quality, we leverage recent advances in natural language processing such as cross-lingual embeddings, pre-training and multilingual learning. Cross-lingual embeddings have shown striking improvements on a range of cross-lingual natural language under-standing tasks (Devlin et al., 2019; Conneau et al., 2019; Wu and Dredze, 2019; Pires et al., 2019). Other work has shown that the pre-training and fine-tuning approaches also help improve genera-tion performance (Dong et al., 2019; Song et al., 2019; Lawrence et al., 2019; Rothe et al., 2019). Finally, multilingual models, where a single model is trained to translate from multiple source lan-guages into multiple target languages, are achiev-ing increasingly better results in machine transla-tion (Johnson et al., 2017; Firat et al., 2017; Aha-roni et al., 2019; Arivazhagan et al., 2019)."
  },
  {
    "id": 1179,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/facebookresearch/cc_net",
    "section_title": "4 Experimental Setting 4.1 Data",
    "add_info": "2 https://github.com/facebookresearch/cc_net",
    "text": "Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET [Cite_Footnote_2] (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web). We use jamr to parse En-glish sentences into AMR. For multilingual de-coder pretraining, we take thirty million sentences from each language split of CCNET ."
  },
  {
    "id": 1180,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/jflanigan/jamr",
    "section_title": "4 Experimental Setting 4.1 Data",
    "add_info": "3 https://github.com/jflanigan/jamr",
    "text": "Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web). We use jamr [Cite_Footnote_3] to parse En-glish sentences into AMR. For multilingual de-coder pretraining, we take thirty million sentences from each language split of CCNET ."
  },
  {
    "id": 1181,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/facebookresearch/m-amr2text",
    "section_title": "4 Experimental Setting 4.2 Models",
    "add_info": "5 https://github.com/facebookresearch/m-amr2text",
    "text": "We implement our models in fairseq-py (Ott et al., 2019). We use large Transformer (Vaswani et al., 2017) sequence-to-sequence models and train all models for 50 epochs with LayerDrop (Fan et al., 2019b), which takes around 2 days. We initialize all weights with the pretrained models. When com-bining crosslingual word embeddings and encoder and decoder pretraining, we initialize all weights with pretraining, then use crosslingual word embed-dings. We do not perform extensive hyperparame-ter search, but experimented with various learning rate values to maintain stable training with pre-trained initialization. To generate, we decode with beam search with beam size 5. Our pretrained mod-els are available for download. [Cite_Footnote_5]"
  },
  {
    "id": 1182,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.seas.upenn.edu/\u02dcstrctlrn/MSTParser/MSTParser.html",
    "section_title": "6 Experiment 6.2 Baseline Models",
    "add_info": "7 http://www.seas.upenn.edu/\u02dcstrctlrn/MSTParser/MSTParser.html",
    "text": "We use the publicly available implementation of MSTParser [Cite_Footnote_7] (with modifications to the feature com-putation) and its default settings, so the feature weights of the projective and non-projective parsers are trained by the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2006)."
  },
  {
    "id": 1183,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/BLLIP/bllip-parser",
    "section_title": "4 Datasets and models",
    "add_info": "1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model \u201dWSJ+Gigaword-v2\u201d.",
    "text": "We use English-French and English-German data from WMT2014 (Bojar et al., 2014). We take 4M English sentences from the English-German data to train E2E and PE2PE. For the neural parser (E2P), we construct the training corpus following the recipe of Vinyals et al. (2015). We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [Cite_Footnote_1] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus. We use WSJ Section 22 as our development set and section 23 as the test set, where we obtain an F1-score of 89.6, competitive with the previously-published 90.5 (Table 4)."
  },
  {
    "id": 1184,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/isi-nlp/ZophRNN",
    "section_title": "4 Datasets and models",
    "add_info": "2 We use the toolkit: https://github.com/isi-nlp/ZophRNN",
    "text": "Model Architecture. For all experiments [Cite_Footnote_2] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2."
  },
  {
    "id": 1185,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/evalb/",
    "section_title": "6 Extract Syntactic Trees from Encoder 6.2 Evaluation",
    "add_info": "3 http://nlp.cs.nyu.edu/evalb/",
    "text": "1. The EVALB tool [Cite_Footnote_3] to calculate the labeled bracketing F1-score."
  },
  {
    "id": 1186,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/timtadh/zhang-shasha",
    "section_title": "6 Extract Syntactic Trees from Encoder 6.2 Evaluation",
    "add_info": "4 https://github.com/timtadh/zhang-shasha",
    "text": "2. The zxx package [Cite_Footnote_4] to calculate Tree edit dis-tance (TED) (Zhang and Shasha, 1989)."
  },
  {
    "id": 1187,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/jkkummerfeld/berkeley-parser-analyser",
    "section_title": "6 Extract Syntactic Trees from Encoder 6.2 Evaluation",
    "add_info": "5 https://github.com/jkkummerfeld/berkeley-parser-analyser",
    "text": "3. The Berkeley Parser Analyser [Cite_Footnote_5] (Kummerfeld et al., 2012) to analyze parsing error types."
  },
  {
    "id": 1188,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www-01.ibm.com/software/data/infosphere/streams/",
    "section_title": "3 The System",
    "add_info": "IBM. (2012). InfoSphere Streams, from http://www-01.ibm.com/software/data/infosphere/streams/",
    "text": "For accuracy and speed, we built our real-time data processing infrastructure on the IBM\u2019s InfoSphere Streams platform (IBM, 2012)  , which enables us to write our own analysis and visualization modules and assemble them into a real-time processing pipeline. Streams applications are highly scalable so we can adjust our system to handle higher volume of data by adding more servers and by distributing processing tasks. Twitter traffic often balloons during big events (e.g. televised debates or primary election days) and stays low between events, making high scalability strongly desirable. Figure 1 shows our system\u2019s architecture and its modules. Next, we introduce our data source and each individual module."
  },
  {
    "id": 1189,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://ai.tencent.com/ailab/nlp/dialogue/datasets/grayscale_data_release.zip",
    "section_title": "4 Experimental Setup 4.3 Implementation Details",
    "add_info": "1 Related resources can be found at https://ai.tencent.com/ailab/nlp/dialogue/datasets/grayscale_data_release.zip",
    "text": "For grayscale data construction, we train a seq2seq generation model and build a BM25 retrieval sys-tem using the training set for each dataset. We con-sider the top 100 responses from BM25 retrieval and the top 5 responses from seq2seq generation (via beam search) as the grayscale responses. To facilitate further research, we have made our col-lected grayscale data publicly available. [Cite_Footnote_1] During training, we use these grayscale responses in a way adaptive to the training matching model. At each training epoch, ten different grayscale responses are used: the top 5 retrieval responses ranked by the current matching model and all 5 seq2seq gen-eration responses. We experiment our new training approach on four latest state-of-the-art models as follows: Specifically, we first pre-train a model with objec-tive L ran only then switch to L Uni . We find that such a treatment makes the training process more stable."
  },
  {
    "id": 1190,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/huminghao16/RE3QA",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/huminghao16/RE3QA",
    "text": "We evaluate our approach on four datasets. On TriviaQA-Wikipedia and TriviaQA-unfiltered datasets (Joshi et al., 2017), we achieve 75.2 F1 and 71.2 F1 respectively, outperforming previ-ous best approaches. On SQuAD-document and SQuAD-open datasets, both of which are modified versions of SQuAD (Rajpurkar et al., 2016), we obtain 14.8 and 4.1 absolute gains on F1 score over prior state-of-the-art results. Moreover, our ap-proach surpasses the pipelined baseline with faster inference speed on both TriviaQA-Wikipedia and SQuAD-document. Source code is released for fu-ture research exploration [Cite_Footnote_1] ."
  },
  {
    "id": 1191,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/google-research/bert",
    "section_title": "4 Experimental Setup",
    "add_info": "5 https://github.com/google-research/bert",
    "text": "Data preprocessing Following Clark and Gard-ner (2018), we merge small paragraphs into a sin-gle paragraph of up to a threshold length in Triv-iaQA and SQuAD-open. The threshold is set as 200 by default. We manually tune the number of retrieved paragraphs K for each dataset, and set the number of retrieved segments N as 8. Follow-ing Devlin et al. (2018), we set the window length l as 384 L q 3 so that L x is 384 and set the stride r as 128, where L q is the question length. We also calculate the answer recall after document prun-ing, which indicates the performance upper bound. Model settings We initialize our model us-ing two publicly available uncased versions of BERT [Cite_Footnote_5] : BERT BASE and BERT LARGE , and refer readers to Devlin et al. (2018) for details on model sizes. We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 10% steps to fine-tune the network for 2 epochs. The batch size is 32 and a dropout probability of 0.1 is used. The number of blocks J used for early-stopped re-triever is 3 for base model and 6 for large model by default. The number of proposed answers M is 20, while the threshold of NMS M \u21e4 is 5. Dur-ing inference, we tune the weights for retrieving, reading, and reranking, and set them as 1.4, 1, 1.4. Evaluation metrics We use mean average pre-cision (MAP) and top-N to evaluate the retriev-ing component. As for evaluating the performance of reading and reranking, we measure the exact match (EM) accuracy and F1 score calculated be-tween the final prediction and gold answers."
  },
  {
    "id": 1192,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/kovvalsky/LangPro",
    "section_title": "References",
    "add_info": "1 https://github.com/kovvalsky/LangPro",
    "text": "L ang P ro is an automated theorem prover for natural language. [Cite_Footnote_1] Given a set of premises and a hypothesis, it is able to prove semantic relations between them. The prover is based on a version of an-alytic tableau method specially designed for natural logic. The proof procedure op-erates on logical forms that preserve lin-guistic expressions to a large extent. The nature of proofs is deductive and transpar-ent. On the FraCaS and SICK textual en-tailment datasets, the prover achieves high results comparable to state-of-the-art."
  },
  {
    "id": 1193,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://goo.gl/language/wiki-atomic-edits",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "\u2022 A new corpus (WikiAtomicEdits) of 26M atomic insertions and 17M atomic deletions covering 8 languages (\u00a73 and \u00a74):  http://goo.gl/language/wiki-atomic-edits ."
  },
  {
    "id": 1194,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://fasttext.cc/docs/en/crawl-vectors.html",
    "section_title": "6 Language Modeling Analysis 6.1 Predicting Insertion Locations",
    "add_info": "7 https://fasttext.cc/docs/en/crawl-vectors.html",
    "text": "Models. We evaluate two models. First, we evaluate a standard language modeling baseline (General LM), in which we simply insert the phrase p at every possible point in s and chose the index which yields the lowest perplexity. We use the LSTM language model from Jozefowicz et al. (2016), which obtained SOTA results on lan-guage modeling on the one billion words bench-mark for English (Chelba et al., 2013). We train this language model for each language on an aver-age of \u223c 500 million tokens from Wikipedia. Sec-ond, we evaluate a discriminative model specifi-cally trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a context-dependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the rep-resentation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initial-ized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018). [Cite_Footnote_7] We hold out 50K and 10K insertion edits for each lan-guage as development and test sets, and use the re-maining edits (insertions and deletions) as training data. This provides us with at least 1 million ex-amples for training in each language (cf. Table 2). See Supplementary Material for additional details. Results. Table 8 shows the accuracy of each model for each language. We see that the discri-minitve model trained on insertions directly per-forms better than the general LM by at least 1% absolute accuracy on every language, and by 3.8% absolute on average. It is worth emphasizing that this performance improvement is despite the fact that the general LM was trained with, on average, four times the number of tokens and is a much larger model\u2013the general LM has \u223c 2 billion pa-rameters (Jozefowicz et al., 2016) compared to \u223c 1 million for the discriminative model."
  },
  {
    "id": 1195,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://goo.gl/language/wiki-atomic-edits",
    "section_title": "Description of Data Release",
    "add_info": null,
    "text": "Our full corpus is available for download at  http://goo.gl/language/wiki-atomic-edits . The data contains 26M atomic insertions and 17M atomic deletions covering 8 languages. All sen-tences (both the original sentence s, and the edited sentence e(s)) have been POS-tagged and depen-dency parsed (Andor et al., 2016) as well as scored using a SOTA LM (Jozefowicz et al., 2016). We also release the 5K 5-way human insertion annota-tions for English, and 1K 3-way annotations each for Spanish and German, as described in \u00a74."
  },
  {
    "id": 1196,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://dev.twitter.com/docs/streaming-api/methods",
    "section_title": "4 Contextually-similar Pair Generation",
    "add_info": "1 https://dev.twitter.com/docs/streaming-api/methods",
    "text": "We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were col-lected from September 2010 to January 2011 via the Twitter API. [Cite_Footnote_1] From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O\u2019Connor et al. (2010)). We use the Aspell dictionary (v6.06) to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length \u2265 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are mor-phophonemically similar to the OOV type, follow-ing settings in Han and Baldwin (2011)."
  },
  {
    "id": 1197,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://aspell.net/",
    "section_title": "4 Contextually-similar Pair Generation",
    "add_info": "2 http://aspell.net/",
    "text": "We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were col-lected from September 2010 to January 2011 via the Twitter API. From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O\u2019Connor et al. (2010)). We use the Aspell dictionary (v6.06) [Cite_Footnote_2] to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length \u2265 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are mor-phophonemically similar to the OOV type, follow-ing settings in Han and Baldwin (2011)."
  },
  {
    "id": 1198,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/RUC-WSM/WD-Match",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "The source code of WD-Match is available at  https://github.com/RUC-WSM/WD-Match"
  },
  {
    "id": 1199,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://nlp.stanford.edu/projects/snli",
    "section_title": "4 Experiments 4.1 Datasets and Metrics",
    "add_info": "3 https://nlp.stanford.edu/projects/snli",
    "text": "SNLI [Cite_Footnote_3] is a benchmark for natural language in-ference. In SNLI, each data record is a premise-hypothesis-label triple. The premise and hypoth-esis are two sentences and the label could be \u201cen-tailment\u201d, \u201cneutral\u201d, \u201ccontradiction\u201d, or \u201c-\u201d. In our experiments, following the practices in (Bowman et al., 2015), the data with label \u201c-\u201d are ignored. We follow the original dataset partition. Accuracy is used as the evaluation metric for this dataset."
  },
  {
    "id": 1200,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://data.allenai.org/scitail/",
    "section_title": "4 Experiments 4.1 Datasets and Metrics",
    "add_info": "4 http://data.allenai.org/scitail/",
    "text": "SciTail [Cite_Footnote_4] is an entailment dataset based on multiple-choice science exams and web sentences. Each record is a premise-hypothesis-label triple. The label is \u201centailment\u201d or \u201cneutral\u201d, because sci-entific factors cannot contradict. We follow the original dataset partition. Accuracy are used as the evaluation metric for this dataset."
  },
  {
    "id": 1201,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA",
    "section_title": "4 Experiments 4.1 Datasets and Metrics",
    "add_info": "5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA",
    "text": "TrecQA [Cite_Footnote_5] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included. The raw version has 82 ques-tions in the development set and 100 questions in the test set. Mean average precision (MAP) and mean reciprocal rank (MRR) are used as the evalu-ation metrics for this task."
  },
  {
    "id": 1202,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://www.microsoft.com/en-us/download/details.aspx?id=52419",
    "section_title": "4 Experiments 4.1 Datasets and Metrics",
    "add_info": "6 https://www.microsoft.com/en-us/download/details.aspx?id=52419",
    "text": "WikiQA [Cite_Footnote_6] is a retrieval-based question answer-ing dataset based on Wikipedia. We follow the data split of original paper. This dataset consists of 20.4k training pairs, 2.7k development pairs, and 6.2k testing pairs. We use MAP and MRR as the evaluation metrics for this task."
  },
  {
    "id": 1203,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "4 Experiments 4.2 Experimental Setup",
    "add_info": "7 https://github.com/google-research/bert",
    "text": "Please note that we remove the character embed-ding and position embedding in our experiments; in WD-Match(BERT), F is a pre-trained BERT-base [Cite_Footnote_7] model, M is an MLP. Please note that for easing of combining with WD-Match, BERT was only used to extract the sentence features separately in our experiments. The G module for four models are identical: a non-linear projection layer and a linear projection layer."
  },
  {
    "id": 1204,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.ehealthmd.com/library/acltears",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "First, our proposed method obtains Web pages that include both an acronym and its definitions. Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question.  http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym \u201cACL\u201d without its definition in three different meanings found in blogs"
  },
  {
    "id": 1205,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/",
    "section_title": "5 Experiment Setup",
    "add_info": "5 http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/",
    "text": "1. LSA: This approach was reported in Landauer and Dumais (1997). We use pre-trained word em-beddings based on LSA [Cite_Footnote_5] . The vocabulary size is 100,000."
  },
  {
    "id": 1206,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "5 Experiment Setup",
    "add_info": "6 http://nlp.stanford.edu/projects/glove/",
    "text": "2. GloVe: We use pre-trained vectors avaiable from the GloVe project [Cite_Footnote_6] . The vocabulary size in this case is 2,195,904."
  },
  {
    "id": 1207,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/",
    "section_title": "5 Experiment Setup",
    "add_info": "7 https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/",
    "text": "3. Dependency Weights: We use pre-trained vectors [Cite_Footnote_7] weighted using dependency distance, as given in Levy and Goldberg (2014). The vocabulary size is 174,015."
  },
  {
    "id": 1208,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/archive/p/Word2Vec/",
    "section_title": "5 Experiment Setup",
    "add_info": "8 https://code.google.com/archive/p/Word2Vec/",
    "text": "4. Word2Vec: use pre-trained Google word vectors. These were trained using Word2Vec tool [Cite_Footnote_8] on the Google News corpus. The vocabulary size for Word2Vec is 3,000,000. To interact with these pre-trained vectors, as well as compute various features, we use gensim library (R\u030cehu\u030ar\u030cek and Sojka, 2010)."
  },
  {
    "id": 1209,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://sary.sourceforge.net",
    "section_title": "4 Experimental Settings",
    "add_info": "1 http://sary.sourceforge.net",
    "text": "The system was implemented in Java, however it handled the suffix arrays through an external C li-brary called Sary. [Cite_Footnote_1] All experiments were conducted on a 2 GHz Core2Duo T7200 machine with 2 GB RAM."
  },
  {
    "id": 1210,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://pytorch.org/text/_modules/torchtext/data/utils.html",
    "section_title": "6 Experimental Setup",
    "add_info": "9 https://pytorch.org/text/_modules/torchtext/data/utils.html",
    "text": "Setting up the encoder. We normalize the text by lower casing, removing special characters, etc. [Cite_Footnote_9] For each task, we construct separate 1-Gram vo-cabulary (U) and initialize a trainable randomly sampled token embedding (U \u00d7 d e ) from N (0, 1). Similarly, we randomly initialize a (d s-max \u00d7 d e ) positional embedding."
  },
  {
    "id": 1211,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://nlp.stanford.edu/projects/nmt/",
    "section_title": "6 Experiments",
    "add_info": "2 Datasets used in this work can be found at https://nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense",
    "text": "The proposed approach provides a new paradigm and understanding of data augmentation for text generation. To evaluate that our approach can mimic the effect of data augmentation, we con-duct experiments on two text generation tasks \u2013 neural machine translation and conversational re-sponse generation. We compare our approach with two most popular data augmentation methods (one token-level and one sentence-level augmentation method) that can be applied on various text genera-tion tasks: \u2022 Masked Language model (MLM): We use a pre-trained BERT (Devlin et al., 2019; Wolf et al., 2020) and randomly choose 15% of the words for each sentence. BERT takes in these masked words to predict these masked positions with new words. We augment one sample from each original train-ing sample. Thus the data size increases to twice of the original one. Note that we only augment the English side of translation datasets. \u2022 Back-translation (BT): For neural machine trans-lation, we employ a fixed target-to-source transla-tion model trained on the original dataset. For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [Cite_Footnote_2] . We again augment one sam-ple from each original training sample."
  },
  {
    "id": 1212,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense",
    "section_title": "6 Experiments",
    "add_info": "2 Datasets used in this work can be found at https://nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense",
    "text": "The proposed approach provides a new paradigm and understanding of data augmentation for text generation. To evaluate that our approach can mimic the effect of data augmentation, we con-duct experiments on two text generation tasks \u2013 neural machine translation and conversational re-sponse generation. We compare our approach with two most popular data augmentation methods (one token-level and one sentence-level augmentation method) that can be applied on various text genera-tion tasks: \u2022 Masked Language model (MLM): We use a pre-trained BERT (Devlin et al., 2019; Wolf et al., 2020) and randomly choose 15% of the words for each sentence. BERT takes in these masked words to predict these masked positions with new words. We augment one sample from each original train-ing sample. Thus the data size increases to twice of the original one. Note that we only augment the English side of translation datasets. \u2022 Back-translation (BT): For neural machine trans-lation, we employ a fixed target-to-source transla-tion model trained on the original dataset. For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [Cite_Footnote_2] . We again augment one sam-ple from each original training sample."
  },
  {
    "id": 1213,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh",
    "section_title": "6 Experiments 6.1 Neural Machine Translation",
    "add_info": "3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh",
    "text": "We use translation benchmarks IWSLT14 En\u2013De, En\u2013Fr, En\u2013It, and IWSLT15 En\u2013Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [Cite_Footnote_3] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set. The IWSLT15 dataset is the same as that used in Luong et al. (2015a), and the validation and test sets are tst2012 and tst2013, respectively."
  },
  {
    "id": 1214,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.uniroma1.it/wsdeval/training-data",
    "section_title": "5 Dataset Generation",
    "add_info": "4 We use the version released at http://nlp.uniroma1.it/wsdeval/training-data, with sense annotations that leverage WordNet 3.0.",
    "text": "G ENE S IS is able to generate substitutes starting from a word in its context. Thus, starting from a source dataset C of target words in context, we can exploit G ENE S IS to produce ranked lists of sub-stitutes and, associating the generated substitutes with the targets, obtain silver datasets for the lexi-cal substitution task. To this end, first, we finetune G ENE S IS on a gold dataset for the lexical substi-tution task; then, we give as input to the finetuned model the corpus C, generating as output a list of replacements for each input instance. The input instances, associated with the generated substitutes, constitute the silver corpus. To ensure the quality of the generated substitutes, we apply a similar-ity threshold \u03bb on the ranking step of G ENE S IS (cf. Section 3), keeping only the substitutes whose similarity to the target is higher than \u03bb. As source dataset C we exploit SemCor (Miller et al., 1993), a manually annotated corpus where instances are sense-tagged according to the WordNet sense in-ventory [Cite_Footnote_4] . While it is typically used as a training corpus for English Word Sense Disambiguation (WSD), as we show, its manually-curated sense dis-tribution is also beneficial for lexical substitution. Indeed, having a frequency of target words that follows a sense distribution involves the generation of substitutes for different senses of the same word, helping lexical substitution systems finetuned on the silver dataset to generalize more effectively."
  },
  {
    "id": 1215,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/heartexlabs/label-studio#try-out-label-studio",
    "section_title": "9 Qualitative Evaluation",
    "add_info": "8 The sample size is significant with respect to the source dataset with confidence level of 95% and a margin error of \u00b1 5. The annotation interface was developed through Label Studio https://github.com/heartexlabs/label-studio#try-out-label-studio. layer dropout, when keeping dropout fixed at 0.1. The chosen value is 0.6.",
    "text": "Annotation Task We set up a test where an anno-tator is provided with a target word in context and a set of substitutes that are equally distributed among the gold and the generated ones. The annotator is required to select, if there are any, all the substitutes that are not suitable replacements for the target in the given context. We select three annotators with certified proficiency in English and previous experi-ence in linguistic annotation tasks and present them with a sample of 322 test instances drawn from the LST dataset [Cite_Footnote_8] . The annotators are asked to select the inappropriate substitutes from an anonymized shuffled set of three gold and three generated substi-tutes, obtained with G ENE S IS trained on CT T . For all the instances, the gold substitutes do not appear in the generated ones and vice versa. The annota-tion guidelines are reported in the supplementary material (Section D)."
  },
  {
    "id": 1216,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "3 Discourse vs. non-discourse usage",
    "add_info": "2 http://mallet.cs.umass.edu",
    "text": "Sections 0 and 1 of the PDTB were used for de-velopment of the features described in the previous section. Here we report results using a maximum entropy classifier [Cite_Footnote_2] using ten-fold cross-validation over sections 2-22."
  },
  {
    "id": 1217,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://sourceforge.net/projects/zgen/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under lin-ear complexity. Compared with parsers that use dy-namic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard lineariza-tion task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addi-tion to syntactic tree construction. Experimental re-sults show that the transition-based linearization sys-tem runs an order of magnitude faster than a state-of-the-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at  http://sourceforge.net/projects/zgen/."
  },
  {
    "id": 1218,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "http://stp.lingfil.uu.se/\u02dcnivre/research/Penn2Malt.html",
    "section_title": "4 Experiments",
    "add_info": "1 http://stp.lingfil.uu.se/\u02dcnivre/research/Penn2Malt.html",
    "text": "We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour-nal sections 2\u201321 for training, 22 for development testing and 23 for final testing. Gold-standard de-pendency trees are derived from bracketed sentences in the treebank using Penn2Malt [Cite_Footnote_1] , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of lin-earization, which has been adopted in former liter-als (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline."
  },
  {
    "id": 1219,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://sourceforge.net/projects/zgen/",
    "section_title": "6 Conclusion",
    "add_info": null,
    "text": "We studied transition-based syntactic linearization as an extension to transition-based parsing. Com-pared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sen-tences when given a bag of words. Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-first baseline. We publicly release our code at  http://sourceforge.net/projects/zgen/."
  },
  {
    "id": 1220,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://github.com/chaitanyamalaviya/lang-reps",
    "section_title": "References",
    "add_info": "1 Code and learned vectors are available at http://github.com/chaitanyamalaviya/lang-reps",
    "text": "One central mystery of neural NLP is what neural models \u201cknow\u201d about their subject matter. When a neural machine transla-tion system learns to translate from one language to another, does it learn the syn-tax or semantics of the languages? Can this knowledge be extracted from the sys-tem to fill holes in human scientific knowl-edge? Existing typological databases con-tain relatively full feature specifications for only a few hundred languages. Ex-ploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one neural machine translation (NMT) system from 1017 lan-guages into English, and use this to pre-dict information missing from typological databases. Experiments show that the pro-posed method is able to infer not only syn-tactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages\u2019 geographic and phy-logenetic neighbors. [Cite_Footnote_1]"
  },
  {
    "id": 1221,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/wenlinyao/EMNLP20-SubeventAcquisition",
    "section_title": "References",
    "add_info": "1 Code and the knowledge base are avail-able at https://github.com/wenlinyao/EMNLP20-SubeventAcquisition",
    "text": "Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly su-pervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two obser-vations that 1) subevents are temporally con-tained by the parent event, and 2) the defini-tions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the ini-tial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evalua-tion showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations [Cite_Footnote_1] ."
  },
  {
    "id": 1222,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "5 The Contextual Classifier Using BERT",
    "add_info": "8 Our implementation was based on https://github.com/huggingface/transformers.",
    "text": "In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [Cite_Footnote_8] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs."
  },
  {
    "id": 1223,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.shashca.com",
    "section_title": "4 Experimental Evaluation 4.1 Dataset and Gold Standard",
    "add_info": "5 http://www.shashca.com,http://stepforwardpak.com/",
    "text": "The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news [Cite_Footnote_5] , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard."
  },
  {
    "id": 1224,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://stepforwardpak.com/",
    "section_title": "4 Experimental Evaluation 4.1 Dataset and Gold Standard",
    "add_info": "5 http://www.shashca.com,http://stepforwardpak.com/",
    "text": "The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news [Cite_Footnote_5] , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard."
  },
  {
    "id": 1225,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://hadi763.wordpress.com/",
    "section_title": "4 Experimental Evaluation 4.1 Dataset and Gold Standard",
    "add_info": "6 https://hadi763.wordpress.com/",
    "text": "The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry [Cite_Footnote_6] , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard."
  },
  {
    "id": 1226,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.replysms.com/",
    "section_title": "4 Experimental Evaluation 4.1 Dataset and Gold Standard",
    "add_info": "7 http://www.replysms.com/",
    "text": "The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS [Cite_Footnote_7] and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard."
  },
  {
    "id": 1227,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://roman.urdu.co/",
    "section_title": "4 Experimental Evaluation 4.1 Dataset and Gold Standard",
    "add_info": "8 http://roman.urdu.co/",
    "text": "The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS and blog [Cite_Footnote_8] . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard."
  },
  {
    "id": 1228,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://bit.ly/1OJGL9Q",
    "section_title": "4 Experimental Evaluation 4.2 UrduPhone Evaluation",
    "add_info": "10 We use NLTK-Trainer\u2019s phonetic library http://bit.ly/1OJGL9Q",
    "text": "We compare UrduPhone with Soundex and its variants. [Cite_Footnote_10] These algorithms are used to group words based on their encoding and then evalu-ated against the gold standard. Table 2 shows the results on Web dataset. UrduPhone out-performs Soundex, Caverphone, and Metaphone while Nysiis\u2019s f-measure is comparable to that of UrduPhone. We observe that Nysiis produces a large number of single word clusters (out of 6,943 clusters produced 5,159 have only one word). This gives high precision but recall is low. UrduPhone produces fewer clusters (and fewer one word clus-ters) with high recall."
  },
  {
    "id": 1229,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.snopes.com/",
    "section_title": "3 The P UB H EALTH dataset 3.1 Data collection",
    "add_info": "2 https://www.snopes.com/claims). We compute the mean and standard deviation for Flesch-Kincaid and Dale-Chall scores of claims for LIAR (Wang, 2017), FEVER (Thorne et al., 2018), MultiFC (Augenstein et al., 2019), FAKENEWSNET (Shu et al., 2019b), and also our own fact-checking dataset. The sample sizes used for evaluation for each dataset are as follows, LIAR: 12,791, MultiFC: 34,842, FAKENEWSNET: 23,196, FEVER: 145,449, and 11,832 for our dataset.",
    "text": "Initially, we scraped 39,301 claims, amounting to: 27,578 fact-checked claims from five fact-checking websites (Snopes [Cite_Footnote_2] , Politifact 3 , Truthor-Fiction 4 , FactCheck 5 , and FullFact 6 ); 9,023 news headline claims from the health section and health tags of Associated Press 7 and Reuters News 8 web-sites; and 2,700 claims from the news review site Health News Review (HNR) 9 ."
  },
  {
    "id": 1230,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://medlineplus.gov/encyclopedia.html",
    "section_title": "References",
    "add_info": "12 https://medlineplus.gov/encyclopedia.html",
    "text": "Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, [Cite_Footnote_12] Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. \u2018Centers for Disease Control and Prevention\u2019, \u2018abscess\u2019, \u2018adolescence\u2019, \u2018airborne\u2019, \u2018alimentation\u2019, \u2018alopecia\u2019, \u2018aneurysm\u2019, \u2018anorexia\u2019, \u2018anti-vaxxer\u2019, \u2018arrhythmia\u2019, \u2018bacteria\u2019, \u2018bacterium\u2019, \u2018biohazard\u2019, \u2018bioterrorism\u2019, \u2018bleeding\u2019, \u2018blood pressure\u2019, \u2018chickenpox\u2019, \u2018chloroquine\u2019, \u2018con-tagious\u2019, \u2018death\u2019, \u2018disease\u2019, \u2018embolism\u2019, \u2018endemic\u2019, \u2018environment\u2019, \u2018epidemiology\u2019, \u2018first aid\u2019, \u2018flatten the curve\u2019, \u2018flu\u2019, \u2018gallbladder\u2019, \u2018gangrene\u2019, \u2018heart at-tack\u2019, \u2018heparin\u2019, \u2018hospital\u2019, \u2018hydroxychloroquine\u2019, \u2018hygiene\u2019, \u2018hypertension\u2019, \u2018illness\u2019, \u2018immune\u2019, \u2018in-fant mortality rate\u2019, \u2018infect\u2019, \u2018influenza\u2019, \u2018lactose intolerance\u2019, \u2018liver\u2019, \u2018medicine\u2019, \u2018menstruation\u2019, \u2018mental health\u2019, \u2018nurse\u2019, \u2018organs\u2019, outbreak, pace-maker, \u2018pandemic\u2019, \u2018pathogen\u2019, \u2018patients\u2019, \u2018period poverty\u2019, \u2018public health\u2019, \u2018quarantine\u2019, \u2018sickness\u2019, \u2018smoking\u2019, \u2018stroke\u2019, \u2018surgical\u2019, \u2018tumour\u2019, \u2018vaccine\u2019, \u2018ventilator\u2019, \u2018virus\u2019, \u2018x-ray\u2019. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length"
  },
  {
    "id": 1231,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.thinklocalactpersonal.org.uk/Browse/Informationandadvice/CareandSupportJargonBuster/",
    "section_title": "References",
    "add_info": "13 https://www.thinklocalactpersonal.org.uk/Browse/Informationandadvice/CareandSupportJargonBuster/",
    "text": "Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, [Cite_Footnote_13] National Careers Healthcare Job, and the Mayo Clinic. Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. \u2018Centers for Disease Control and Prevention\u2019, \u2018abscess\u2019, \u2018adolescence\u2019, \u2018airborne\u2019, \u2018alimentation\u2019, \u2018alopecia\u2019, \u2018aneurysm\u2019, \u2018anorexia\u2019, \u2018anti-vaxxer\u2019, \u2018arrhythmia\u2019, \u2018bacteria\u2019, \u2018bacterium\u2019, \u2018biohazard\u2019, \u2018bioterrorism\u2019, \u2018bleeding\u2019, \u2018blood pressure\u2019, \u2018chickenpox\u2019, \u2018chloroquine\u2019, \u2018con-tagious\u2019, \u2018death\u2019, \u2018disease\u2019, \u2018embolism\u2019, \u2018endemic\u2019, \u2018environment\u2019, \u2018epidemiology\u2019, \u2018first aid\u2019, \u2018flatten the curve\u2019, \u2018flu\u2019, \u2018gallbladder\u2019, \u2018gangrene\u2019, \u2018heart at-tack\u2019, \u2018heparin\u2019, \u2018hospital\u2019, \u2018hydroxychloroquine\u2019, \u2018hygiene\u2019, \u2018hypertension\u2019, \u2018illness\u2019, \u2018immune\u2019, \u2018in-fant mortality rate\u2019, \u2018infect\u2019, \u2018influenza\u2019, \u2018lactose intolerance\u2019, \u2018liver\u2019, \u2018medicine\u2019, \u2018menstruation\u2019, \u2018mental health\u2019, \u2018nurse\u2019, \u2018organs\u2019, outbreak, pace-maker, \u2018pandemic\u2019, \u2018pathogen\u2019, \u2018patients\u2019, \u2018period poverty\u2019, \u2018public health\u2019, \u2018quarantine\u2019, \u2018sickness\u2019, \u2018smoking\u2019, \u2018stroke\u2019, \u2018surgical\u2019, \u2018tumour\u2019, \u2018vaccine\u2019, \u2018ventilator\u2019, \u2018virus\u2019, \u2018x-ray\u2019. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length"
  },
  {
    "id": 1232,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://nationalcareers.service.gov.uk/job-categories/healthcare",
    "section_title": "References",
    "add_info": "14 https://nationalcareers.service.gov.uk/job-categories/healthcare",
    "text": "Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, [Cite_Footnote_14] and the Mayo Clinic. Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. \u2018Centers for Disease Control and Prevention\u2019, \u2018abscess\u2019, \u2018adolescence\u2019, \u2018airborne\u2019, \u2018alimentation\u2019, \u2018alopecia\u2019, \u2018aneurysm\u2019, \u2018anorexia\u2019, \u2018anti-vaxxer\u2019, \u2018arrhythmia\u2019, \u2018bacteria\u2019, \u2018bacterium\u2019, \u2018biohazard\u2019, \u2018bioterrorism\u2019, \u2018bleeding\u2019, \u2018blood pressure\u2019, \u2018chickenpox\u2019, \u2018chloroquine\u2019, \u2018con-tagious\u2019, \u2018death\u2019, \u2018disease\u2019, \u2018embolism\u2019, \u2018endemic\u2019, \u2018environment\u2019, \u2018epidemiology\u2019, \u2018first aid\u2019, \u2018flatten the curve\u2019, \u2018flu\u2019, \u2018gallbladder\u2019, \u2018gangrene\u2019, \u2018heart at-tack\u2019, \u2018heparin\u2019, \u2018hospital\u2019, \u2018hydroxychloroquine\u2019, \u2018hygiene\u2019, \u2018hypertension\u2019, \u2018illness\u2019, \u2018immune\u2019, \u2018in-fant mortality rate\u2019, \u2018infect\u2019, \u2018influenza\u2019, \u2018lactose intolerance\u2019, \u2018liver\u2019, \u2018medicine\u2019, \u2018menstruation\u2019, \u2018mental health\u2019, \u2018nurse\u2019, \u2018organs\u2019, outbreak, pace-maker, \u2018pandemic\u2019, \u2018pathogen\u2019, \u2018patients\u2019, \u2018period poverty\u2019, \u2018public health\u2019, \u2018quarantine\u2019, \u2018sickness\u2019, \u2018smoking\u2019, \u2018stroke\u2019, \u2018surgical\u2019, \u2018tumour\u2019, \u2018vaccine\u2019, \u2018ventilator\u2019, \u2018virus\u2019, \u2018x-ray\u2019. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length"
  },
  {
    "id": 1233,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.mayoclinic.org/",
    "section_title": "References",
    "add_info": "15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements",
    "text": "Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. \u2018Centers for Disease Control and Prevention\u2019, \u2018abscess\u2019, \u2018adolescence\u2019, \u2018airborne\u2019, \u2018alimentation\u2019, \u2018alopecia\u2019, \u2018aneurysm\u2019, \u2018anorexia\u2019, \u2018anti-vaxxer\u2019, \u2018arrhythmia\u2019, \u2018bacteria\u2019, \u2018bacterium\u2019, \u2018biohazard\u2019, \u2018bioterrorism\u2019, \u2018bleeding\u2019, \u2018blood pressure\u2019, \u2018chickenpox\u2019, \u2018chloroquine\u2019, \u2018con-tagious\u2019, \u2018death\u2019, \u2018disease\u2019, \u2018embolism\u2019, \u2018endemic\u2019, \u2018environment\u2019, \u2018epidemiology\u2019, \u2018first aid\u2019, \u2018flatten the curve\u2019, \u2018flu\u2019, \u2018gallbladder\u2019, \u2018gangrene\u2019, \u2018heart at-tack\u2019, \u2018heparin\u2019, \u2018hospital\u2019, \u2018hydroxychloroquine\u2019, \u2018hygiene\u2019, \u2018hypertension\u2019, \u2018illness\u2019, \u2018immune\u2019, \u2018in-fant mortality rate\u2019, \u2018infect\u2019, \u2018influenza\u2019, \u2018lactose intolerance\u2019, \u2018liver\u2019, \u2018medicine\u2019, \u2018menstruation\u2019, \u2018mental health\u2019, \u2018nurse\u2019, \u2018organs\u2019, outbreak, pace-maker, \u2018pandemic\u2019, \u2018pathogen\u2019, \u2018patients\u2019, \u2018period poverty\u2019, \u2018public health\u2019, \u2018quarantine\u2019, \u2018sickness\u2019, \u2018smoking\u2019, \u2018stroke\u2019, \u2018surgical\u2019, \u2018tumour\u2019, \u2018vaccine\u2019, \u2018ventilator\u2019, \u2018virus\u2019, \u2018x-ray\u2019. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length"
  },
  {
    "id": 1234,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.mayoclinic.org/symptoms",
    "section_title": "References",
    "add_info": "15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements",
    "text": "Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. \u2018Centers for Disease Control and Prevention\u2019, \u2018abscess\u2019, \u2018adolescence\u2019, \u2018airborne\u2019, \u2018alimentation\u2019, \u2018alopecia\u2019, \u2018aneurysm\u2019, \u2018anorexia\u2019, \u2018anti-vaxxer\u2019, \u2018arrhythmia\u2019, \u2018bacteria\u2019, \u2018bacterium\u2019, \u2018biohazard\u2019, \u2018bioterrorism\u2019, \u2018bleeding\u2019, \u2018blood pressure\u2019, \u2018chickenpox\u2019, \u2018chloroquine\u2019, \u2018con-tagious\u2019, \u2018death\u2019, \u2018disease\u2019, \u2018embolism\u2019, \u2018endemic\u2019, \u2018environment\u2019, \u2018epidemiology\u2019, \u2018first aid\u2019, \u2018flatten the curve\u2019, \u2018flu\u2019, \u2018gallbladder\u2019, \u2018gangrene\u2019, \u2018heart at-tack\u2019, \u2018heparin\u2019, \u2018hospital\u2019, \u2018hydroxychloroquine\u2019, \u2018hygiene\u2019, \u2018hypertension\u2019, \u2018illness\u2019, \u2018immune\u2019, \u2018in-fant mortality rate\u2019, \u2018infect\u2019, \u2018influenza\u2019, \u2018lactose intolerance\u2019, \u2018liver\u2019, \u2018medicine\u2019, \u2018menstruation\u2019, \u2018mental health\u2019, \u2018nurse\u2019, \u2018organs\u2019, outbreak, pace-maker, \u2018pandemic\u2019, \u2018pathogen\u2019, \u2018patients\u2019, \u2018period poverty\u2019, \u2018public health\u2019, \u2018quarantine\u2019, \u2018sickness\u2019, \u2018smoking\u2019, \u2018stroke\u2019, \u2018surgical\u2019, \u2018tumour\u2019, \u2018vaccine\u2019, \u2018ventilator\u2019, \u2018virus\u2019, \u2018x-ray\u2019. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length"
  },
  {
    "id": 1235,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.mayoclinic.org/tests-procedures",
    "section_title": "References",
    "add_info": "15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements",
    "text": "Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. \u2018Centers for Disease Control and Prevention\u2019, \u2018abscess\u2019, \u2018adolescence\u2019, \u2018airborne\u2019, \u2018alimentation\u2019, \u2018alopecia\u2019, \u2018aneurysm\u2019, \u2018anorexia\u2019, \u2018anti-vaxxer\u2019, \u2018arrhythmia\u2019, \u2018bacteria\u2019, \u2018bacterium\u2019, \u2018biohazard\u2019, \u2018bioterrorism\u2019, \u2018bleeding\u2019, \u2018blood pressure\u2019, \u2018chickenpox\u2019, \u2018chloroquine\u2019, \u2018con-tagious\u2019, \u2018death\u2019, \u2018disease\u2019, \u2018embolism\u2019, \u2018endemic\u2019, \u2018environment\u2019, \u2018epidemiology\u2019, \u2018first aid\u2019, \u2018flatten the curve\u2019, \u2018flu\u2019, \u2018gallbladder\u2019, \u2018gangrene\u2019, \u2018heart at-tack\u2019, \u2018heparin\u2019, \u2018hospital\u2019, \u2018hydroxychloroquine\u2019, \u2018hygiene\u2019, \u2018hypertension\u2019, \u2018illness\u2019, \u2018immune\u2019, \u2018in-fant mortality rate\u2019, \u2018infect\u2019, \u2018influenza\u2019, \u2018lactose intolerance\u2019, \u2018liver\u2019, \u2018medicine\u2019, \u2018menstruation\u2019, \u2018mental health\u2019, \u2018nurse\u2019, \u2018organs\u2019, outbreak, pace-maker, \u2018pandemic\u2019, \u2018pathogen\u2019, \u2018patients\u2019, \u2018period poverty\u2019, \u2018public health\u2019, \u2018quarantine\u2019, \u2018sickness\u2019, \u2018smoking\u2019, \u2018stroke\u2019, \u2018surgical\u2019, \u2018tumour\u2019, \u2018vaccine\u2019, \u2018ventilator\u2019, \u2018virus\u2019, \u2018x-ray\u2019. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length"
  },
  {
    "id": 1236,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.mayoclinic.org/drugs-supplements",
    "section_title": "References",
    "add_info": "15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements",
    "text": "Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. \u2018Centers for Disease Control and Prevention\u2019, \u2018abscess\u2019, \u2018adolescence\u2019, \u2018airborne\u2019, \u2018alimentation\u2019, \u2018alopecia\u2019, \u2018aneurysm\u2019, \u2018anorexia\u2019, \u2018anti-vaxxer\u2019, \u2018arrhythmia\u2019, \u2018bacteria\u2019, \u2018bacterium\u2019, \u2018biohazard\u2019, \u2018bioterrorism\u2019, \u2018bleeding\u2019, \u2018blood pressure\u2019, \u2018chickenpox\u2019, \u2018chloroquine\u2019, \u2018con-tagious\u2019, \u2018death\u2019, \u2018disease\u2019, \u2018embolism\u2019, \u2018endemic\u2019, \u2018environment\u2019, \u2018epidemiology\u2019, \u2018first aid\u2019, \u2018flatten the curve\u2019, \u2018flu\u2019, \u2018gallbladder\u2019, \u2018gangrene\u2019, \u2018heart at-tack\u2019, \u2018heparin\u2019, \u2018hospital\u2019, \u2018hydroxychloroquine\u2019, \u2018hygiene\u2019, \u2018hypertension\u2019, \u2018illness\u2019, \u2018immune\u2019, \u2018in-fant mortality rate\u2019, \u2018infect\u2019, \u2018influenza\u2019, \u2018lactose intolerance\u2019, \u2018liver\u2019, \u2018medicine\u2019, \u2018menstruation\u2019, \u2018mental health\u2019, \u2018nurse\u2019, \u2018organs\u2019, outbreak, pace-maker, \u2018pandemic\u2019, \u2018pathogen\u2019, \u2018patients\u2019, \u2018period poverty\u2019, \u2018public health\u2019, \u2018quarantine\u2019, \u2018sickness\u2019, \u2018smoking\u2019, \u2018stroke\u2019, \u2018surgical\u2019, \u2018tumour\u2019, \u2018vaccine\u2019, \u2018ventilator\u2019, \u2018virus\u2019, \u2018x-ray\u2019. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length"
  },
  {
    "id": 1237,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://chywang.github.io/data/emnlp17.zip",
    "section_title": "1 Introduction",
    "add_info": "2 https://chywang.github.io/data/emnlp17.zip",
    "text": "In the experiments, given only 0.6M entities and their respective 2.4M categories in Chinese Wikipedia, our method extracts 1.52M relations with an overall accuracy of 93.6%. The exper-iments also show that our approach outperforms previous methods for both is-a and non-taxonomic relation extraction from Chinese UGCs. The ex-tracted relations and the labeled test set are pub-licly available [Cite_Footnote_2] ."
  },
  {
    "id": 1238,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://download.wikipedia.com/zhwiki/20170120/",
    "section_title": "6 Experiments 6.1 Data Source and Experimental Settings",
    "add_info": "8 http://download.wikipedia.com/zhwiki/20170120/",
    "text": "The data source is downloaded from the Chi-nese Wikipedia dump of the version January 20th, 2017 [Cite_Footnote_8] . Because some Wikipedia pages are not related to entities, we use heuristic rules to fil-ter out disambiguation, redirect, template and list pages. Finally, we obtain 0.6M entities and 2.4M entity-category pairs. The open-source toolkit Fu-danNLP (Qiu et al., 2013) is employed for Chinese NLP analysis. The word embeddings are trained via a Skip-gram model using a large corpus from Wang and He (2016) and set to 100 dimensions."
  },
  {
    "id": 1239,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://knowledgeworks.cn:20313/cndbpedia/api/entityAVP",
    "section_title": "6 Experiments 6.3 Non-taxonomic Relation Extraction",
    "add_info": "10 http://knowledgeworks.cn:20313/cndbpedia/api/entityAVP",
    "text": "Evaluation To evaluate the correctness of ex-tracted relations, we carry out two experimental tests: accuracy test and coverage test. Follow-ing Suchanek et al. (2007), in the accuracy test, we randomly sample 200 relation instances for each relation type and ask human annotators to label. We discard the results if human annota-tors disagree. The coverage test is to determine whether the extracted relations already exist in Chinese knowledge bases. Low coverage score means these relations are not present in existing Chinese knowledge bases. In the experiments, we take CN-DBpedia V2.0 (Xu et al., 2017) as the ground truth knowledge base. Up till February 2017, it contains 41M explicit semantic relations of 9M entities, excluding entity summaries, syn-onyms, etc. We use the CN-DBpedia API [Cite_Footnote_10] to ob-tain relations for each entity and report the cover-age of relation r as:"
  },
  {
    "id": 1240,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "4 Training",
    "add_info": "1 https://code.google.com/p/word2vec/",
    "text": "Our experiments are based on the publicly avail-able word2vec [Cite_Footnote_1] and GloVe packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009. On the smallest 17M word corpus we explored a range of vector dimen-sionalities from 10 to 1000. On the larger 120M and 1.6B word corpus, we trained extended mod-els with a 200-dimensional semantic component and a 100-dimensional syntactic component com-paring to 300-dimensional CBOW, Skip-gram and GloVe models. The parameter, \u03bb, in Equation 11 was set to 0.1 and the recommended window sizes of 5, 10 and 15 words either side of the central word were used as context for the CBOW, Skip-gram and GloVe models respectively."
  },
  {
    "id": 1241,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "4 Training",
    "add_info": "2 http://nlp.stanford.edu/projects/glove/",
    "text": "Our experiments are based on the publicly avail-able word2vec and GloVe [Cite_Footnote_2] packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009. On the smallest 17M word corpus we explored a range of vector dimen-sionalities from 10 to 1000. On the larger 120M and 1.6B word corpus, we trained extended mod-els with a 200-dimensional semantic component and a 100-dimensional syntactic component com-paring to 300-dimensional CBOW, Skip-gram and GloVe models. The parameter, \u03bb, in Equation 11 was set to 0.1 and the recommended window sizes of 5, 10 and 15 words either side of the central word were used as context for the CBOW, Skip-gram and GloVe models respectively."
  },
  {
    "id": 1242,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mattmahoney.net/dc/text.html",
    "section_title": "4 Training",
    "add_info": "3 http://mattmahoney.net/dc/text.html",
    "text": "Our experiments are based on the publicly avail-able word2vec and GloVe packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files [Cite_Footnote_3] suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009. On the smallest 17M word corpus we explored a range of vector dimen-sionalities from 10 to 1000. On the larger 120M and 1.6B word corpus, we trained extended mod-els with a 200-dimensional semantic component and a 100-dimensional syntactic component com-paring to 300-dimensional CBOW, Skip-gram and GloVe models. The parameter, \u03bb, in Equation 11 was set to 0.1 and the recommended window sizes of 5, 10 and 15 words either side of the central word were used as context for the CBOW, Skip-gram and GloVe models respectively."
  },
  {
    "id": 1243,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/facebookresearch/InferSent",
    "section_title": "3 Experiments",
    "add_info": "1 The official implementation available at https://github.com/facebookresearch/InferSent is used. Reported hyperparameters are used except LSTM hid-den state, 1024d is chosen due to hardware limitations.",
    "text": "On the SST dataset, our model (86.66% Acc.) is again on par with tree LSTM (87.27% Acc.) and better than Transformer (85.38% Acc.) as well as Infersent (86.00% Acc.) [Cite_Footnote_1] ."
  },
  {
    "id": 1244,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ChenWu98/Point-Then-Operate",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.",
    "text": "Unsupervised text style transfer aims to al-ter text styles while preserving the content, without aligned data for supervision. Exist-ing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) gener-ated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence op-eration method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We pro-vide comprehensive training objectives to con-trol the fluency, style, and content of the out-puts and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [Cite_Footnote_1]"
  },
  {
    "id": 1245,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.geohive.com/",
    "section_title": "3 Determining Word Feature 3.1 Internal Sub-Features",
    "add_info": "3 http://www.geohive.com/",
    "text": "Our model captures three types of internal sub-features: 1) f 1 : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2 : internal semantic feature of important triggers; 3) f 3 : internal gazetteer feature. 1) f 1 is the basic sub-feature exploited in this look-up gazetteers: lists of names of persons, organizations, locations and other kinds of named entities. This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive [Cite_Footnote_3] , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems . Gazetters have been widely used in NER systems to improve performance."
  },
  {
    "id": 1246,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.scopesys.com/",
    "section_title": "3 Determining Word Feature 3.1 Internal Sub-Features",
    "add_info": "5 http://www.scopesys.com/",
    "text": "Our model captures three types of internal sub-features: 1) f 1 : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2 : internal semantic feature of important triggers; 3) f 3 : internal gazetteer feature. 1) f 1 is the basic sub-feature exploited in this look-up gazetteers: lists of names of persons, organizations, locations and other kinds of named entities. This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems [Cite_Footnote_5] . Gazetters have been widely used in NER systems to improve performance."
  },
  {
    "id": 1247,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "4 Features",
    "add_info": "6 https://code.google.com/p/word2vec/",
    "text": "The second feature captures finer-grained sim-ilarities between related words (e.g., cell and organism). Given the 400-dimensional embed-ding (Baroni et al., 2014) of each content word (lem-matized) in an input sentence, we compute a sentence vector by adding its content lemma vectors. The co-sine similarity between the S (1) and S (2) vectors is then used as an STS feature. Baroni et al. develop the word embeddings using word2vec [Cite_Footnote_6] from a cor-pus of about 2.8 billion tokens, using the Continuous Bag-of-Words ( CBOW ) model proposed by Mikolov et al. (2013)."
  },
  {
    "id": 1248,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/awslabs/fever",
    "section_title": "1 Introduction",
    "add_info": "2 https://github.com/awslabs/fever",
    "text": "To characterize the challenges posed by FEVER we develop a pipeline approach which, given a claim, first identifies relevant documents, then se-lects sentences forming the evidence from the doc-uments and finally classifies the claim w.r.t. ev-idence. The best performing version achieves 31.87% accuracy in verification when requiring correct evidence to be retrieved for claims S UP - PORTED or R EFUTED , and 50.91% if the correct-ness of the evidence is ignored, both indicating the difficulty but also the feasibility of the task. We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces [Cite_Footnote_2] and the baseline system to stimulate further research on verification."
  },
  {
    "id": 1249,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/sheffieldnlp/fever-baselines",
    "section_title": "1 Introduction",
    "add_info": "3 https://github.com/sheffieldnlp/fever-baselines",
    "text": "To characterize the challenges posed by FEVER we develop a pipeline approach which, given a claim, first identifies relevant documents, then se-lects sentences forming the evidence from the doc-uments and finally classifies the claim w.r.t. ev-idence. The best performing version achieves 31.87% accuracy in verification when requiring correct evidence to be retrieved for claims S UP - PORTED or R EFUTED , and 50.91% if the correct-ness of the evidence is ignored, both indicating the difficulty but also the feasibility of the task. We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [Cite_Footnote_3] to stimulate further research on verification."
  },
  {
    "id": 1250,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Adapter-Hub/adapter-transformers",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Standard Transfer Setup The standard way of 2015; Glavas\u030c et al., 2019; Ruder et al., 2019; Wang performing cross-lingual transfer with a state-of-et al., 2020) and later on the full-sentence level the-art large multilingual model such as multilin- (Devlin et al., 2019; Conneau and Lample, 2019; gual BERT or XLM-R is 1) to fine-tune it on la- Cao et al., 2020). More recent models such as mul-belled data of a downstream task in a source lan-tilingual BERT (Devlin et al., 2019)\u2014large Trans-guage and then 2) apply it directly to perform in-former (Vaswani et al., 2017) models pretrained ference in a target language (Hu et al., 2020). A on large amounts of multilingual data\u2014have been downside of this setting is that the multilingual ini-observed to perform surprisingly well when trans-tialisation balances many languages. It is thus not ferring to other languages (Pires et al., 2019; Wu suited to excel at a specific language at inference and Dredze, 2019; Wu et al., 2020) and the cur-time. We propose a simple method to ameliorate 1  https://github.com/Adapter-Hub/adapter-transformers this issue by allowing the model to additionally adapt to the particular target language."
  },
  {
    "id": 1251,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/raosudha89/GYAFC-corpus",
    "section_title": "4 Experimental setup 4.1 Datasets",
    "add_info": "1 https://github.com/raosudha89/GYAFC-corpus",
    "text": "For the intrinsic evaluation, we used the GYAFC dataset. [Cite_Footnote_1] It consists of handcrafted informal-formal sentence pairs in two domains, namely, Entertain-ment & Music (E&M) and Family & Relationship (F&R). Table 1 shows the statistics of the training, validation, and test sets for the GYAFC dataset. In the validation and test sets of GYAFC, each sen-tence has four references. For better exploring the data requirements of different methods to combine rules, we followed Zhang et al. (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, , and Grammarly to help us create a set of rules."
  },
  {
    "id": 1252,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.grammarbot.io/",
    "section_title": "4 Experimental setup 4.1 Datasets",
    "add_info": "2 https://www.grammarbot.io/",
    "text": "For the intrinsic evaluation, we used the GYAFC dataset. It consists of handcrafted informal-formal sentence pairs in two domains, namely, Entertain-ment & Music (E&M) and Family & Relationship (F&R). Table 1 shows the statistics of the training, validation, and test sets for the GYAFC dataset. In the validation and test sets of GYAFC, each sen-tence has four references. For better exploring the data requirements of different methods to combine rules, we followed Zhang et al. (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, [Cite_Footnote_2] , and Grammarly to help us create a set of rules."
  },
  {
    "id": 1253,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.grammarly.com/",
    "section_title": "4 Experimental setup 4.1 Datasets",
    "add_info": "3 https://www.grammarly.com/",
    "text": "For the intrinsic evaluation, we used the GYAFC dataset. It consists of handcrafted informal-formal sentence pairs in two domains, namely, Entertain-ment & Music (E&M) and Family & Relationship (F&R). Table 1 shows the statistics of the training, validation, and test sets for the GYAFC dataset. In the validation and test sets of GYAFC, each sen-tence has four references. For better exploring the data requirements of different methods to combine rules, we followed Zhang et al. (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, , and Grammarly [Cite_Footnote_3] to help us create a set of rules."
  },
  {
    "id": 1254,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://visagetechnologies.com/products-and-services/visagesdk/facetrack",
    "section_title": "2 Method 2.1 Dataset and Feature Extraction",
    "add_info": "Technologies Visage. 2016. Face tracking. https://visagetechnologies.com/products-and-services/visagesdk/facetrack. Accessed: 2016-03- 10.",
    "text": "To extract face and head movement information from the video, a face-tracker (Visage, 2016)  was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea-tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow. The first six values represent head location and orientation. The next six values represent vertical movement of the outer (\u201co \u201d), middle (\u201cm \u201d), or inner (\u201ci \u201d) portion of the right (\u201cr \u201d) or left (\u201cl \u201d) eyebrows. The final values rep-resent horizontal movement of the eyebrows."
  },
  {
    "id": 1255,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cs.toronto.edu/",
    "section_title": "References",
    "add_info": null,
    "text": "A Appendix: Supplemental Material In Section 2.3, we made use of a freely available CPM implementation available from  http://www.cs.toronto.edu/ \u223c jenn/CPM/ in MAT-LAB, Version 8.5.0.197613 (R2015a)."
  },
  {
    "id": 1256,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://wikipedia.org/",
    "section_title": "1 Introduction",
    "add_info": "1 http://wikipedia.org/",
    "text": "In recent years, NE extraction has been performed with machine learning based methods. However, such methods cannot cover all of NEs in texts. Therefore, it is necessary to extract NEs from ex-isting resources and use them to identify more NEs. There are many useful resources on the Web. We fo-cus on Wikipedia [Cite_Footnote_1] as the resource for acquiring NEs. Wikipedia is a free multilingual online encyclope-dia and a rapidly growing resource. In Wikipedia, a large number of NEs are described in titles of ar-ticles with useful information such as HTML tree structures and categories. Each article links to other related articles. According to these characteristics, they could be an appropriate resource for extracting NEs."
  },
  {
    "id": 1257,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "3 Graph-based CRFs for NE Categorization in Wikipedia",
    "add_info": null,
    "text": "Charles Sutton. 2006. GRMM: A graphical models toolkit.  http://mallet.cs.umass.edu."
  },
  {
    "id": 1258,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.chasen.org/\u02dctaku/software/TinySVM/",
    "section_title": "4 Experiments 4.2 Experimental settings",
    "add_info": "2 http://www.chasen.org/\u02dctaku/software/TinySVM/",
    "text": "S model C model R R model I model Figure 3: An example of graphs constructed by combination of defined cliques. S, C, R in the model names mean that corresponding model has Sibling, Cousin, Relative cliques respectively. In each model, classification is performed on each con-nected subgraph. SVMs We introduce two models by SVMs (model I and model P). In model I, each anchor text is clas-sified independently. In model P, we ordered the anchor texts in a linear-chain sequence. Then, we perform a history-based classification along the se-quence, in which j \u2212 1-th classification result is used in j-th classification. We use TinySVM with a linear-kernel. One-versus-rest method is used for multi-class classification. To perform training and testing with SVMs, we use TinySVM [Cite_Footnote_2] with a linear-kernel, and one-versus-rest is used for multi-class classification. We used the cost of constraint vio-lation C = 1."
  },
  {
    "id": 1259,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mecab.sourceforge.net/",
    "section_title": "4 Experiments 4.2 Experimental settings",
    "add_info": "3 http://mecab.sourceforge.net/",
    "text": "Features for CRFs and SVMs The features used in the classification with CRFs and SVMs are shown in Table 2. Japanese morphological analyzer MeCab [Cite_Footnote_3] is used to obtain morphemes."
  },
  {
    "id": 1260,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "3 Graph-based CRFs for NE Categorization in Wikipedia",
    "add_info": "Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.",
    "text": "CRFs In order to investigate which type of clique boosts classification performance, we perform ex-periments on several CRFs models that are con-structed from combinations of defined cliques. Re-sulting models of CRFs evaluated on this experi-ments are SCR, SC, SR, CR, S, C, R and I (indepen-dent). Figure 3 shows representative graphs of the eight models. When the graph are disconnected by reducing the edges, the classification is performed on each connected subgraph. We call it an example. We name the examples according the graph struc-ture: \u201dloopy examples\u201d are subgraphs including at least one cycle; \u201dlinear chain or tree examples\u201d are subgraphs including not a cycle but at least an edge; \u201done node examples\u201d are subgraphs without edges. Table 1 shows the distribution of the examples of each model. Since SCR, SC, SR and CR model have loopy examples, TRP approximate inference is nec-essary. To perform training and testing with CRFs, we use GRMM (Sutton, 2006)  with TRP. We set the Gaussian Prior variances for weights as \u03c3 2 = 10 in all models."
  },
  {
    "id": 1261,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.chasen.org/\u02dctaku/software/TinySVM/",
    "section_title": "4 Experiments 4.2 Experimental settings",
    "add_info": "2 http://www.chasen.org/\u02dctaku/software/TinySVM/",
    "text": "S model C model R R model I model Figure 3: An example of graphs constructed by combination of defined cliques. S, C, R in the model names mean that corresponding model has Sibling, Cousin, Relative cliques respectively. In each model, classification is performed on each con-nected subgraph. SVMs We introduce two models by SVMs (model I and model P). In model I, each anchor text is clas-sified independently. In model P, we ordered the anchor texts in a linear-chain sequence. Then, we perform a history-based classification along the se-quence, in which j \u2212 1-th classification result is used in j-th classification. We use TinySVM with a linear-kernel. One-versus-rest method is used for multi-class classification. To perform training and testing with SVMs, we use TinySVM [Cite_Footnote_2] with a linear-kernel, and one-versus-rest is used for multi-class classification. We used the cost of constraint vio-lation C = 1."
  },
  {
    "id": 1262,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mecab.sourceforge.net/",
    "section_title": "4 Experiments 4.2 Experimental settings",
    "add_info": "3 http://mecab.sourceforge.net/",
    "text": "Features for CRFs and SVMs The features used in the classification with CRFs and SVMs are shown in Table 2. Japanese morphological analyzer MeCab [Cite_Footnote_3] is used to obtain morphemes."
  },
  {
    "id": 1263,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://cnts.uia.ac.be/conll2003/ner/",
    "section_title": "6 Experiments 6.1 Dataset and Evaluation",
    "add_info": null,
    "text": "We test the effectiveness of our technique on the CoNLL 2003 English named en-tity recognition dataset downloadable from  http://cnts.uia.ac.be/conll2003/ner/. The data comprises Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). The data is separated into a training set, a development set (testa), and a test set (testb). The training set contains 945 documents, and approximately 203,000 tokens and the test set has 231 documents and approximately 46,000 tokens. Performance on this task is evaluated by measuring the precision and recall of annotated entities (and not tokens), combined into an F1 score. There is no partial credit for labeling part of an entity sequence correctly; an incorrect entity boundary is penalized as both a false positive and as a false negative."
  },
  {
    "id": 1264,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/lifengjin/dimi_emnlp18",
    "section_title": "5 Model",
    "add_info": "2 https://github.com/lifengjin/dimi_emnlp18.",
    "text": "Specifically, \u03c4each tree is a set {\u03c4 , \u03c4 , \u03c4 [Cite_Footnote_2] , \u03c4 11 , \u03c4 12 , \u03c4 21 , ...} of category node labels \u03c4 \u03b7 where \u03b7 \u2208 {1,2} \u2217 defines a path of left or right branches from the root to that node. Category labels for every pair of left and right children \u03c4 \u03b71 ,\u03c4 \u03b72 are drawn from a multinomial distribution defined by the grammar G and the category of the parent \u03c4 \u03b7 : where \u03b4 x is a Kronecker delta function equal to 1 at value x and 0 elsewhere, and terminals have null expansions P G (a b | w) = P G (a b | \u22a5) = ~a, b=\u22a5, \u22a5\u007f for w \u2208 W. 4"
  },
  {
    "id": 1265,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.yelp.com/dataset/download",
    "section_title": "B Additional Experimental Details B.1 Self-attentive Sentence Classification",
    "add_info": "2 https://www.yelp.com/dataset/download",
    "text": "Dataset Three tasks are conducted on three pub-lic sentence classification datasets. Author profiling (Age dataset ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset [Cite_Footnote_2] ) is to predict the number of stars the user assigned to by analysis their reviews. Tex-tual entailment (SNLI dataset 3 ) is to tell whether the semantics in the two sentences are entailment or contradiction or neutral. Following Lin et al. (2017), the train / validate / test split of Age is 68485 / 4000 / 4000, Yelp is 500K / 2000 / 2000, SNLI is 550K / 10K / 10K."
  },
  {
    "id": 1266,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://skylion007.github.io/OpenWebTextCorpus/",
    "section_title": "B Additional Experimental Details B.3 ELECTRA",
    "add_info": "9 https://skylion007.github.io/OpenWebTextCorpus/",
    "text": "Dataset Following the official code of Clark et al. (2020), ELECTRA models are pretrained on the OpenWebTextCorpus [Cite_Footnote_9] dataset, an open source ef-fort to reproduce OpenAI\u2019s WebText dataset. Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents. The pretrained model is then finetuned and evaluated on GLUE benchmark . GLUE contains a variety of tasks covering tex-tual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), senti-ment (SST), and linguistic acceptability (CoLA). Our evaluation metrics are Spearman correlation for STS, Matthews correlation for CoLA, and ac-curacy for the other GLUE tasks."
  },
  {
    "id": 1267,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/rikdz/GraphWriter",
    "section_title": "B Additional Experimental Details B.3 ELECTRA",
    "add_info": "12 https://github.com/rikdz/GraphWriter",
    "text": "Experiment settings The ELECTRA-small model we implemented follow all official settings 11 except that it is fully-trained on one GTX 1080Ti GPU for 6 days. The ELECTRA-small model has 12 layers with 4 heads in every layer\u2019s attention. For our method, the stepsize is set to 0.01 and the repulsive term \u03b1 is set to 0.1. The repulsive learning of attention is only applied to the pre-training stage. The fine-tuning remains the same with the original one. B.4 GraphWriter Dataset Experiments are conducted on the Ab-stract GENeration DAtaset (AGENDA) (Koncel-Kedziorski et al., 2019), a dataset of knowledge graphs paired with scientific abstracts. It consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences. We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings [Cite_Footnote_12] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer. We reproduce their results and keep all settings the same when applying the proposed repulsive attention. The SVGD update rule is used in our algorithm and applied to all lay-ers. The stepsize is set to 0.1 and the repulsive weight is set to 0.01 in this experiment. The model is trained on one TITAN Xp GPU."
  },
  {
    "id": 1268,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://taku910.github.io/cabocha/",
    "section_title": "1 Introduction",
    "add_info": "1 http://taku910.github.io/cabocha/.",
    "text": "Syntactic structures are usually represented as dependencies between chunks called bunsetsus. A bunsetsu is a Japanese grammatical and phono-logical unit that consists of one or more con-tent words such as a noun, verb, or adverb fol-lowed by a sequence of zero or more function words such as auxiliary verbs, postpositional par-ticles, or sentence-final particles. Most publicly available Japanese parsers, including CaboCha [Cite_Footnote_1] (Kudo et al., 2002) and KNP (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure. Such parsers are generally highly accurate and have been widely used in various NLP applications."
  },
  {
    "id": 1269,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP",
    "section_title": "1 Introduction",
    "add_info": "2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.",
    "text": "Syntactic structures are usually represented as dependencies between chunks called bunsetsus. A bunsetsu is a Japanese grammatical and phono-logical unit that consists of one or more con-tent words such as a noun, verb, or adverb fol-lowed by a sequence of zero or more function words such as auxiliary verbs, postpositional par-ticles, or sentence-final particles. Most publicly available Japanese parsers, including CaboCha (Kudo et al., 2002) and KNP [Cite_Footnote_2] (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure. Such parsers are generally highly accurate and have been widely used in various NLP applications."
  },
  {
    "id": 1270,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.cl.cs.titech.ac.jp/\u223cryu-i/syncha/",
    "section_title": "1 Introduction",
    "add_info": "3 http://www.cl.cs.titech.ac.jp/\u223cryu-i/syncha/.",
    "text": "Therefore, predicate-argument structure analysis is usually implemented as a post-processor of bunsetsu-based syntactic parser, not just for as-signing grammatical functions, but for identifying constituents, such as an analyzer SynCha [Cite_Footnote_3] (Iida et al., 2011), which uses the parsing results from CaboCha. We assume that using a word as a pars-ing unit instead of a bunsetsu chunk helps to main-tain consistency between syntactic structure anal-ysis and predicate-argument structure analysis."
  },
  {
    "id": 1271,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/cambridgeltl/adversarial-postspec",
    "section_title": "6 Conclusion and Future Work",
    "add_info": null,
    "text": "In future work, we will explore more sophis-ticated adversarial models such as Cycle-GAN (Zhu et al., 2017). Moreover, we will experiment with bootstrapping approaches to extract new lexi-cal constraints from post-specialized embeddings. We also plan to extend the method to asymmet-ric relations (e.g., hypernymy) and to more target (resource-lean) languages. The code is available at  https://github.com/cambridgeltl/adversarial-postspec."
  },
  {
    "id": 1272,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.statmt.org/wmt14/translation-task.html",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "5 https://www.statmt.org/wmt14/translation-task.html",
    "text": "Pre-training data settings. The ZH-EN sentence-level parallel dataset contains 2.0M sentence pairs with 54.8M Chinese words and 60.8M English words. 4 We use WMT14 EN-DE translation dataset as the EN-DE sentence-level parallel dataset which consists of 4.4M sentence pairs. [Cite_Footnote_5]"
  },
  {
    "id": 1273,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/messense/jieba-rs",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "6 https://github.com/messense/jieba-rs",
    "text": "All Chinese sentences are segmented by Jieba [Cite_Footnote_6] while all English and German sentences are tok-enized by Moses scripts (Koehn et al., 2007). For ZH-EN (EN-DE) translation, we merge the source and target sentences of the parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) opera-tions (Sennrich et al., 2016)."
  },
  {
    "id": 1274,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.casmacat.eu/corpus/news-commentary.html",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "9 http://www.casmacat.eu/corpus/news-commentary.html",
    "text": "\u2022 News, which is from News Commentary v11 corpus. [Cite_Footnote_9] We use news-test2015 and news-"
  },
  {
    "id": 1275,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/sameenmaruf/selective-attn/tree/master/data",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "10 https://github.com/sameenmaruf/selective-attn/tree/master/data",
    "text": "All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019). [Cite_Footnote_10] Simi-lar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pre-training datasets. See Appendix A for more statis-tics of the fine-tuning datasets."
  },
  {
    "id": 1276,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT",
    "section_title": "4 Experimentation 4.1 Experimental Settings",
    "add_info": "11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT",
    "text": "Model settings. We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [Cite_Footnote_11] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively. The number of heads in multi-head attention is 8 and the dropout rate is 0.1. In pre-training, we train the models for 500K steps on four V100 GPUs with batch-size 8192. We use Adam (Kingma and Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.98 for optimiza-tion, and learning rate as 1, the warm-up step as 16K. In fine-tuning, we fine-tune the models for 200K steps on a single V100 GPU with batch-size 8192, learning rate 0.3, and warm-up step 4K. In inferring, we set the beam size to 5. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality."
  },
  {
    "id": 1277,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/williamFalcon/",
    "section_title": "References",
    "add_info": "W.A. Falcon. 2019. Pytorch lightning. https://github.com/williamFalcon/ pytorch-lightning .",
    "text": "Since we essentially cherry-picked good results, its only fair to show a similarly cherry-picked negative example of M A U DE . We sampled from responses where M A U DE scores are negatively correlated with human annotations on Inquisitive-ness metric (5% of cases), and we show one of those responses in Figure 6. We notice how both DistilBERT-NLI and M A U DE fails to recognize the duplication of utterances which leads to a low overall score. This suggests there still exists room for improvement in developing M A U DE , possibly by training the model to detect degeneracy in the context. E Hyperparameters and Training Details We performed rigorous hyperparameter search to tune our model M A U DE . We train M A U DE with downsampling, as we observe poor results when we run the recurrent network on top of 768 dimensions. Specifically, we downsample to 300 dimensions, which is the same used by our baselines RUBER and InferSent in their respective encoder represen-tations. We also tested with the choice of either learning a PCA to downsample the BERT represen-tations vs learning the mapping D g (Equation 4), and found the latter producing better results. We keep the final decoder same for all models, which is a two layer MLP with hidden layer of size 200 dimensions and dropout 0.2. For BERT-based mod-els (DistilBERT-NLI and M A U DE ), we use Hug-gingFace Transformers (Wolf et al., 2019) to first fine-tune the training dataset on language model objective. We tested with training on frozen fine-tuned representations in our initial experiments, but fine-tuning end-to-end lead to better ablation scores. For all models we train using Adam optimizer with 0.0001 as the learning rate, early stopping till vali-dation loss doesn\u2019t improve. For the sake of easy reproducibility, we use Pytorch Lightning (Falcon, 2019)  framework. We used 8 Nvidia-TitanX GPUs on a DGX Server Workstation to train faster using Pytorch Distributed Data Parallel (DDP)."
  },
  {
    "id": 1278,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/gucci-j/light-transformer-emnlp2021",
    "section_title": "E Performance in SQ U AD",
    "add_info": "1 Our code is publicly available here: https://github.com/gucci-j/light-transformer-emnlp2021",
    "text": "Masked language modeling ( MLM ), a self-supervised pretraining objective, is widely used in natural language processing for learn-ing text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocab-ulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve down-stream performance (e.g. next sentence predic-tion). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretrain-ing objectives based on token-level classifica-tion tasks as replacements of MLM . Empirical results on G LUE and SQ U AD show that our proposed methods achieve comparable or bet-ter performance to MLM using a B ERT - BASE architecture. We further validate our methods using smaller models, showing that pretrain-ing a model with 41% of the B ERT - BASE \u2019s pa-rameters, B ERT - MEDIUM results in only a 1% drop in G LUE scores with our best objective. [Cite_Footnote_1]"
  },
  {
    "id": 1279,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.nltk.org/",
    "section_title": "2 Pretraining Tasks",
    "add_info": "2 We use the Natural Language Toolkit\u2019s stop word list: https://www.nltk.org/.",
    "text": "Masked Token Type Classification (T OKEN T YPE ): Our fourth objective is a four-way classi-fication, aiming to predict whether a token is a stop word, [Cite_Footnote_2] a digit, a punctuation mark, or a content word. Therefore, the task can be seen as a simpli-fied version of POS tagging. We regard any tokens that are not included in the first three categories as content words. We mask 15% of tokens in each sample with a special [MASK] token and compute the cross-entropy loss over the masked ones only not to make the task trivial. For example, if we com-pute the token-level loss over unmasked tokens, a model can easily recognize the four categories as we only have a small number of non-content words in the vocabulary."
  },
  {
    "id": 1280,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/huggingface/datasets",
    "section_title": "3 Experimental Setup",
    "add_info": "4 https://github.com/huggingface/datasets",
    "text": "Models: We use B ERT (Devlin et al., 2019) ( BASE ) as our basis model by replacing the MLM and NSP objectives with one of our five token-level pretraining tasks in all our experiments. We also consider two smaller models from Turc et al. (2019), MEDIUM and SMALL , where we reduce the size of the following components compared to the BASE model: (1) hidden layers; (2) hidden size; (3) feed-forward layer size; and (4) attention heads. More specifically, MEDIUM has eight hidden layers and attention heads, while SMALL has four hid-den layers and eight attention heads. The size of feed-forward and hidden layers for both models are 2048 and 512, respectively. Pretraining Data: We pretrain all models on the English Wikipedia and BookCorpus (Zhu et al., 2015) (WikiBooks) using the datasets library. [Cite_Footnote_4] Implementation Details: We pretrain and fine-tune our models with two NVIDIA Tesla V100 (SXM2 - 32GB) with a batch size of 32 for BASE and 64 for MEDIUM and SMALL . We pretrain all our models for up to five days each due to limited access to computational resources and funds for running experiments. We save a checkpoint of each model every 24 hours. Evaluation: We evaluate our approaches on G LUE (Wang et al., 2019) and SQ U AD (Rajpurkar et al., 2016) benchmarks. To measure performance in downstream tasks, we fine-tune all models for five times each with a different random seed."
  },
  {
    "id": 1281,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.nltk.org/",
    "section_title": "E Performance in SQ U AD",
    "add_info": "7 A stop word category is based on the Natural Language Toolkit\u2019s stop word list: https://www.nltk.org/.",
    "text": "Masked Token Type Classification (T OKEN T YPE ): Our fourth task is a four-way classifi-cation task that identifies whether a token is a stop word [Cite_Footnote_7] , a digit, a punctuation mark, or a content word. We regard any tokens that are not included in the first three categories as content words. We mask 15% of tokens in each sample with a special [MASK] token and compute the cross-entropy loss over the masked ones only not to make the task trivial: if we compute the token-level loss, includ-ing unmasked tokens, a model can easily recognize the four categories of tokens as we have a small number of tokens for non-content words. In this task, a model should be able to identify the distinc-tion between different types of tokens; therefore, the task can be seen as a simplified version of POS tagging."
  },
  {
    "id": 1282,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/huggingface/datasets",
    "section_title": "C Experimental Setup C.1 Model Architecture",
    "add_info": "8 https://github.com/huggingface/datasets",
    "text": "C.2 Data Following Devlin et al. (2019), we use the English Wikipedia and BookCorpus (Zhu et al., 2015) data (WikiBooks) downloaded from the datasets library [Cite_Footnote_8] . We remove headers for the English Wikipedia and extract training samples with a max-imum length of 512. For the BookCorpus, we concatenate sentences such that the total number of tokens is less than 512. For the English Wikipedia, we extract one sample from articles whose length is less than 512. We tokenize text using byte-level Byte-Pair-Encoding (Sennrich et al., 2016). The resulting corpus consists of 8.1 million samples and 2.7 billion tokens in total."
  },
  {
    "id": 1283,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/gucci-j/light-transformer-emnlp2021",
    "section_title": "C Experimental Setup C.3 Implementation Details",
    "add_info": null,
    "text": "We implement our models using PyTorch (Paszke et al., 2019) and the transformers library (Wolf et al., 2020). We pretrain our models with two NVIDIA Tesla V100 (SXM2 - 32GB) and use one for fine-tuning. Our code is publicly available on GitHub:  https://github.com/gucci-j/light-transformer-emnlp2021."
  },
  {
    "id": 1284,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/artetxem/vecmap",
    "section_title": "5 Experiments",
    "add_info": "1 https://github.com/artetxem/vecmap",
    "text": "We use the datasets used by Artetxe et al. (2017), consisting of three language pairs: English-Italian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014); the other datasets were created by Artetxe et al. (2017). Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b)) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictio-nary consisting only of numeral identity transla-tions (such as 2-2, 3-3, et cetera) as in Artetxe et al. (2017). [Cite_Footnote_1] However, because the methods pre-sented in this work feature tunable hyperparame-ters, we use a portion of the training set as devel-opment data. 2 In all experiments, a single target word is predicted for each source word, and full points are awarded if it is one of the listed correct translations. On average, the number of transla-tions for each source (non-English) word was 1.2 for English-Italian, 1.3 for English-German, and 1.4 for English-Finnish."
  },
  {
    "id": 1285,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://L2R.cs.uiuc.edu/\u02dccogcomp/software.php",
    "section_title": "5 Experiments 5.2 Textual Entailment",
    "add_info": "3 http://L2R.cs.uiuc.edu/\u02dccogcomp/software.php",
    "text": "The second column of Table 2 lists the resources used to generate features corresponding to each hid-den variable type. For word-mapping variables, the features include a WordNet based metric (WNSim), indicators for the POS tags and negation identifiers. We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger [Cite_Footnote_3] ) to generate features. For edge the entailment and paraphrase identification tasks. See Section 4 for an explanation of the hidden variable types. The linguistic resources used to generate features are abbreviated as follows \u2013 POS: Part of speech, Coref: Canonical coreferent entities; NE: Named Entity, ED: Edit distance, Neg: Negation markers, DEP: mapping variables, we include the features of the corresponding word mapping variables, scaled by the word similarity of the words forming the edge."
  },
  {
    "id": 1286,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://groups.google.com/group/hoo-nlp/",
    "section_title": "3 Experiments and Results",
    "add_info": "2 Available at http://groups.google.com/group/hoo-nlp/ after registration.",
    "text": "We experimentally test our M 2 method in the con-text of the HOO shared task. The HOO test data [Cite_Footnote_2] consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff to extract system edits. We obtain each system\u2019s output and segment it at the sentence level accord-ing to the gold standard sentence segmentation. The source sentences, system hypotheses, and correc-tions are tokenized using the Penn Treebank stan-dard (Marcus et al., 1993). The character edit offsets are automatically converted to token offsets. We set the parameter u to 2, allowing up to two unchanged words per edit. The results are shown in Table 1. Note that the M 2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. We can see that the M 2 scorer results in higher scores than the offi-cial scorer for all systems, showing that the official scorer missed some valid edits. For example, the M 2 scorer finds 155 valid edits for the UI system compared to 141 found by the official scorer, and 83 valid edits for the NU system, compared to 78 by the official scorer. We manually inspect the output of the scorers and find that the M 2 scorer indeed ex-tracts the correct edits matching the gold standard where possible. Examples are shown in Table 2."
  },
  {
    "id": 1287,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.gnu.org/s/wdiff/",
    "section_title": "3 Experiments and Results",
    "add_info": "4 http://www.gnu.org/s/wdiff/",
    "text": "We experimentally test our M 2 method in the con-text of the HOO shared task. The HOO test data consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff [Cite_Footnote_4] to extract system edits. We obtain each system\u2019s output and segment it at the sentence level accord-ing to the gold standard sentence segmentation. The source sentences, system hypotheses, and correc-tions are tokenized using the Penn Treebank stan-dard (Marcus et al., 1993). The character edit offsets are automatically converted to token offsets. We set the parameter u to 2, allowing up to two unchanged words per edit. The results are shown in Table 1. Note that the M 2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. We can see that the M 2 scorer results in higher scores than the offi-cial scorer for all systems, showing that the official scorer missed some valid edits. For example, the M 2 scorer finds 155 valid edits for the UI system compared to 141 found by the official scorer, and 83 valid edits for the NU system, compared to 78 by the official scorer. We manually inspect the output of the scorers and find that the M 2 scorer indeed ex-tracts the correct edits matching the gold standard where possible. Examples are shown in Table 2."
  },
  {
    "id": 1288,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.comp.nus.edu.sg/software/",
    "section_title": "5 Conclusion",
    "add_info": null,
    "text": "We have presented a novel method, called Max-Match (M 2 ), for evaluating grammatical error cor-rection. Our method computes the sequence of phrase-level edits that achieves the highest over-lap with the gold-standard annotation. Experi-ments on the HOO data show that our method overcomes deficiencies in the current evaluation method. The M 2 scorer is available for download at  http://nlp.comp.nus.edu.sg/software/."
  },
  {
    "id": 1289,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://stackexchange.com/sites",
    "section_title": "6 Experimental Setup",
    "add_info": "2 https://stackexchange.com/sites",
    "text": "MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset where [Cite_Footnote_2] to 10 news articles share a single summary. We consider every pair of articles that share a summary to be a similar document pair. Table 2 shows summary statistics of the two document ranking datasets."
  },
  {
    "id": 1290,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://archive.org/details/stackexchange",
    "section_title": "6 Experimental Setup",
    "add_info": "3 https://archive.org/details/stackexchange",
    "text": "StackExchange 2 is an online question answer-ing platform and has been used as a benchmark in previous work (dos Santos et al., 2015; Shah et al., 2018; Perkins and Yang, 2019). We took the June 2019 data dumps [Cite_Footnote_3] of the AskUbuntu and Su-perUser subdomains of the platform and combined them to form our dataset."
  },
  {
    "id": 1291,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.nltk.org/",
    "section_title": "C Implementation Details",
    "add_info": "6 https://www.nltk.org/",
    "text": "Text Span Extraction. Sentences are ex-tracted from the documents using the sentence tokenizer from the nltk Python package [Cite_Footnote_6] (Bird et al., 2009)."
  },
  {
    "id": 1292,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/fchollet/keras",
    "section_title": "3 System 3.3 Technical implementation",
    "add_info": "Franois Chollet. 2015. keras. https://github.com/fchollet/keras.",
    "text": "The visualisation application is a client-server sys-tem with a web interface. It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002). Heatmaps and t-SNE results are plotted with the matplotlib (Hunter, 2007) library and sklearn (Pedregosa et al., 2011). The main deep learning framework is Keras (Chollet, 2015)  , but there is also a preliminary attempt to include PyTorch (Paszke et al., 2017) models. Statisti-cal measures were calculated using scipy (Jones et al., 2001\u2013)."
  },
  {
    "id": 1293,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/pallets/flask",
    "section_title": "3 System 3.3 Technical implementation",
    "add_info": "Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.",
    "text": "The visualisation application is a client-server sys-tem with a web interface. It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018)  framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002). Heatmaps and t-SNE results are plotted with the matplotlib (Hunter, 2007) library and sklearn (Pedregosa et al., 2011). The main deep learning framework is Keras (Chollet, 2015), but there is also a preliminary attempt to include PyTorch (Paszke et al., 2017) models. Statisti-cal measures were calculated using scipy (Jones et al., 2001\u2013)."
  },
  {
    "id": 1294,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.cs.utexas.edu/users/ml/nldata/geoquery",
    "section_title": "4 Experiment",
    "add_info": "Mooney, Raymond. Geoquery 1996.Data. http://www.cs.utexas.edu/users/ml/nldata/geoquery. html (accessed February 13, 2010).",
    "text": "To test the coverage and precision of Locutus, I have customized it to answer questions from the G EOQUERY 250 corpus (Mooney, 1996)  , which consists of a database of geographical information paired with 250 English sentences requesting in-formation from that database. 25 of these sentences are held out for the purposes of another study, and I have not examined the behavior of Locutus with respect to these sentences. I ran the other 225 sen-tences through Locutus, keeping track of which sentences Locutus built at least one query for. For each of those sentences, I also tracked the follow-ing:"
  },
  {
    "id": 1295,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/mlpc-ucsd/BERT_Convolutions",
    "section_title": "3 Integrating lightweight convolutions 3.2 Composite attention and lightweight convolution experiments",
    "add_info": "2 Code is available at https://github.com/mlpc-ucsd/BERT_Convolutions, built upon the Huggingface Transformers library (Wolf et al., 2020).",
    "text": "Pre-training To maximize similarity with De-vlin et al. (2019), we pre-trained models on the BookCorpus (Zhu et al., 2015) and WikiText-103 datasets (Merity et al., 2017) using masked lan-guage modeling. Small models were pre-trained for 125,000 steps, with batch size 128 and learn-ing rate 0.0003. Full pre-training and fine-tuning details are outlined in Appendix A.1. [Cite_Footnote_2] Evaluation Models were evaluated on the GLUE benchmark, a suite of sentence classification tasks including natural language inference (NLI), gram-maticality judgments, sentiment classification, and textual similarity (Wang et al., 2018). For each task, we ran ten fine-tuning runs and used the model with the best score on the development set. We report scores on the GLUE test set. Development scores and statistics for all experiments are reported in Appendix A.2."
  },
  {
    "id": 1296,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/SimonHFL/CWEB",
    "section_title": "1 Introduction",
    "add_info": "3 https://github.com/SimonHFL/CWEB",
    "text": "Contributions: We (i) release a new dataset, CWEB (Corrected Websites), of website data that is corrected for grammatical errors; [Cite_Footnote_3] (ii) system-atically compare it to previously released GEC corpora; (iii) benchmark current state-of-the-art GEC approaches on this data and demonstrate that they are heavily biased towards existing datasets with high error density, even after fine-tuning on our target domain; (iv) perform an analysis showing that a factor behind the performance drop is the inability of systems to rely on a strong internal language model in low error density domains."
  },
  {
    "id": 1297,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://commoncrawl.org/",
    "section_title": "2 CWEB Dataset",
    "add_info": "4 https://commoncrawl.org/",
    "text": "Crawl [Cite_Footnote_4] dataset and represent a wide range of data seen online such as blogs, magazines, corporate or educational websites. These include texts writ-ten by native or non-native English speakers and professional as well as amateur online writers. Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they We compare our data with existing GEC corpora which cover a range of domains and proficiency lev-els. Table 3 presents a number of different statistics and Table 4 their error-type frequencies. 10 3.1 English as a second language (ESL) JFLEG (Napoles et al., 2017) The JHU Fluency-Extended GUG corpus consists of sentences writ-ten by English language learners (with different proficiency levels and L1s) for the TOEFL\u00ae exam, tuned on CWEB data. Fine-tuning yields substantial improvements, but scores are still worse than on ESL domains. Scores are calculated against each individual annotator and averaged. sary). vanced speakers. CWEB and AESW in particular stand out, with edits that largely retain the seman-tics of a sentence and that result in more subtle improvements."
  },
  {
    "id": 1298,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/miso-belica/jusText",
    "section_title": "2 CWEB Dataset",
    "add_info": "5 https://github.com/miso-belica/jusText",
    "text": "Crawl dataset and represent a wide range of data seen online such as blogs, magazines, corporate or educational websites. These include texts writ-ten by native or non-native English speakers and professional as well as amateur online writers. Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText [Cite_Footnote_5] tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they We compare our data with existing GEC corpora which cover a range of domains and proficiency lev-els. Table 3 presents a number of different statistics and Table 4 their error-type frequencies. 10 3.1 English as a second language (ESL) JFLEG (Napoles et al., 2017) The JHU Fluency-Extended GUG corpus consists of sentences writ-ten by English language learners (with different proficiency levels and L1s) for the TOEFL\u00ae exam, tuned on CWEB data. Fine-tuning yields substantial improvements, but scores are still worse than on ESL domains. Scores are calculated against each individual annotator and averaged. sary). vanced speakers. CWEB and AESW in particular stand out, with edits that largely retain the seman-tics of a sentence and that result in more subtle improvements."
  },
  {
    "id": 1299,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/SimonHFL/CWEB",
    "section_title": "2 CWEB Dataset",
    "add_info": null,
    "text": "To avoid copyright restrictions, we split the col-lected paragraphs into sentences and shuffle all sentences in order to break the original and co-herent structure that would be needed to repro-duce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Scha\u0308fer, 2015; Bie-mann et al., 2007). The data is available at  https://github.com/SimonHFL/CWEB . covering a range of topics. Texts have been cor-rected for grammatical errors and fluency."
  },
  {
    "id": 1300,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "2 CWEB Dataset",
    "add_info": "9 https://spacy.io/",
    "text": "The texts are tokenized using SpaCy [Cite_Footnote_9] and au-tomatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017)."
  },
  {
    "id": 1301,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.cs.sunysb.edu/\u02dcjunkang/connotation_wordnet",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "ConnotationWordNet, the final outcome of our study, is a new lexical resource that has conno-tation labels over both words and senses follow-ing the structure of WordNet. The lexicon is pub-licly available at:  http://www.cs.sunysb.edu/\u02dcjunkang/connotation_wordnet.) In what follows, we will first describe the net-work of words and senses (Section 2), then intro-duce the representation of the network structure as pairwise Markov Random Fields, and a loopy be-lief propagation algorithm as collective inference (Section 3). We then present comprehensive eval-uation (Section 4 & 5 & 6), followed by related work (Section 7) and conclusion (Section 8)."
  },
  {
    "id": 1302,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://cs.stanford.edu/",
    "section_title": "7 Conclusion",
    "add_info": "11 To facilitate future work, all of our models are publicly available at http://cs.stanford.edu/ \u223c valentin/.",
    "text": "Future work could explore unifying these tech-niques with other state-of-the-art approaches. It may be useful to scaffold on both data and model com-plexity, e.g., by increasing head automata\u2019s number of states (Alshawi and Douglas, 2000). We see many opportunities for improvement, considering the poor performance of oracle training relative to the super-vised state-of-the-art, and in turn the poor perfor-mance of unsupervised state-of-the-art relative to the oracle models. [Cite_Footnote_11] To this end, it would be instructive to understand both the linguistic and statistical na-ture of the sweet spot, and to test its universality."
  },
  {
    "id": 1303,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/asiddhant/Active-NLP",
    "section_title": "1 Introduction",
    "add_info": "1 Code for all of our models and for running active learn-ing experiments can be found at https://github.com/asiddhant/Active-NLP",
    "text": "In this paper, we present a large-scale study [Cite_Footnote_1] , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL). For each task we consider, with multiple datasets, multiple models, and mul-tiple acquisition functions. Moreover, in all ex-periments, we set hyper-parameters on warm-start data, allowing for a more honest assessment. This paper does not seek to champion any one approach but instead to ask, is there any single method that we can reliably expect to work out-of-the-box on a new problem?"
  },
  {
    "id": 1304,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://flan.cs.pitt.edu/~hwa/align/align.html",
    "section_title": "4 Conclusion and Future Work",
    "add_info": "4 The annotation interface is open to public. Please visit http://flan.cs.pitt.edu/~hwa/align/align.html",
    "text": "In summary, we have presented an annotation envi-ronment for acquiring word alignments between En-glish and Chinese as well as Part-Of-Speech tags for Chinese. The system is in place and the annotation process is underway. [Cite_Footnote_4]"
  },
  {
    "id": 1305,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/lxucs/multilingual-sl",
    "section_title": "4 Experiments",
    "add_info": "1 Code is available at https://github.com/lxucs/multilingual-sl.",
    "text": "We implement three different settings for the baseline. [Cite_Footnote_1] BL-Direct is the direct zero-shot trans-fer without utilizing unlabeled data of TLs. BL-Single trains gold data of English and silver data of only one TL per model; it simply selects predic-tions of all unlabeled data as silver labels, without considering any uncertainties. BL-Joint is similar to BL-Single but instead train with all TLs jointly."
  },
  {
    "id": 1306,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cs.waikato.ac.nz/ml/weka/",
    "section_title": "2 Related Work and Experimental Framework",
    "add_info": "2 Weka software (http://www.cs.waikato.ac.nz/ml/weka/)",
    "text": "Section 3 describes the features used by our pre-dictor. Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights. To this end, we use SVM-Regression [Cite_Footnote_2] (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system. We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown cor-pus (Section 5)."
  },
  {
    "id": 1307,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/daiquocnguyen/R-MeN",
    "section_title": "5 Conclusion",
    "add_info": null,
    "text": "We propose a new KG embedding model, named R-MeN, where we integrate transformer self-attention mechanism-based memory interactions with a CNN decoder to capture the potential dependencies in the KG triples effectively. Experimental results show that our proposed R-MeN obtains the new state-of-the-art performances for both the triple classification and search personalization tasks. In future work, we plan to extend R-MeN for multi-hop knowledge graph reasoning. Our code is available at:  https://github.com/daiquocnguyen/R-MeN."
  },
  {
    "id": 1308,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://stanfordnlp.github.io/CoreNLP/",
    "section_title": "3 Methodology",
    "add_info": "2 https://stanfordnlp.github.io/CoreNLP/",
    "text": "We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }. Addition-ally, let\u2019s say that each sentence q i in introduction is composed of words {w 1 , w 2 , ...w l qi }, where l q is the length of q i . We consider a match(q i ,p j ), if there is an exact string match of a sequence of words {w k , w k+1 , ..w l qi } between the sentence q i and passage p j . If this sequence is either a noun phrase, verb phrase, adjective phrase or a named entity in p j , as recognized by CoreNLP or BAN-NER, we select it as an answer span A. Addition-ally, we use p j as the passage P and form a cloze question Q from the answer bearing sentence q i by replacing A with a placeholder. As a result, we obtain passage-question-answer (P, Q, A) triples (Table 1 shows an example). As a post-processing step, we prune out (P,Q,A) triples where the word overlap between the question (Q) and pas-sage (P) is less than [Cite_Footnote_2] words (after excluding the stop words)."
  },
  {
    "id": 1309,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://banner.sourceforge.net",
    "section_title": "3 Methodology",
    "add_info": "3 http://banner.sourceforge.net",
    "text": "We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer [Cite_Footnote_3] (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }. Addition-ally, let\u2019s say that each sentence q i in introduction is composed of words {w 1 , w 2 , ...w l qi }, where l q is the length of q i . We consider a match(q i ,p j ), if there is an exact string match of a sequence of words {w k , w k+1 , ..w l qi } between the sentence q i and passage p j . If this sequence is either a noun phrase, verb phrase, adjective phrase or a named entity in p j , as recognized by CoreNLP or BAN-NER, we select it as an answer span A. Addition-ally, we use p j as the passage P and form a cloze question Q from the answer bearing sentence q i by replacing A with a placeholder. As a result, we obtain passage-question-answer (P, Q, A) triples (Table 1 shows an example). As a post-processing step, we prune out (P,Q,A) triples where the word overlap between the question (Q) and pas-sage (P) is less than words (after excluding the stop words)."
  },
  {
    "id": 1310,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/georgwiese/biomedical-qa",
    "section_title": "4 Experiments & Results 4.1 Datasets",
    "add_info": "5 https://github.com/georgwiese/biomedical-qa",
    "text": "We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [Cite_Footnote_5] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions. For this setting, there are only 899 questions for training. Since this is already a low-resource problem we only report results using 5-fold cross-validation on all the available data. We report Mean Reciprocal Rank (MRR) on the fac-toid questions, and F1 score for the list questions."
  },
  {
    "id": 1311,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/brmson/question-classification",
    "section_title": "4 Experiments & Results 4.3 Analysis",
    "add_info": "7 https://github.com/brmson/question-classification",
    "text": "Performance on question types: Figure 2 shows the average gain in F1 score for different types of questions, when we pretrain on the clozes compared to the supervised case. This analysis is done on the 10% split of the SQuAD training set. We consider two classifications of each ques-tion \u2013 one determined on the first word (usually a wh-word) of the question (Figure 2 (bottom)) and one based on the output of a separate ques-tion type classifier [Cite_Footnote_7] adapted from (Li and Roth, 2002). We use the coarse grain labels namely Abbreviation (ABBR), Entity (ENTY), Descrip-tion (DESC), Human (HUM), Location (LOC), Numeric (NUM) trained on a Logistic Regres-sion classification system . While there is an im-provement across the board, we find that abbrevi-ation questions in particular receive a large boost. Also, \u201dwhy\u201d questions show the least improve-ment, which is in line with our expectation, since these usually require reasoning or world knowl-edge which cloze questions rarely require."
  },
  {
    "id": 1312,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/antonisa/inflection",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/antonisa/inflection.",
    "text": "Recent years have seen exceptional strides in the task of automatic morphological inflec-tion generation. However, for a long tail of languages the necessary resources are hard to come by, and state-of-the-art neural methods that work well under higher resource settings perform poorly in the face of a paucity of data. In response, we propose a battery of im-provements that greatly improve performance under such low-resource conditions. First, we present a novel two-step attention architecture for the inflection decoder. In addition, we in-vestigate the effects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination. The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points. [Cite_Footnote_1] Also, we identify the crucial factors for suc-cess with cross-lingual transfer for morpho-logical inflection: typological similarity and a common representation across languages."
  },
  {
    "id": 1313,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.inuktitutcomputing.ca/",
    "section_title": "1 Introduction",
    "add_info": "2 http://www.inuktitutcomputing.ca/",
    "text": "Additionally, they could be very useful for building educational applications for languages of under-represented communities (along with their inverse, morphological analyzers). Encouraging examples are the Yupik morphological analyzer (Schwartz et al., 2019) and the Inuktitut educa-tional tools from the respective Native Peoples communities. [Cite_Footnote_2] The social impact of such applica-tions can be enormous, effectively raising the sta-tus of the languages slightly closer to the level of the dominant regional language."
  },
  {
    "id": 1314,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/vojtsek/joint-induction",
    "section_title": "Origin Locale Expensiveness",
    "add_info": "2 https://github.com/vojtsek/joint-induction",
    "text": "Our experimental code is available on GitHub. [Cite_Footnote_2]"
  },
  {
    "id": 1315,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/deepmipt/ner",
    "section_title": "3 Method 3.3 Slot Tagger Model Training",
    "add_info": "7 https://github.com/deepmipt/ner",
    "text": "We model the slot tagging task as sequence tag-ging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of re-spective tags (Lample et al., 2016). [Cite_Footnote_7] The output layer of the tagger network gives softmax proba-bility distributions over possible tags. To further increase recall, we add an inference-time rule \u2013 if slot candidate is split \u2013 it is just ranked for relevance multiple times (with respect to multiple contexts). the most probable predicted tag is \u2018O\u2019 (i.e., no slot) and the second most probable tag has a probability higher than a preset threshold \ud835\udc47 tag , the second tag is chosen as a prediction instead. As we discuss in Section 6, this threshold is crucial for achieving substantial recall improvement."
  },
  {
    "id": 1316,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk",
    "section_title": "5 Experiments 5.1 Datasets and Experimental Setup",
    "add_info": "9 We used the ATIS data version from https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk.",
    "text": "\u2022 ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [Cite_Footnote_9]"
  },
  {
    "id": 1317,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://spacy.io",
    "section_title": "5 Experiments 5.1 Datasets and Experimental Setup",
    "add_info": "10 https://spacy.io",
    "text": "As sources of weak supervision providing slot can-didates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) \u2013 a union of labels pro-vided by both parsers is used in all our setups. In ad-dition, to explore combined sources on the named-entity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy. [Cite_Footnote_10] To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings."
  },
  {
    "id": 1318,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/vojtsek/joint-induction",
    "section_title": "7 Conclusion",
    "add_info": "16 https://github.com/vojtsek/joint-induction",
    "text": "We present a novel approach for weakly supervised natural language understanding in dialogue systems that discovers domain-relevant slots and tags them in a standalone fashion. Our method removes the need for annotated training data by using off-the-shelf linguistic annotation models. Experiments on five datasets in four domains mark a signifi-cant improvement in intrinsic NLU performance over previous weakly supervised approaches; in particular, we vastly improve the slot recall. The usefulness of slots discovered by our method is further confirmed in a full dialogue response gener-ation application. Code used for our experiments is available on GitHub. [Cite_Footnote_16]"
  },
  {
    "id": 1319,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/qiangning/MATRES",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/qiangning/MATRES",
    "text": "The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal order-ing of events. [Cite_Footnote_1] It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and 20 more for evaluation) from TimeBank 2 (Puste-jovsky et al., 2003), AQUAINT 3 (Graff, 2002) and Platinum (UzZaman et al., 2013)."
  },
  {
    "id": 1320,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://catalog.ldc.upenn.edu/LDC2006T08",
    "section_title": "2 Our Baseline Model",
    "add_info": "2 https://catalog.ldc.upenn.edu/LDC2006T08",
    "text": "Our pairwise temporal ordering model re-ceives as input a sequence X [0,n) of n tokens (or subword units for BERT-like models) i.e. {x 0 ,x 1 ,...,x n\u22121 }, representing the input text. A subsequence span i is defined by start i , end i \u2208 [0,n). Subsequences span 1 and span [Cite_Footnote_2] represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2."
  },
  {
    "id": 1321,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2002T31",
    "section_title": "1 Introduction",
    "add_info": "3 https://catalog.ldc.upenn.edu/LDC2002T31",
    "text": "The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal order-ing of events. 1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and 20 more for evaluation) from TimeBank 2 (Puste-jovsky et al., 2003), AQUAINT [Cite_Footnote_3] (Graff, 2002) and Platinum (UzZaman et al., 2013)."
  },
  {
    "id": 1322,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.timeml.org/publications/timeMLdocs/timeml_1.2.1.html",
    "section_title": "3 Multi-task Learning 3.2 Auxiliary Datasets",
    "add_info": "6 http://www.timeml.org/publications/timeMLdocs/timeml_1.2.1.html.",
    "text": "We also use a closer and complementary tem-poral annotation dataset, i.e. the Timebank and Aquaint annotations involving timex re-lations (timex-event, event-timex, timex-timex) (Ning et al., 2018; Goyal and Durrett, 2019). [Cite_Footnote_6] We expect the model to greatly benefit from being ex-posed to the timex relations in an MTL framework by learning about temporality in general and by adding specificity of the event-event temporal re-lations from the MATRES annotations. Figure 2 shows an example of the data annotated with an event-timex relation."
  },
  {
    "id": 1323,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/UKPLab/emnlp2018-novel-metaphors",
    "section_title": "3 Corpus",
    "add_info": "2 https://github.com/UKPLab/emnlp2018-novel-metaphors",
    "text": "Using crowdsourcing (Amazon Mechanical Turk), we first conduct a pilot study to choose among four different annotations methods. We then employ the best method to collect annotations and create an additional novelty score between 1 (novel) and \u22121 (conventionalized) for each token labeled as metaphor in the VUAMC. Note that non-content words like prepositions and auxiliary verbs (have, be, do) are filtered out beforehand. Our annotations/scores can be integrated into the original VUAMC resource. We make the annota-tions and scripts to embed them into the original corpus publicly available. [Cite_Footnote_2]"
  },
  {
    "id": 1324,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://saifmohammad.com/WebPages/BestWorst.html",
    "section_title": "3 Corpus 3.2 Corpus Creation",
    "add_info": "3 http://saifmohammad.com/WebPages/BestWorst.html",
    "text": "After filtering out prepositions and auxiliary verbs (have, be, do) using the POS tags supplied by the VUAMC, we collect annotations covering 15,180 metaphors in total (Table 1). We only in-clude workers located in the US. For creation of the best\u2013worst scaling tuples, and for aggregation of the annotations, we use the scripts provided by Kiritchenko and Mohammad (2016). [Cite_Footnote_3] We use a best-worst scaling factor of 1.5 and four items per tuple. Thus, each metaphor appears in six differ-ent best-worst scaling comparisons. This results in 22,770 best\u2013worst scaling items to be annotated."
  },
  {
    "id": 1325,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://www.macmillandictionary.com/dictionary/american/go-through#go-through",
    "section_title": "3 Corpus 3.3 Analysis",
    "add_info": "4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60",
    "text": "Before we conduct a more in-depth analysis of the annotated metaphors, we show some exam-ples. In Table 3, we list four novel and four con-ventionalized metaphors (as annotated). A good example for a novel metaphor is the description of \u201cwords [...] as a coat-hanger\u201d in Table 3 (3). This usage cannot be found in dictionaries, and clearly constitutes creative language use. In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [Cite_Footnote_4]"
  },
  {
    "id": 1326,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://www.macmillandictionary.com/dictionary/american/",
    "section_title": "3 Corpus 3.3 Analysis",
    "add_info": "4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60",
    "text": "Before we conduct a more in-depth analysis of the annotated metaphors, we show some exam-ples. In Table 3, we list four novel and four con-ventionalized metaphors (as annotated). A good example for a novel metaphor is the description of \u201cwords [...] as a coat-hanger\u201d in Table 3 (3). This usage cannot be found in dictionaries, and clearly constitutes creative language use. In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [Cite_Footnote_4]"
  },
  {
    "id": 1327,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/spotify/annoy",
    "section_title": "3 Corpus an indication of novelty of metaphoric use.",
    "add_info": "5 https://github.com/spotify/annoy",
    "text": "To analyze the relation between novelty and concreteness, we first extend the concreteness list by Brysbaert et al. (2014) using a technique sim-ilar to Mohler et al. (2014). For a given token t, we extract 20 approximate nearest neighbors nn(t) from Google News Embeddings (Mikolov et al., 2013) using Annoy. [Cite_Footnote_5] The concreteness value for t is then computed by averaging its neighbors\u2019 con-creteness values from the concreteness list."
  },
  {
    "id": 1328,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://github.com/google-research/electra",
    "section_title": "3 Experiments 3.3 Computational Efficiency",
    "add_info": "2 https://github.com/google-research/electra",
    "text": "Wall-clock time. We compare the number of train-ing steps per second. For direct comparison, we modify the ELECTRA reference code [Cite_Footnote_2] . For TPU v3 with 8 TPU cores, ELECTRA and SCRIPT achieve 31.3 and 22.7 training iterations per sec-ond with a mean MXU utilization of 14.93% and 17.91% for small models, respectively."
  },
  {
    "id": 1329,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://Skylion007.github.io/OpenWebTextCorpus",
    "section_title": "3 Experiments",
    "add_info": "Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.",
    "text": "Hence, we train and evaluate two SCRIPT mod-els \u201csmall\u201d and \u201cbase\u201d with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019)  with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details."
  },
  {
    "id": 1330,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://Skylion007.github.io/OpenWebTextCorpus",
    "section_title": "3 Experiments 3.2 Efficient Pseudo-Log-Likelihood Scoring",
    "add_info": "Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.",
    "text": "Table 2: Comparison of small and base models on the GLUE dev set. The models were trained on the OpenWebText corpus (Gokaslan and Cohen, 2019)  for 1, 000, 000 and 766, 000 steps, respectively. The GLUE task scores are means of 8 runs over a set of random seeds. SCRIPT outperforms ELECTRA while enjoying a simple architecture and learning algorithm."
  },
  {
    "id": 1331,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/wxjiao/UncSamp",
    "section_title": "References",
    "add_info": "1 The source code is available at https://github.com/wxjiao/UncSamp",
    "text": "Self-training has proven effective for improv-ing NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we em-pirically show is sub-optimal. In this work, we propose to improve the sampling proce-dure by selecting the most informative mono-lingual sentences to complement the paral-lel data. To this end, we compute the un-certainty of monolingual sentences using the bilingual dictionary extracted from the paral-lel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not pro-vide additional gains. Accordingly, we de-sign an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English\u21d2German and English\u21d2Chinese datasets demonstrate the ef-fectiveness of the proposed approach. Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [Cite_Footnote_1]"
  },
  {
    "id": 1332,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/neulab/compare-mt",
    "section_title": "2 Observing Monolingual Uncertainty 2.2 Experimental Setup",
    "add_info": "3 https://github.com/neulab/compare-mt",
    "text": "Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by Sacre-BLEU (Post, 2018) . For the En\u21d2Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improve-ment with paired bootstrap resampling (Koehn, 2004) using compare-mt [Cite_Footnote_3] (Neubig et al., 2019)."
  },
  {
    "id": 1333,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/clab/fast_align",
    "section_title": "2 Observing Monolingual Uncertainty 2.3 Effect of Uncertain Data",
    "add_info": "5 https://github.com/clab/fast_align",
    "text": "Self-training v.s. Uncertainty. In this experi-ment, we first adopted fast-align 5 to establish word alignments between source and target words in the authentic parallel corpus and used the alignments to build the bilingual dictionary A b . Then we used the bilingual dictionary to compute the data uncer-tainty expressed in Equation (3) for the sentences in the monolingual data set. After that, we ranked all the 40M monolingual sentences and grouped them 38 into [Cite_Footnote_5] equally-sized bins (i.e., 8M sentences per bin) according to their uncertainty scores. At last, we performed self-training with each bin of monolingual 36 data."
  },
  {
    "id": 1334,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://transmart.qq.com/index",
    "section_title": "References",
    "add_info": "6 https://transmart.qq.com/index",
    "text": "In this work, we demonstrate the necessity of distin-guishing monolingual sentences for self-training in NMT, and propose an uncertainty-based sampling strategy to sample monolingual data. By sampling monolingual data with relatively high uncertainty, our method outperforms random sampling signifi-cantly on the large-scale WMT English\u21d2German and English\u21d2Chinese datasets. Further analyses demonstrate that our uncertainty-based sampling approach does improve the translation quality of high uncertainty sentences and also benefits the prediction of low-frequency words at the target side. The proposed technology has been applied to TranSmart [Cite_Footnote_6] (Huang et al., 2021), an interactive ma-chine translation system in Tencent, to improve the performance of its core translation engine. Future work includes the investigation on the confirmation bias issue of self-training and the effect of decoding strategies on self-training sampling. Grants Council of the Hong Kong Special Admin-istrative Region, China (CUHK 2410021, Research Impact Fund (RIF), R5034-18; CUHK 14210717, General Research Fund), and Tencent AI Lab Rhi-noBird Focused Research Program (GF202036). We sincerely thank the anonymous reviewers for their insightful suggestions on various aspects of this work."
  },
  {
    "id": 1335,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/clab/fast_align",
    "section_title": "A Appendix A.2 Linguistic Properties",
    "add_info": "8 https://github.com/clab/fast_align",
    "text": "Coverage. Coverage measures the ratio of source words being aligned by any target words (Tu et al., 2016). Firstly, we trained an alignment model on the authentic parallel data by fast-align [Cite_Footnote_8] . Then we used the alignment model to force-align the mono-lingual sentences and the synthetic target sentences. Next, we calculated the coverage of each source sentence, and report the averaged coverage of each data bin. The lower coverage of monolingual sen-tences in bin 5 indicates that they are not aligned as well as the other bins."
  },
  {
    "id": 1336,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://kheafield.com/code/kenlm/",
    "section_title": "A Appendix A.3 Comparison with Related Work",
    "add_info": "9 https://kheafield.com/code/kenlm/",
    "text": "For DWF, we ranked the monolingual data by word rarity (Platanios et al., 2019) of sentences and also selected the top 80M monolingual data for self-training. For S RC LM, we trained an N -gram language model (Heafield, 2011) [Cite_Footnote_9] on the source sentences in the bitext and measured the distance between each monolingual sentence to the bitext source sentences by cross-entropy. Similarly, we se-lected 8M monolingual data with the lowest cross-entropy for self-training."
  },
  {
    "id": 1337,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://paraphrase.org",
    "section_title": "1 Introduction",
    "add_info": "2 PPDB is a large paraphrase database derived from static bilingual translation data available at: http://paraphrase.org",
    "text": "Most current approaches to lexical simplifica-tion heavily rely on corpus statistics and surface level features, such as word length and corpus-based word frequencies (read more in \u00a75). Two of the most commonly used assumptions are that simple words are associated with shorter lengths and higher frequencies in a corpus. However, these assumptions are not always accurate and are often the major source of errors in the simplifi-cation pipeline (Shardlow, 2014). For instance, the word foolishness is simpler than its meaning-preserving substitution folly even though foolish-ness is longer and less frequent in the Google 1T Ngram corpus (Brants and Franz, 2006). In fact, we found that 21% of the 2272 meaning-equivalent word pairs randomly sampled from PPDB [Cite_Footnote_2] (Ganitkevitch et al., 2013) had the simpler word longer than the complex word, while 14% had the simpler word less frequent."
  },
  {
    "id": 1338,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/mounicam/lexical_simplification",
    "section_title": "2 Constructing A Word-Complexity Lexicon with Human Judgments",
    "add_info": "3 Download at https://github.com/mounicam/lexical_simplification",
    "text": "We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators. [Cite_Footnote_3] Despite the actual larger En-glish vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus was effective for simplifica-tion purposes (see experiments in \u00a74) as our neural ranking model (\u00a73) can estimate the complexity of any word or phrase even out-of-vocabulary."
  },
  {
    "id": 1339,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/ldc2006t13",
    "section_title": "2 Constructing A Word-Complexity Lexicon with Human Judgments",
    "add_info": "4 https://catalog.ldc.upenn.edu/ldc2006t13",
    "text": "We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators. Despite the actual larger En-glish vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus [Cite_Footnote_4] was effective for simplifica-tion purposes (see experiments in \u00a74) as our neural ranking model (\u00a73) can estimate the complexity of any word or phrase even out-of-vocabulary."
  },
  {
    "id": 1340,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "3 Neural Readability Ranking Model for Words and Phrases 3.2 Features",
    "add_info": "7 https://code.google.com/archive/p/word2vec/ ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional rep-resentations by applying Gaussian radial basis functions.",
    "text": "For an input pair of words/phrases hw a ,w b i, we include individual features f(w 1 ), f(w 2 ) and the differences f(w a )\u2212f(w b ). We also use pair-wise features f(hw a , w b i) including cosine simi-larity cos(\u2192\u2212w a , \u2192\u2212w b ) and the difference \u2212\u2192w a \u2212 \u2192\u2212w b between the word2vec (Mikolov et al., 2013) em-bedding of the input words. The embeddings for a mutli-word phrase are obtained by averaging the embeddings of all the words in the phrase. We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package. [Cite_Footnote_7]"
  },
  {
    "id": 1341,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/tmt/",
    "section_title": "4 Experimental Settings",
    "add_info": "3 http://nlp.stanford.edu/software/tmt/.",
    "text": "Implementations. To implement our models, we rely on MALLET (McCallum, 2002) for C-LDA and the Stanford Topic Modeling Toolbox [Cite_Footnote_3] for L-LDA. In both cases, we run 1000 iterations of Gibbs sam-pling, using default values for all hyperparameters."
  },
  {
    "id": 1342,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://wordnetcode.princeton.edu/standoff-files/core-wordnet.txt",
    "section_title": "4 Experimental Settings",
    "add_info": "4 A subset of WordNet restricted to the 5000 most fre-quently used word senses. Available from: http://wordnetcode.princeton.edu/standoff-files/core-wordnet.txt",
    "text": "This method yields 7901 labeled adjective-noun phrases. They are divided into development and test data according to a sampling procedure that respects the following criteria: (i) Both sets must contain all attributes with an equal number of phrases for each attribute; (ii) phrases with both elements con-tained in CoreWordNet [Cite_Footnote_4] are preferred, while others are only considered if necessary to satisfy the first criterion. This procedure yields 496/345 phrases in the development/test set, distributed over 206 at-tributes . Training data. The pseudo-documents are collec-ted from dependency paths obtained from section 2 of the parsed pukWaC corpus (Baroni et al., 2009)."
  },
  {
    "id": 1343,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "4 Experimental Settings",
    "add_info": "Andrew Kachites McCallum. 2002. MAL-LET: A machine learning for language toolkit. http://mallet.cs.umass.edu.",
    "text": "Implementations. To implement our models, we rely on MALLET (McCallum, 2002)  for C-LDA and the Stanford Topic Modeling Toolbox 3 for L-LDA. In both cases, we run 1000 iterations of Gibbs sam-pling, using default values for all hyperparameters."
  },
  {
    "id": 1344,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.nltk.org/_modules/nltk/tokenize/casual.html",
    "section_title": "2 Profiling with Abstract Features",
    "add_info": "1 Using the NLTK tokenizer http://www.nltk.org/_modules/nltk/tokenize/casual.html",
    "text": "\u2022 PunctA Generalization of PunctC (A for ag-gressive), converting different types of punc-tuation to classes: emoticons [Cite_Footnote_1] to \u2018E\u2019 and emo-jis 2 to \u2018J\u2019, other punctuation to \u2018P\u2019."
  },
  {
    "id": 1345,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://pypi.python.org/pypi/emoji/",
    "section_title": "2 Profiling with Abstract Features",
    "add_info": "2 https://pypi.python.org/pypi/emoji/",
    "text": "\u2022 Shape Transforms uppercase characters to \u2018U\u2019, lowercase characters to \u2018L\u2019, digits to \u2018D\u2019 and all other characters to \u2018X\u2019. Repetitions of transformed characters are condensed to a maximum of [Cite_Footnote_2] for greater generalization."
  },
  {
    "id": 1346,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/bplank/bleaching-text",
    "section_title": "3 Experiments 3.1 Lexical vs Bleached Models",
    "add_info": null,
    "text": "For the multilingual embeddings model we use the mean embedding representation from the sys-tem of (Plank, 2017) and add max, std and cover-age features. We create multilingual embeddings by projecting monolingual embeddings to a single multilingual space for all five languages using a recently proposed SVD-based projection method with a pseudo-dictionary (Smith et al., 2017). The monolingual embeddings are trained on large amounts of in-house Twitter data (as much data as we had access to, i.e., ranging from 30M tweets for French to 1,500M tweets in Dutch, with a word type coverage between 63 and 77%). This results in an embedding space with a vocabulary size of 16M word types. All code is available at  https://github.com/bplank/bleaching-text."
  },
  {
    "id": 1347,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.hlt.utdallas.edu/\u223cady",
    "section_title": "4 Experiments",
    "add_info": "6 ECB is available at http://www.hlt.utdallas.edu/\u223cady.",
    "text": "Datasets One dataset we employed is the au-tomatic content extraction (ACE) (ACE-Event, 2005). However, the utilization of the ACE corpus for the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using a restricted set of event types such as LIFE , BUSI - NESS , CONFLICT , and JUSTICE . Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus [Cite_Footnote_6] to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution. One important step in the cre-ation process of this corpus consists in finding sets of related documents that describe the same semi-nal event such that the annotation of coreferential event mentions across documents is possible. For this purpose, we selected from the GoogleNews archive various topics whose description contains keywords such as commercial transaction, attack, death, sports, terrorist act, election, arrest, natu-ral disaster, etc. The entire annotation process for creating the ECB resource is described in (Bejan and Harabagiu, 2008). Table 1 lists several basic statistics extracted from these two corpora."
  },
  {
    "id": 1348,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://news.google.com/",
    "section_title": "4 Experiments",
    "add_info": "7 http://news.google.com/",
    "text": "Datasets One dataset we employed is the au-tomatic content extraction (ACE) (ACE-Event, 2005). However, the utilization of the ACE corpus for the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using a restricted set of event types such as LIFE , BUSI - NESS , CONFLICT , and JUSTICE . Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution. One important step in the cre-ation process of this corpus consists in finding sets of related documents that describe the same semi-nal event such that the annotation of coreferential event mentions across documents is possible. For this purpose, we selected from the GoogleNews archive [Cite_Footnote_7] various topics whose description contains keywords such as commercial transaction, attack, death, sports, terrorist act, election, arrest, natu-ral disaster, etc. The entire annotation process for creating the ECB resource is described in (Bejan and Harabagiu, 2008). Table 1 lists several basic statistics extracted from these two corpora."
  },
  {
    "id": 1349,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://crfpp.sourceforge.net/",
    "section_title": "5 Experiments 5.2 Experimental setup",
    "add_info": null,
    "text": "The CRF model we compare with was trained with the CRF++ tool, available at  http://crfpp.sourceforge.net/. The model is equiva-lent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic cat-egories in a window of [-2, +2] with respect to the current token, plus bigrams of concept tags (see (Hahn et al., 2008) and the CRF++ web site for more details)."
  },
  {
    "id": 1350,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/31248",
    "section_title": "4 Empirical Evaluation 4.1 Experimental Setup",
    "add_info": "1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248",
    "text": "We have obtained the digital publications of 9474 Journal of Comparative Neurology (JCN) [Cite_Footnote_1] articles from 1982 to 2005. We have converted the PDF format into plain text, maintaining paragraph breaks (some errors still occur though). A simple heuristic based approach identifies semantic sec-tions of the paper (e.g, Introduction, Results, Dis-cussion). As most experimental descriptions appear in the Results section, we only process the Results section. A neuroscience expert manually annotated the data records in the Results section of 58 re-search articles. The total number of sentences in the Results section of the 58 files is 6630 (averag-ing 114.3 sentences per article)."
  },
  {
    "id": 1351,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "4 Empirical Evaluation 4.1 Experimental Setup",
    "add_info": "McCallum, A.K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.",
    "text": "We used the machine learning package MALLET (McCallum, 2002)  to conduct the CRF model training and labeling."
  },
  {
    "id": 1352,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://medialab.di.unipi.it/Project/Parser/DgAnnotator/",
    "section_title": "10 Adaptation Track",
    "add_info": "DgAnnotator. 2006. http://medialab.di.unipi.it/Project/Parser/DgAnnotator/.",
    "text": "By visual inspection using DgAnnotator (DgAnnotator, 2006)  , the parses looked generally correct. Most of the errors seemed due to improper handling of conjunctions and disjunctions. The collection in fact contains several phrases like:"
  },
  {
    "id": 1353,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://sourceforge.net/projects/mstparser/",
    "section_title": "8 Performance",
    "add_info": "Mstparser 0.4.3. 2007. http://sourceforge.net/projects/mstparser/",
    "text": "The experiments were performed on a 2.4 Ghz AMD Opteron machine with 32 GB RAM. Train-ing the parser using the 2 nd -order perceptron on the English corpus required less than 3 GB of memory and about one hour for each iteration over the whole dataset. Parsing the English test set required 39.97 sec. For comparison, we tested the MST parser version 0.4.3 (Mstparser, 2007)  , configured for second-order, on the same data: training took 73.9 minutes to perform 10 iterations and parsing took 97.5 sec. MST parser achieved:"
  },
  {
    "id": 1354,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.clsp.jhu.edu/PIRE/ssr",
    "section_title": "2 Approach 2.1 Data",
    "add_info": "1 The Spontaneous Speech Reconstruction corpus can be downloaded from http://www.clsp.jhu.edu/PIRE/ssr.",
    "text": "We conducted our experiments on the recently re-leased Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a medium-sized set of disfluency annotations atop Fisher conversational telephone speech data (Cieri et al., 2004) [Cite_Footnote_1] . Advantages of the SSR data include"
  },
  {
    "id": 1355,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.uco.es/kdis/mllresources/",
    "section_title": "5:361\u2013397. Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong",
    "add_info": "2 http://www.uco.es/kdis/mllresources/",
    "text": "Datasets: Figure 3 and 4 shows the distribution of the instance numbers of the categories in the datasets RCV1 and Delicious [Cite_Footnote_2] . In both datasets, the instance numbers of the categories have long-tail distribution."
  },
  {
    "id": 1356,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/",
    "section_title": "3 Corpus Annotation",
    "add_info": "1 Data available at http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/",
    "text": "The corpus of stories for children was drawn from the fables collection of (McIntyre and Lapata, 2009) [Cite_Footnote_1] and annotated as described in (Bethard et al., 2012). In this section we illustrate the main annotation princi-ples for coherent temporal annotation. As an example story, consider:"
  },
  {
    "id": 1357,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "http://www.bethard.info/data/fables-100-temporal-dependency.xml",
    "section_title": "3 Corpus Annotation",
    "add_info": "2 Available from http://www.bethard.info/data/fables-100-temporal-dependency.xml",
    "text": "Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyre-Lapata collection and measured inter-annotator agree-ment by Krippendorff\u2019s Alpha for nominal data (Krip-pendorff, 2004; Hayes and Krippendorff, 2007). For the resulting annotated corpus annotators achieved Alpha of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering rela-tion labels. Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [Cite_Footnote_2] could be used to train a temporal dependency parsing model."
  },
  {
    "id": 1358,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/lijuncen/Sentiment-and-Style-Transfer",
    "section_title": "3 Experiments 3.1 Datasets",
    "add_info": "2 https://github.com/lijuncen/Sentiment-and-Style-Transfer",
    "text": "We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [Cite_Footnote_2] and the IMDb movie review dataset provided by Dai et al. (2019)."
  },
  {
    "id": 1359,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "https://github.com/raosudha89/GYAFC-corpus",
    "section_title": "3 Experiments 3.1 Datasets",
    "add_info": "4 https://github.com/raosudha89/GYAFC-corpus",
    "text": "We also evaluate our methods on a formality style transfer dataset, Grammarly\u2019s Yahoo Answers Formality Corpus (GYAFC), [Cite_Footnote_4] introduced in Rao and Tetreault (2018). Although it is a parallel cor-pus, we treat it as an unaligned corpus in our ex-periments. In order to compare to previous work, we chose the Family & Relationships category for our experiments. Datasets statistics are shown in Table 1."
  },
  {
    "id": 1360,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://fasttext.cc/",
    "section_title": "3 Experiments 3.2 Experimental Details",
    "add_info": "5 https://fasttext.cc/",
    "text": "Following previous work, we measure the style transfer accuracy using a FastText [Cite_Footnote_5] (Joulin et al., 2017) style classifier trained on the respective train-ing set of each dataset. To measure content preser-vation, we use SIM and BLEU as metrics where self-SIM and self-BLEU are computed between the source sentences and system outputs, while ref-SIM and ref-BLEU are computed between the system outputs and human references when avail-able. To measure the fluency we use a pre-trained GPT-2 model to compute the perplexity. Our gen-erator, GPT-2, has 1.5 billion parameters, and we train on a GTX 1080 Ti GPU for about 12 hours."
  },
  {
    "id": 1361,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.statmt.org/wmt11/",
    "section_title": "5 Experiments 5.2 P@k Word Translation",
    "add_info": "2 http://www.statmt.org/wmt11/",
    "text": "Next we evaluated our method on a word transla-tion task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were ex-tracted from the publicly available WMT11 [Cite_Footnote_2] cor-pus. The experiments were done for two sets of translation: English to Spanish and Spanish to En-glish. (Mikolov et al., 2013b) extracted the top 6K most frequent words and translated them with Google Translate. They used the top 5K pairs to train a translation matrix, and evaluated their method on the remaining 1K. As our English and Spanish vectors are already aligned we don\u2019t need the 5K training pairs and use only the 1K test pairs."
  },
  {
    "id": 1362,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://www.cs.bgu.ac.il/\u02dcyoavg/software/projectivize",
    "section_title": "4 Experiments",
    "add_info": "4 https://www.cs.bgu.ac.il/\u02dcyoavg/software/projectivize",
    "text": "The benefits of non-projective examples for train-ing projective parsers are evaluated on the 73 tree-banks of the UD 2.0 (Nivre et al., 2017b,a). Three methods to exploit non-projective trees (instead of discarding them) are contrasted: learning on the trees projectivized using Eisner (1996)\u2019s algo-rithm, learning on pseudo-projectivized examples (Nivre and Nilsson, 2005) and learning on the non-projective trees, with the minimum-cost oracle de-scribed in \u00a73. Projectivization is based on Yoav Goldberg\u2019s code. [Cite_Footnote_4] For pseudo-projectivization, the M ALT P ARSER 1.9 implementation is used, with the head encoding scheme. For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle)."
  },
  {
    "id": 1363,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://perso.limsi.fr/aufrant",
    "section_title": "4 Experiments",
    "add_info": "5 https://perso.limsi.fr/aufrant",
    "text": "The benefits of non-projective examples for train-ing projective parsers are evaluated on the 73 tree-banks of the UD 2.0 (Nivre et al., 2017b,a). Three methods to exploit non-projective trees (instead of discarding them) are contrasted: learning on the trees projectivized using Eisner (1996)\u2019s algo-rithm, learning on pseudo-projectivized examples (Nivre and Nilsson, 2005) and learning on the non-projective trees, with the minimum-cost oracle de-scribed in \u00a73. Projectivization is based on Yoav Goldberg\u2019s code. For pseudo-projectivization, the M ALT P ARSER 1.9 implementation is used, with the head encoding scheme. For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source [Cite_Footnote_5] implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle)."
  },
  {
    "id": 1364,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/bionlplab/heart_failure_mortality",
    "section_title": "References",
    "add_info": null,
    "text": "Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Fur-thermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict pa-tient survival outcomes. We show that hid-den layers yield notably more accurate pre-dictions than predefined features, outperform-ing the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at  https://github.com/bionlplab/heart_failure_mortality."
  },
  {
    "id": 1365,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/sebp/scikit-survival",
    "section_title": "4 Experiments 4.2 Metrics",
    "add_info": "1 https://github.com/sebp/scikit-survival",
    "text": "Both C-index and AUC assign a random model 0.5 and a perfect model 1. We measure all-time C-index, C-index at 30 days (C-index@30), and AUC at 30 days and 365 days (AUC@30 and AUC@365) to show the models\u2019 performances dealing with different time-to-events [Cite_Footnote_1] . Model C-index C-index@30 AUC@30 AUC@365 CPH + Feature Set 1 0.499 \u00b1 0.025 0.504 \u00b1 0.032 0.503 \u00b1 0.037 CPH + Feature Set 2 0.621 \u00b1 0.014 0.632 \u00b1 0.033 0.642 \u00b1 0.030 CPH + Hidden Features 0.674 \u00b1 0.023 0.696 \u00b1 0.022 0.710 \u00b1 0.022 MLP + Feature Set 1 0.502 \u00b1 0.023 0.509 \u00b1 0.026 0.509 \u00b1 0.030 MLP + Feature Set 2 0.658 \u00b1 0.010 0.671 \u00b1 0.025 0.685 \u00b1 0.023 MLP + Hidden Features 0.704 \u00b1 0.017 0.726 \u00b1 0.020 0.744 \u00b1 0.019 LSTM + Sequential HF 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 0.501 \u00b1 0.028 0.642 \u00b1 0.030 0.697 \u00b1 0.026 0.501 \u00b1 0.032 0.683 \u00b1 0.008 0.734 \u00b1 0.018 0.742 \u00b1 0.023 Model C-index C-index@30 AUC@30 BERT-Base (Devlin et al., 2019) 0.603 \u00b1 0.115 0.611 \u00b1 0.123 0.618 \u00b1 0.134 BioBert (Lee et al., 2020) 0.701 \u00b1 0.021 0.714 \u00b1 0.027 0.734 \u00b1 0.029 ClinicalBert (Alsentzer et al., 2019) 0.692 \u00b1 0.019 0.705 \u00b1 0.023 0.723 \u00b1 0.025 BlueBert (Peng et al., 2019) 0.713 \u00b1 0.019 0.735 \u00b1 0.024 0.755 \u00b1 0.024 CheXbert (Smit et al., 2020) 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 AUC@365 0.620 \u00b1 0.136 0.739 \u00b1 0.028 0.727 \u00b1 0.029 0.756 \u00b1 0.021 0.742 \u00b1 0.023"
  },
  {
    "id": 1366,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/havakv/pycox",
    "section_title": "4 Experiments 4.3 Training",
    "add_info": "2 https://github.com/havakv/pycox",
    "text": "We perform a grid search to find the optimal hyper-parameters based on the metrics and use them for all configurations. The learning rate is set to 0.0001 with an Adam optimizer. We iterate the training process for 100 epochs with batch size 256 and early stop if the validation loss does not decrease. The dropout rate is 0.6. We perform five-fold cross-validation to produce 95% confidence intervals for each metric. The training, validation and test splits are 70%, 10%, 20%, respectively. We use pycox and PyTorch to implement the framework [Cite_Footnote_2] . The end-to-end training takes about 30 minutes with NVIDIA Tesla P100 16 GB GPU, mainly due to feature extraction."
  },
  {
    "id": 1367,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://qwone.com/~jason/20Newsgroups/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "2 http://qwone.com/~jason/20Newsgroups/",
    "text": "Topic categorization. From the 20 Newsgroups [Cite_Footnote_2] dataset, we examine four binary classification tasks. We end up with binary classification problems, where we classify a document according to two related categories: comp.sys: ibm.pc.hardware vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space and alt.atheism vs. soc.religion.christian. We use the 20NG dataset from Python."
  },
  {
    "id": 1368,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.cornell.edu/~ainur/data.html",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "3 http://www.cs.cornell.edu/~ainur/data.html",
    "text": "Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) [Cite_Footnote_3] , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) ."
  },
  {
    "id": 1369,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.jhu.edu/~mdredze/datasets/sentiment/",
    "section_title": "4 Experiments 4.1 Datasets",
    "add_info": "4 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/",
    "text": "Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) [Cite_Footnote_4] ."
  },
  {
    "id": 1370,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics",
    "section_title": "References",
    "add_info": "1 The code is available at https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics",
    "text": "Recent prompt-based approaches allow pre-trained language models to achieve strong per-formances on few-shot finetuning by reformu-lating downstream tasks as a language mod-eling problem. In this work, we demon-strate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., mod-els incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is sig-nificantly less present in the zero-shot evalu-ation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that pre-serves pretraining weights is effective in mit-igating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics. [Cite_Footnote_1]"
  },
  {
    "id": 1371,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://github.com/mozillazg/python-pinyin",
    "section_title": "4 Experimental Setup 4.3 Evaluation Metrics",
    "add_info": "4 http://github.com/mozillazg/python-pinyin",
    "text": "Rhyme For SongCi, usually, there is only one group of rhyming words in one sample. As the example shown in Table 1, the pronunciation of the red rhyming words are \u201czhu\u201d, \u201cyu\u0308\u201d, \u201cdu\u201d, and \u201cgu\u201d respectively, and the rhyming phoneme is \u201cu\u201d. For the generated samples, we first use the tool pinyin [Cite_Footnote_4] to get the pronunciations (PinYin) of the words in the rhyming positions, and then conduct the evaluation. For Shakespeare\u2019s Sonnets corpus, the rhyming rule is clear \u201cABAB CDCD EFEF GG\u201d and there are 7 groups of rhyming tokens. For the generated samples, we employ the CMU Pronounc-ing Dictionary (Speech@CMU, 1998) to obtain the phonemes of the words in the rhyming posi-tions. For example, the phonemes for word \u201casleep\u201d and \u201csteep\u201d are [\u2019AH0\u2019, \u2019S\u2019, \u2019L\u2019, \u2019IY1\u2019, \u2019P\u2019] and [\u2019S\u2019, \u2019T\u2019, \u2019IY1\u2019, \u2019P\u2019] respectively. And then we can conduct the evaluation by counting the overlapping units from both the original words and the extracted phonemes group by group. We report the Macro-F1 and Micro-F1 numbers in the results tables as well. Integrity Since the format in our task is strict and rigid, thus the number of words to be predicted is also pre-defined. Our model must organize the language using the limited positions, thus sentence integrity may become a serious issue. For exam-ple, the integrity of \u201clove is not love . h/si\u201d is much better than\u201clove is not the . h/si\u201d. To con-duct the evaluation of sentence integrity, we design a straightforward method by calculating the pre-diction probability of the punctuation characters before h/si given the prefix tokens:"
  },
  {
    "id": 1372,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.speech.cs.cmu.edu/cgi-bin/cmudict",
    "section_title": "4 Experimental Setup 4.3 Evaluation Metrics",
    "add_info": "5 http://www.speech.cs.cmu.edu/cgi-bin/cmudict",
    "text": "Rhyme For SongCi, usually, there is only one group of rhyming words in one sample. As the example shown in Table 1, the pronunciation of the red rhyming words are \u201czhu\u201d, \u201cyu\u0308\u201d, \u201cdu\u201d, and \u201cgu\u201d respectively, and the rhyming phoneme is \u201cu\u201d. For the generated samples, we first use the tool pinyin to get the pronunciations (PinYin) of the words in the rhyming positions, and then conduct the evaluation. For Shakespeare\u2019s Sonnets corpus, the rhyming rule is clear \u201cABAB CDCD EFEF GG\u201d and there are 7 groups of rhyming tokens. For the generated samples, we employ the CMU Pronounc-ing Dictionary [Cite_Footnote_5] (Speech@CMU, 1998) to obtain the phonemes of the words in the rhyming posi-tions. For example, the phonemes for word \u201casleep\u201d and \u201csteep\u201d are [\u2019AH0\u2019, \u2019S\u2019, \u2019L\u2019, \u2019IY1\u2019, \u2019P\u2019] and [\u2019S\u2019, \u2019T\u2019, \u2019IY1\u2019, \u2019P\u2019] respectively. And then we can conduct the evaluation by counting the overlapping units from both the original words and the extracted phonemes group by group. We report the Macro-F1 and Micro-F1 numbers in the results tables as well. Integrity Since the format in our task is strict and rigid, thus the number of words to be predicted is also pre-defined. Our model must organize the language using the limited positions, thus sentence integrity may become a serious issue. For exam-ple, the integrity of \u201clove is not love . h/si\u201d is much better than\u201clove is not the . h/si\u201d. To con-duct the evaluation of sentence integrity, we design a straightforward method by calculating the pre-diction probability of the punctuation characters before h/si given the prefix tokens:"
  },
  {
    "id": 1373,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.speech.cs.cmu.edu/cgi-bin/cmudict",
    "section_title": "3 Automatic Humor Recognition 3.1 Humor-Specific Stylistic Features",
    "add_info": "2 Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict",
    "text": "To extract this feature, we identify and count the number of alliteration/rhyme chains in each exam-ple in our data set. The chains are automatically ex-tracted using an index created on top of the CMU pronunciation dictionary [Cite_Footnote_2] ."
  },
  {
    "id": 1374,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://wndomains.itc.it",
    "section_title": "3 Automatic Humor Recognition 3.1 Humor-Specific Stylistic Features",
    "add_info": "3 W ORD N ET D OMAINS assigns each synset in W ORD N ET with one or more \u201cdomain\u201d labels, such as S PORT , M EDICINE , E CONOMY . See http://wndomains.itc.it.",
    "text": "To form a lexicon required for the identification of this feature, we extract from W ORD N ET D OMAINS [Cite_Footnote_3] all the synsets labeled with the domain S EXUALITY . The list is further processed by removing all words with high polysemy (\u2265 4). Next, we check for the presence of the words in this lexicon in each sen-tence in the corpus, and annotate them accordingly. Note that, as in the case of antonymy, W ORD N ET coverage is not complete, and the adult slang fea-ture cannot always be identified. antonymy, adult slang) are present in the same sen-tence, as for instance the following one-liner:"
  },
  {
    "id": 1375,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://pushshift.io/",
    "section_title": "A.4 Reddit Dataset",
    "add_info": null,
    "text": "We use a subset of Reddit comments from 2006- 2018 scraped from  https://pushshift.io/. We con-struct a dictionary containing the 10,000 most popular words and preprocess the dataset by re-moving deleted posts, out-of-vocabulary tokens, profanity, comments with less than 10 upvotes, and comments with over 400 tokens."
  },
  {
    "id": 1376,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/raylin1000/dropbert",
    "section_title": "3 Experiments",
    "add_info": "3 https://github.com/raylin1000/dropbert",
    "text": "Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koc\u030cisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 2.0 (Rajpurkar et al., 2018). We use the NABERT [Cite_Footnote_3] (Numerically Augmented BERT) model with an additional reasoning type to allow \u201cNo Answer\u201d as an answer to accommodate the SQuAD 2.0 dataset which has about 40,000 \u201cNo Answer\u201d questions. Each training session lasted 30 epochs with 50,000 instances sampled per epoch. Three training ses-sions were conducted per sampling method and the EM and F1 scores shown are averaged over those three sessions. Note that NarrativeQA is evaluated using only ROUGE F1 score. Due to GPU mem-ory constraints, we are limited to a batch size of 4, so we are unable replicate the Uniform Batches configuration of MRQA (requires a batch size of 8 to fit 1 instance from each of the 8 datasets)."
  },
  {
    "id": 1377,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://leaderboard.allenai.org/orb/submissions/public",
    "section_title": "3 Experiments",
    "add_info": "4 https://leaderboard.allenai.org/orb/submissions/public Language Processing (EMNLP).",
    "text": "ORB Evaluation Finally, Table 4 shows that our model trained with dynamic sampling and het-erogeneous batches significantly outperforms the previous ORB state-of-the-art NABERT baseline model (submitted on 11/12/2019 on the leader-board site [Cite_Footnote_4] )."
  },
  {
    "id": 1378,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://nlp.csai.tsinghua.edu.cn/\u02dcly/systems/TsinghuaAligner/TsinghuaAligner.html",
    "section_title": "5 Experiments 5.1 Setup",
    "add_info": "1 http://nlp.csai.tsinghua.edu.cn/\u02dcly/systems/TsinghuaAligner/TsinghuaAligner.html",
    "text": "The training corpus consists of 1.2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD cor-pus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. [Cite_Footnote_1] The evalu-ation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002)."
  },
  {
    "id": 1379,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Adaxry/ss_on_decoding_steps",
    "section_title": "1 Introduction",
    "add_info": "3 Codes are available at https://github.com/Adaxry/ss_on_decoding_steps.",
    "text": "The main contributions of this paper can be sum-marized as follows [Cite_Footnote_3] :"
  },
  {
    "id": 1380,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2010S01",
    "section_title": "4 Experiments 4.1 Experimental Setup",
    "add_info": "David Graff, Shudong Huang, Ingrid Carta-gena, Kevin Walker, , and Christopher Cieri. 2010. Fisher spanish speech (ldc2010s01). https://catalog.ldc.upenn.edu/LDC2010S01.",
    "text": "We used Fisher Spanish corpus (Graff et al., 2010)  to perform Spanish speech to English text transla-tion. And we followed previous works (Inaguma et al., 2019) for pre-processing steps, and 40/160 hours of train set, standard dev-test are used for the experiments. Byte-pair-encoding (BPE) (Kudo and Richardson, 2018) was applied to the target transcriptions to form 10K subwords as the tar-get of the translation part. Spanish word embed-dings were obtained from FastText pre-trained on Wikipedia (Bojanowski et al., 2016), and 8000 Spanish words were used in the recognition part."
  },
  {
    "id": 1381,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/clarkkev/deep-coref",
    "section_title": "2 Evaluation Setup",
    "add_info": "3 https://github.com/clarkkev/deep-coref",
    "text": "The first resolver, the Stanford neural resolver (Clark and Manning, 2016) [Cite_Footnote_3] , takes as input a set of entity mentions identified for a given document by a rule-based MD system and trains using reinforce-ment learning a simple mention ranker consisting of three hidden layers of ReLU units and a final layer that is fully-connected."
  },
  {
    "id": 1382,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/FilippoC/disc-span-parser-release",
    "section_title": "1 Introduction",
    "add_info": "4 https://github.com/FilippoC/disc-span-parser-release",
    "text": "We release the C++/Python implementation of the parser. [Cite_Footnote_4]"
  },
  {
    "id": 1383,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/andreasvc/disco-dop",
    "section_title": "4 Experiments 4.3 Evaluation",
    "add_info": "11 https://github.com/andreasvc/disco-dop",
    "text": "We evaluate our parser on the test sets of the three treebanks. We report F-measure and discontinuous F-measure as computed using the disco-dop tool [Cite_Footnote_11] with standard parameters in Table 2. ants of our parsers produced exactly the same re-sults in all settings. This may be expected as their cover of the original treebanks are almost similar. Second, surprisingly, the O(n 3 ) parser produced better results in terms of F-measure than other vari-ants in all cases. We report labeled discontinuous constituent recall and precision measures for the fully supervised model in Table 3. We observe that while the O(n 5 ) and O(n 6 ) have an better recall than the O(n 3 ) parser, their precision is drastically lower. This highlights a benefit of restricting the search space: the parser can retrieve less erroneous constituents leading to an improved overall preci-sion."
  },
  {
    "id": 1384,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://jgibblda.sourceforge.net/",
    "section_title": "3 Methodology 3.1 Encoding distributional semantics",
    "add_info": "1 http://jgibblda.sourceforge.net/",
    "text": "We employ the publicly available implementa-tion of LDA, JGibbLDA2 [Cite_Footnote_1] (Phan et al., 2008), which has two main execution methods: param-eter estimation (model building) and inference for new data (classification of a new document)."
  },
  {
    "id": 1385,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.sogou.com/labs/dl/c.html",
    "section_title": "5 Experiments 5.1 Evaluation as a ranking problem",
    "add_info": "2 http://www.sogou.com/labs/dl/c.html",
    "text": "Data preparation The data is composed of two parts: source documents and background knowl-edge. For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news [Cite_Footnote_2] including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Mil-itary. We then randomly but equally select 600 articles as the set of source documents from 9 top-ics without data bias. We use all the other 16,599 documents of the same corpus as the source of background knowledge, and then introduce a well-known Chinese open source tool (Che et al., 2010) to extract the triples of background knowledge from the raw text automatically. So the back-ground knowledge also distributes evenly across the same 9 topics. We use the same tool to extract the triples of source documents too."
  },
  {
    "id": 1386,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "5 Experiments 5.1 Evaluation as a ranking problem",
    "add_info": "3 https://code.google.com/p/word2vec/",
    "text": "Baseline systems As Zhang et al. (2014) argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval (Man-ning et al., 2008) and entity linking (Han et al., 2011; Sen, 2012), as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link struc-ture. For better comparison, we implement three methods as baselines, which have been proved ef-fective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with the publicly available tool word2vec [Cite_Footnote_3] ."
  },
  {
    "id": 1387,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Compare",
    "url": "http://ai.stanford.edu/",
    "section_title": "4 Extraction Methods Analysis 4.1 Judging Rule Correctness",
    "add_info": "3 http://ai.stanford.edu/ \u223c rion/swn/",
    "text": "The middle columns of Table 2 present, for each extraction method, the obtained percentage of cor-rect rules (precision) and their estimated absolute number. This number is estimated by multiplying the number of annotated correct rules for the ex-traction method by the sampling proportion. In to-tal, we estimate that our resource contains 5.6 mil-lion correct rules. For comparison, Snow\u2019s pub-lished extension to WordNet [Cite_Footnote_3] , which covers simi-lar types of terms but is restricted to synonyms and hyponyms, includes 400,000 relations."
  },
  {
    "id": 1388,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/OnlpLab/morphodetection",
    "section_title": "table, given a few forms of the same lexeme. 2",
    "add_info": "3 Our code is available on https://github.com/OnlpLab/morphodetection.",
    "text": "We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [Cite_Footnote_3]"
  },
  {
    "id": 1389,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "6 BERT-based Trees VS Parser-provided Trees",
    "add_info": "5 https://spacy.io/",
    "text": "Setup. We experimented on two datasets from SemEval 2014 (Pontiki et al., 2014), which con-sist of reviews and comments from two categories: L APTOP and R ESTAURANT . We adopt the stan-dard evaluation metrics: Accuracy and Macro-Averaged F1. We follow the instructions of Zhang et al. (2019) to run the experiments 5 times with random initialization and report the averaged per-formance. We denote the original PWCN with rel-ative position information as PWCN-Pos, and that utilizes dependency trees constructed by SpaCy [Cite_Footnote_5] as PWCN-Dep. SpaCy has reported an UAS of 94.5 on English PTB and so it can serve as a good reference for human-designed dependency schema. We also compare our model against two trivial trees (left-chain and right-chain trees). For our model, we feed the corpus into BERT and ex-tract dependency trees with the best performing setting: Eisner+Dist. For parsing, we introduce an inductive bias to favor short dependencies (Eisner and Smith, 2010). To ensure a fair comparison, we induce the root word from the impact matrix F instead of using the gold root. Specifically, we se-lect the root word x k based on the simple heuristic arg max i P Tj=1 f(x i , x j )."
  },
  {
    "id": 1390,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.inoh.org:8083/ontology-viewer/",
    "section_title": "3 Architecture 3.1 Text View",
    "add_info": "INOH, 2004. INOH Ontology Viewer Website. http://www.inoh.org:8083/ontology-viewer/.",
    "text": "In a traditional ontology browser, the user starts looking for concepts of interest by typing words and phrases into a search field. This is the model for several existing tools, including the VisDic viewer for WordNet (Hor\u00e1k and Smr\u017e, 2004), the INOH ontology viewer (INOH, 2004)  , and the Gene Ontology viewer presented by Koike and Takagi (2004), among others."
  },
  {
    "id": 1391,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm",
    "section_title": "3 Architecture 3.1 Text View",
    "add_info": "Xinhuanet. 2006. US accused of blocking approval of new UN human rights body. http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm.",
    "text": "SconeEdit improves on this browsing paradigm by giving a user who is unfamiliar with the knowl-edge base an easy way to start exploring. Rather than generating a series of guesses at what may be Figure 2. Excerpt from Text View, with Search and Text Tabs covered by the KB, the user can load natural lan-guage text into SconeEdit from a file or the system clipboard. We take an article from Xinhuanet News Service (Xinhuanet, 2006)  as an example. Figure 2 shows an excerpt of this text after it has been loaded."
  },
  {
    "id": 1392,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://en.wikipedia.org/wiki/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "To overcome this problem, we employ a novel ad- 1  https://en.wikipedia.org/wiki/ versarial NA instance generation strategy at entity- Wikipedia:Lists_of_popular_pages_by_ WikiProject level, which requires RE models to pay more atten-Path-Level Adversarial NA Instance. To test the each reasoning text path. The second challenge is model ability of cross-document reasoning in the that RE models need to synthesize all information presence of noise in closed setting (see Sec. 3), we in multiple text paths to obtain the final relation. generate NA reasoning text paths for both human-"
  },
  {
    "id": 1393,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/PlusLabNLP/ee-wiki-bias",
    "section_title": "1 Introduction",
    "add_info": "2 https://github.com/PlusLabNLP/ee-wiki-bias",
    "text": "To facilitate the study, we collect a corpus that contains demographic information, personal life de-scription, and career description from Wikipedia. [Cite_Footnote_2] We first detect events in the collected corpus using a state-of-the-art event extraction model (Han et al., 2019). Then, we extract gender-distinct events with a higher chance to occur for one group than the other. Next, we propose a calibration technique to offset the potential confounding of gender biases in the event extraction model, enabling us to fo-cus on the gender biases at the corpus level. Our contributions are three-fold:"
  },
  {
    "id": 1394,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/rujunhan/EMNLP-2019",
    "section_title": "2 Experimental Setup",
    "add_info": "3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.",
    "text": "Dataset. Our collected corpus contains demo-graphics information and description sections of celebrities from Wikipedia. Table 2 shows the statistics of the number of celebrities with Career or Personal Life sections in our corpora, together with all celebrities we collected. In this work, we only explored celebrities with Career or Personal Life sections, but there are more sections (e.g., Pol-itics and Background and Family) in our collected other defines an event as a complex structure includ-ing a trigger, arguments, time, and location (Ahn, 2006). The corpus following the former definition usually has much broader coverage, while the lat-ter can provide richer information. For broader coverage, we choose a state-of-the-art event detec-tion model that focuses on detecting event trigger words by Han et al. (2019). [Cite_Footnote_3] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|. Then, we separately construct dic-tionaries E m = {e m1 : |e m1 |,...,e mM : |e mM |} and E f = {e f1 : |e f1 |, ..., e fF : |e fF |} mapping events to their frequency for male and female respectively."
  },
  {
    "id": 1395,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "3 Detecting Gender Biases in Events",
    "add_info": "8 We use spaCy (https://spacy.io/) to tokenize the corpus and remove stop words.",
    "text": "To show the effectiveness of using events as a lens for gender bias analysis, we compute WEAT scores on the raw texts and detected events sepa-rately. For the former, we take all tokens excluding stop words. [Cite_Footnote_8] Together with gender attributes from Caliskan et al. (2017), we calculate and show the"
  },
  {
    "id": 1396,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://peixianc.me/amfcodes.zip",
    "section_title": "2 Approach 2.2 Nonparametric Bayesian Modeling 2.2.1 Nonparametric Bayesian Formulation",
    "add_info": "1 We implement the system for relation extraction, based on the code at http://peixianc.me/amfcodes.zip.",
    "text": "Prediction. After learning [Cite_Footnote_1] , we use the expec-tation E(P (y i,j )) in Eq.(9) to complete the entries in Y test . Finally, we can acquire Top-N predicted relations via ranking the values E(P (y i,j )), given entity pair i, for different relations j."
  },
  {
    "id": 1397,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www/eml-research.de/nlp/download/wikirelations.php",
    "section_title": "4 Representation 4.2 Selectional preference features",
    "add_info": "1 http://www/eml-research.de/nlp/download/wikirelations.php",
    "text": "To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [Cite_Footnote_1] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links."
  },
  {
    "id": 1398,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/PKUnlp-icler/GAIN",
    "section_title": "References",
    "add_info": null,
    "text": "Document-level relation extraction aims to ex-tract relations among entities within a docu-ment. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN con-structs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interac-tion among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer re-lations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at  https://github.com/PKUnlp-icler/GAIN."
  },
  {
    "id": 1399,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/headacheboy/data-of-multimodal-sarcasm-detection",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/headacheboy/data-of-multimodal-sarcasm-detection",
    "text": "\u2022 We create a new dataset for multi-modal Twitter sarcasm detection and release it [Cite_Footnote_1] ."
  },
  {
    "id": 1400,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/rycolab/info-theoretic-probing",
    "section_title": "References",
    "add_info": null,
    "text": "The success of neural networks on a diverse set of NLP tasks has led researchers to ques-tion how much these networks actually \u201cknow\u201d about natural language. Probes are a nat-ural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annota-tions in that linguistic task from the network\u2019s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that us-ing simpler models as probes is better; the logic is that simpler models will identify lin-guistic structure, but not learn the task it-self. We propose an information-theoretic op-erationalization of probing as estimating mu-tual information that contradicts this received wisdom: one should always select the high-est performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the lin-guistic information inherent in the represen-tation. The experimental portion of our pa-per focuses on empirically estimating the mu-tual information between a linguistic property and BERT, comparing these estimates to sev-eral baselines. We evaluate on a set of ten typologically diverse languages often under-represented in NLP research\u2014plus English\u2014 totalling eleven languages. Our implementa-tion is available in  https://github.com/rycolab/info-theoretic-probing."
  },
  {
    "id": 1401,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/OceanskySun/",
    "section_title": "4 Experiments and Results 4.3 Baselines",
    "add_info": "6 https://github.com/OceanskySun/",
    "text": "For GRAFT-Net (Sun et al., 2018), we use the implementation published by the author; [Cite_Footnote_6] how-ever, we retrieve data with a simpler process, as described in Section 4.4."
  },
  {
    "id": 1402,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/PravallikaRao/SpellChecker",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/PravallikaRao/SpellChecker",
    "text": "\u2022 We create synthetic datasets [Cite_Footnote_1] of noisy and correct word mappings for Hindi and Telugu by collecting highly probable spelling errors and inducing noise in clean corpus."
  },
  {
    "id": 1403,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://ltrc.iiit.ac.in/download.php",
    "section_title": "4 Experiments and Results 4.1 Dataset details",
    "add_info": "2 https://ltrc.iiit.ac.in/download.php",
    "text": "Due to lack of data with error patterns in Indic languages, we have built a synthetic dataset that SCMIL is trained on. Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words [Cite_Footnote_2] and most frequent Telugu words . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists. Thus, the Hindi and Telugu data lists consist of words and phrases consisting maximum of five words."
  },
  {
    "id": 1404,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://en.wiktionary.org/Frequencylists/Telugu",
    "section_title": "4 Experiments and Results 4.1 Dataset details",
    "add_info": "3 https://en.wiktionary.org/Frequencylists/Telugu",
    "text": "Due to lack of data with error patterns in Indic languages, we have built a synthetic dataset that SCMIL is trained on. Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words and most frequent Telugu words [Cite_Footnote_3] . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists. Thus, the Hindi and Telugu data lists consist of words and phrases consisting maximum of five words."
  },
  {
    "id": 1405,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://ltrc.iiit.ac.in/download.php",
    "section_title": "4 Experiments and Results 4.3 Results and Analysis",
    "add_info": "4 https://ltrc.iiit.ac.in/download.php",
    "text": "The rule-based spell-checker for Hindi, HIN-SPELL (Singh et al., 2015) reported an accuracy of 77.9% on a data of 870 misspelled words ran-domly collected from books, newspapers and peo-ple etc. Te data used in Singh et al. (2015) and the HINSPELL system are not available. Hence, we implemented HINSPELL using Shabdanjali dic-tionary [Cite_Footnote_4] consisting of 32952 Hindi word. This system when tested on our Hindi synthetic dataset, gave an accuracy is 72.3%. This accuracy being lower than the original HINSPELL accuracy can be accounted to larger size of testing data and in-clusion of out-of-vocabulary words. Thus, SCMIL outperforms HINSPELL by reporting an accuracy of 85.4%."
  },
  {
    "id": 1406,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://git.io/JU0JJ",
    "section_title": "References",
    "add_info": "1 Our code is available at https://git.io/JU0JJ.",
    "text": "Exploiting visual groundings for language un-derstanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a con-stituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via R EINFORCE and derives the learning signal only from the alignment of images and sen-tences. While their model is relatively accu-rate overall, its error distribution is very un-even, with low performance on certain con-stituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely in-sufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model estab-lishes a new state of the art, outperforming its non-grounded version and, thus, confirm-ing the effectiveness of visual groundings in constituency grammar induction. It also sub-stantially outperforms the previous grounded model, with largest improvements on more \u2018abstract\u2019 categories (e.g., +55.1% recall on VPs). [Cite_Footnote_1]"
  },
  {
    "id": 1407,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://git.io/Jf3nn",
    "section_title": "4 Experiments 4.2 Settings and hyperparameters",
    "add_info": "5 https://git.io/Jf3nn.",
    "text": "We adopt parameter settings suggested by the authors for the baseline models. For VG-NSL we run the authors\u2019 code. [Cite_Footnote_5] We re-implement C-PCFG using automatic differentiation (Eisner, 2016) to speed up training. Our VC-PCFG com-prises a parsing model and an image-text match-ing model. The parsing model has the same pa-rameters as the baseline C-PCFG; the image-text matching model has the same parameters as the baseline VG-NSL. Concretely, the parsing model has 30 nonterminals and 60 preterminals. Each of them is represented by a 256-dimensional vec-tor. The inference model q \u03c6 (z|w) uses a single-layer BiLSTM. It has a 512-dimensitional hidden state and relies on 512-dimensitional word em-beddings. We apply a max-pooling layer over the hidden states of the BiLSTM and then ob-tain 64-dimensitional mean vectors \u00b5 \u03c6 (w) and log-variances log \u03c3 \u03c6 (w) by using an affine layer. The image-text matching model projects visual fea-tures into 512-dimensitional feature vectors and encodes spans as 512-dimensitional vectors. Our span representation model is another single-layer BiLSTM, with the same hyperparameters as in the inference model. \u03b1 for visually grounded learning is set to 0.001. We implement VC-PCFG relying on Torch-Struct (Rush, 2020), and optimize it us-ing Adam (Kingma and Ba, 2015) with the learning rate set to 0.01, \u03b2 1 = 0.75, and \u03b2 2 = 0.999. All parameters are initialized with Xavier uniform ini-tializer (Glorot and Bengio, 2010)."
  },
  {
    "id": 1408,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://code.google.com/p/egret-parser",
    "section_title": "3 Parsing 3.1 Parsing Overview",
    "add_info": "4 http://code.google.com/p/egret-parser",
    "text": "For English, the two most widely referenced parsers are the Stanford Parser and Berkeley Parser. In this work, we compare the Stanford Parser\u2019s CFG model, with the Berkeley Parser\u2019s latent variable model. In previous reports, it has been noted (Kummerfeld et al., 2012) that the la-tent variable model of the Berkeley parser tends to have the higher accuracy of the two, so if the accu-racy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret, [Cite_Footnote_4] which achieves nearly iden-tical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other el-ement in the phrase."
  },
  {
    "id": 1409,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://plata.ar.media.kyoto-u.ac.jp/tool/EDA",
    "section_title": "3 Parsing 3.1 Parsing Overview",
    "add_info": "5 http://plata.ar.media.kyoto-u.ac.jp/tool/EDA",
    "text": "For Japanese, our first method uses the MST-based pointwise dependency parser of Flannery et al. (2011), as implemented in the Eda toolkit. [Cite_Footnote_5] In order to convert dependencies into phrase-structure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing. Both models are trained on the Japanese Word Dependency Treebank (Mori et al., 2014)."
  },
  {
    "id": 1410,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://code.google.com/p/nile",
    "section_title": "4 Alignment 4.1 Alignment Overview",
    "add_info": "6 http://code.google.com/p/nile",
    "text": "As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [Cite_Footnote_6] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation."
  },
  {
    "id": 1411,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/IntelLabs/academic-budget-bert",
    "section_title": "References",
    "add_info": "1 Our code is publicly available at: https://github.com/IntelLabs/academic-budget-bert",
    "text": "While large language models \u00e0 la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimiza-tions, design choices, and hyperparameter tun-ing, it is possible to produce models that are competitive with BERT BASE on GLUE tasks at a fraction of the original pretraining cost. [Cite_Footnote_1]"
  },
  {
    "id": 1412,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/tensorflow/models/tree/master/official/nlp/bert",
    "section_title": "3 Combining Efficient Training Methods 3.2 Combined Speedup",
    "add_info": "3 https://github.com/tensorflow/models/tree/master/official/nlp/bert",
    "text": "We compare our optimized framework to the offi-cial implementation of Devlin et al. (2019). [Cite_Footnote_3] Ta-ble 1 shows that using the official code to train BERT BASE could take almost 6 days under our hard-ware assumptions (Section 2), and a large model might require close to a month of non-stop compu-tation. In contrast, our recipe significantly speeds up training, allowing one to train BERT LARGE with the original number of steps (1M) in a third of the time (8 days), or converge in 2-3 days by enlarging the batch size. While larger batch sizes do not guar-antee convergence to models of equal quality, they are generally recommended (Ott et al., 2018; Liu et al., 2019), and present a more realistic starting point for our next phase (hyperparameter tuning) given our 24-hour constraint."
  },
  {
    "id": 1413,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://cordova.apache.org",
    "section_title": "3 My Turn To Read App",
    "add_info": "3 https://cordova.apache.org",
    "text": "Mobile versions of MTTR are built using Apache Cordova [Cite_Footnote_3] \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR."
  },
  {
    "id": 1414,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://readium.org",
    "section_title": "3 My Turn To Read App",
    "add_info": "4 https://readium.org",
    "text": "Mobile versions of MTTR are built using Apache Cordova \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium [Cite_Footnote_4] , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR."
  },
  {
    "id": 1415,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.idpf.org/epub",
    "section_title": "3 My Turn To Read App 3.1 Read Aloud eBooks",
    "add_info": "5 http://www.idpf.org/epub",
    "text": "We use the EPUB format [Cite_Footnote_5] to create what we call a \u201cRead Aloud eBook\" used by MTTR. To link the text in the book to the synchronized audio, we use SMIL (Synchronized Multimedia Integration Lan-guage), as defined in the EPUB Media Overlays specification. The complete process for generat-ing a Read Aloud eBook is as follows: book MP3 file for this chapter. The alignment is done using the Kaldi ASR toolkit (Povey et al., 2011) and the LibriSpeech acoustic models (Panayotov et al., 2015). The result-ing word-level alignment is used to compute the beginning and end timestamps for each sentence. We use Sequitur G2P (Bisani and Ney, 2008) to phonetically transcribe out-of-vocabulary words. The transcriptions are checked manually and added to the lexicon used for forced alignment."
  },
  {
    "id": 1416,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/lxml/lxml",
    "section_title": "3 My Turn To Read App 3.1 Read Aloud eBooks",
    "add_info": "6 https://github.com/lxml/lxml",
    "text": "1. We use lxml [Cite_Footnote_6] to extract the plain text from the original eBook EPUB. We then break up paragraphs into sentences and create a map-ping between sentence identifiers and token indices where sentences start and end."
  },
  {
    "id": 1417,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/aerkalov/ebooklib",
    "section_title": "3 My Turn To Read App 3.1 Read Aloud eBooks",
    "add_info": "7 https://github.com/aerkalov/ebooklib",
    "text": "3. We use ebooklib [Cite_Footnote_7] to generate a new EPUB file with sentences linked to time segments in the relevant MP3 file using SMIL."
  },
  {
    "id": 1418,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://myturntoread.org",
    "section_title": "5 Discussion & Future Work",
    "add_info": "9 https://myturntoread.org.",
    "text": "My Turn To Read is currently in beta and we plan to release freely-available web [Cite_Footnote_9] and mobile (iOS & Android) versions in August of 2019 with the public-domain book The Adventures of Pinocchio. We plan to add more books in subsequent releases."
  },
  {
    "id": 1419,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/p/word2vec/",
    "section_title": "3 Representation of Words and Compounds",
    "add_info": "5 https://code.google.com/p/word2vec/",
    "text": "In this work we use word embeddings of Mikolov et al. (2013a) to represent the seman-tics of words and compounds. We chose an En-glish Wikipedia dump as our corpus. After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [Cite_Footnote_5] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words. Compounds\u2019 sizes are restricted to two (i.e. bigrams) for the sake of sim-plicity and to respect the evaluation set standards. The compounds and word embeddings are then used as supervised signals to learn a composition function."
  },
  {
    "id": 1420,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.elastic.co/products/elasticsearch",
    "section_title": "3 Approach",
    "add_info": "2 https://www.elastic.co/products/elasticsearch",
    "text": "For a question with q words Q = {w tQ } qt=1 along with its N answer choices C = {C n } Nn=1 where C n = {w Ct } ct=1 , the essential-term selector chooses a subset of essential terms E \u2282 Q, which are then concatenated with each C n to formulate a query. The query for each answer choice, E+C n , is sent to the retriever (e.g. Elastic Search [Cite_Footnote_2] ), and the top K retrieved sentences based on the scores returned by the retriever are then concatenated into the evidence passage P n = {w tP } tK=1 ."
  },
  {
    "id": 1421,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/nijianmo/arc-etrr-code",
    "section_title": "4 Experiments",
    "add_info": null,
    "text": "In this section, we first discuss the performance of the essential term selector, ET-Net, on a public dataset. We then discuss the performance of the whole retriever-reader pipeline, ET-RR, on mul-tiple open-domain datasets.For both the ET-Net and ET-RR models, we use 96-dimensional hid-den states and 1-layer BiLSTMs in the sequence modeling layer. A dropout rate of 0.4 is applied for the embedding layer and the BiLSTMs\u2019 out-put layer. We use adamax (Kingma and Ba, 2014) with a learning rate of 0.02 and batch size of 32. The model is trained for 100 epochs. Our code is released at  https://github.com/nijianmo/arc-etrr-code."
  },
  {
    "id": 1422,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2",
    "section_title": "2 Data",
    "add_info": "2 http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2",
    "text": "For the experiments in this paper, we used a full dump of Wikipedia from September 4, 2010. [Cite_Footnote_2] In-cluded in this dump is a total of 10,355,226 articles, of which 1,019,490 have been geotagged. Excluding various types of special-purpose articles used pri-marily for maintaining the site (specifically, redirect articles and articles outside the main namespace), the dump includes 3,431,722 content-bearing arti-cles, of which 488,269 are geotagged."
  },
  {
    "id": 1423,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://download.freebase.com/wex/",
    "section_title": "2 Data",
    "add_info": "3 http://download.freebase.com/wex/",
    "text": "It is necessary to process the raw dump to ob-tain the plain text, as well as metadata such as geo-tagged coordinates. Extracting the coordinates, for example, is not a trivial task, as coordinates can be specified using multiple templates and in mul-tiple formats. Automatically-processed versions of the English-language Wikipedia site are provided by Metaweb, [Cite_Footnote_3] which at first glance promised to signif-icantly simplify the preprocessing. Unfortunately, these versions still need significant processing and they incorrectly eliminate some of the important metadata. In the end, we wrote our own code to process the raw dump. It should be possible to ex-tend this code to handle other languages with little difficulty. See Lieberman and Lin (2009) for more discussion of a related effort to extract and use the geotagged articles in Wikipedia."
  },
  {
    "id": 1424,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/GeoText/",
    "section_title": "2 Data",
    "add_info": "4 http://www.ark.cs.cmu.edu/GeoText/",
    "text": "Geo-tagged Microblog Corpus As a second eval-uation corpus on a different domain, we use the corpus of geotagged tweets collected and used by Eisenstein et al. (2010). [Cite_Footnote_4] It contains 380,000 mes-sages from 9,500 users tweeting within the 48 states of the continental USA."
  },
  {
    "id": 1425,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.comp.nus.edu.sg/\u02dcnlp/sw/m2scorer.tar.gz",
    "section_title": "4 Experiments 4.1 Dataset and Evaluation",
    "add_info": "1 http://www.comp.nus.edu.sg/\u02dcnlp/sw/m2scorer.tar.gz",
    "text": "We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014). We report final performance on the CoNLL-14 test set without alternatives, and an-alyze model performance on the CoNLL-13 devel-opment set (Dahlmeier et al., 2013). We use the de-velopment and validation sets for model selection. The sizes of all datasets in number of sentences are shown in Table 1. We report performance in F 0.5 -measure, as calculated by the m2scorer\u2014 the official implementation of the scoring metric in the shared task. [Cite_Footnote_1] Given system outputs and gold-standard edits, m2scorer computes the F 0.5 measure of a set of system edits against a set of gold-standard edits."
  },
  {
    "id": 1426,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Pzoom522/L1-Refinement",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "To demonstrate the effectiveness of our method, we select four state-of-the-art baselines and con-duct comprehensive evaluations in both supervised and unsupervised settings. Our experiments in-volve ten languages from diverse branches/families and embeddings trained on corpora of different domains. In addition to the standard Bilingual Lex-icon Induction (BLI) benchmark, we also investi-gate a downstream task, namely cross-lingual trans-fer for Natural Language Inference (NLI). In all setups tested, our algorithm significantly improves the performance of strong baselines. Finally, we provide an intuitive visualisation illustrating why ` 1 loss is more robust than it ` 2 counterpart when refining CLWEs (see Fig. 1). Our code is avail-able at  https://github.com/Pzoom522/L1-Refinement."
  },
  {
    "id": 1427,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://www.netlib.org/ode/vode.f",
    "section_title": "4 Experimental Setup 4.3 Implementation Details of Algorithm 1",
    "add_info": "3 http://www.netlib.org/ode/vode.f",
    "text": "The CSLS scheme with a neighbourhood size of 10 (CSLS-10) is adopted to build synthetic dictio-naries via the input CLWEs. A variable-coefficient ordinary differential equation (VODE) solver [Cite_Footnote_3] was implemented for the system described in Eq. (7). Suggested by Trendafilov (2003), we set the maxi-mum order at 15, the smoothness coefficient \u03b1 in Eq. (5) at 1e8, the threshold in Eq. (8) at 1e-5, and performed the integration with a fixed time interval of 1e-6. An early-stopping design was adopted to ensure computation completed in a reasonable time: in addition to the two default stopping criteria in \u00a7 3, integration is terminated if R dt reaches 5e-3 (dt is the differentiation term in Eq. (7))."
  },
  {
    "id": 1428,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://stackoverflow.com/",
    "section_title": "1 Introduction",
    "add_info": "1 https://stackoverflow.com/",
    "text": "A typical example for CQA is shown in Table 1. In this example, Answer [Cite_Footnote_1] is a good answer, be-cause it provides helpful information, e.g., \u201ccheck it to the traffic dept\u201d. Although Answer 2 is rele-vant to the question, it does not contain any useful information so that it should be regarded as a bad answer."
  },
  {
    "id": 1429,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/pku-wuwei/QCN",
    "section_title": "1 Introduction",
    "add_info": "3 An implementation of our model is available at https://github.com/pku-wuwei/QCN.",
    "text": "\u2022 Our proposed Question Condensing Net-works (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effec-tiveness of our model. [Cite_Footnote_3]"
  },
  {
    "id": 1430,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://alt.qcri.org/semeval2015/task3/index.php?id=data-and-tools",
    "section_title": "3 Proposed Model 3.1 Word-Level Embedding",
    "add_info": "4 http://alt.qcri.org/semeval2015/task3/index.php?id=data-and-tools",
    "text": "Word-level embeddings are composed of two components: GloVe (Pennington et al., 2014) word vectors trained on the domain-specific unan-notated corpus provided by the task [Cite_Footnote_4] , and con-volutional neural network-based character embed-dings which are similar to (Kim et al., 2016). Web text in CQA forums differs largely from normal-ized text in terms of spelling and grammar, so specifically trained GloVe vectors can model word interactions more precisely. Character embedding has proven to be very useful for out-of-vocabulary (OOV) words, so it is especially suitable for noisy web text in CQA."
  },
  {
    "id": 1431,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.qatarliving.com/forum",
    "section_title": "4 Experimental Setup 4.1 Datasets",
    "add_info": "5 http://www.qatarliving.com/forum",
    "text": "We use two community question answering datasets from SemEval (Nakov et al., 2015, 2017) to evaluate our model. The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [Cite_Footnote_5] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description. There are also some metadata associated with them, e.g., user ID, date of posting, and the question category. We do not use the metadata because they failed to boost performance in our model. Since the SemEval 2017 dataset is an updated version of SemEval 2016 6 , and shares the same evaluation metrics with SemEval 2016, we choose to use the SemEval 2017 dataset for evaluation."
  },
  {
    "id": 1432,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://ilk.uvt.nl/sabine/chunklink/",
    "section_title": "2 Unsupervised Relation Extraction Problem 2.1 Context Vector and Feature Design",
    "add_info": null,
    "text": "1 Software available at  http://ilk.uvt.nl/sabine/chunklink/"
  },
  {
    "id": 1433,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://alpage.inria.fr/\u02dcsagot/lefff-en.html",
    "section_title": "2 Methods",
    "add_info": "3 Lexique des Formes Fle\u0301chies du Franc\u0327ais: http://alpage.inria.fr/\u02dcsagot/lefff-en.html",
    "text": "Morphological classification. The continuous word representations are used to train a logis-tic regression classifier 2 for each morphological feature: gender and number for noun and adjec-tives; tense for verbs (with labels: present, fu-ture, imperfect, or simple past). Word labels are taken from the Lefff French morphological lexicon (Sagot, 2010) [Cite_Footnote_3] . To ensure a fair comparison be-tween context-independent and context-dependent embedding classification, words that are ambigu-ous with respect to a given feature are excluded from the respective classifier\u2019s training and test data."
  },
  {
    "id": 1434,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://www.github.com/tzshi/bubble-parser-acl21",
    "section_title": "Appendix B Implementation Details",
    "add_info": null,
    "text": "Our implementation (  https://www.github.com/tzshi/bubble-parser-acl21 ) is based on PyTorch."
  },
  {
    "id": 1435,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.nlm.nih.gov/mesh",
    "section_title": "1. Introduction",
    "add_info": "1 MeSH is available at http://www.nlm.nih.gov/mesh. MeSH 2003 was used in this research.",
    "text": "Node Number Terms C18.452.297 diabetes mellitus insulin-dependent diabetes C18.452.297.267 mellitus C18.452.297.267.960 wolfram syndrome Table 1. Subtree of MeSH [Cite_Footnote_1] tree. Node numbers represent hierarchical structure of terms"
  },
  {
    "id": 1436,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.conexor.com",
    "section_title": "2. Information for Term Specificity 2.2. Contextual Information",
    "add_info": "Conexor. 2004. Conexor Functional Dependency Grammar Parser. http://www.conexor.com",
    "text": "T = {t k |1\u2264 k \u2264 n} (3) the distribution of predicates which have the terms as arguments, and the distribution of modi- where t k is a term and n is total number of terms. fiers of the terms are contextual information. In next step, a discrete random variable X is de-other words. Contrary, domain specific terms don\u2019t tend to be modified by other words, be- X ={x k |1\u2264 k \u2264 n} (4) cause they have sufficient information in them- p(x k ) = Prob(X = x k ) selves (Caraballo, 1999B). Under this assumption, we use probabilistic distribution of modifiers as where x k is an event of a term t k occurs in corpus, contextual information. Because domain specific p(x k ) is the probability of event x k . The informa-terms, unlike general words, are rarely modified tion quantity, I(x k ), gained after observing the in corpus, it is important to collect statistically event x k , is defined by the logarithmic function. sufficient modifiers from given corpus. Therefore Finally I(x k ) is used as specificity value of t k as accurate text processing, such as syntactic pars- equation (5). ing, is needed to extract modifiers. As Cara-Spec(t k ) \u2248 I(x k ) = \u2212log p(x k ) (5) ballo\u2019s work was for general words, they extracted only rightmost prenominals as context In equation (5), we can measure specificity of information. We use Conexor functional depend- t k , by estimating p(x k ). We describe three estimat-ency parser (Conexor, 2004)  to analyze the struc- ing methods of p(x k ) in following sections. ture of sentences. Among many dependency 3.1. Compositional Information based functions defined in Conexor parser, \u201cattr\u201d and \u201cmod\u201d functions are used to extract from analyzed structures. If a term or Method (Method 1) modifiers modifiers In this section, we describe a method using com-of the term do not occur in corpus, specificity of positional information introduced in section 2.1. the term can not be measured using contextual This method is divided into two steps: In the first information step, specificity values of all words are measured independently. In the second step, the specificity"
  },
  {
    "id": 1437,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.uni-weimar.de/medien/webis/corpora",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.uni-weimar.de/medien/webis/corpora",
    "text": "The contribution of this paper is three-fold: First, through distant supervision we acquire a large cor-pus with 28,689 argumentative text segments from the online debate portal idebate.org. The corpus covers 14 separate domains with strongly varying feature distributions. It will be made freely avail-able to other researchers. [Cite_Footnote_1] Second, we obtain a ro-bust classifier for argumentativeness, providing ev-idence that distant supervision does not only save money and time, but also benefits the effectiveness of cross-domain and cross-register argumentation mining. Third, we evaluate\u2014for the first time\u2014 the robustness of several features in classifying ar-gumentativeness across domains and registers."
  },
  {
    "id": 1438,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.uni-weimar.de/medien/webis/corpora",
    "section_title": "3 Mining Argumentative Text through Distant Supervision 3.2 The Webis-Debate-16 Corpus",
    "add_info": "2 http://www.uni-weimar.de/medien/webis/corpora",
    "text": "As a result of applying the defined mapping func-tions, we obtained a large argumentation mining corpus, called Webis-Debate-16. The corpus con-tains 28,689 text segments from the 14 themes of idebate.org (23,880 argumentative, 4809 non-argumentative). Each theme is assumed to represent one domain. Table 2 lists the distribution of docu-ments over the domains in the corpus. Regarding the number of annotated text segments, Webis-Debate-16 is the largest dataset published so far for argu-mentation mining. While our review corpus from (Wachsmuth et al., 2014b) is even larger, its anno-tations are restricted to sentiment-related argumen-tation. Table 3 compares Webis-Debate-16 to other real argumentation mining corpora, namely, the Es-says corpus (Stab and Gurevych, 2014a), the Web discourse corpus (Habernal and Gurevych, 2015), the European Court of Human Rights (ECHR) cor-pus (Palau and Moens, 2009), and the Araucaria cor-pus (Reed and Rowe, 2004). The Webis-Debate-16 corpus will be made freely available online. [Cite_Footnote_2]"
  },
  {
    "id": 1439,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://jmcauley.ucsd.edu/data/amazon/links.html",
    "section_title": "6 Experiment 6.1 Dataset and Experimental Setup",
    "add_info": "5 http://jmcauley.ucsd.edu/data/amazon/links.html",
    "text": "For word vector initialization, we train word em-beddings with word2vec (Mikolov et al., 2013) on the Yelp Challenge dataset 4 for the restaurant do-main and on the Amazon reviews dataset 5 (McAuley et al., 2015) for the laptop domain. The Yelp dataset contains 2.2M restaurant reviews with 54K vocab-ulary size. For the Amazon reviews, we only ex-tracted the electronic domain that contains 1M re-views with 590K vocabulary size. We vary differ-ent dimensions for word embeddings and chose 300 for both domains. Empirical sensitivity studies on different dimensions of word embeddings are also conducted. Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007). Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02. We also try with varying context window size, and use 3 for the laptop domain and [Cite_Footnote_5] for the restaurant domain, re-spectively. All parameters are chosen by cross vali-dation."
  },
  {
    "id": 1440,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.chokkan.org/software/crfsuite/",
    "section_title": "6 Experiment 6.1 Dataset and Experimental Setup",
    "add_info": "Naoaki Okazaki. 2007. CRFsuite: a fast im-plementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.",
    "text": "For word vector initialization, we train word em-beddings with word2vec (Mikolov et al., 2013) on the Yelp Challenge dataset 4 for the restaurant do-main and on the Amazon reviews dataset 5 (McAuley et al., 2015) for the laptop domain. The Yelp dataset contains 2.2M restaurant reviews with 54K vocab-ulary size. For the Amazon reviews, we only ex-tracted the electronic domain that contains 1M re-views with 590K vocabulary size. We vary differ-ent dimensions for word embeddings and chose 300 for both domains. Empirical sensitivity studies on different dimensions of word embeddings are also conducted. Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007)  . Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02. We also try with varying context window size, and use 3 for the laptop domain and for the restaurant domain, re-spectively. All parameters are chosen by cross vali-dation."
  },
  {
    "id": 1441,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/berlino/tensor2struct-public",
    "section_title": "\u2217 Equal contribution.",
    "add_info": "1 Our implementations are available at https://github.com/berlino/tensor2struct-public.",
    "text": "\u2022 We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [Cite_Footnote_1]"
  },
  {
    "id": 1442,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/fxsjy/jieba",
    "section_title": "4 Experiments",
    "add_info": "1 https://github.com/fxsjy/jieba",
    "text": "The statistics of the three corpora are shown in Table 1. The first two corpora were preprocessed by using the python jieba segmenter [Cite_Footnote_1] for word seg-mentation and filtering. The third corpus SemEval is in English and can be tokenized by white spaces. Stop words and words appeared only once or in less than two documents were removed to allevi-ate data sparsity. Next, TF-IDF (term frequency-inverse document frequency) was used to extract the features from text. TF-IDF is a numerical s-tatistic method that is designed to reflect how im-portant a word is to a document in a corpus. In our experiments, we set the dimension of each text representation to 2,000 according to the ranking of the TF-IDF weights with each dimension of term-frequency(TF) features. After that, the text rep-resentations are fed into the proposed INN-RER method."
  },
  {
    "id": 1443,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/complementizer/news-tls",
    "section_title": "References",
    "add_info": null,
    "text": "Previous work on automatic news timeline summarization (TLS) leaves an unclear pic-ture about how this task can generally be ap-proached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection and date sum-marization, and to the previous lack of appro-priate evaluation metrics for the full TLS task. In this paper, we compare different TLS strate-gies using appropriate evaluation frameworks, and propose a simple and effective combina-tion of methods that improves over the state-of-the-art on all tested benchmarks. For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets. The dataset will be made available at  https://github.com/complementizer/news-tls."
  },
  {
    "id": 1444,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/HeidelTime/heideltime",
    "section_title": "3 Strategies for Timeline Summarization Defining the Set of Dates",
    "add_info": "2 https://github.com/HeidelTime/heideltime",
    "text": "First, we identify the set of possible dates to in-clude in a timeline. We obtain these from (i) the publication dates of all articles in A and (ii) tex-tual references of dates in sentences in A, such as \u2019last Monday\u2019, or \u201912 April\u2019. We use the tool Hei-delTime [Cite_Footnote_2] (Stro\u0308tgen and Gertz, 2013) to detect and resolve textual mentions of dates. Date Selection Next, we select the l most important dates. We compare the following date selection methods in-troduced by Tran et al. (2013a):"
  },
  {
    "id": 1445,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/GuyAllard/markov_clustering",
    "section_title": "3 Strategies for Timeline Summarization Clustering",
    "add_info": "4 We use the implementation and default parameters from https://github.com/GuyAllard/markov_clustering",
    "text": "The edge weight is set to the cosine similarity be-tween the TF-IDF bag-of-words vectors of a 1 and a 2 . The constraint on the publication dates ensures that clusters do not have temporal gaps. Further-more, it reduces the number of similarity compu-tations between pairs of articles considerably. We run MCL on this graph and obtain clusters by iden-tifying the connected components in the resulting connectivity matrix [Cite_Footnote_4] ."
  },
  {
    "id": 1446,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://edition.cnn.com/specials/world/fast-facts",
    "section_title": "4 Dataset",
    "add_info": "6 http://edition.cnn.com/specials/world/fast-facts",
    "text": "Ground-Truth Timelines: We obtain ground-truth timelines from CNN Fast Facts [Cite_Footnote_6] , which has a collection of several hundred timelines grouped in categories, e.g., \u2018people\u2019 or \u2018disasters\u2019. We pick all timelines of the \u2018people\u2019 category and a small number from other categories."
  },
  {
    "id": 1447,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/thunlp/RSN",
    "section_title": "References",
    "add_info": null,
    "text": "Open relation extraction (OpenRE) aims to extract relational facts from the open-domain corpus. To this end, it discovers relation pat-terns between named entities and then clusters those semantically equivalent patterns into a united relation cluster. Most OpenRE meth-ods typically confine themselves to unsuper-vised paradigms, without taking advantage of existing relational facts in knowledge bases (KBs) and their high-quality labeled instances. To address this issue, we propose Relational Siamese Networks (RSNs) to learn similar-ity metrics of relations from labeled data of pre-defined relations, and then transfer the relational knowledge to identify novel rela-tions in unlabeled data. Experiment results on two real-world datasets show that our frame-work can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at  https://github.com/thunlp/RSN."
  },
  {
    "id": 1448,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.wikipedia.org/",
    "section_title": "2 The Structural Semantic Relatedness Measure 2.1 Knowledge Sources",
    "add_info": "1 http://www.wikipedia.org/",
    "text": "1. Wikipedia [Cite_Footnote_1] , a large-scale online encyc-lopedia, its English version includes more than 3,000,000 concepts and new articles are added quickly and up-to-date. Wikipedia contains rich semantic knowledge in the form of hyperlinks between Wikipedia articles, such as Polysemy (disambiguation pages), Synonym (redirect pages) and Associative relation (hyperlinks between Wikipedia articles). In this paper, we extract the semantic relatedness sr between Wikipedia con-cepts using the method described in Milne and Witten(2008): where a and b are the two concepts of interest, A and B are the sets of all the concepts that are re-spectively linked to a and b, and W is the entire Wikipedia. For demonstration, we show the se-mantic relatedness between four selected con-cepts in Table 1."
  },
  {
    "id": 1449,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://wordnet.princeton.edu/",
    "section_title": "2 The Structural Semantic Relatedness Measure 2.1 Knowledge Sources",
    "add_info": "2 http://wordnet.princeton.edu/",
    "text": "3. NE Co-occurrence Corpus, a corpus of documents for capturing the social relatedness between named entities. According to the fuzzy set theory (Baeza-Yates et al., 1999), the degree of named entities co-occurrence in a corpus is a measure of the relatedness between them. For example, in Google search results, the \u201cChicago Bulls\u201d co-occurs with \u201cNBA\u201d in more than 7,900,000 web pages, while only co-occurs with \u201cEMNLP\u201d in less than 1,000 web pages. So the co-occurrence statistics can be used to measure the social relatedness between named entities. In this paper, given a NE Co-occurrence Corpus D, the social relatedness scr between two named entities ne 1 and ne 2 is measured using the Google Similarity Distance (Cilibrasi and Vitanyi, 2007): where D 1 and D 2 are the document sets corres-pondingly containing ne 1 and ne [Cite_Footnote_2] . An example of social relatedness is shown in Table 3, which is computed using the Web corpus through Google."
  },
  {
    "id": 1450,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.ldc.org",
    "section_title": "4 Features",
    "add_info": "2 http://www.ldc.org",
    "text": "Verb pattern The ATB includes morphological analyses for each verb resulting from the Buckwal-ter [Cite_Footnote_2] analyzer. Semitic languages such as Arabic have a rich templatic morphology, and this analy-sis includes the root and pattern information of each verb. This feature is of particular scientific interest because it is unique to the Semitic languages, and has an interesting potential correlation with argu-ment structure."
  },
  {
    "id": 1451,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/clinc/oos-eval",
    "section_title": "4 Experimental Settings 4.1 Dataset: Multi-Domain Intent Detection",
    "add_info": "3 https://github.com/clinc/oos-eval.",
    "text": "We use a recently-released dataset, CLINC150, [Cite_Footnote_3] for multi-domain intent detection (Larson et al., 2019). The CLINC150 dataset defines 150 types of intents in total (i.e., N = 150), where there are 10 different domains and 15 intents for each of them. Table 2 shows the dataset statistics."
  },
  {
    "id": 1452,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/",
    "section_title": "4 Experimental Settings 4.3 Model Training and Configurations",
    "add_info": "5 We use https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.",
    "text": "We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [Cite_Footnote_5] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments. We combine three NLI datasets, SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque et al., 2011) from the GLUE benchmark (Wang et al., 2018) to pre-train our proposed model."
  },
  {
    "id": 1453,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/UKPLab/sentence-transformers",
    "section_title": "4 Experimental Settings 4.3 Model Training and Configurations",
    "add_info": "5 We use https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.",
    "text": "We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [Cite_Footnote_5] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments. We combine three NLI datasets, SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque et al., 2011) from the GLUE benchmark (Wang et al., 2018) to pre-train our proposed model."
  },
  {
    "id": 1454,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/connorbrinton/polyai-models/releases/tag/v1.0",
    "section_title": "4 Experimental Settings 4.3 Model Training and Configurations",
    "add_info": "6 https://github.com/connorbrinton/polyai-models/releases/tag/v1.0.",
    "text": "\u2022 Non-BERT classifier: We also test a state-of-the-art fast embedding-based classifier, \u201cUSE+ConveRT\u201d (Henderson et al., 2019; Casanueva et al., 2020), in the \u201call domains\u201d setting. Casanueva et al. (2020) showed that the \u201cUSE+ConveRT\u201d outperformed a BERT classifier on the CLINC150 dataset, while it was not evaluated along with the OOS detec-tion task. We modified their original code [Cite_Footnote_6] to apply the uncertainty-based OOS detection."
  },
  {
    "id": 1455,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/salesforce/DNNC-few-shot-intent",
    "section_title": "4 Experimental Settings 4.3 Model Training and Configurations",
    "add_info": "8 Our code is available at https://github.com/salesforce/DNNC-few-shot-intent.",
    "text": "\u2022 Proposed method: [Cite_Footnote_8] \u201cDNNC\u201d is our pro-posed method, and \u201cDNNC-scratch\u201d is with-out the NLI pre-training in Section 3.3. \u201cDNNC-joint\u201d is our joint approach on top of top-k retrieval by Emb-kNN (Section 3.4)."
  },
  {
    "id": 1456,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://github.com/clinc/oos-eval",
    "section_title": "References",
    "add_info": "9 https://github.com/clinc/oos-eval.",
    "text": "This work is supported in part by NSF under grants III-1763325, III-1909323, and SaTC-1930941. We thank Huan Wang, Wenpeng Yin for their insightful discussions, and the anonymous reviewers for their helpful and thoughtful comments. We also thank Jin Qu, Tian Xie, Xinyi Yang, and Yingbo Zhou for their support in the deployment of DNNC into the internal system. Appendix A Training Details Dataset preparation To use the CLINC150 dataset (Larson et al., 2019) [Cite_Footnote_9] in our ways, espe-cially for the single-domain experiments, we pro-vide preprocessing scrips accompanied with our code."
  },
  {
    "id": 1457,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/",
    "section_title": "References",
    "add_info": "10 https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.",
    "text": "General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [Cite_Footnote_10] The example code to train the NLI-style models is also available. We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs."
  },
  {
    "id": 1458,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/UKPLab/sentence-transformers",
    "section_title": "References",
    "add_info": "10 https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.",
    "text": "General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [Cite_Footnote_10] The example code to train the NLI-style models is also available. We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs."
  },
  {
    "id": 1459,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/huggingface/transformers/tree/master/examples/text-classification",
    "section_title": "References",
    "add_info": "11 https://github.com/huggingface/transformers/tree/master/examples/text-classification.",
    "text": "General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. The example code to train the NLI-style models is also available. [Cite_Footnote_11] We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs."
  },
  {
    "id": 1460,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json",
    "section_title": "References",
    "add_info": "12 https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json.",
    "text": "General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. The example code to train the NLI-style models is also available. We use the roberta-base configuration [Cite_Footnote_12] for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs."
  },
  {
    "id": 1461,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py",
    "section_title": "References",
    "add_info": "13 https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py.",
    "text": "Text pre-processing For all the RoBERTa-based models, we used the RoBERTa roberta-base\u2019s tokenizer provided in the transformers library. [Cite_Footnote_13] We did not perform any additional pre-processing in our experiments."
  },
  {
    "id": 1462,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/jasonwei20/eda_nlp",
    "section_title": "B Data Augmentation",
    "add_info": "14 https://github.com/jasonwei20/eda_nlp.",
    "text": "EDA Classifier-EDA uses the following four data augmentation techniques in Wei and Zou (2019): synonym replacement, random insertion, random swap, and random deletion. We follow the publicly available code. [Cite_Footnote_14] For every training example, we empirically set one augmentation based on every technique. We apply each technique separately to the original sentence and therefore every training example will have four augmentations. The proba-bility of a word in an utterance being edited is set to 0.1 for all the techniques."
  },
  {
    "id": 1463,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/princeton-nlp/LM-BFF",
    "section_title": "References",
    "add_info": "2 Our implementation is publicly available at https://github.com/princeton-nlp/LM-BFF.",
    "text": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot perfor-mance solely by leveraging a natural-language prompt and a few task demonstrations as in-put context. Inspired by their findings, we study few-shot learning in a more practical sce-nario, where we use smaller language models for which fine-tuning is computationally effi-cient. We present LM-BFF\u2014better few-shot fine-tuning of language models \u2014a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt genera-tion; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a sys-tematic evaluation for analyzing few-shot per-formance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dra-matically outperform standard fine-tuning pro-cedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and do-main expertise, and hence constitutes a strong task-agnostic method for few-shot learning. [Cite_Footnote_2]"
  },
  {
    "id": 1464,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.quora.com/q/quoradata/",
    "section_title": "B Datasets",
    "add_info": "12 https://www.quora.com/q/quoradata/",
    "text": "For SNLI (Bowman et al., 2015) and datasets from GLUE (Wang et al., 2019), including SST-2 (Socher et al., 2013), CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), QNLI (Ra-jpurkar et al., 2016), RTE (Dagan et al., 2005; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MRPC (Dolan and Brock-ett, 2005), QQP [Cite_Footnote_12] and STS-B (Cer et al., 2017), we follow Zhang et al. (2021) and use their original development sets for testing. For datasets which re-quire a cross-validation evaluation\u2014MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), MPQA (Wiebe et al., 2005), Subj (Pang and Lee, 2004)\u2014we sim-ply randomly sample 2,000 examples as the testing set and leave them out from training. For SST-5 (Socher et al., 2013) and TREC (Voorhees and Tice, 2000), we use their official test sets. We show dataset statistics in Table B.1."
  },
  {
    "id": 1465,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/UKPLab/sentence-transformers",
    "section_title": "C Experimental Details - C.3 Fine-tuning with demonstrations",
    "add_info": "14 https://github.com/UKPLab/sentence-transformers",
    "text": "When using demonstrations, we sample 16 dif-ferent sets of demonstrations for each input and average the predicted log probability for each class during inference. We find that further increasing the number of samples does not bring substantial improvement. Additional, we have tried different aggregation methods like taking the result with the maximum confidence and we did not find a meaningful improvement. For selective demonstra-tions, we take roberta-large-nli-stsb mean-tokens [Cite_Footnote_14] from Reimers and Gurevych (2019) as our sentence embedding model."
  },
  {
    "id": 1466,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html",
    "section_title": "4 Experimental setup",
    "add_info": "2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html",
    "text": "We applied our improved model and Shimbo and Hara\u2019s original model to the EDR corpus (EDR, 1995). We also ran the Kurohashi-Nagao parser (KNP) 2.0 [Cite_Footnote_2] , a widely-used Japanese dependency parser to which Kurohashi and Nagao\u2019s (1994) rule-based coordination analysis method is built in. For comparison with KNP, we focus on bun-setsu-level coordinations. A bunsetsu is a chunk formed by a content word followed by zero or more non-content words like particles."
  },
  {
    "id": 1467,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www2.nict.go.jp/r/r312/EDR/index.html",
    "section_title": "4 Experimental setup",
    "add_info": "EDR, 1995. The EDR dictionary. NICT. http://www2.nict.go.jp/r/r312/EDR/index.html.",
    "text": "We applied our improved model and Shimbo and Hara\u2019s original model to the EDR corpus (EDR, 1995)  . We also ran the Kurohashi-Nagao parser (KNP) 2.0 , a widely-used Japanese dependency parser to which Kurohashi and Nagao\u2019s (1994) rule-based coordination analysis method is built in. For comparison with KNP, we focus on bun-setsu-level coordinations. A bunsetsu is a chunk formed by a content word followed by zero or more non-content words like particles."
  },
  {
    "id": 1468,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/donnyslin/KCAT",
    "section_title": "References",
    "add_info": "1 Code is available at https://github.com/donnyslin/KCAT",
    "text": "Fine-grained Entity Typing is a tough task which suffers from noise samples extracted from distant supervision. Thousands of man-ually annotated samples can achieve greater performance than millions of samples gen-erated by the previous distant supervision method. Whereas, it\u2019s hard for human be-ings to differentiate and memorize thousands of types, thus making large-scale human la-beling hardly possible. In this paper, we in-troduce a Knowledge-Constraint Typing An-notation Tool (KCAT [Cite_Footnote_1] ), which is efficient for fine-grained entity typing annotation. KCAT reduces the size of candidate types to an ac-ceptable range for human beings through en-tity linking and provides a Multi-step Typ-ing scheme to revise the entity linking result. Moreover, KCAT provides an efficient Anno-tator Client to accelerate the annotation pro-cess and a comprehensive Manager Module to analyse crowdsourcing annotations. Experi-ment shows that KCAT can significantly im-prove annotation efficiency, the time consump-tion increases slowly as the size of type set ex-pands."
  },
  {
    "id": 1469,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://translate.google.com/",
    "section_title": "4 The Newsela Cross-Lingual Simplification Dataset 4.1 Cross-Lingual Segment Alignment",
    "add_info": "2 https://translate.google.com/",
    "text": "As a result, we adapt a monolingual text simpli-fication aligner for cross-lingual alignment. MAS-SAlign (Paetzold et al., 2017) is a Python li-brary designed to align segments of different length within comparable corpora of the same lan-guage. It employs a vicinity-driven search ap-proach, based on the assumption that the order in which information appears is roughly constant in simple and complex texts. A similarity ma-trix is created between the paragraphs/sentences of aligned documents/paragraphs using a standard bag-of-words TF-IDF model. It finds a starting point to begin the search for an alignment path, allowing long-distance alignment skips, capturing 1-N and N-1 alignments. To leverage this align-ment flexibility, we apply MASSAlign to English articles and Spanish articles machine translated into English by Google translate. [Cite_Footnote_2] An important property of Google translated articles is that they are aligned 1-1 at the sentence level. This lets us deterministically find the Spanish replacement for the aligned Google translated English version re-turned by MASSAlign. Translation quality is high for this language pair, and even noisy translated ar-ticles contain enough signal to construct the simi-larity matrix required by MASSAlign."
  },
  {
    "id": 1470,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/cocoxu/simplification",
    "section_title": "5 Experiment Settings 5.1 Evaluation Metrics",
    "add_info": "3 https://github.com/cocoxu/simplification",
    "text": "SARI (Xu et al., 2016) [Cite_Footnote_3] is designed to evalu-ate text simplification systems by comparing sys-tem output against references and against the input sentence. It explicitly measures the goodness of words that are added, deleted and kept by the sys-tems. Xu et al. (2016) showed that BLEU shows high correlation with human scores for grammati-cality and meaning preservation and SARI shows high correlation with human scores for simplic-ity. In the cross-lingual setting, we cannot directly compare the Spanish input with English hypothe-ses and references, therefore we use the baseline machine translation of Spanish into English as a pseudo-source text. The resulting SARI score directly measures the improvement over baseline machine translation."
  },
  {
    "id": 1471,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/mmautner/readability",
    "section_title": "5 Experiment Settings 5.1 Evaluation Metrics",
    "add_info": "4 https://github.com/mmautner/readability",
    "text": "In addition to BLEU and SARI, we report Pear-son\u2019s correlation coefficient (PCC) to measure the strength of the linear relationship between the complexity of our system outputs and the com-plexity of reference translations. Heilman et al. (2008) use it to evaluate the performance of read-ing difficulty prediction. Here we estimate the reading grade level complexity of MT outputs and reference translations using the Automatic Read-ability Index (ARI) [Cite_Footnote_4] score, which combines evi-dence from the number of characters per word and number of words per sentence using hand-tuned weights (Senter and Smith, 1967):"
  },
  {
    "id": 1472,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/manuyavuz/TurkishMDSDataSet_alpha",
    "section_title": "4 Experimental Setup 4.1 Data Set",
    "add_info": "1 The data set can be retrieved from the following github repository: https://github.com/manuyavuz/TurkishMDSDataSet_alpha",
    "text": "One of the greatest challenges for MDS studies in Turk-ish is that there does not exist a manually annotated data set. In this study, we have collected and manually annotated a Turkish MDS data set, which is publicly available for future studies [Cite_Footnote_1] ."
  },
  {
    "id": 1473,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/chrishokamp/constrained_decoding",
    "section_title": "3 Grid Beam Search",
    "add_info": "1 Our implementation of GBS is available at https://github.com/chrishokamp/constrained_decoding",
    "text": "The beams at the top level of the grid (beams where c = numConstraints) contain hypothe-ses which cover all of the constraints. Once a hy-pothesis on the top level generates the EOS token, it can be added to the set of finished hypotheses. The highest scoring hypothesis in the set of fin-ished hypotheses is the best sequence which cov-ers all constraints. [Cite_Footnote_1]"
  },
  {
    "id": 1474,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.opensubtitles.org/",
    "section_title": "A.3 English-Portuguese",
    "add_info": "5 http://www.opensubtitles.org/",
    "text": "Our English-Portuguese training corpus consists of 28.5 Million segments from the Europarl, JRC-Aquis (Steinberger et al., 2006) and OpenSubti-tles [Cite_Footnote_5] corpora."
  },
  {
    "id": 1475,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/data/glove.840B.300d.zip",
    "section_title": "4 Experiment 4.1 Implementation Details",
    "add_info": "2 Downloaded from http://nlp.stanford.edu/data/glove.840B.300d.zip.",
    "text": "We use the tokenizer from Stanford CoreNLP (Manning et al., 2014) to preprocess each passage and question. The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used through-out our model. For word embedding, we use pre-trained case-sensitive GloVe embeddings [Cite_Footnote_2] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com-pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas-sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment. The hidden vec-tor length is set to 75 for all layers. The hidden size used to compute attention scores is also 75. We also apply dropout (Srivastava et al., 2014) be-tween layers with a dropout rate of 0.2. The model is optimized with AdaDelta (Zeiler, 2012) with an initial learning rate of 1. The \u03c1 and used in AdaDelta are 0.95 and 1e \u22126 respectively."
  },
  {
    "id": 1476,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://stanford-qa.com",
    "section_title": "4 Experiment 4.2 Main Results",
    "add_info": "3 Downloaded from http://stanford-qa.com",
    "text": "Two metrics are utilized to evaluate model perfor-mance: Exact Match (EM) and F1 score. EM measures the percentage of the prediction that matches one of the ground truth answers exactly. F1 measures the overlap between the prediction and ground truth answers which takes the max-imum F1 over all of the ground truth answers. The scores on dev set are evaluated by the offi-cial script [Cite_Footnote_3] . Since the test set is hidden, we are re-quired to submit the model to Stanford NLP group to obtain the test scores."
  },
  {
    "id": 1477,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://stanford-qa.com",
    "section_title": "4 Experiment 4.2 Main Results",
    "add_info": "4 Extracted from SQuAD leaderboard http://stanford-qa.com on Feb. 6, 2017.",
    "text": "Table 2 shows exact match and F1 scores on the dev and test set of our model and competing ap-proaches [Cite_Footnote_4] . The ensemble model consists of 20 training runs with the identical architecture and hyper-parameters. At test time, we choose the an-swer with the highest sum of confidence scores amongst the 20 runs for each question. As we can see, our method clearly outperforms the baseline and several strong state-of-the-art systems for both single model and ensembles."
  },
  {
    "id": 1478,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://aka.ms/Splash_dataset",
    "section_title": "References",
    "add_info": null,
    "text": "We study the task of semantic parse correction with natural language feedback. Given a natu-ral language utterance, most semantic parsing systems pose the problem as one-shot transla-tion where the utterance is mapped to a cor-responding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feed-back to correct the system when it generates an inaccurate interpretation of an initial utter-ance. We focus on natural language to SQL systems and construct, SPLASH , a dataset of ut-terances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the cor-rection task and show that incorporating such a rich form of feedback can significantly im-prove the overall semantic parsing accuracy while retaining the flexibility of natural lan-guage interaction. While we estimated hu-man correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at  https://aka.ms/Splash_dataset."
  },
  {
    "id": 1479,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/rshin/seq2struct",
    "section_title": "3 Dataset Construction 3.1 Generating Questions and Incorrect SQL Pairs",
    "add_info": "1 https://github.com/rshin/seq2struct",
    "text": "To generate erroneous SQL interpretations of questions in Spider, we opted for using the output of a text-to-SQL parser to ensure that our dataset reflect the actual distribution of errors that contem-porary parsers make. This is a more realistic setup than artificially infusing errors in the gold SQL. We use the Seq2Struct parser (Shin, 2019) [Cite_Footnote_1] to generate erroneous SQL interpretations. Seq2Struct com-bines grammar-based decoder of Yin and Neubig (2017) with a self-attention-based schema encod-ing and it reaches a parsing accuracy of 42.94% on the development set of Spider."
  },
  {
    "id": 1480,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/cosmozhang/Modular_Neural_CRF",
    "section_title": "6 Experimental Evaluation 6.2 Q1: Monolithic vs. Modular Learning",
    "add_info": "4 https://github.com/cosmozhang/Modular_Neural_CRF",
    "text": "Target Sentiment task The results are summa-rized in Tab. 1. We also compared our models with recently published state-of-the-art models on these datasets. To help ensure a fair comparison with Ma et al. which does not use inference, we also included the results of our model without the CRF layer (denoted LSTM-Ti(g)). All of our models beat the state-of-the-art results by a large margin. The source code and experimental setup are avail-able online [Cite_Footnote_4] ."
  },
  {
    "id": 1481,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu/",
    "section_title": "3 Models 3.1 Latent variable context models",
    "add_info": "2 We use the estimation methods provided by the MAL-LET toolkit, available from http://mallet.cs.umass.edu/.",
    "text": "In this paper we train LDA models of P (w|c) and P (c|w). In the former case, the analogy to document modelling is that each context type plays the role of a \u201cdocument\u201d consisting of all the words observed in that context in a corpus; for P (c|w) the roles are reversed. The models are trained by Gibbs sampling using the efficient procedure of Yao et al. (2009). The empirical estimates for distributions over words and latent variables are derived from the assignment of topics over the training corpus in a single sam-pling state. For example, to model P(w|c) we cal-culate: where f zw is the number of words of type w as-signed topic z, f zc is the number of times z is associ-ated with context c, f z\u00b7 and f \u00b7c are the marginal topic and context counts respectively, N is the number of word types and \u03b1 and \u03b2 parameterise the Dirichlet prior distributions over P (z|c) and P (w|z). Follow-ing the recommendations of Wallach et al. (2009) we use asymmetric \u03b1 and symmetric \u03b2; rather than using fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method. [Cite_Footnote_2] We use standard set-tings for the number of training iterations (1000), the length of the burnin period before hyperparameter estimation begins (200 iterations) and the frequency of hyperparameter estimation (50 iterations)."
  },
  {
    "id": 1482,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.nlpado.de/\u02dcsebastian/sigf.html",
    "section_title": "5 Experiment 2: Lexical substitution 5.1 Data",
    "add_info": "5 We use the software package available at http://www.nlpado.de/\u02dcsebastian/sigf.html.",
    "text": "Previous authors have partitioned the dataset in various ways. Erk and Pado\u0301 (2008) use only a sub-set of the data where the target is a noun headed by a verb or a verb heading a noun. Thater et al. (2010) discard sentences which their parser cannot parse and paraphrases absent from their training cor-pus and then optimise the parameters of their model through four-fold cross-validation. Here we aim for complete coverage on the dataset and do not perform any parameter tuning. We use two measures to eval-uate performance: Generalised Averaged Precision (Kishida, 2005) and Kendall\u2019s \u03c4 b rank correlation coefficient, which were used for this task by Thater et al. (2010) and Dinu and Lapata (2010), respec-tively. Generalised Averaged Precision (GAP) is a precision-like measure for evaluating ranked pre-dictions against a gold standard. \u03c4 b is a variant of Kendall\u2019s \u03c4 that is appropriate for data containing tied ranks. We do not use the \u201cprecision out of ten\u201d measure that was used in the original Lexical Substi-tution Task; this measure assigns credit for the pro-portion of the first 10 proposed paraphrases that are present in the gold standard and in the context of ranking attested substitutes it is unclear how to ob-tain non-trivial results for target words with 10 or fewer possible substitutes. We calculate statistical significance of performance differences using strati-fied shuffling (Yeh, 2000). [Cite_Footnote_5]"
  },
  {
    "id": 1483,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html",
    "section_title": "3 Features in CRFs",
    "add_info": "6 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html",
    "text": "We use an array of features in CRFs which are ei-ther derived or borrowed from the taxonomy that a Japanese tokenizer called JUMAN and KNP , [Cite_Footnote_6] a Japanese dependency parser (aka Kurohashi-Nagao Parser), make use of in characterizing the output they produce: both JUMAN and KNP are part of the compression model we build."
  },
  {
    "id": 1484,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu",
    "section_title": "5 Evaluation Setup",
    "add_info": "Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.",
    "text": "We extracted lead sentences both from the brief and from its source article, and aligned them, us-ing what is known as the Smith-Waterman algorithm (Smith and Waterman, 1981), which produced 1,401 pairs of summary and source sentence. For the ease of reference, we call the corpus so produced \u2018NICOM\u2019 for the rest of the paper. A part of our sys-tem makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006  ). Throughout the experiments, we call our approach \u2018Generic Sen-tence Trimmer\u2019 or GST."
  },
  {
    "id": 1485,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.isi.edu/\u02dcoch/YASMET.html",
    "section_title": "3 Adjacency Pairs 3.4 Results",
    "add_info": "5 http://www.isi.edu/\u02dcoch/YASMET.html",
    "text": "We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training. To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET. [Cite_Footnote_5]"
  },
  {
    "id": 1486,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.isi.edu/\u02dcravichan/YASMET.html",
    "section_title": "3 Adjacency Pairs 3.4 Results",
    "add_info": "6 http://www.isi.edu/\u02dcravichan/YASMET.html",
    "text": "Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smooth-ing and using incremental feature selection as de-scribed in (Berger et al., 1996) and implemented in the yasmetFS package. [Cite_Footnote_6] Note that restricting our-selves to only backward looking features decreases the performance significantly, as we can see in Ta-ble 2."
  },
  {
    "id": 1487,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/serenayj/ABCD-ACL2021",
    "section_title": "1 Introduction",
    "add_info": "1 ABCD is available at https://github.com/serenayj/ABCD-ACL2021.",
    "text": "The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. [Cite_Footnote_1]"
  },
  {
    "id": 1488,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/serenayj/DeSSE",
    "section_title": "3 Datasets 3.1 DeSSE",
    "add_info": "2 DeSSE and MinWiki are available at https://github.com/serenayj/DeSSE.",
    "text": "DeSSE is collected in an undergraduate social science class, where students watched video clips about race relations, and wrote essays in a blog environment to share their opinions with the class. It was created to support analysis of student writ-ing, so that different kinds of feedback mechanisms can be developed regarding sentence organization. Students have difficulty with revision to address lack of clarity in their writing (Kuhn et al., 2016), such as non-specific uses of connectives, run on sentences, repetitive statements and the like. These make DeSSE different from corpus with expert written text, such as Wikipedia and newspaper. The annotation process is unique in that it involves iden-tifying where to split a source complex sentence into distinct clauses, and how to rephrase each re-sulting segment as a semantically complete simple sentence, omitting any discourse connectives. It differs from corpora that identify discourse units within sentences, such as RST-DT (Carlson et al., 2003) and PTDB (Prasad et al., 2008), because clauses are explicitly rewritten as simple sentences. It differs from split-and-rephrase corpora such as MinWikiSplit, because of the focus in DeSSE on rephrased simple sentences that have a one-to-one correspondence to tensed clauses in the original complex sentence. DeSSE is also used for connec-tive prediction tasks, as in (Gao et al., 2021). [Cite_Footnote_2]"
  },
  {
    "id": 1489,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.informatik.uni-trier.de/\u223cley/db/",
    "section_title": "1 Introduction",
    "add_info": "2 http://www.informatik.uni-trier.de/\u223cley/db/",
    "text": "We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP [Cite_Footnote_2] -record/citation-text pairs and global GE criteria of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008). Such sys-tems achieve very high accuracy close to 90% F1 on semi-structured domains similar to ours. Our trained alignment model can be used to directly align new record-text pairs to create a labeling of the texts. Empirical results demonstrate a 20.6% error reduction in token labeling accuracy com-pared to a strong baseline method that employs a set of high-precision alignments. Furthermore, we provide a 63.8% error reduction compared to IBM Model 4 (Brown et al., 1993). Alignments learned by our model are used to train a linear-chain CRF extractor. We obtain an error reduction of 35.1% over a previous state-of-the-art extraction method that uses heuristically generated alignments."
  },
  {
    "id": 1490,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt",
    "section_title": "1 Introduction",
    "add_info": "3 Expectation criteria used in our experiments are listed at http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt.",
    "text": "We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP -record/citation-text pairs and global GE criteria [Cite_Footnote_3] of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008). Such sys-tems achieve very high accuracy close to 90% F1 on semi-structured domains similar to ours. Our trained alignment model can be used to directly align new record-text pairs to create a labeling of the texts. Empirical results demonstrate a 20.6% error reduction in token labeling accuracy com-pared to a strong baseline method that employs a set of high-precision alignments. Furthermore, we provide a 63.8% error reduction compared to IBM Model 4 (Brown et al., 1993). Alignments learned by our model are used to train a linear-chain CRF extractor. We obtain an error reduction of 35.1% over a previous state-of-the-art extraction method that uses heuristically generated alignments."
  },
  {
    "id": 1491,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.cs.umass.edu/\u223ckedarb/dbie",
    "section_title": "4 Experiments",
    "add_info": "6 The data set can be found at http://www.cs.umass.edu/\u223ckedarb/dbie cite data.sgml.",
    "text": "For each DBLP record we searched on the web for matching citation texts using the first author\u2019s last name and words in the title. Each citation text found is manually labeled for evaluation purposes. An example of a matching DBLP record-citation text pair is shown in Table 3. Our data set [Cite_Footnote_6] con-tains 522 record-text pairs for 260 DBLP entries."
  },
  {
    "id": 1492,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt",
    "section_title": "4 Experiments",
    "add_info": "8 A complete list of expectation criteria is available at http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt.",
    "text": "As is common practice (Haghighi and Klein, 2006; Mann and McCallum, 2008), we simulate user-specified expectation criteria through statis-tics on manually labeled citation texts. For ex-traction criteria, we select for each label, the top N extraction features ordered by mutual informa-tion (MI) with that label. Also, we aggregate the alignment features of record tokens whose align-ment with a target text token results in a correct label assignment. The top N alignment features that have maximum MI with this correct label-ing are selected as alignment criteria. We bin tar-get expectations of these criteria into 11 bins as [0.05, 0.1, 0.2, 0.3, . . . , 0.9, 0.95]. In our exper-iments, we set N = 10 and use a fixed weight w = 10.0 for all expectation criteria (no tuning of parameters was performed). Table 4 shows a sample of GE criteria used in our experiments. [Cite_Footnote_8]"
  },
  {
    "id": 1493,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "http://www.wordgumbo.com/ie/cmp/iedata.txt",
    "section_title": "1 Introduction 1.1 Related Work",
    "add_info": "1 http://www.wordgumbo.com/ie/cmp/iedata.txt",
    "text": "According to Campbell (2003), the methods based on comparisons of cognate lists and sound corre-spondences are the most popular approaches em-ployed for establishing relationships between lan-guages. Barbanc\u0327on et al. (2013) emphasize the va-riety of computational methods used in this field, and state that the differences in datasets and ap-proaches cause difficulties in the evaluation of the results regarding the reconstruction of the phylo-genetic tree of languages. Linguistic phylogeny reconstruction proves especially useful in histor-ical and comparative linguistics, as it enables the analysis of language evolution. Ringe et al. (2002) propose a computational method for evolutionary tree reconstruction based on a \u201cperfect phylogeny\u201d algorithm; using a Bayesian phylogeographic ap-proach, Alekseyenko et al. (2012), continuing the work of Atkinson et al. (2005), model the expan-sion of the Indo-European language family and find support for the hypothesis which places its homeland in Anatolia; Atkinson and Gray (2006) analyze language divergence dates and argue for the usage of computational phylogenetic meth-ods in the question of Indo-European age and ori-gins. Using modified versions of Swadesh\u2019s lists [Cite_Footnote_1] , Dyen et al. (1992) investigate the classification of Indo-European languages by applying a lexicosta-tistical method."
  },
  {
    "id": 1494,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://dexonline.ro",
    "section_title": "2 Methodology and Algorithm 2.2 Algorithm 2.2.1 Preprocessing",
    "add_info": "3 http://dexonline.ro",
    "text": "Step 3. Lemmatization. We use lemmas for identifying words\u2019 definitions in dictionaries and for computing adequate distances between words and their cognates or etymons. We use the Dex-online [Cite_Footnote_3] machine-readable dictionary to lemmatize Romanian words."
  },
  {
    "id": 1495,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.sapere.it/sapere/dizionari",
    "section_title": "2 Methodology and Algorithm 2.2 Algorithm 2.2.2 Relationships Identification",
    "add_info": "4 Italian: http://www.sapere.it/sapere/dizionari",
    "text": "In our previous work (Ciobanu and Dinu, 2014a) we applied this method on a Romanian dic-tionary, while here we extract cognates from Ro-manian corpora. We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries [Cite_Footnote_4] to extract etymology-related information and Google Trans-late to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well. We selected these six languages for the following reason: the first four in our list are Romance languages, and our intuition is that there are numerous words in these languages which share a common ancestor with Romanian words. We investigate the cog-nate pairs for Turkish because many French words were imported in both Romanian and Turkish in the 19 th century, and we believe that accounting for Romanian-Turkish cognates would provide a more accurate result for the similarity of these lan-guages. As for English, we decided to investigate the cognate pairs for this language in order to ana-lyze to what extent the influence of English on Ro-manian increases across time. In Table 2 we report the number of Romanian words having an etymon or a cognate pair in the six related languages."
  },
  {
    "id": 1496,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://translate.google.com",
    "section_title": "2 Methodology and Algorithm 2.2 Algorithm 2.2.2 Relationships Identification",
    "add_info": "5 http://translate.google.com",
    "text": "In our previous work (Ciobanu and Dinu, 2014a) we applied this method on a Romanian dic-tionary, while here we extract cognates from Ro-manian corpora. We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries to extract etymology-related information and Google Trans-late [Cite_Footnote_5] to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well. We selected these six languages for the following reason: the first four in our list are Romance languages, and our intuition is that there are numerous words in these languages which share a common ancestor with Romanian words. We investigate the cog-nate pairs for Turkish because many French words were imported in both Romanian and Turkish in the 19 th century, and we believe that accounting for Romanian-Turkish cognates would provide a more accurate result for the similarity of these lan-guages. As for English, we decided to investigate the cognate pairs for this language in order to ana-lyze to what extent the influence of English on Ro-manian increases across time. In Table 2 we report the number of Romanian words having an etymon or a cognate pair in the six related languages."
  },
  {
    "id": 1497,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://nlp.unibuc.ro/resources/rosim.pdf",
    "section_title": "3 Experiments and Results 3.1 Romanian Evolution 3.1.2 Results",
    "add_info": "6 The complete ranking of similarity is available online at http://nlp.unibuc.ro/resources/rosim.pdf.",
    "text": "In this subsection we present and analyze the main results drawn from our research. In Table 3 we list the output of our method for each corpus, with and without diacritics [Cite_Footnote_6] . We report the similarity between Romanian and related languages, provid-ing the average value of the three metrics used. In the %words column we provide, for each corpus, the percentage of words having an etymon or a cognate pair in a given language (the typical mea-sure used in lexicostatistical comparison, i.e., the 0 distance function). The results for the Romanian datasets are plotted in Figure 4 and Figure 5."
  },
  {
    "id": 1498,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://cnts.uia.ac.be/conll2000/",
    "section_title": "5 Classifier Features 5.4 Word class features",
    "add_info": "2 http://cnts.uia.ac.be/conll2000/",
    "text": "Syntactic-chunk label: The value of this feature is a B-I-O style label indicating what kind of syntactic chunk the word is contained in, e.g. noun phrase, verb phrase, or prepositional phrase. These are assigned using a word-chunking SVM-based system trained on the CoNLL-2000 data [Cite_Footnote_2] (which uses the lowest nodes of the Penn TreeBank syntactic trees to break sentences into base phrases)."
  },
  {
    "id": 1499,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cs.ualberta.ca/~lindek/minipar.htm",
    "section_title": "5 Classifier Features 5.5 Governing features",
    "add_info": "3 http://www.cs.ualberta.ca/~lindek/minipar.htm",
    "text": "These features attempt to include some simple dependency information from the surrounding words, using the dependency parses produced by Minipar [Cite_Footnote_3] . These features aim to identify events that are expressed as phrases or that require knowledge of the surrounding phrase to be iden-tified."
  },
  {
    "id": 1500,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://timex2.mitre.org/tern.html",
    "section_title": "5 Classifier Features 5.6 Temporal features",
    "add_info": "4 http://timex2.mitre.org/tern.html",
    "text": "Time chunk label: The value of this feature is a B-I-O label indicating whether or not this word is contained in a temporal annotation. The temporal annotations are produced by a word-chunking SVM-based system trained on the temporal ex-pressions (TIMEX2 annotations) in the TERN 2004 data [Cite_Footnote_4] . In addition to identifying expres-sions like Monday and this year, the TERN data identifies event-containing expressions like the time she arrived at her doctor's office."
  },
  {
    "id": 1501,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.org/~taku/software/TinySVM/",
    "section_title": "6 Classifier Parameters",
    "add_info": "6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/",
    "text": "The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [Cite_Footnote_6] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking."
  },
  {
    "id": 1502,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.org/~taku/software/yamcha/",
    "section_title": "6 Classifier Parameters",
    "add_info": "6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/",
    "text": "The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [Cite_Footnote_6] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking."
  },
  {
    "id": 1503,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ybz79/AMR2text",
    "section_title": "4 Experiments 4.1 Settings",
    "add_info": "3 Our code is available at https://github.com/ybz79/AMR2text",
    "text": "Data and preprocessing We conduct our experi-ments with two benchmark datasets: LDC2015E85 and LDC2017T10. The two datasets contain 16833 and 36521 training samples, and they use a common development set with 1368 samples and a common test set with 1371 samples. We segment natural words in both AMR graphs and references into sub-words. As a result, a word node in AMR graphs may be divided into several sub-word nodes. We use a special edge subword to link the corresponding sub-word nodes. Then, for each AMR graph, we find its correspond-ing line graph and generate G c and G e respectively. Training details For model parameters, the number of graph encoding layers is fixed to 6, and the representation dimension d is set to 512. We set the graph neighborhood order K = 1,2 and 4 for both G c and G e . The Transformer decoder is based on Open-NMT (Klein et al., 2018), with 6 layers, 512 dimensions and 8 heads. We use Adam (Kingma and Ba, 2015) as our optimizer and \u03b2 = (0.9,0.98). The learning rate is varied over the course of training, similar with Vaswani et al. (2017): where t denotes the accumulative training steps, and w indicates the warmup steps. We use w = 16000 and the coefficient \u03b3 is set to 0.75. As for batch size, we use 80 for LDC2015E86 and 120 for LDC2017T10. [Cite_Footnote_3]"
  },
  {
    "id": 1504,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://www.perspectiveapi.com",
    "section_title": "2 Related Work",
    "add_info": "1 https://www.perspectiveapi.com",
    "text": "Adversarial examples are powerful tools to in-vestigate the vulnerabilities of a deep learning model (Szegedy et al., 2014). While this line of research has recently received a lot of attention in the deep learning community, it has a long history in machine learning, going back to adversarial at-tacks on linear spam classifiers (Dalvi et al., 2004; Lowd and Meek, 2005). Hosseini et al. (2017) show that simple modifications, such as adding spaces or dots between characters, can drasti-cally change the toxicity score from Google\u2019s perspective API [Cite_Footnote_1] . Belinkov and Bisk (2018) show that character-level machine translation sys-tems are overly sensitive to random character ma-nipulations, such as keyboard typos. They manipu-late every word in a sentence with synthetic or nat-ural noise. However, throughout our experiments, we care about the degree of distortion in a sen-tence, and look for stronger adversaries which can increase the loss within a limited budget. Instead of randomly perturbing text, we propose an effi-cient method, which can generate adversarial text using the gradients of the model with respect to the input."
  },
  {
    "id": 1505,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.di.unipi.it/\u02dcgulli/",
    "section_title": "4 Experiments",
    "add_info": "3 https://www.di.unipi.it/\u02dcgulli/",
    "text": "In principle, HotFlip could be applied to any dif-ferentiable character-based classifier. Here, we fo-cus on the CharCNN-LSTM architecture (Kim et al., 2016), which can be adapted for classifica-tion via a single dense layer after the last recur-rent hidden unit. We use the AG\u2019s news dataset [Cite_Footnote_3] , which consists of 120,000 training and 7,600 test instances from four equal-sized classes: World, Sports, Business, and Science/Technology. The ar-chitecture consists of a 2-layer stacked LSTM with 500 hidden units, a character embedding size of 25, and 1000 kernels of width 6 for temporal convolutions. This classifier was able to outper-form (Conneau et al., 2017), which has achieved the state-of-the-art result on some benchmarks, on AG\u2019s news. The model is trained with SGD and gradient clipping, and the batch size was set to 64. We used 10% of the training data as the develop-ment set, and trained for a maximum of 25 epochs. We only allow character changes if the new word does not exist in the vocabulary, to avoid changes that are more likely to change the meaning of text. The adversary uses a beam size of 10, and has a budget of maximum of 10% of characters in the document. In Figure 1, we plot the success rate of the adversary against an acceptable confidence score for the misclassification. That is, we con-sider the adversary successful only if the classifier misclassifies the instance with a given confidence score. For this experiment, we create adversarial examples for 10% of the test set."
  },
  {
    "id": 1506,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/teapot123/JASen",
    "section_title": "References",
    "add_info": "1 Our code and data are available at https://github.com/teapot123/JASen.",
    "text": "Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this pa-per, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each as-pect/sentiment without using any labeled ex-amples. Existing methods are either designed only for one of the sub-tasks, neglecting the benefit of coupling both, or are based on topic models that may contain overlapping concepts. We propose to first learn hsentiment, aspecti joint topic embeddings in the word embedding space by imposing regularizations to encour-age topic distinctiveness, and then use neural models to generalize the word-level discrim-inative information by pre-training the clas-sifiers with embedding-based predictions and self-training them on unlabeled data. Our com-prehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets [Cite_Footnote_1] ."
  },
  {
    "id": 1507,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://www.yelp.com/dataset/challenge",
    "section_title": "5 Evaluation 5.1 Experimental Setup",
    "add_info": "2 https://www.yelp.com/dataset/challenge",
    "text": "\u2022 Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge [Cite_Footnote_2] . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment po-larity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it)."
  },
  {
    "id": 1508,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.nltk.org/",
    "section_title": "5 Evaluation 5.1 Experimental Setup",
    "add_info": "3 https://www.nltk.org/",
    "text": "Preprocessing and Hyperparameter Setting. To preprocess the training corpus D, we use the word tokenizer provided by NLTK [Cite_Footnote_3] . We also use a phrase mining tool, AutoPhrase (Shang et al., 2017), to extract meaningful phrases such as \u201cgreat wine\u201d and \u201cnumeric keypad\u201d such that they can capture complicated semantics in a single text unit. We use the benchmark validation set to fine-tune the hy-perparameters: embedding dimension = 100, local context window size h = 5, \u03bb g = 2.5, \u03bb r = 1, training epoch = 5. For neural model pre-training, we set T = 20. A CNN model is trained for each sub-task: aspect extraction and sentiment classifi-cation. Each model uses 20 feature maps for filters with window sizes of 2, 3, and 4. SGD is used with 1e \u2212 3 as the learning rate in both pre-training and self-training and the batch size is set to 16."
  },
  {
    "id": 1509,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ictnlp/GS4NMT",
    "section_title": "4 Experiment 4.1 Settings",
    "add_info": "1 Experiment code: https://github.com/ictnlp/GS4NMT",
    "text": "We carry out experiments on Chinese-to-English translation. [Cite_Footnote_1] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped. We use NIST 2002 (MT 02) as the vali-dation set and NIST 2003-2006 (MT 03-08) as the test sets. We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the transla-tion task."
  },
  {
    "id": 1510,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip",
    "section_title": "4 Experiments 4.1 Session Based Task Oriented Parsing",
    "add_info": null,
    "text": "We open source SB-TOP in the follow-ing link:  http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip. More information about the dataset can be found in the Table ?? in the Appendix."
  },
  {
    "id": 1511,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://wordnet.princeton.edu/glosstag.shtml",
    "section_title": "2 Semantic Representation of Concepts 2.1 Collecting contextual information",
    "add_info": "2 http://wordnet.princeton.edu/glosstag.shtml",
    "text": "Figure 1 illustrates the process of obtaining a set of relevant Wikipedia pages T b as contextual informa-tion for a given concept b = (p,s). Let L p be the set containing p and all the Wikipedia pages hav-ing an outgoing link to p, and R s be the set con-sisting of s and all other synsets that are in its di-rect neighbourhood. We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [Cite_Footnote_2] . Let B be a function mapping each WordNet synset s 0 to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise. Hence, B(R s ) = \u222a s 0 \u2208R s B(s 0 ). Then, our con-textual information is the set of Wikipedia pages T b = L p \u222a B(R s ). In the case either p or s is not present in the concept b, we take the contextual in-formation as T b = B(R s ) or T b = L p , respectively."
  },
  {
    "id": 1512,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://lcl.uniroma1.it/nasari/",
    "section_title": "6 Conclusions",
    "add_info": null,
    "text": "In this paper we presented a novel semantic approach, called N ASARI , for effective vector representation of arbitrary WordNet synsets and Wikipedia pages. The strength of our approach lies in its combination of complementary knowl-edge from different types of resource, while at the same time it also benefits from an effective vec-tor representation with two novel features: lexi-cal specificity for the calculation of vector weights and a semantically-aware dimensionality reduc-tion. N ASARI attains state-of-the-art performance on multiple standard benchmarks in word similarity as well as Wikipedia sense clustering. We release the representations obtained for all the Wikipedia pages and WordNet synsets in  http://lcl.uniroma1.it/nasari/. As future work we plan to integrate N ASARI into BabelNet and apply our representation to a mul-tilingual setting, enabling the comparison of pairs of concepts across languages. We also intend to use our approach on the task of multilingual Word Sense Disambiguation."
  },
  {
    "id": 1513,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.globalWordnet.org/gwa/Wordnet_table.html",
    "section_title": "3 Experiments 3.1 Publicly available Wordnets",
    "add_info": "2 http://www.globalWordnet.org/gwa/Wordnet_table.html",
    "text": "The PWN is the oldest and the biggest available Wordnet. It is also free. Wordnets in many languages are being constructed and developed [Cite_Footnote_2] . However, only a few of these Wordnets are of high quality and free for downloading. The EuroWord-net (Vossen, 1998) is a multilingual database with Wordnets in European languages (e.g., Dutch, Ital-ian and Spanish). The AsianWordnet provides a platform for building and sharing Wordnets for Asian languages (e.g., Mongolian, Thai and Viet-namese). Unfortunately, the progress in building most of these Wordnets is slow and they are far from being finished."
  },
  {
    "id": 1514,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.asianWordnet.org/progress",
    "section_title": "3 Experiments 3.1 Publicly available Wordnets",
    "add_info": "3 http://www.asianWordnet.org/progress",
    "text": "The PWN is the oldest and the biggest available Wordnet. It is also free. Wordnets in many languages are being constructed and developed . However, only a few of these Wordnets are of high quality and free for downloading. The EuroWord-net (Vossen, 1998) is a multilingual database with Wordnets in European languages (e.g., Dutch, Ital-ian and Spanish). The AsianWordnet [Cite_Footnote_3] provides a platform for building and sharing Wordnets for Asian languages (e.g., Mongolian, Thai and Viet-namese). Unfortunately, the progress in building most of these Wordnets is slow and they are far from being finished."
  },
  {
    "id": 1515,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://compling.hss.ntu.edu.sg/omw/",
    "section_title": "3 Experiments 3.2 Experimental results and discussion",
    "add_info": "4 http://compling.hss.ntu.edu.sg/omw/",
    "text": "For the IWND approach, we use all [Cite_Footnote_4] Wordnets as intermediate resources. The number of Wordnet synsets we create using the IWND approach are presented in Table 4. We only construct Wordnet synsets for ajz, asm and dis using the IWND ap-proach because these languages are not supported by MT."
  },
  {
    "id": 1516,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://www.python.org/",
    "section_title": "1 Introduction 1.2 Design Goals",
    "add_info": "1 https://www.python.org/",
    "text": "\u2022 Minimal requirements on the underlying server structure: Sisyphus only requires Python 3 [Cite_Footnote_1] with a few basic packages and a Unix-type operating system."
  },
  {
    "id": 1517,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/jhclark/ducttape",
    "section_title": "2 Related Work",
    "add_info": "2 https://github.com/jhclark/ducttape",
    "text": "The toolkit that seems to be most similar to our approach is Ducttape [Cite_Footnote_2] , the successor of LonnyBin (Clark and Lavie, 2010). It is well designed and covers many useful points. However, we miss a more flexible configuration of the workflow e.g. workflows that adjust to the outputs of finished jobs are not supported. This does not allow to trig-ger parts of the workflow only if current computa-tions show that they are required."
  },
  {
    "id": 1518,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/rwth-i6/sisyphus",
    "section_title": "7 Conclusion",
    "add_info": "4 https://github.com/rwth-i6/sisyphus",
    "text": "We presented overview of our novel workflow manager Sisyphus. Features like automatic er-ror detection, efficient usage of computational re-sources, scalability, easy of reproducibility, abil-ity to share work with others have been proven to be extremely helpful for our research. The large collection of tools for Python can be used with-out modification for editing, debugging, and docu-menting the workflow, since it is written in Python. It is freely available online [Cite_Footnote_4] under the Mozilla License v2.0 to encourage the adoption by other groups."
  },
  {
    "id": 1519,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/zhanglu-cst/ClassKG",
    "section_title": "4 Experiments 4.3 Experimental Settings",
    "add_info": "1 https://github.com/zhanglu-cst/ClassKG",
    "text": "The training and evaluation are performed on NVIDIA RTX 2080Ti. In the subgraph annota-tor, we use a three-layer GIN (Xu et al., 2019). We first train it with our self-supervised task 10 6 iter-ations and then finetune it 10 epochs. We set the batch size of self-supervision/finetuning to 50/256. In classifier training, we set the batch size to 4/8 for long/short texts. Both the subgraph annotator and the text classifier use AdamW (Loshchilov and Hutter, 2019) as optimizer. Their learning rates are 1e-4 and 2e-6, respectively. The classifier uses bert-base-uncased for short texts and longformer-base-4096 for long texts. For keywords extraction, we select top 100 keywords per class in each it-eration. The hyperparameter M is set to 4. The keywords set change threshold is set to 0.1. Our code has already been released. [Cite_Footnote_1]"
  },
  {
    "id": 1520,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://fever.ai/",
    "section_title": "5 Related Work",
    "add_info": "4 http://fever.ai/",
    "text": "The majority of previous works deal with tex-tual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direc-tion, where evidence sentences come from 5.4 mil-lion Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. docu-ment retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More re-cently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking [Cite_Footnote_4] . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study abstractive summarization (Zhang et al., 2019)."
  },
  {
    "id": 1521,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://goo.gl/7KG2U",
    "section_title": "3 A Resource for German Derivation",
    "add_info": "1 Downloadable from: http://goo.gl/7KG2U",
    "text": "For German, there are several resources with derivational information. We use version 1.3 of DE RIV B ASE (Zeller et al., 2013), [Cite_Footnote_1] a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 non-singleton derivational families. It has a precision of 84% and a recall of 71%. Its higher coverage com-pared to C ELEX (Baayen et al., 1996) and IMSL EX (Fitschen, 2004) makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas."
  },
  {
    "id": 1522,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://goo.gl/bFokI",
    "section_title": "5 Experimental Evaluation",
    "add_info": "2 Downloadable from: http://goo.gl/bFokI",
    "text": "Experiments. We evaluate the impact of smooth-ing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German G UR 350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analo-gously to the well-known Rubenstein and Good-enough (1965) dataset for English. [Cite_Footnote_2] We predict semantic similarity as cosine similarity. We make a prediction for a word pair if both words are repre-sented in the semantic space and their vectors have a non-zero similarity."
  },
  {
    "id": 1523,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.daviddlewis.com/resources/",
    "section_title": "5 Results 5.1 Reuters-21578",
    "add_info": "Lewis, D. et al. (1987). Reuters-21578. http://www.daviddlewis.com/resources/",
    "text": "We used the \u201cModApte\u201d split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987)  . The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents D u ). Of the 135 poten-tial topic categories only the 10 most frequent cate-gories are used (Joachims, 1999). Categories outside the 10 most frequent were collapsed into one class and assigned a label \u201cother\u201d. For each document i in the training and test sets, we extract features x i in the following manner: stop-words are removed fol-lowed by the removal of case and information about inflection (i.e., stemming) (Porter, 1980). We then compute TFIDF features for each document (Salton and Buckley, 1987). All graphs were constructed us-ing cosine similarity with TFIDF features."
  },
  {
    "id": 1524,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "2 Probability Space",
    "add_info": "3 https://github.com/google-research/bert",
    "text": "Paragraph-level model In paragraph level models, we assume that for a given question against a document d, each of its paragraphs p 1 , . . . , p K independently selects a pair of answer positions (i k ,j k ), which are the begin and end of the answer from paragraph p k . In the case that p k does not support answering the question q, special NULL positions are selected (follow-ing the SQuAD 2.0 BERT implementation [Cite_Footnote_3] ). Thus, the set of possible outcomes \u2126 in the paragraph-level probability space is the set of lists of begin/end position pairs, one from each paragraph: {[(i 1 , j 1 ), . . . , (i K , j K )]}, where i k and j k range over positions in the respective paragraphs."
  },
  {
    "id": 1525,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/allenai/document-qa",
    "section_title": "5 Experiments 5.1 Data and Implementation",
    "add_info": "6 https://github.com/allenai/document-qa",
    "text": "Two datasets are used in this paper: TriviaQA (Joshi et al., 2017) in its Wikipedia formulation, and NarrativeQA (summaries setting) (Koc\u030cisky\u0301 et al., 2018). Using the same preprocessing as Clark and Gardner (2018) for TriviaQA-Wiki [Cite_Footnote_6] , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al. (2019), for NarrativeQA we define the possible answer string sets A using Rouge-L (Lin, 2004) similarity with crouwdsourced abstractive answer strings. We use identical data preprocessing and the evaluation script provided by the authors."
  },
  {
    "id": 1526,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt",
    "section_title": "5 Applying V-measure 5.1 Document Clustering",
    "add_info": "S. Strassel and M. Glenn. 2003. Creating the annotated tdt-4 y2003 evaluation corpus. http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt.",
    "text": "Clustering techniques have been used widely to sort documents into topic clusters. We reproduce such an experiment here to demonstrate the usefulness of V-measure. Using a subset of the TDT-4 cor-pus (Strassel and Glenn, 2003)  (1884 English news wire and broadcast news documents manually la-beled with one of 12 topics), we ran clustering experiments using k-means clustering (McQueen, 1967) and evaluated the results using V-Measure, VI and Q 0 \u2013 those measures that satisfied the de-sirable properties defined in section 4. The top-ics and relative distributions are as follows: Acts of Violence/War (22.3%), Elections (14.4%), Diplo-matic Meetings (12.9%), Accidents (8.75%), Natu-ral Disasters (7.4%), Human Interest (6.7%), Scan-dals (6.5%), Legal Cases (6.4%), Miscellaneous (5.3%), Sports (4.7), New Laws (3.2%), Science and Discovery (1.4%)."
  },
  {
    "id": 1527,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://turing.iimas.unam.mx/wix/MexSeg",
    "section_title": "3 Morphological Segmentation Datasets",
    "add_info": "2 Our datasets can be found to-gether with the code of our models at http://turing.iimas.unam.mx/wix/MexSeg.",
    "text": "Final splits. In order to make follow-up work on minimal-resource settings for morphological segmentation easily comparable, we provide pre-defined splits of our datasets [Cite_Footnote_2] . 40% of the data constitute the test sets. Of the remaining data, we use 20% for development and the rest for training. The final numbers of words per dataset and lan-guage are shown in Table 2."
  },
  {
    "id": 1528,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/nyu-mll/jiant/tree/bert-friends-exps",
    "section_title": "5 Models and Experimental Details",
    "add_info": "3 https://github.com/nyu-mll/jiant/tree/bert-friends-exps",
    "text": "We implement our models using the jiant toolkit, [Cite_Footnote_3] which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. Appendix A presents addi-tional details."
  },
  {
    "id": 1529,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "5 Models and Experimental Details",
    "add_info": "4 https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "We implement our models using the jiant toolkit, which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. [Cite_Footnote_4] Appendix A presents addi-tional details."
  },
  {
    "id": 1530,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://lucene.apache.org/java/",
    "section_title": "5 Experiments",
    "add_info": "2 http://lucene.apache.org/java/",
    "text": "We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [Cite_Footnote_2] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set. We also used a set of 340 QA-type sentence pairs from RTE02-03 and 195 pairs from RTE04 by convert-ing the hypothesis sentences into question form to create additional set of q/a pairs. In total, we cre-ated labeled training dataset X L of around 7600 q/a pairs . We evaluated the performance of graph-based QA system using a set of 202 questions from the TREC04 as testing dataset (Voorhees, 2003), (Prager et al., 2000). We retrieved around 20 can-didate sentences for each of the 202 test questions and manually labeled each q/a pair as true/false en-tailment to compile 4037 test data."
  },
  {
    "id": 1531,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/subhadarship/learning-to-unjumble",
    "section_title": "References",
    "add_info": "1 The code is available at https://github.com/subhadarship/learning-to-unjumble.",
    "text": "State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train models that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our approach. Applying random shuffling strategy on the word-level, we found that our approach enables the RoBERTa model achieve better performance on 4 out of 7 GLUE tasks. Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations. [Cite_Footnote_1]"
  },
  {
    "id": 1532,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/scripts",
    "section_title": "4 Experiments 4.2 Dataset for Shuffled-Token Detection",
    "add_info": "2 Timestamp May 9th, 2020. We used the https://github.com/scripts from NVIDIA/DeepLearningExamples/tree/ master/PyTorch/LanguageModeling/BERT# getting-the-data to extract the data.",
    "text": "We extracted 133K articles from Wikidump. [Cite_Footnote_2] We used each paragraph in the extracted text as a data sample for our model. We filtered out samples that were either spaces-only or had more than 512 tokens after tokenizing with the pretrained RobertaTokenizer of the roberta-base model. We finally randomly split the samples into 1.3M for training and 14K for validation."
  },
  {
    "id": 1533,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/huggingface/transformers/blob/v2.8.0/examples/run_glue.py",
    "section_title": "4 Experiments 4.4 Downstream Evaluation",
    "add_info": "4 The default hyperparameters are as in https://github.com/huggingface/transformers/blob/v2.8.0/examples/run_glue.py.",
    "text": "We evaluate our approach on 7 GLUE tasks us-ing the metrics outlined in Table 1. We use the same set of hyperparameters for fine-tuning for downstream tasks for each approach for a fair com-parison. Methods for comparison to our approach include (a) the baseline approach where the training objective is detecting masked tokens, and (b) the plain pre-trained RoBERTa base model. The values of hyperparameters used for GLUE fine-tuning are outlined in Table 2. The rest of the hyperparameters are set to default values. [Cite_Footnote_4]"
  },
  {
    "id": 1534,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://www.cs.ualberta.ca/\u02dcab31/langid/",
    "section_title": "5 Application to machine transliteration 5.1 Data",
    "add_info": "1 Our tagged data are available online at http://www.cs.ualberta.ca/\u02dcab31/langid/.",
    "text": "The English-Hindi corpus of names (Li et al., 2009; MSRI, 2009) contains a test set of 1000 names rep-resented in both the Latin and Devanagari scripts. We manually classified these names as being of ei-ther Indian or non-Indian origin, occasionally resort-ing to web searches to help disambiguate them. [Cite_Footnote_1] We discarded those names that fell into both categories (e.g. \u201cMaya\u201d) as well as those that we could not confidently classify. In total, we discarded 95 of these names, and randomly selected 95 names from the training set that we could confidently classify to complete our corpus of 1000 names. Of the 1000 names, 546 were classified as being of Indian origin and the remaining 454 were classified as being of non-Indian origin; the names have an average length of 7.0 characters."
  },
  {
    "id": 1535,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm",
    "section_title": "3 Language identification with SVMs",
    "add_info": "C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a li-brary for support vector machines. Software available at http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm.",
    "text": "In our experiments, we used the LIBLINEAR (Fan et al., 2008) package for the linear kernel and the LIBSVM (Chang and Lin, 2001)  package for the RBF and sigmoid kernels. We discarded any peri-ods and parentheses, but kept apostrophes and hy-phens, and we converted all letters to lower case. We removed very short names of length less than two. For all data sets, we held out 10% of the data as the test set. We then found optimal parameters for each kernel type using 10-fold cross-validation on the remaining training set. This yielded optimum maximum n-gram lengths of four for single names and five for full names. Using the optimal parame-ters, we constructed models from the entire training data and then tested the models on the held-out test set."
  },
  {
    "id": 1536,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/google/sentencepiece",
    "section_title": "3 Experimental Setup 3.3 Architecture and Optimization",
    "add_info": "3 https://github.com/google/sentencepiece",
    "text": "All experiments are performed with the Trans-former architecture (Vaswani et al., 2017) using the open-source Tensorflow-Lingvo implementa-tion (Shen et al., 2019). Specifically, we use the Transformer Big model containing 375M parame-ters (6 layers, 16 heads, 8192 hidden dimension) (Chen et al., 2018) and a shared source-target Sen-tencePiece model (SPM) [Cite_Footnote_3] (Kudo and Richardson, 2018). We use a vocabulary size of 32k for the bilingual models and 64k for the multilingual mod-els. Different SPMs are trained depending on the set of languages supported by the model."
  },
  {
    "id": 1537,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/antonisa/embeddings",
    "section_title": "References",
    "add_info": "1 Available at https://github.com/antonisa/embeddings.",
    "text": "Most of recent work in cross-lingual word em-beddings is severely Anglocentric. The vast majority of lexicon induction evaluation dic-tionaries are between English and another lan-guage, and the English embedding space is se-lected by default as the hub when learning in a multilingual setting. With this work, how-ever, we challenge these practices. First, we show that the choice of hub language can sig-nificantly impact downstream lexicon induc-tion and zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to in-clude all language pairs using triangulation, and create new dictionaries for under-represented languages. [Cite_Footnote_1] Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we iden-tify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English."
  },
  {
    "id": 1538,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://github.com/antonisa/at",
    "section_title": "3 New LI Evaluation Dictionaries",
    "add_info": "9 Available unable to filter dictionaries in the following 11 languages: aze,https://github.com/antonisa/atembeddings. bel, ben, bos, lit, mkd, msa, sqi, tam, tha, tel.",
    "text": "3.2 Dictionaries for all Language Pairs through matical gender is a more complicated matter: it is not Triangulation uncommon for word translations to be of different gram-matical gender across languages). Our second method for creating new dictionaries is Hence, we devise a filtering method for removing bla-inspired by phrase table triangulation ideas from the tant mistakes when triangulating morphologically rich pre-neural MT community (Wang et al., 2006; Levin-languages. We rely on automatic morphological tagging boim and Chiang, 2015). The concept can be easily which we can obtain for most of the MUSE languages, explained with an example, visualized in Figure 1. Con-using the StanfordNLP toolkit (Qi et al., 2020). 10 The sider the Portuguese (Pt) word trabalho which, ac-morphological tagging uses the Universal Dependen-cording to the MUSE Pt\u2013En dictionary, has the words cies feature set (Nivre et al., 2016) making the tagging job and work as possible En translations. In turn, these comparable across almost all languages. Our filtering two En words can be translated to 4 and 5 Czech (Cs) technique iterates through the bridged dictionaries: for a words respectively. By utilizing the transitive property given source word, if we find a target word with the ex- (which translation should exhibit) we can identify the set act same morphological analysis, we filter out all other of 5 possible Cs translations for the Pt word trabalho. translations with the same lemma but different tags. In Following this simple triangulation approach, we cre-the case of feature mismatch (for instance, Greek uses 2 ate 4,704 new dictionaries over pairs between the 50 numbers, 4 cases and 3 genders while Italian has 2 num-languages of the MUSE dictionaries. [Cite_Footnote_9] For consistency, bers, 2 genders, and no cases) or if we only find a partial we keep the same train and test splits as with MUSE, so tag match over a feature subset, we filter out transla-that the source-side types are equal across all dictionar-tions with disagreeing tags. We ignore the grammatical ies with the same source language. gender and verb form features, as they are not directly Triangulating through English (which is unavoid-comparable cross-lingually. Coming back to our Greek\u2013 able, due to the relative paucity of non-English-centric Italian example, this means that for the form \u03b5\u03b9\u03c1\u03b7\u03bd\u03b9\u03ba\u1f79\u03c2 dictionaries) is suboptimal \u2013 English is morphologi-we would only keep pacifico as a candidate transla-cally poor and lacks corresponding markings for gen-tion (we show more examples in Table 1). der, case, or other features that are explicitly marked in morphologically-rich languages map to the same En-ingly, we find that bridged dictionaries between mor-glish form. Similarly, gendered nouns or adjectives phologically rich languages require a lot more filter-in gendered languages map to English forms that lack ing. For instance more than 80% of the entries of the gender information. For example, the MUSE Greek\u2013 Urdu-Greek dictionary get filtered out. On average, the English dictionary lists the word peaceful as the trans-languages with more filtered entries are Urdu (62.4%), lation for all \u03b5\u03b9\u03c1\u03b7\u03bd\u03b9\u03ba\u1f79\u03c2, \u03b5\u03b9\u03c1\u03b7\u03bd\u03b9\u03ba\u1f75, \u03b5\u03b9\u03c1\u03b7\u03bd\u03b9\u03ba\u1f79, \u03b5\u03b9\u03c1\u03b7\u03bd\u03b9\u03ba\u1f71, Turkish (61.1%), and German (58.6%). On the other which are the male, female, and neutral (singular and hand, much fewer entries are removed from dictionaries plural) inflections of the same adjective. Equivalently, with languages like Dutch (36.2%) or English (38.1%). the English\u2013Italian dictionary translates peaceful into sum of the GH distances between the source\u2013hub and of the evaluation set. In our 10-languages experiments, target\u2013hub spaces. On our distant languages experiment, a language different than the source and the target yields the correlation coefficient between P@1 and GH is 0.45, the best accuracy for over 93% of the evaluation sets, while it is slightly lower (0.34) for our 10-languages with the difference being statistically significant in more experiment. Figure 3 shows two high correlation exam-than half such cases. Similarly, in the distant-languages ples, namely Gl\u2013En and En\u2013Hi."
  },
  {
    "id": 1539,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/ccsasuke/umwe",
    "section_title": "4 Lexicon Induction Experiments 4.1 Methods and Setup",
    "add_info": "12 https://github.com/ccsasuke/umwe is English in only 17 instances (less than 20% of the",
    "text": "We train and evaluate all models starting with pre-trained Wikipedia FastText embeddings for all lan-guages (Grave et al., 2018). We focus on the minimally supervised scenario which only uses similar character strings between any languages for supervision in order to mirror the hard, realistic scenario of not having anno-tated training dictionaries between the languages. We learn MWE with the MAT+MPSR method using the pub-licly available code, [Cite_Footnote_12] aligning several language subsets varying the hub language. We decided against compar-ing to the incremental hub (IHS) method of Heyman et al. (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. We also do not com-pare to UMH, as we consider it conceptually similar to MAT+MPSR and no code is publicly available. For BWE experiments, we use MUSEs 14 (MUSE, semisupervised) and VecMap 15 systems, and we additionally compare them to MAT+MPSR for completeness."
  },
  {
    "id": 1540,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/artetxem/vecmap",
    "section_title": "4 Lexicon Induction Experiments 4.1 Methods and Setup",
    "add_info": "13 We refer the reader to Table 2 from Heyman et al. (2019) which compares to MAT+MPSR, and to Table 7 of their appendix 14 https://github.com/facebookresearch/MUSE which shows the dramatic influence of language order. 15 https://github.com/artetxem/vecmap",
    "text": "We train and evaluate all models starting with pre-trained Wikipedia FastText embeddings for all lan-guages (Grave et al., 2018). We focus on the minimally supervised scenario which only uses similar character strings between any languages for supervision in order to mirror the hard, realistic scenario of not having anno-tated training dictionaries between the languages. We learn MWE with the MAT+MPSR method using the pub-licly available code, aligning several language subsets varying the hub language. We decided against compar-ing to the incremental hub (IHS) method of Heyman et al. (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. [Cite_Footnote_13] We also do not com-pare to UMH, as we consider it conceptually similar to MAT+MPSR and no code is publicly available. For BWE experiments, we use MUSEs 14 (MUSE, semisupervised) and VecMap 15 systems, and we additionally compare them to MAT+MPSR for completeness."
  },
  {
    "id": 1541,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/xijiz/cfgen",
    "section_title": "References",
    "add_info": null,
    "text": "Past progress on neural models has proven that named entity recognition is no longer a prob-lem if we have enough labeled data. How-ever, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual ex-amples by the interventions on the existing observational examples to enhance the origi-nal dataset. Experiments across three datasets show that our method improves the generaliza-tion ability of models under limited observa-tional examples. Besides, we provide a theo-retical foundation by using a structural causal model to explore the spurious correlations be-tween input features and output labels. We in-vestigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Inter-estingly, we find that the non-spurious corre-lations are more located in entity representa-tion rather than context representation. As a result, our method eliminates part of the spu-rious correlations between context representa-tion and output labels. The code is available at  https://github.com/xijiz/cfgen."
  },
  {
    "id": 1542,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.ccks2019.cn/?pageid=62",
    "section_title": "3 Experiments 3.1 Dataset",
    "add_info": "1 http://www.ccks2019.cn/?pageid=62",
    "text": "CNER [Cite_Footnote_1] CNER is a Chinese clinical NER dataset in the CCKS-2019 challenge, including anatomy, disease, imaging examination, laboratory examina-tion, drug, and operation. We extract 1650 avail-able medical records from CNER, which contains entities of the disease type only."
  },
  {
    "id": 1543,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://labelstud.io/",
    "section_title": "3 Experiments 3.1 Dataset",
    "add_info": "2 https://labelstud.io/",
    "text": "IDiag For guaranteeing the diversity of the ex-perimental data, we use Label Studio [Cite_Footnote_2] to create a new medical NER dataset. We collect 12127 health record images from the hospital, which are converted into text paragraphs by optical character recognition (OCR). We hire some people to anno-tate diagnoses in these text paragraphs. To ensure the high quality of the dataset, we removed 539 data examples in the final dataset. It is worth noting that the distribution of IDiag, compared to CNER, has a big difference due to error text recognition from OCR."
  },
  {
    "id": 1544,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.cluebenchmarks.com/",
    "section_title": "3 Experiments 3.1 Dataset",
    "add_info": "3 https://www.cluebenchmarks.com/",
    "text": "CLUENER (Xu et al., 2020) In addition to the medical NER datasets, we also use a conven-tional NER dataset CLUENER released by CLUE organization [Cite_Footnote_3] , which is a well-defined and fine-grained dataset for named entity recognition in Chi-nese, including 10 categories like Person Name, Organzation, Book, etc. We extract 12090 avail-able instances from this dataset."
  },
  {
    "id": 1545,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html",
    "text": "Consider the publicly available EHR note [Cite_Footnote_1] in Figure 1. The note is divided into the 10 sections found in that EHR (Problems, Medications, His-tory, etc.). This example provides insight into why section prediction can be a difficult task. Although most of the headers appear to be bold, there is also plenty of bold text which is not the main header (e.g. see the History section). Additionally, in some cases (e.g. Allergies section) there is no text under the header at all. This makes it difficult to segment the data appropriately. Finally, although medications have their own section, they also ap-pear in the Plan section. Other issues that are not exposed in this example include: 1) Section order is not consistent across EHRs, 2) Headers may be missing, 3) Common features of headers (e.g. bold or colon) are not guaranteed to appear."
  },
  {
    "id": 1546,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://en.wikipedia.org/wiki/Category:Clinical_medicine",
    "section_title": "3 Data 3.1 Medical Literature (MedLit)",
    "add_info": "2 Articles under the \u2018Clinical Medicine\u2019 category (https://en.wikipedia.org/wiki/Category:Clinical_medicine).",
    "text": "The medical literature dataset consists of passages from textbooks, guidelines, and a subset of med-ically relevant Wikipedia articles [Cite_Footnote_2] . The number of sentences per source type is shown in Table 1. In total there are four sources in this dataset: Wikipedia and licensed content from DynaMed, Elsevier, and Wiley publishers."
  },
  {
    "id": 1547,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "4 Method and Results",
    "add_info": "5 https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "For the GRU RNN, we use the Adam optimizer, a batch size of 32, dropout of 0.2, and embedding size 300. We experimented with other parameter values on the development set, but these worked best. We ran each model for 50 epochs\u2014enough for the training loss to converge. For our BERT ex-periments we use a PyTorch implementation [Cite_Footnote_5] with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam op-timizer, a batch size of 32, dropout of 0.1, and em-bedding size 768. All text is cut off to the first 128 word-pieces. We experimented with differ-ent numbers of epochs, and chose the model that performed best on the dev set (usually one tuned at 10 epochs or fewer). Statistical significance was computed using McNemar\u2019s test. We exper-imented with both section classification and sen-tence classification as described in the following subsections."
  },
  {
    "id": 1548,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/uclmr/jack/blob/master/conf/nli/esim.yaml",
    "section_title": "3 Overview 3.2 Distinguishing Features",
    "add_info": "7 For instance, see https://github.com/uclmr/jack/blob/master/conf/nli/esim.yaml",
    "text": "Declarative Model Definition. Implementing different kinds of MR models can be repetitive, tedious, and error-prone. Most neural architec-tures are built using a finite set of basic building blocks for encoding sequences, and realising inter-action between sequences (e.g. via attention mech-anisms). For such a reason, J ACK allows to de-scribe these models at a high level, as a composi-tion of simpler building blocks [Cite_Footnote_7] , leaving concrete implementation details to the framework."
  },
  {
    "id": 1549,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/uclmr/jack/tree/master/notebooks",
    "section_title": "6 Demo",
    "add_info": null,
    "text": "We created three tutorial Jupyter notebooks at  https://github.com/uclmr/jack/tree/master/notebooks to demo J ACK \u2019s use cases. The quick start notebook shows how to quickly set up, load and run the existing systems for QA and NLI. The model training notebook demonstrates training, testing, evaluating and saving QA and NLI models programmatically. However, normally the user will simply use the provided training script from command line. The model implementation notebook delves deeper into implementing new models from scratch by writing all modules for a custom model."
  },
  {
    "id": 1550,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://farm2.user.srcf.net/research/bagel/",
    "section_title": "5 Experimental Setup 5.1 Data set",
    "add_info": "7 Available for download at: http://farm2.user.srcf.net/research/bagel/.",
    "text": "We performed our experiments on the BAGEL data set of Mairesse et al. (2010), which fits our usage scenario in a spoken dialogue sys-tem and is freely available. [Cite_Footnote_7] It contains a to-tal of 404 sentences from a restaurant informa-tion domain (describing the restaurant location, food type, etc.), which correspond to 202 dia-logue acts, i.e., each dialogue act has two para-phrases. Restaurant names, phone numbers, and other \u201cnon-enumerable\u201d properties are abstracted \u2013 replaced by an \u201cX\u201d symbol \u2013 throughout the gen-eration process. Note that while the data set con-tains alignment of source SVPs to target phrases, we do not use it in our experiments."
  },
  {
    "id": 1551,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/UFAL-DSG/tgen",
    "section_title": "8 Conclusions and Further Work",
    "add_info": "11 https://github.com/UFAL-DSG/tgen",
    "text": "The generator source code, along with config-uration files for experiments on the BAGEL data set, is available for download on Github. [Cite_Footnote_11]"
  },
  {
    "id": 1552,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/amoudgl/short-jokes-dataset",
    "section_title": "2 Related Work 3.1 Style selection and groupings",
    "add_info": "Abhinav Moudgil. 2017. short jokes dataset. https://github.com/amoudgl/short-jokes-dataset. [On-line; accessed 1-Oct-2019].",
    "text": "Style & dataset #S Split #L Label (distribution) B Domain Public Task . Formality NTERPERS GYAFC (Rao and Tetreault, 2018) 224k given 2 formal (50%), informal (50%) Y web N clsf. Politeness I StanfPolite (Danescu et al., 2013) 10k given 2 polite (49.6%), impolite (50.3%) Humor ShortHumor (CrowdTruth, 2016) 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017)  463k random 2 humor (50%), non-humor (50%) IGURATIVE Sarcasm SarcGhosh (Ghosh and Veale, 2016) 43k given 2 sarcastic (45%), non-sarcastic (55%) EmoBank valence (Buechel and Hahn, 2017) 10k random 1 negative, positive EmoBank arousal (Buechel and Hahn, 2017) 10k random 1 calm, excited EmoBank dominance (Buechel and Hahn, 2017) 10k random 1 being_controlled, being_in_control FFECTIVE DailyDialog (Li et al., 2017) 102k given 7 noemotion(83%), happy(12%).. Offense HateOffensive (Davidson et al., 2017) 24k given 3 hate(6.8%), offensive(76.3%).. A Romance ShortRomance 2k random 2 romantic (50%), non-romantic (50%) Sentiment SentiBank (Socher et al., 2013) 239k given 2 positive (54.6%), negative (45.4%) Gender PASTEL (Kang et al., 2019) 41k given 3 Female (61.2%), Male (38.0%).. 41k given 8 35-44 (15.3%), 25-34 (42.1%).. ERSONAL Age PASTEL (Kang et al., 2019)"
  },
  {
    "id": 1553,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://ufal.mff.cuni.cz/udpipe",
    "section_title": "3 Discourse segmentation 3.1 Binary task",
    "add_info": "3 http://ufal.mff.cuni.cz/udpipe",
    "text": "(1) [But maintaining the key components (. . .)] 1 [\u2013 a stable exchange rate and high levels of imports \u2013] 2 [will consume enormous amounts (. . .).] [Cite_Footnote_3]"
  },
  {
    "id": 1554,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.sfu.ca/\u02dcmtaboada",
    "section_title": "5 Corpora",
    "add_info": "6 https://www.sfu.ca/\u02dcmtaboada",
    "text": "For English, we use three corpora, allowing us to evaluate how robust is our model across do-mains. First, we report results on the RST-DT (from now on called En-DT), the most widely used corpus for this task. This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report perfor-mance on the SFU review corpus [Cite_Footnote_6] (En-SFU-DT) containing product reviews, and on the instruc-tional corpus (En-Instr-DT) (Subba and Di Euge-nio, 2009) built on instruction manuals. 7"
  },
  {
    "id": 1555,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://bitbucket.org/chloebt/discourse/",
    "section_title": "9 Conclusion",
    "add_info": null,
    "text": "We proposed new discourse segmenters that make use of resources available for many languages and domains. We investigated the usefulness of syn-tactic information when derived from dependency parse trees, and showed that this information is not as useful as expected, and that gold POS tags give as high results as using predicted constituent trees. We also showed that scores are lowered when considering a realistic setting, relying on predicted tokenization and not assuming gold sen-tences. We make our code available at  https://bitbucket.org/chloebt/discourse/."
  },
  {
    "id": 1556,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/asahi417/kex",
    "section_title": "References",
    "add_info": "1 Source code to reproduce our experimental results, includ-ing a keyword extraction library, are available in the following repository: https://github.com/asahi417/kex",
    "text": "Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we perform an exhaustive and large-scale empirical compar-ison of both statistical and graph-based term weighting methods in the context of keyword extraction. Our analysis reveals some interest-ing findings such as the advantages of the less-known lexical specificity with respect to tf-idf, or the qualitative differences between statisti-cal and graph-based methods. Finally, based on our findings we discuss and devise some suggestions for practitioners. [Cite_Footnote_1]"
  },
  {
    "id": 1557,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/LIAAD/KeywordExtractor-Datasets",
    "section_title": "3 Experimental Setting",
    "add_info": "6 All the datasets were fetched from a public data reposi-tory for keyword extraction data: https://github.com/LIAAD/KeywordExtractor-Datasets: KPCrowd (Marujo et al., 2013), Inspec (Hulth, 2003), Krapivin2009 (Krapivin et al., 2009), SemEval2017 (Augenstein et al., 2017), kdd (Gollapalli and Caragea, 2014), www (Gollapalli and Caragea, 2014), wiki20 (Medelyan and Witten, 2008), PubMed (Schutz et al., 2008), Schutz2008 (Schutz et al., 2008), citeulike180 (Medelyan et al., 2009), fao30 and fao780 (Medelyan and Witten, 2008), guyen2007 (Nguyen and Kan, 2007), and SemEval2010 (Kim et al., 2010).",
    "text": "Datasets. To evaluate the keyword extraction methods, we consider 15 different public datasets in English. [Cite_Footnote_6] Each entry in a dataset consists of a source document and a set of gold keyphrases, where the source document is processed through the pipeline described in Section 3 and the gold keyphrase set is filtered to include only phrases which appear in its candidate set. Table 1 pro-vides high-level statistics of each dataset, including length and number of keyphrases 7 (both average and standard deviation)."
  },
  {
    "id": 1558,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://pypi.org/project/segtok/",
    "section_title": "3 Experimental Setting",
    "add_info": "8 https://pypi.org/project/segtok/",
    "text": "Preprocessing. Before running keyword extrac-tion on each dataset, we apply standard text pre-processing operations. The documents are first tok-enized into words by segtok [Cite_Footnote_8] , a python library for tokenization and sentence splitting. Then, each word is stemmed to reduce it to its base form for comparison purpose by Porter Stemmer from NLTK (Bird et al., 2009), a widely used python li-brary for text processing. Part-of-speech annotation is carried out using NLTK tagger. To select a can-didate phrase set P d , following the literature (Wan and Xiao, 2008b), we consider contiguous nouns in the document d that form a noun phrase satisfying the regular expression ( ADJECTIVE )*( NOUN )+. We then filter the candidates with a stopword list taken from the official YAKE implementation (Campos et al., 2020). Finally, for the statistical methods and the graph-based methods based on them (i.e., LexRank and TFIDFRank), we compute citeulike180 6.6 9.5 18.0 15.2 23.0 fao30 17.3 16.0 24.0 20.7 26.0 fao780 9.3 3.2 11.7 10.5 12.4 kdd 11.7 7.0 11.2 11.6 10.6 theses100 5.6 9.4 6.60.9 10.7 wiki20 13.0 13.0 17.0 21.0 13.0 www 12.2 8.1 11.9 12.2 10.6 prior statistics including term frequency (tf), tf-idf, and LDA by Gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010) within each dataset."
  },
  {
    "id": 1559,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/LIAAD/yake",
    "section_title": "3 Experimental Setting",
    "add_info": "10 https://github.com/LIAAD/yake",
    "text": "Preprocessing. Before running keyword extrac-tion on each dataset, we apply standard text pre-processing operations. The documents are first tok-enized into words by segtok , a python library for tokenization and sentence splitting. Then, each word is stemmed to reduce it to its base form for comparison purpose by Porter Stemmer from NLTK (Bird et al., 2009), a widely used python li-brary for text processing. Part-of-speech annotation is carried out using NLTK tagger. To select a can-didate phrase set P d , following the literature (Wan and Xiao, 2008b), we consider contiguous nouns in the document d that form a noun phrase satisfying the regular expression ( ADJECTIVE )*( NOUN )+. We then filter the candidates with a stopword list taken from the official YAKE implementation [Cite_Footnote_10] (Campos et al., 2020). Finally, for the statistical methods and the graph-based methods based on them (i.e., LexRank and TFIDFRank), we compute citeulike180 6.6 9.5 18.0 15.2 23.0 fao30 17.3 16.0 24.0 20.7 26.0 fao780 9.3 3.2 11.7 10.5 12.4 kdd 11.7 7.0 11.2 11.6 10.6 theses100 5.6 9.4 6.60.9 10.7 wiki20 13.0 13.0 17.0 21.0 13.0 www 12.2 8.1 11.9 12.2 10.6 prior statistics including term frequency (tf), tf-idf, and LDA by Gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010) within each dataset."
  },
  {
    "id": 1560,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/declare-lab/MIME",
    "section_title": "References",
    "add_info": null,
    "text": "Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochas-ticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at  https://github.com/declare-lab/MIME ."
  },
  {
    "id": 1561,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/facebookresearch/EmpatheticDialogues",
    "section_title": "4 Experimental Settings 4.1 Dataset",
    "add_info": "1 https://github.com/facebookresearch/EmpatheticDialogues",
    "text": "We evaluate our method on E MPATHETIC D IA - LOGUES [Cite_Footnote_1] (Rashkin et al., 2018), a dataset that con-tains 24,850 open-domain dyadic conversations be-tween two users, where one responds emphatically to the other. For our experiments, we use the 8:1:1 train/validation/test split, defined by the authors of this dataset. Each sample consists of a context \u2014 defined by an excerpt of a full conversation and the emotion of the user \u2014 and the empathetic response to the last utterance in the context. There are a total of 32 different emotion categories roughly uniformly distributed across the dataset."
  },
  {
    "id": 1562,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Oneplus/tamr",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Our code and the alignments for LDC2014T12 dataset are publicly available at  https://github.com/Oneplus/tamr"
  },
  {
    "id": 1563,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://senseclusters.sourceforge.net/",
    "section_title": "3 Vector Approaches",
    "add_info": "1 http://senseclusters.sourceforge.net/",
    "text": "SenseClusters [Cite_Footnote_1] is an unsupervised knowledge-lean word sense disambiguation package The pack-age uses clustering algorithms to group similar in-stances of target words and label them with the ap-propriate sense. The clustering algorithms include Agglomerative, Graph partitional-based, Partitional biased agglomerative and Direct k-way clustering. The clustering can be done in either vector space where the vectors are clustered directly or similar-ity space where vectors are clustered by finding the pair-wise similarities among the contexts. The fea-ture options available are first and second-order co-occurrence, unigram and bigram vectors. First-order vectors are highly frequent words, unigrams or bi-grams that co-occur in the same window of context as the target word. Second-order vectors are highly frequent words that occur with the words in their re-spective first order vector."
  },
  {
    "id": 1564,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "http://www.d.umn.edu/tpederse/namedata.html",
    "section_title": "5 Data 5.3 Conflate Test Dataset",
    "add_info": "2 http://www.d.umn.edu/tpederse/namedata.html",
    "text": "We create our dataset using name-conflate [Cite_Footnote_2] to extract instances containing the conflate words from the 2005 Medline Baseline. Table 4 shows our cur-rent set of conflated words with their corresponding number of test (test) and training (train) instances. We refer to the conflated words as their pseudowords throughout the paper."
  },
  {
    "id": 1565,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cuitools.sourceforge.net",
    "section_title": "References",
    "add_info": null,
    "text": "Our experiments were conducted using CuiTools v0.15, which is freely available from  http://cuitools.sourceforge.net."
  },
  {
    "id": 1566,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.di.unipi.it/~gulli/AG",
    "section_title": "4 Experiments",
    "add_info": "1 http://www.di.unipi.it/~gulli/AG corpus of news articles.html",
    "text": "1. AG\u2019s news corpus [Cite_Footnote_1] ,a news article corpus with categorized articles from more than 2,000 news sources. We use the dataset with 4 largest classes constructed in (Zhang et al., 2015)."
  },
  {
    "id": 1567,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://ai.stanford.edu/~amaas/data/sentiment/",
    "section_title": "4 Experiments",
    "add_info": "2 http://ai.stanford.edu/~amaas/data/sentiment/",
    "text": "2. IMDB movie review dataset [Cite_Footnote_2] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011)."
  },
  {
    "id": 1568,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://ana.cachopo.org/datasets-for-single-label-text-categorization",
    "section_title": "4 Experiments",
    "add_info": "3 http://ana.cachopo.org/datasets-for-single-label-text-categorization",
    "text": "3. 20 Newsgroups (20NG for short), an email collection dataset categorized into 20 news groups. Simiar to (Dai and Le, 2015), we use the post-processed version [Cite_Footnote_3] , in which attach-ments, PGP keys and some duplicates are re-moved."
  },
  {
    "id": 1569,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://huggingface.co/qa",
    "section_title": "3 Analysis 3.1 Are generations grounded in retrieval?",
    "add_info": "10 https://huggingface.co/qa",
    "text": "Other systems also have this issue, possibly due to source-reference divergence and train-validation overlap: We note that this issue is not unique to our system \u2014 other systems on the KILT leaderboard like BART + DPR and RAG actually perform worse than their no-retrieval counterpart (BART) in generation quality, as shown in Table 1. Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by Jernite (2020). [Cite_Footnote_10] A possible explanation for this issue is high source-reference divergence, a common problem in table-to-text generation (Wiseman et al., 2017; Tian et al., 2019). In Table 2 and Table 4, we measure the n-gram overlap of top-ranked gold validation answers (Gold Ans) with predicted retrievals. This overlap is low and similar to that of our generations, which we suspect encourages our model to ignore retrievals. A second explanation is the large amount of train-validation overlap (Section 3.2), which eliminates the need for retrieval."
  },
  {
    "id": 1570,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/google-research/language/tree/master/language/realm",
    "section_title": "7:453\u2013466. A.1 Training & Model Details",
    "add_info": "19 https://github.com/google-research/language/tree/master/language/realm",
    "text": "All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM [Cite_Footnote_19] and the Routing Transformer. Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search). Since our retriever is frozen, we do not use the document index refresher available in their codebase."
  },
  {
    "id": 1571,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/google-research/google-research/tree/master/routing_transformer",
    "section_title": "7:453\u2013466. A.1 Training & Model Details",
    "add_info": "20 https://github.com/google-research/google-research/tree/master/routing_transformer REALM predicted the correct retrieval. Notice very small differences in generation quality (R-L) as well as the fraction of n-grams (n-g) in the generation overlap-ping with retrievals predicted by C -REALM (vs pre-dicted retr.). To control for overlap due to stopwords, we also add n-gram overlaps with the randomly sam-pled retrievals. ble 13, notice very little difference with and without retrieval. no effect other than being a symbol of an economic entity or symbol of an era or location, and nothing more. For example look at Sears, Sears didn\u2019t care what went on inside, it was all about the _appearance_ of its location, the prestige of the location, the facilities and so on. It didn\u2019t care about how long it took it to operate, it was about how much people would pay to go see it. Sears was a landmark as a cultural movement and other big companies followed suit, so if you want to see a building you\u2019ve never seen before, you have to go see Sears, just like you have to see a Toyota Camry for Toyota Camry. They used to be all about building new factories, some of them if I recall, but now that they\u2019re bigger, that means that more factory jobs are coming to them. You\u2019ve probably seen them in stores as stores where people buy and sell stuff, so there aren\u2019t that many places for them to come from. Instead, it\u2019s just for show, a symbol of rich people.",
    "text": "All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM and the Routing Transformer. [Cite_Footnote_20] Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search). Since our retriever is frozen, we do not use the document index refresher available in their codebase."
  },
  {
    "id": 1572,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://lucene.apache.org",
    "section_title": "3 Approach",
    "add_info": "2 https://lucene.apache.org",
    "text": "KBs (e.g., ARC), we retrieve the top n sentences from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We im-plemented this using the BM25 IR model with the default parameters in Lucene [Cite_Footnote_2] . For reading com-prehension datasets where the question is associ-ated with a text passage (e.g., MultiRC), all the sentences in this passage become candidates."
  },
  {
    "id": 1573,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html",
    "section_title": "3 Approach 3.1 Ranking of Candidate Justification Sets",
    "add_info": "4 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html",
    "text": "Relevance (R) We use the Lucene implementa-tion [Cite_Footnote_4] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer. In particular, we form a query that concate-nates the question and candidate answer, and use as underlying document collection (necessary to com-pute document statistics such as inverse document frequencies (IDF)) either: sentences in the entire KB (for ARC), or all sentences in the correspond-ing passage in the case of reading comprehension (MultiRC). The arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set."
  },
  {
    "id": 1574,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/thompsonb/prism",
    "section_title": "1 Introduction",
    "add_info": "1 Except for Gujarati, where we had no training data. 2 https://github.com/thompsonb/prism",
    "text": "\u2022 Outperforms or ties with prior metrics and language pairs (Ganitkevitch and Callison-Burch, several contrastive neural methods on the 2014). Paraphrase tables were, in turn, used in MT segment-level WMT 2019 MT metrics task metrics to reward systems for paraphrasing words in every language pair; [Cite_Footnote_1] (Banerjee and Lavie, 2005) or phrases (Zhou et al.,"
  },
  {
    "id": 1575,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://casmacat.eu/corpus/",
    "section_title": "1 Introduction 4.2 Model Training",
    "add_info": "5 http://casmacat.eu/corpus/ Bootstrap resampling (Koehn, 2004; Graham et al., global-voices.html 2014) is used to estimate confidence intervals for",
    "text": "Our data comes primarily from WikiMatrix (Schwenk et al., 2019), Global Voices, [Cite_Footnote_5] EuroParl"
  },
  {
    "id": 1576,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://casmacat.eu/corpus/global-voices.html",
    "section_title": "B Data Details for Replication",
    "add_info": "10 http://casmacat.eu/corpus/global-voices.html",
    "text": "Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, [Cite_Footnote_10] EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh\u2013English and Kazakh\u2013Russian data from WMT, to be able to evaluate on Kazakh."
  },
  {
    "id": 1577,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://nlp.ffzg.hr/resources/corpora/setimes/",
    "section_title": "B Data Details for Replication",
    "add_info": "11 http://nlp.ffzg.hr/resources/corpora/setimes/",
    "text": "Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, [Cite_Footnote_11] United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh\u2013English and Kazakh\u2013Russian data from WMT, to be able to evaluate on Kazakh."
  },
  {
    "id": 1578,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/bleurt",
    "section_title": "C.5 Baselines",
    "add_info": "12 https://github.com/google-research/bleurt",
    "text": "We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d [Cite_Footnote_12] We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors."
  },
  {
    "id": 1579,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Tiiiger/bert_score",
    "section_title": "C.5 Baselines",
    "add_info": "13 https://github.com/Tiiiger/bert_score",
    "text": "We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors. [Cite_Footnote_13]"
  },
  {
    "id": 1580,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://data.statmt.org/wmt19/translation-task/wmt19-submitted-data-v3.tgz",
    "section_title": "C.5 Baselines",
    "add_info": "14 http://data.statmt.org/wmt19/translation-task/wmt19-submitted-data-v3.tgz",
    "text": "The remaining baseline results are computed using the metric scores as submitted to (Ma et al., 2019) [Cite_Footnote_14]"
  },
  {
    "id": 1581,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/rycolab/homophony-as-renyi-entropy",
    "section_title": "5 Results, Discussion and Conclusion 7",
    "add_info": "7 Our code is available at https://github.com/rycolab/homophony-as-renyi-entropy.",
    "text": "5 Results, Discussion and Conclusion [Cite_Footnote_7]"
  },
  {
    "id": 1582,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://opennlp.sourceforge.net/",
    "section_title": "6 Evaluation: Cloze 6.2 Training and Test Data",
    "add_info": "1 http://opennlp.sourceforge.net/",
    "text": "We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepo-sitional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and ar-gument head words. We use the OpenNLP [Cite_Footnote_1] coref-erence engine to resolve entity mentions."
  },
  {
    "id": 1583,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://www.ml4nlp.de/code-and-data/treex2conll",
    "section_title": "4 Evaluation 4.1 Datasets and Preprocessing",
    "add_info": "1 see http://www.ml4nlp.de/code-and-data/treex2conll",
    "text": "The evaluation datasets for English and Chi-nese are those from the CoNLL Shared Task 2009 (Hajic\u030c et al., 2009) (henceforth CoNLL-ST). Their annotation in the CoNLL-ST is not identi-cal, but the guidelines for \u201ccore\u201d semantic roles are similar (Kingsbury et al., 2004), so we eval-uate only on core roles here. The data for the second language pair is drawn from the Prague Czech-English Dependency Treebank 2.0 (Hajic\u030c et al., 2012), which we converted to a format simi-lar to that of CoNLL-ST [Cite_Footnote_1] . The original annotation uses the tectogrammatical representation (Hajic\u030c, 2002) and an inventory of semantic roles (or func-tors), most of which are interpretable across vari-ous predicates. Also note that the syntactic anno-tation of English and Czech in PCEDT 2.0 is quite similar (to the extent permitted by the difference in the structure of the two languages) and we can use the dependency relations in our experiments."
  },
  {
    "id": 1584,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://ufal.mff.cuni.cz/",
    "section_title": "5 Results 5.2 Argument Classification",
    "add_info": "2 http://ufal.mff.cuni.cz/ \u223c toman/pcedt/en/functors.html",
    "text": "Most of the labels [Cite_Footnote_2] are self-explanatory: Pa-tient (PAT), Actor (ACT), Time (TWHEN), Effect (EFF), Location (LOC), Manner (MANN), Ad-dressee (ADDR), Extent (EXT). CPHR marks the nominal part of a complex predicate, as in \u201cto have [a plan] CPHR \u201d, and DIR3 indicates destination."
  },
  {
    "id": 1585,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Extend",
    "url": "http://psd.museum.upenn.edu/",
    "section_title": "2 CDLI and the Annotations",
    "add_info": "PSD: The Pennsylvania Sumerian Dictionary. 2006. http://psd.museum.upenn.edu/",
    "text": "In the study of the Ur III corpus, the most exhaus-tive infrastructure and documentation for lemmati-zation is that provided for \u201cthe Open Richly Anno-tated Cuneiform Corpus (Oracc)\u201d (ORACC, 2014). The lemmatizer for the Oracc system is accessed via an Emacs interface designed to encourage si-multaneous transliteration and lemmatization by a human expert. The process begins with the human expert submitting an unlemmatized transliteration in a format called ATF (ASCII Transliteration For-mat). This format is the standard interchange for-mat for transliteration across many projects dealing in and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the Pennsylvania Sumerian Dictionary (PSD, 2006)  , and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014)). Via the Emacs interface, the transliteration is sub-mitted to the linguistic annotatation system, which identifies an existing project-specific glossary based on directives provided by the human expert in the transliteration, and returns a preliminary lemmatiza-tion whose completeness and content depends on the referenced project glossary. The transliterator may then modify any automatically-generated lemmata, or, in the case of new words or new senses in which existing words used, manually lemmatize the word to allow the lemmatizer to \u201charvest\u201d the new lemma and add it to the glossary. Oracc\u2019s lemmatizer also performs normalization and morphological analysis in order to automatically and consistently identify words in the text. The lemmatizer is not designed to \u201clearn\u201d new insights or induce new rules regard-ing Sumerian morphology on the basis of new lem-mata harvested from submissions, but rather serves as a mechanism to consistently apply rules that have been harvested."
  },
  {
    "id": 1586,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Extend",
    "url": "http://oracc.museum.upenn.edu/dcclt/",
    "section_title": "2 CDLI and the Annotations",
    "add_info": "DCCLT - Digital Corpus of Cuneiform Lexical Texts. 2014. http://oracc.museum.upenn.edu/dcclt/",
    "text": "In the study of the Ur III corpus, the most exhaus-tive infrastructure and documentation for lemmati-zation is that provided for \u201cthe Open Richly Anno-tated Cuneiform Corpus (Oracc)\u201d (ORACC, 2014). The lemmatizer for the Oracc system is accessed via an Emacs interface designed to encourage si-multaneous transliteration and lemmatization by a human expert. The process begins with the human expert submitting an unlemmatized transliteration in a format called ATF (ASCII Transliteration For-mat). This format is the standard interchange for-mat for transliteration across many projects dealing in and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the Pennsylvania Sumerian Dictionary (PSD, 2006), and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014)  ). Via the Emacs interface, the transliteration is sub-mitted to the linguistic annotatation system, which identifies an existing project-specific glossary based on directives provided by the human expert in the transliteration, and returns a preliminary lemmatiza-tion whose completeness and content depends on the referenced project glossary. The transliterator may then modify any automatically-generated lemmata, or, in the case of new words or new senses in which existing words used, manually lemmatize the word to allow the lemmatizer to \u201charvest\u201d the new lemma and add it to the glossary. Oracc\u2019s lemmatizer also performs normalization and morphological analysis in order to automatically and consistently identify words in the text. The lemmatizer is not designed to \u201clearn\u201d new insights or induce new rules regard-ing Sumerian morphology on the basis of new lem-mata harvested from submissions, but rather serves as a mechanism to consistently apply rules that have been harvested."
  },
  {
    "id": 1587,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/clab/lstm-parser/tree/easy-to-use",
    "section_title": "5 DeepCx neural network architecture 5.3 Implementation details",
    "add_info": "2 https://github.com/clab/lstm-parser/tree/easy-to-use.",
    "text": "DeepCx is implemented using a refactored version of the LSTM parser codebase that performs iden-tically to the original. [Cite_Footnote_2] The neural network frame-work, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02\u201321; development: 22)."
  },
  {
    "id": 1588,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/duncanka/lstm-causality-tagger",
    "section_title": "5 DeepCx neural network architecture 5.3 Implementation details",
    "add_info": "3 https://github.com/duncanka/lstm-causality-tagger.",
    "text": "The code for DeepCx is available on GitHub. [Cite_Footnote_3] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser."
  },
  {
    "id": 1589,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.ims.uni-stuttgart.de/schmid",
    "section_title": "2 Feature Annotation",
    "add_info": "1 The complete annotation program is available from the author\u2019s home page at http://www.ims.uni-stuttgart.de/schmid",
    "text": "Besides the slash features, we used other fea-tures in order to improve the parsing accuracy of the PCFG, inspired by the work of Klein and Man-ning (2003). The most important ones of these features [Cite_Footnote_1] will now be described in detail. Sec-tion 4.3 shows the impact of these features on labeled bracketing accuracy and empty category prediction."
  },
  {
    "id": 1590,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/jzhou316/Unsupervised-Sentence-Summarization",
    "section_title": "3 Experimental Setup",
    "add_info": "1 Code available at https://github.com/jzhou316/Unsupervised-Sentence-Summarization.",
    "text": "For the contextual matching model\u2019s similarity function S, we adopt the forward language model of ELMo (Peters et al., 2018) to encode tokens to corresponding hidden states in the sequence, re-sulting in a three-layer representation each of di-mension 512. The bottom layer is a fixed char-acter embedding layer, and the above two layers are LSTMs associated with the generic unsuper-vised language model trained on a large amount of text data. We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [Cite_Footnote_1] The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024. It is trained using dropout rate 0.5 and SGD combined with gradient clipping."
  },
  {
    "id": 1591,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://opennlp.sourceforge.net",
    "section_title": "2 Background 2.2 Discourse in Textual Entailment",
    "add_info": "3 http://opennlp.sourceforge.net",
    "text": "A number of systems have tried to address the question of coreference in RTE as a preprocessing step prior to inference proper, with most systems using off-the-shelf coreference resolvers such as JavaRap (Qiu et al., 2004) or OpenNLP [Cite_Footnote_3] . Gen-erally, anaphoric expressions were textually re-placed by their antecedents. Results were in-conclusive, however, with several reports about errors introduced by automatic coreference res-olution (Agichtein et al., 2008; Adams et al., 2007). Specific evaluations of the contribution of coreference resolution yielded both small nega-tive (Bar-Haim et al., 2008) and insignificant pos-itive (Chambers et al., 2007) results."
  },
  {
    "id": 1592,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/cindyxinyiwang/TrDec_pytorch",
    "section_title": "References",
    "add_info": "1 Our code is available at https://github.com/cindyxinyiwang/TrDec_pytorch.",
    "text": "Recent advances in Neural Machine Transla-tion (NMT) show that adding syntactic infor-mation to NMT systems can improve the qual-ity of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like constituency and dependency parse trees. This is often done via a standard RNN decoder that operates on a lin-earized target tree structure. However, it is an open question of what specific linguistic for-malism, if any, is the best structural represen-tation for NMT. In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [Cite_Footnote_1]"
  },
  {
    "id": 1593,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Compare",
    "url": "http://www.phontron.com/kftt",
    "section_title": "5 Experiments",
    "add_info": "Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.",
    "text": "Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011)  , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible. Details are in Tab. 1. English sentences are parsed using Ckylark (Oda et al., 2015) for the constituency parse trees, and Stanford Parser (de Marneffe et al., 2006; Chen and Manning, 2014) for the dependency parse trees. We use byte-pair encoding (Sennrich et al., 2016) with 8K merge operations on ja-en, 4K merge operations on or-en, and 24K merge operations on de-en."
  },
  {
    "id": 1594,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/sjcfr/CWVAE",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at  https://github.com/sjcfr/CWVAE."
  },
  {
    "id": 1595,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://mstparser.sourceforge.net",
    "section_title": "5 Evaluation Results 5.1 Syntactic Dependency Parsers",
    "add_info": "8 It\u2019s freely available at http://mstparser.sourceforge.net.",
    "text": "The parser is basically based on the MSTParser [Cite_Footnote_8] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser. 1) Chen et al. (2008) used fea-tures derived from short dependency pairs based on large-scale auto-parsed data to enhance depen-dency parsing. Here, the same features are used, though all dependency pairs rather than short de-pendency pairs are extracted along with the de-pendency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large im-provement for English and Czech. Here, the same features are also used as word clusters are gen-erated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transition-based parser based on maximum entropy as in Zhao and Kit (2008). We still use the similar fea-ture notations of that work."
  },
  {
    "id": 1596,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "http://svmlight.joachims.org/",
    "section_title": "3 Exploiting Comparable Corpora 3.1 Multilingual Domain Model w il evaluated in the corpus T l .",
    "add_info": "3 We adopted the efficient implementation freely available at http://svmlight.joachims.org/.",
    "text": "As Kernel Methods are the state-of-the-art su-pervised framework for learning and they have been successfully adopted to approach the TC task (Joachims, 2002), we chose this framework to per-form all our experiments, in particular Support Vector Machines [Cite_Footnote_3] . Taking into account the exter-nal knowledge provided by a MDM it is possible estimate the topic similarity among two texts ex-pressed in different languages, with the following kernel: where D is defined as in equation 1."
  },
  {
    "id": 1597,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://multiwordnet.itc.it",
    "section_title": "4 Exploiting Bilingual Dictionaries",
    "add_info": "4 Available at http://multiwordnet.itc.it.",
    "text": "MultiWordNet [Cite_Footnote_4] . It is a multilingual computa-tional lexicon, conceived to be strictly aligned with the Princeton WordNet. The available lan-guages are Italian, Spanish, Hebrew and Roma-nian. In our experiment we used the English and the Italian components. The last version of the Italian WordNet contains around 58,000 Italian word senses and 41,500 lemmas organized into 32,700 synsets aligned whenever possible with WordNet English synsets. The Italian synsets are created in correspondence with the Princeton WordNet synsets, whenever possible, and seman-tic relations are imported from the corresponding English synsets. This implies that the synset index structure is the same for the two languages."
  },
  {
    "id": 1598,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/intersun/LightningDOT",
    "section_title": "References",
    "add_info": "1 Code and pre-training checkpoints are available at https://github.com/intersun/LightningDOT.",
    "text": "Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow infer-ence speed due to enormous computation cost mainly from cross-modal attention in Trans-former architecture. When applied to real-life applications, such latency and computa-tion demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most ma-ture scenario of V+L application, which has been widely studied even prior to the emer-gence of recent pre-trained models. We pro-pose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, with-out sacrificing accuracy. LightningDOT re-moves the time-consuming cross-modal atten-tion by pre-training on three novel learning objectives, extracting feature indexes offline, and employing instant dot-product matching with further re-ranking, which significantly speeds up retrieval process. In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000\u00d7 mag-nitude of computational hours. [Cite_Footnote_1]"
  },
  {
    "id": 1599,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/yumeng5/LOTClass",
    "section_title": "References",
    "add_info": "1 Source code can be found at https://github.com/yumeng5/LOTClass.",
    "text": "Current text classification methods typically require a good number of human-labeled doc-uments as training data, which can be costly and difficult to obtain in real applications. Hu-mans can perform classification without see-ing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on un-labeled data, without using any labeled doc-uments. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as rep-resentation learning models for document clas-sification. Our method (1) associates semanti-cally related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% ac-curacy on four benchmark datasets including topic and sentiment classification without us-ing any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name [Cite_Footnote_1] ."
  },
  {
    "id": 1600,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://ictclas.org/",
    "section_title": "3 Experiment 3.1 Data set",
    "add_info": "3 http://ictclas.org/",
    "text": "We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS [Cite_Footnote_3] and Stan-ford Chinese word segmenter , which are widely used in NLP related to word segmentation. Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours. ICTCLAS, on the other hand, is a HMM-based Chinese word segmenter. Another baseline is Li and Sun (2009), which also uses punctua-tion in their semi-supervised framework. F-score is used as the accuracy measure. The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to cor-rectly segment out of vocabulary words."
  },
  {
    "id": 1601,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://nlp.stanford.edu/projects/chinese-nlp.shtml\\\\#cws",
    "section_title": "3 Experiment 3.1 Data set",
    "add_info": "4 http://nlp.stanford.edu/projects/chinese-nlp.shtml\\\\#cws",
    "text": "We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS and Stan-ford Chinese word segmenter [Cite_Footnote_4] , which are widely used in NLP related to word segmentation. Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours. ICTCLAS, on the other hand, is a HMM-based Chinese word segmenter. Another baseline is Li and Sun (2009), which also uses punctua-tion in their semi-supervised framework. F-score is used as the accuracy measure. The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to cor-rectly segment out of vocabulary words."
  },
  {
    "id": 1602,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss",
    "section_title": "3 Model 3.2 ReWE",
    "add_info": "1 https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss",
    "text": "In the experiment, we have explored two cases for the ReWE loss : the minimum square error (MSE) [Cite_Footnote_1] and the cosine embedding loss (CEL) . Finally, the NLL loss and the ReW E loss are com-bined to form the training objective using a posi-tive trade-off coefficient, \u03bb:"
  },
  {
    "id": 1603,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://pytorch.org/docs/stable/nn.html#torch.nn.CosineEmbeddingLoss",
    "section_title": "3 Model 3.2 ReWE",
    "add_info": "2 https://pytorch.org/docs/stable/nn.html#torch.nn.CosineEmbeddingLoss",
    "text": "In the experiment, we have explored two cases for the ReWE loss : the minimum square error (MSE) and the cosine embedding loss (CEL) [Cite_Footnote_2] . Finally, the NLL loss and the ReW E loss are com-bined to form the training objective using a posi-tive trade-off coefficient, \u03bb:"
  },
  {
    "id": 1604,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/ijauregiCMCRC/ReWENMT",
    "section_title": "4 Experiments",
    "add_info": "3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWENMT",
    "text": "We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [Cite_Footnote_3] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017). We have also explored the use of sub-word units learned with byte pair encoding (BPE) (Sennrich et al., 2016). All the preprocess-ing steps, hyperparameter values and training pa-rameters are described in detail in the supplemen-tary material to ease reproducibility of our results."
  },
  {
    "id": 1605,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/cookielee77/CLARE",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We evaluate CLARE on text classification, nat-ural language inference, and sentence paraphrase tasks, by attacking finetuned BERT models (De-vlin et al., 2019). Extensive experiments and hu-man evaluation results show that CLARE outper-forms baselines in terms of attack success rate, tex-tual similarity, fluency, and grammaticality, and strikes a better balance between attack success rate and preserving input-output similarity. Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at  https://github.com/cookielee77/CLARE."
  },
  {
    "id": 1606,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://www.languagetool.org/",
    "section_title": "3 Experiments 3.2 Datasets and Evaluation",
    "add_info": "7 https://www.languagetool.org/",
    "text": "\u2022 Grammar error (GErr): the absolute num-ber of increased grammatical errors in the suc-cessful adversarial example, compared to the original text. Following (Zang et al., 2020; Morris et al., 2020b), we calculate this by the LanguageTool (Naber et al., 2003). [Cite_Footnote_7]"
  },
  {
    "id": 1607,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/cookielee77/CLARE",
    "section_title": "6 Conclusion",
    "add_info": null,
    "text": "We have presented CLARE, a contextualized ad-versarial example generation model for text. It uses contextualized knowledge from pretrained masked language models, and can generate ad-versarial examples that are natural, fluent and grammatical. With three contextualized perturba-tion patterns, Replace, Insert and Merge in our arsenal, CLARE can produce outputs of varied lengths and achieves a higher attack success rate than baselines and with fewer edits. Human eval-uation shows significant advantages of CLARE in terms of textual similarity, fluency and gram-maticality. We release our code and models at  https://github.com/cookielee77/CLARE."
  },
  {
    "id": 1608,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "A Appendix A.1 Additional Experiment Details",
    "add_info": "10 https://github.com/huggingface/transformers",
    "text": "Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers [Cite_Footnote_10] (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively. We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler and BERTAt-tack are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set. In the adver-sarial training (\u00a74.3), the small TextCNN victim model (Kim, 2014) has 128 embedding size and 100 filters for 3, 4, 5 window size with 0.5 dropout, resulting in 7M parameters."
  },
  {
    "id": 1609,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/jind11/TextFooler",
    "section_title": "A Appendix A.1 Additional Experiment Details",
    "add_info": "11 https://github.com/jind11/TextFooler",
    "text": "Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively. We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler [Cite_Footnote_11] and BERTAt-tack are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set. In the adver-sarial training (\u00a74.3), the small TextCNN victim model (Kim, 2014) has 128 embedding size and 100 filters for 3, 4, 5 window size with 0.5 dropout, resulting in 7M parameters."
  },
  {
    "id": 1610,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/LinyangLee/BERT-Attack",
    "section_title": "A Appendix A.1 Additional Experiment Details",
    "add_info": "12 https://github.com/LinyangLee/BERT-Attack",
    "text": "Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively. We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler and BERTAt-tack [Cite_Footnote_12] are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set. In the adver-sarial training (\u00a74.3), the small TextCNN victim model (Kim, 2014) has 128 embedding size and 100 filters for 3, 4, 5 window size with 0.5 dropout, resulting in 7M parameters."
  },
  {
    "id": 1611,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/UKPLab/acl2016-supersense-embeddings",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/UKPLab/acl2016-supersense-embeddings",
    "text": "\u2022 We are the first to provide a joint word-and supersense-embedding model, which we make publicly available [Cite_Footnote_1] for the research com-munity. This provides an insight into the word and supersense positions in the vector space"
  },
  {
    "id": 1612,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/kutschkem/SmithHeilmann_fork/tree/master/MIRATagger",
    "section_title": "2 Related Work 2.2 Supersense Tagging",
    "add_info": "2 https://github.com/kutschkem/SmithHeilmann_fork/tree/master/MIRATagger",
    "text": "Supersenses, also known as lexicographer files or semantic fields, were originally used to organize lexical-semantic resources (Fellbaum, 1990). The supersense tagging task was introduced by Cia-ramita and Johnson (2003) for nouns and later expanded for verbs (Ciaramita and Altun, 2006). Their state-of-the-art system is trained and eval-uated on the SemCor data (Miller et al., 1994) with an F-score of 77.18%, using a hidden Markov model. Since then, the system, resp. its reimple-mentation by Heilman [Cite_Footnote_2] , was widely used in applied tasks (Agirre et al., 2011; Surdeanu et al., 2011; Laparra and Rigau, 2013). Supersense taggers have then been built also for Italian (Picca et al., 2008), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2013). Tsvetkov et al. (2015) proposes the us-age of SemCor supersense frequencies as a way to evaluate word embedding models, showing that a good alignment of embedding dimensions to super-senses correlates with performance of the vectors in word similarity and text classification tasks. Re-cently, Johannsen et al. (2014) introduced a task of multiword supersense tagging on Twitter. On their newly constructed dataset, they show poor do-main adaptation performance of previous systems, achieving a maximum performance with a search-based structured prediction model (Daume\u0301 III et al., 2009) trained on both Twitter and SemCor data. In parallel, Schneider and Smith (2015) expanded a multiword expression (MWE) annotated corpus of online reviews with supersense information, fol-lowing an alternative annotation scheme focused on MWE. Similarly to Johannsen et al. (2014), they find that SemCor may not be a sufficient re-source for supersense tagging adaption to different domains. Therefore, in our work, we explore the potential of using an automatically annotated Ba-belfied Wikipedia corpus (Scozzafava et al., 2015) for this task."
  },
  {
    "id": 1613,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://github.com/coastalcph/supersense-data-twitter",
    "section_title": "5 Building a Supersense Tagger 5.1 Experimental Setup",
    "add_info": "5 https://github.com/coastalcph/supersense-data-twitter",
    "text": "We implement a window-based approach with a multi-channel multi-layer perceptron model using the Theano framework (Bastien et al., 2012). With a sliding window of size [Cite_Footnote_5] for the sequence learning setup we extract for each word the following seven feature vectors:"
  },
  {
    "id": 1614,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py",
    "section_title": "6 Using Supersense Embeddings in Document Classification Tasks 6.1 Experimental Setup",
    "add_info": "7 https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py",
    "text": "Keras demo [Cite_Footnote_7] , into which we incorporate the su-persense information. Figure 3 displays our net-work architecture. First, we use three channels of word embeddings on the plain textual input. The first channel are the 300-dimensional word em-beddings obtained from our enriched Wikipedia corpus. The second embedding channel consists of 41-dimensional vectors capturing the cosine simi-larity of the word to each supersense embedding. The third channel contains the vector of relative frequencies of the word occurring in the enriched Wikipedia together with its supersense, i.e. provid-ing the background supersense distribution for the word. Each of the document embeddings is then convoluted with the filter size of 3, followed by a pooling layer of length 2 and fed into a long-short-term-memory (LSTM) layer. In parallel, we feed as input a processed document text, where the words are replaced by their predicted super-senses. Given that we have the Wikipedia-based supersense embeddings in the same vector space as the word embeddings, we can now proceed to creating the 300-dimensional embedding channel also for the supersense text. As in the plain text channels, we feed also these embeddings into the convolutional and LSTM layers in a similar fashion. Afterwards, we concatenate all LSTM outputs and feed them into a standard fully connected neural network layer, followed by the sigmoid for the bi-nary output. The following subsections discuss our results on a range of classification tasks: subjectiv-ity prediction, sentiment polarity classification and metaphor detection."
  },
  {
    "id": 1615,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://www.cs.cornell.edu/people/pabo/movie-review-data/",
    "section_title": "6 Using Supersense Embeddings in Document Classification Tasks 6.3 Subjectivity Classification",
    "add_info": "9 https://www.cs.cornell.edu/people/pabo/movie-review-data/",
    "text": "Pang and Lee (2004) demonstrate that the subjec-tivity detection can be a useful input for a sen-timent classifier. They compose a publicly avail-able dataset [Cite_Footnote_9] of 5000 subjective and 5000 objec-tive sentences, classifying them with a reported accuracy of 90-92% and further show that predict-ing this information improves the end-level sen-timent classification on a movie review dataset. Kim (2014) and Wang and Manning (2013) fur-ther improve the performance through different machine learning methods. Supersenses are a nat-ural candidate for subjectivity prediction, as we hypothesize that the nouns and verbs in the sub-jective and objective sentences often come from different semantic classes (e.g. VERB . FEELING vs. VERB . COGNITION ). We employ the same archi-tecture as in previous task, automatically annotat-ing the words in the documents with their super-senses. Our results are reported in Table 9. The supersenses (SUPER) provide an additional infor-mation, improving the model performance by up to 2% over word embeddings (WORDS). The dif-ference between both systems is significant. Based on a manual error analysis, the supersense informa-tion contributes here in a similar manner as in the previous case. Subjective sentences contain more verbs of supersense PERCEPTION , while objective ones more frequently feature the supersenses POS - SESSION and SOCIAL . Nouns in the subjective cat-egory are characterized by supersenses COMMUNI - CATION and ATTRIBUTE , while in objective ones the PERSON and POSSESSION are more frequent."
  },
  {
    "id": 1616,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.cs.cmu.edu/\u02dcytsvetko/metaphor/datasets.zip",
    "section_title": "6 Using Supersense Embeddings in Document Classification Tasks 6.4 Metaphor Identification",
    "add_info": "10 http://www.cs.cmu.edu/\u02dcytsvetko/metaphor/datasets.zip",
    "text": "Supersenses have recently been shown to provide improvements in metaphor prediction tasks (Ger-shman et al., 2014), as they hold the informa-tion of coarse semantic concepts. Turney et al. (2011) explore the task of discriminating literal and metaphoric adjective-noun expressions. They report an accuracy of 79% on a small dataset rated by five annotators. Tsvetkov et al. (2013) pursue this work further by constructing and publishing a dataset of 985 literal and 985 methaphorical adjective-noun pairs [Cite_Footnote_10] and classify them. Gersh-man et al. (2014) further expand on this work using 64-dimensional vector-space word representations constructed by Faruqui and Dyer (2014) for clas-sification. They report a state-of-the-art F-score of 85% with random decision forests, including also abstractness and imageability features (Wil-son, 1988) and supersenses from WordNet, aver-aged across senses."
  },
  {
    "id": 1617,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/UKPLab/acl2016-supersense-embeddings",
    "section_title": "8 Conclusions and Future Work",
    "add_info": "11 https://github.com/UKPLab/acl2016-supersense-embeddings",
    "text": "We have presented a novel joint embedding set of words and supersenses, which provides a new insight into the word and supersense positions in the vector space. We demonstrated the utility of these embeddings for predicting supersenses and manifested that the supersense enrichment can lead to a significant improvement in a range of down-stream classification tasks, using our embeddings in a neural network model. The outcomes of this work are available to the research community. [Cite_Footnote_11] . In follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as SemCor (Miller et al., 1994) and STREUSLE (Schneider and Smith, 2015) to examine the impact of the corpus choice in detail and extend the train-ing data beyond WordNet vocabulary. Moreover, the coarse semantic categorization contained in su-persenses was shown to be preserved in translation (Schneider et al., 2013), making them a perfect can-didate for a multilingual adaptation of the vector space, e.g. extending Faruqui and Dyer (2014)."
  },
  {
    "id": 1618,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://phontron.com/lader",
    "section_title": "6 Experiments",
    "add_info": "7 Available open-source: http://phontron.com/lader",
    "text": "For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types of pre-ordering: original order with F 0 \u2190 F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3-step), and the proposed model with latent derivations (lader). [Cite_Footnote_7] Except when stated oth-erwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10 \u22123 (chosen through cross-validation)."
  },
  {
    "id": 1619,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.d.umn.edu/~tpederse/data.html",
    "section_title": "5 Word Translation Disambiguation 6.1 Experiment 1: WSD Benchmark Data",
    "add_info": "2 http://www.d.umn.edu/~tpederse/data.html.",
    "text": "We first applied BB, MB-B, and MB-D to translation of the English words \u2018line\u2019 and \u2018interest\u2019 using a benchmark data [Cite_Footnote_2] . The data mainly consists of articles in the Wall Street Journal and it is designed for conducting Word"
  },
  {
    "id": 1620,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://encarta.msn.com/default.asp",
    "section_title": "5 Word Translation Disambiguation 6.2 Experiment 2: Yarowsky\u2019s Words",
    "add_info": "4 http://encarta.msn.com/default.asp",
    "text": "For each of the words, we extracted about 200 sentences containing the word from the Encarta [Cite_Footnote_4] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great Encyclopedia Chinese corpus as unclassified data in Chinese. We defined, for each translation, a seed word in English as a classified example (cf., Table 5)."
  },
  {
    "id": 1621,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.whlib.ac.cn/sjk/bkqs.htm",
    "section_title": "5 Word Translation Disambiguation 6.2 Experiment 2: Yarowsky\u2019s Words",
    "add_info": "5 http://www.whlib.ac.cn/sjk/bkqs.htm",
    "text": "For each of the words, we extracted about 200 sentences containing the word from the Encarta English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great Encyclopedia [Cite_Footnote_5] Chinese corpus as unclassified data in Chinese. We defined, for each translation, a seed word in English as a classified example (cf., Table 5)."
  },
  {
    "id": 1622,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/yfeng21/notaprediction",
    "section_title": "References",
    "add_info": "1 Our datasets for the NOTA task are released at https://github.com/yfeng21/notaprediction",
    "text": "This paper discusses the importance of uncov-ering uncertainty in end-to-end dialog tasks and presents our experimental results on uncer-tainty classification on the processed Ubuntu Dialog Corpus [Cite_Footnote_1] . We show that instead of re-training models for this specific purpose, we can capture the original retrieval model\u2019s un-derlying confidence concerning the best pre-diction using trivial additional computation."
  },
  {
    "id": 1623,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://tedlab.mit.edu/\u223cdr/Tgrep2/",
    "section_title": "2 Related Work",
    "add_info": "1 http://tedlab.mit.edu/\u223cdr/Tgrep2/",
    "text": "There are several specialized tools for indexing and querying treebanks. (See Bird et al. (2005) for an overview and critical comparisons.) TGrep2 [Cite_Footnote_1] is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts. It allows Boolean expressions over nodes and regular expressions in-side nodes. Matching uses a binary index and is performed recursively starting at the top node in the query. TIGERSearch is associated with the German syntactic corpus TIGER. The tool is more typed than TGrep2 and allows search over discontinuous con-stituents that are common in German. TIGERSearch stores the corpus in a Prolog-like logical form and searches using unification matching. LPath is an extension of XPath with three features: immedi-ate precedence, subtree scoping and edge alignment. The queries are executed in an SQL database (Lai and Bird, 2004). Other tree query languages include CorpusSearch, Gsearch, Linguist\u2019s Search Engine, Netgraph, TIQL, VIQTORYA etc."
  },
  {
    "id": 1624,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/",
    "section_title": "2 Related Work",
    "add_info": "2 http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/",
    "text": "There are several specialized tools for indexing and querying treebanks. (See Bird et al. (2005) for an overview and critical comparisons.) TGrep2 is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts. It allows Boolean expressions over nodes and regular expressions in-side nodes. Matching uses a binary index and is performed recursively starting at the top node in the query. TIGERSearch [Cite_Footnote_2] is associated with the German syntactic corpus TIGER. The tool is more typed than TGrep2 and allows search over discontinuous con-stituents that are common in German. TIGERSearch stores the corpus in a Prolog-like logical form and searches using unification matching. LPath is an extension of XPath with three features: immedi-ate precedence, subtree scoping and edge alignment. The queries are executed in an SQL database (Lai and Bird, 2004). Other tree query languages include CorpusSearch, Gsearch, Linguist\u2019s Search Engine, Netgraph, TIQL, VIQTORYA etc."
  },
  {
    "id": 1625,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://agtk.sourceforge.net/",
    "section_title": "2 Related Work",
    "add_info": "3 http://agtk.sourceforge.net/",
    "text": "Bird and Liberman (2001) introduce an abstract general annotation approach, based on annotation graphs. [Cite_Footnote_3] The model is best suited for speech data, where time constraints are limited within an inter-val, but it is unnecessarily complex for supporting annotations on written text."
  },
  {
    "id": 1626,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.nlm.nih.gov/pubs/factsheets/medline.html",
    "section_title": "3 The Layered Query Language",
    "add_info": "4 http://www.nlm.nih.gov/pubs/factsheets/medline.html",
    "text": "\u2022 Scalability to large collections such as MED-LINE (containing millions of documents). [Cite_Footnote_4]"
  },
  {
    "id": 1627,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.ncbi.nlm.nih.gov/LocusLink",
    "section_title": "3 The Layered Query Language",
    "add_info": "7 http://www.ncbi.nlm.nih.gov/LocusLink",
    "text": "Word, POS and shallow parse layers are sequen-tial (the latter can skip or span multiple words). The gene/protein layer assigns IDs from the LocusLink database of gene names. [Cite_Footnote_7] For a given gene there are as many LocusLink IDs as the number of organisms it is found in (e.g., 4 in the case of the gene Bcl-2)."
  },
  {
    "id": 1628,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Extend",
    "url": "http://www.nlm.nih.gov/mesh/meshhome.html",
    "section_title": "3 The Layered Query Language",
    "add_info": "8 http://www.nlm.nih.gov/mesh/meshhome.html",
    "text": "The MeSH layer contains entities from the hier-archical medical ontology MeSH (Medical Subject Headings). [Cite_Footnote_8] The MeSH annotations on Figure 1 are overlapping (share the word cell) and hierarchical both ways: spanning, since blood cell (with MeSH id D001773) orthographically spans the word cell (id A11), and ontologically, since blood cell is a kind of cell and cell death (id D016923) is a kind of Bio-logical Phenomena."
  },
  {
    "id": 1629,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/Leechikara/Dialogue-Based-Anti-Fraud",
    "section_title": "References",
    "add_info": "1 https://github.com/Leechikara/Dialogue-Based-Anti-Fraud",
    "text": "Identity fraud detection is of great importance in many real-world scenarios such as the finan-cial industry. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in loan applications and propose to solve this problem with a novel interactive dialogue system which consists of two modules. One is the knowledge graph (KG) constructor organizing the personal infor-mation for each loan applicant. The other is structured dialogue management that can dy-namically generate a series of questions based on the personal KG to ask the applicants and determine their identity states. We also present a heuristic user simulator based on problem analysis to evaluate our method. Experiments have shown that the trainable dialogue system can effectively detect fraudsters, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexi-ble, which can help promote real-world appli-cations. [Cite_Footnote_1]"
  },
  {
    "id": 1630,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://www.ownthink.com",
    "section_title": "2 Knowledge Graph Constructor",
    "add_info": "3 https://www.ownthink.com",
    "text": "There are four types of personal information in a Chinese loan application form: \u201cSchool\u201d, \u201cCom-pany\u201d, \u201cResidence\u201d and \u201cBirthPlace\u201d. To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG [Cite_Footnote_3] and crawl triplets that are directly related to them. How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked. Thus we use wealthy geographic information about or-ganizations and locations in electronic maps (e.g., Amap ) to complete the KG."
  },
  {
    "id": 1631,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/YerevaNN/WARP",
    "section_title": "1 Introduction",
    "add_info": "1 Our implementation is publicly available at: https://github.com/YerevaNN/WARP",
    "text": "In this paper, we introduce a novel technique to find optimal prompts. We call our method WARP: Word-level Adversarial RePrograming [Cite_Footnote_1] . The method is inspired by adversarial reprogramming (Elsayed et al., 2019) \u2014 a method of adding ad-versarial perturbations to an input image that re-programs a pretrained neural network to perform classification on a task other than the one it was originally trained for."
  },
  {
    "id": 1632,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs",
    "section_title": "4 Experiments on GLUE 4.1 Tasks",
    "add_info": "4 https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs",
    "text": "Table 1: Test set results on GLUE Benchmark. The results are obtained from the GLUE Evaluation server. The subscript next to TinyBERT corresponds to the number of layers in the model. WARP for RTE, STS-B and MRPC are intialized from the MNLI parameters. Results for WNLI are not shown, although they are counted in the averaged GLUE score (AVG column). The last column # shows the number of trainable parameters. WARP\u2019s average performance is higher than all models with up to three orders of magnitude more trainable parameters. Fully fine-tuned RoBERTa and the current state-of-the-art method (DeBERT) score higher by 6.5 and 9.2 points, respectively. Linear Classifier 64.2 78.1 74.9 59.2 88.4 WARP 0 70.9 78.8 77.1 72.2 89.8 WARP 1 83.9 87.6 81.6 72.6 93.8 WARP 2 85.4 88.0 81.5 69.7 94.3 WARP [Cite_Footnote_4] 86.9 92.4 83.1 68.2 95.9 WARP 8 87.6 93.0 83.8 72.9 95.4 WARP init 86.8 90.4 83.6 80.1 96.0 WARP 20 88.2 93.5 84.5 75.8 96.0 WARP MNLI 86.3"
  },
  {
    "id": 1633,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html",
    "section_title": "References",
    "add_info": "8 https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.htmlels. [MASK] means the prompts are intialized with the word embedding of same token, and MNLI means the prompt is initialized with the prompts of out best MNLI run.",
    "text": "We disable all dropouts inside Transformer. We use huggingface implementation of AdamW op-timizer with weight decay disabled. The gradi-ent is normalized to the value 1.0. For the batch sampling we use bucketing with padding noise of 0.1. In order to use the device memory more ef-fectively, we also set maximum number of tokens per batch to 2048. The maximum sequence length is truncated to 512 tokens. We enable mixed preci-sion and pad all sequence lengths to the multiples of 8 for the effective usage of TensorCores [Cite_Footnote_8] ."
  },
  {
    "id": 1634,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://linear-transformers.com",
    "section_title": "4 Experimental Setup 4.3 Implementation",
    "add_info": "1 https://linear-transformers.com",
    "text": "We re-implement the Transformer and use the orig-inal implementation of the LT. [Cite_Footnote_1] All models are trained to minimise cross-entropy with the AdamW optimiser (Loshchilov and Hutter, 2019). We use 300-D GloVe embeddings (Pennington et al., 2014) which are passed through a linear projection layer with size d model . All experiments were performed on a GPU GeForce GTX 1080 Ti. Details on the im-plementation, hyperparameters and reproducibility are available in the Appendix. Our implementation is publicly available."
  },
  {
    "id": 1635,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/pkhdipraja/towards-incremental-transformers",
    "section_title": "4 Experimental Setup 4.3 Implementation",
    "add_info": "2 https://github.com/pkhdipraja/towards-incremental-transformers",
    "text": "We re-implement the Transformer and use the orig-inal implementation of the LT. All models are trained to minimise cross-entropy with the AdamW optimiser (Loshchilov and Hutter, 2019). We use 300-D GloVe embeddings (Pennington et al., 2014) which are passed through a linear projection layer with size d model . All experiments were performed on a GPU GeForce GTX 1080 Ti. Details on the im-plementation, hyperparameters and reproducibility are available in the Appendix. Our implementation is publicly available. [Cite_Footnote_2]"
  },
  {
    "id": 1636,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.comet.ml/docs/python-sdk/introduction-optimizer/",
    "section_title": "Training details",
    "add_info": "4 https://www.comet.ml/docs/python-sdk/introduction-optimizer/",
    "text": "We also apply label smoothing (Szegedy et al., 2016) with = 0.1 for sequence classification to make the model more robust for incremental pro-cessing. For OOV words, we randomly replace tokens by \"UNK\" token with p = 0.02 during training and use it for testing (\u017dilka and Jurc\u030c\u00edc\u030cek, 2015). We perform hyperparameter search using Comet\u2019s Bayesian search algorithm [Cite_Footnote_4] , maximising F1 score for sequence tagging and accuracy for sequence classification on the validation set. The hyperparameter search trials are limited to 20 for all of our experiments. The hyperparameters for LT were also used for LT+R. We use similar hyper-parameters for LT+R+CM and LT+R+CM+D. We set the seed to 42119392 for all of our experiments."
  },
  {
    "id": 1637,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/natashamjaques/neural_chat/tree/master/BatchRL",
    "section_title": "4 Learning from talking to humans 4.2 Hosting real-time conversations online",
    "add_info": null,
    "text": "Figure 2 shows a screenshot of the interface, which includes buttons that allow users to give manual feedback on responses they particularly liked or disliked. Users were encouraged to use these buttons, and we sum these manual votes to create an overall votes score. After chatting, users were asked to provide a Likert scale rating of the bot\u2019s conversation quality, fluency, diversity, contingency/relatedness, and empathy. The code for the RL models is available in open-source at  https://github.com/natashamjaques/neural_chat/tree/master/BatchRL. Using the server, we collected a batch of human interaction data containing 46,061 pairs of user input and agent response. Because humans may use inappropriate language with bots online (see (Horton, 2016)), we filtered this data to remove 1 character responses, profanities, and invalid inputs for a remaining total of 45,179 response pairs. This filtering step is important to ensure undesirable human behavior is not learned by the RL algorithms. The offline data was used to train the RL models as described in Section 3."
  },
  {
    "id": 1638,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://ai.tencent.com/ailab/Encoding_Conversation_Context_",
    "section_title": "References",
    "add_info": "1 Our datasets are released at: http://ai.tencent.com/ailab/Encoding_Conversation_Context_ for_Neural_Keyphrase_Extraction_from_ Microblog_Posts.html",
    "text": "Existing keyphrase extraction methods suffer from data sparsity problem when they are con-ducted on short and informal texts, especially microblog messages. Enriching context is one way to alleviate this problem. Considering that conversations are formed by reposting and re-plying messages, they provide useful clues for recognizing essential content in target posts and are therefore helpful for keyphrase iden-tification. In this paper, we present a neural keyphrase extraction framework for microblog posts that takes their conversation context into account, where four types of neural encoders, namely, averaged embedding, RNN, attention, and memory networks, are proposed to repre-sent the conversation context. Experimental results on Twitter and Weibo datasets [Cite_Footnote_1] show that our framework with such encoders outper-forms state-of-the-art approaches."
  },
  {
    "id": 1639,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://trec.nist.gov/data/tweets/",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "4 http://trec.nist.gov/data/tweets/",
    "text": "Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track [Cite_Footnote_4] . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its \u201cin reply to status id\u201d included. Recursively, we searched the \u201cin re-ply to\u201d tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014."
  },
  {
    "id": 1640,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://developer.twitter.com/en/docs/tweets/search/api-reference/get-saved_searches-show-id",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "5 http://developer.twitter.com/en/docs/tweets/search/api-reference/get-saved_searches-show-id",
    "text": "Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track . To recover conver-sations, we used Tweet Search API [Cite_Footnote_5] to retrieve full information of a tweet with its \u201cin reply to status id\u201d included. Recursively, we searched the \u201cin re-ply to\u201d tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014."
  },
  {
    "id": 1641,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://open.weibo.com/wiki/Trends/hourly",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "6 http://open.weibo.com/wiki/Trends/hourly",
    "text": "Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its \u201cin reply to status id\u201d included. Recursively, we searched the \u201cin re-ply to\u201d tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags [Cite_Footnote_6] on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014."
  },
  {
    "id": 1642,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.open.weibo.com/wiki/2/search/topics",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "7 http://www.open.weibo.com/wiki/2/search/topics",
    "text": "Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its \u201cin reply to status id\u201d included. Recursively, we searched the \u201cin re-ply to\u201d tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API [Cite_Footnote_7] to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014."
  },
  {
    "id": 1643,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cs.cmu.edu/\u02dcark/TweetNLP/",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "9 http://www.cs.cmu.edu/\u02dcark/TweetNLP/",
    "text": "We preprocessed Twitter dataset with Twitter NLP tool [Cite_Footnote_9] (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization. For Weibo dataset, we used NLPIR tool 10 (Zhang et al., 2003) for Chi-nese word segmentation. In particular, Weibo con-versations have an relatively wide range (from 3 to 8,846 words), e.g., one conversation could contain up to 447 messages. If use the maximum length of all conversations as the input length for en-coders, padding the inputs will lead to a sparse ma-trix. Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of messages and then encode the pro-duced summaries. In contrast, we do not sum-marize Twitter conversations because their length range is much narrower (from 4 to 1,035 words)."
  },
  {
    "id": 1644,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://openie.allenai.org/",
    "section_title": "2 Framework Description 2.2 Answer-relevant Relation Extraction",
    "add_info": "1 http://openie.allenai.org/",
    "text": "We utilize an off-the-shelf toolbox of OpenIE [Cite_Footnote_1] to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in dif-ferent granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure 2 shows n-ary relations extracted from OpenIE. As we can see, OpenIE extracts multiple relations for complex sentences. Here we select the most infor-mative relation according to three criteria in the order of descending importance: (1) having the maximal number of overlapped tokens between the answer and the relation; (2) being assigned the highest confidence score by OpenIE; (3) contain-ing maximum non-stop words. As shown in Fig-ure 2, our criteria can select answer-relevant rela-tions (waved in Figure 2), which is especially use-ful for sentences with extraneous information. In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context."
  },
  {
    "id": 1645,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://res.qyzhou.me/redistribute.zip",
    "section_title": "3 Experimental Setting 3.1 Dataset & Metrics",
    "add_info": "2 https://res.qyzhou.me/redistribute.zip",
    "text": "We conduct experiments on the SQuAD dataset (Rajpurkar et al., 2016). It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the arti-cles. We employ two different data splits by fol-lowing (Zhou et al., 2017) [Cite_Footnote_2] and (Du et al., 2017) . In (Zhou et al., 2017), the original SQuAD de-velopment set is evenly divided into dev and test sets, while (Du et al., 2017) treats SQuAD devel-opment set as its test set and splits SQuAD training set into a training set (90%) and a development set (10%). We also filter out questions which do not have any overlapped non-stop words with the cor-responding sentences and perform some prepro-cessing steps, such as tokenization and sentence splitting. The data statistics are given in Table 3."
  },
  {
    "id": 1646,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/UKPLab/acl2020-confidence-regularization",
    "section_title": "References",
    "add_info": "1 The code is available at https://github.com/UKPLab/acl2020-confidence-regularization",
    "text": "Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brit-tle against test cases outside the training dis-tribution. Recently, several proposed debias-ing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the ex-pense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the result-ing models on broader types of examples be-yond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debias-ing method, called confidence regularization, which discourage models from exploiting bi-ases while enabling them to receive enough incentive to learn from all the training ex-amples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy. [Cite_Footnote_1]"
  },
  {
    "id": 1647,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/TalSchuster/FeverSymmetric",
    "section_title": "4 Experimental Setup 4.2 Fact Verification",
    "add_info": "3 https://github.com/TalSchuster/FeverSymmetric",
    "text": "Fever-Symmetric Schuster et al. (2019) intro-duce this dataset to demonstrate that FEVER mod-els mostly rely on the claim-only bias, i.e., the occurrence of words and phrases in the claim that are biased toward certain labels. The dataset is manually constructed such that relying on cues of the claim can lead to incorrect predictions. We evaluate the models on the two versions (version 1 and 2) of their test sets. [Cite_Footnote_3]"
  },
  {
    "id": 1648,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html",
    "section_title": "300",
    "add_info": "1 We used the random sampling code available at http://people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html (Banerjee et al., 2005).",
    "text": "We sampled [Cite_Footnote_1] 100 objects from each of the ten dis-tributions (clusters), and made a dataset of 1,000 ob-jects in total."
  },
  {
    "id": 1649,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.ofai.at/",
    "section_title": "7 Experiments",
    "add_info": "2 We used the Matlab script downloaded from http://www.ofai.at/ \u223c dominik.schnitzer/mp/.",
    "text": "For comparison, we also tested two recently pro-posed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012) [Cite_Footnote_2] , and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the co-sine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, we computed both the regularized Laplacian ker-nel (Chebotarev and Shamis, 1997; Smola and Kon-dor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels."
  },
  {
    "id": 1650,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://archive.ics.uci.edu/ml/",
    "section_title": "7 Experiments 7.2 Document classification 7.2.1 Task and dataset",
    "add_info": null,
    "text": "Two multiclass document classification datasets were used: Reuters Transcribed and Mini News-groups, distributed at  http://archive.ics.uci.edu/ml/. The properties of the datasets are summarized in Ta-ble 2."
  },
  {
    "id": 1651,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "References",
    "add_info": null,
    "text": "Recent progress in natural language process-ing has been driven by advances in both model architecture and model pretraining. Trans-former architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this ca-pacity for a wide variety of tasks. Trans-formers is an open-source library with the goal of opening up these advances to the wider machine learning community. The li-brary consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a cu-rated collection of pretrained models made by and available for the community. Trans-formers is designed to be extensible by re-searchers, simple for practitioners, and fast and robust in industrial deployments. The li-brary is available at  https://github.com/huggingface/transformers."
  },
  {
    "id": 1652,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/huggingface/transformers",
    "text": "Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub [Cite_Footnote_1] . Detailed documentation and tutorials are available on Hugging Face\u2019s website ."
  },
  {
    "id": 1653,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/huggingface/swift-coreml-transformers",
    "section_title": "5 Deployment",
    "add_info": "3 https://github.com/huggingface/swift-coreml-transformers",
    "text": "Finally, as Transformers become more widely used in all NLP applications, it is increasingly im-portant to deploy to edge devices such as phones or home electronics. Models can use adapters to convert models to CoreML weights that are suit-able to be embedded inside a iOS application, to enable on-the-edge machine learning. Code is also made available [Cite_Footnote_3] . Similar methods can be used for Android devices."
  },
  {
    "id": 1654,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/changzhisun/AntNRE",
    "section_title": "1 Introduction",
    "add_info": "1 Our implementation is available at https://github.com/changzhisun/AntNRE.",
    "text": "To further utilize the structure of the graph, we also propose assigning different weights on graph edges. In particular, we introduce a binary relation classification task, which is to determine whether the two entities form a valid relation. Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary rela-tion classification, which makes the proposed ad-jacency matrix more explanatory. To summarize, the main contributions of this work are [Cite_Footnote_1]"
  },
  {
    "id": 1655,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/Tom556/OrthogonalTransformerProbing",
    "section_title": "3 Method",
    "add_info": null,
    "text": "Scaling Vector d\u00affor a specific objective, allowing probing for multiple linguistic tasks simultaneously. In this work, an individual Orthogonal Transfor-mation V is trained for each language, facilitating multi-language probing. This approach assumes that the representations are isomorphic across lan-guages; we examine this claim in our experiments. Our implementation is available at GitHub:  https://github.com/Tom556/OrthogonalTransformerProbing."
  },
  {
    "id": 1656,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://search.carrot2.org/stable/search",
    "section_title": "4 Evaluation 4.3 Comparative Evaluation",
    "add_info": "4 http://search.carrot2.org/stable/search [Last access: 15/05/2013].",
    "text": "With respect to implementation, we used the Carrot2 APIs [Cite_Footnote_4] which are freely available for STC, LINGO and the classical BIK. It is worth notic-ing that all implementations in Carrot2 are tuned to extract exactly 10 clusters. For OPTIMSRC, we reproduced the results presented in the paper of (Carpineto and Romano, 2010) as no imple-mentation is freely available. The results are il-lustrated in Table 2 including both F \u03b2 -measure and F b 3 . They evidence clear improvements of our methodology when compared to state-of-the-art text-based PRC algorithms, over both datasets and all evaluation metrics. But more important, even when the p-context vector is small (p = 3), the adapted GK-means outperforms all other ex-isting text-based PRC which is particularly impor-tant as they need to perform in real-time."
  },
  {
    "id": 1657,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/malllabiisc/pra-oda",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We term this procedure as On-Demand Aug-mentation (ODA), because the search can be per-formed during test time in an on-demand man-ner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013), and vector space random walk PRA (Gard-ner et al., 2014) are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; Gardner et al., 2014). Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments sug-gest that ODA provides better performance than (Gardner et al., 2013) and nearly the same pre-diction performance as provided by (Gardner et al., 2014), but in both cases with the added ad-vantage of faster running time and greater flex-ibility due to its online and on-demand nature. The code along with the results can be obtained at  https://github.com/malllabiisc/pra-oda."
  },
  {
    "id": 1658,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/facebookresearch/mlqa",
    "section_title": "References",
    "add_info": "1 MLQA is publicly available at https://github.com/facebookresearch/mlqa",
    "text": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challeng-ing. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned ex-tractive QA evaluation benchmark intended to spur research in this area. [Cite_Footnote_1] MLQA contains QA instances in 7 languages, English, Ara-bic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K in-stances in English and 5K in each other lan-guage, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance."
  },
  {
    "id": 1659,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/facebookresearch/LASER",
    "section_title": "2 The MLQA corpus 2.1 Parallel Sentence Mining",
    "add_info": "3 https://github.com/facebookresearch/LASER",
    "text": "To detect parallel sentences we use the LASER toolkit, [Cite_Footnote_3] which achieves state-of-the-art perfor-mance in mining parallel sentences (Artetxe and Schwenk, 2019). LASER uses multilingual sen-tence embeddings and a distance or margin cri-terion in the embeddings space to detect parallel sentences. The reader is referred to Artetxe and Schwenk (2018) and Artetxe and Schwenk (2019) for a detailed description. See Appendix A.6 for further details and statistics on the number of par-allel sentences mined for all language pairs."
  },
  {
    "id": 1660,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.unicode.org/reports/tr44/tr44-4.html#General_Category_Values",
    "section_title": "4 Cross-lingual QA Experiments 4.1 Evaluation Metrics for Multilingual QA",
    "add_info": "6 http://www.unicode.org/reports/tr44/tr44-4.html#General_Category_Values",
    "text": "Most extractive QA tasks use Exact Match (EM) and mean token F1 score as performance metrics. The widely-used SQuAD evaluation also performs the following answer-preprocessing operations: i) lowercasing, ii) stripping (ASCII) punctuation iii) stripping (English) articles and iv) whitespace to-kenisation. We introduce the following modifica-tions for fairer multilingual evaluation: Instead of stripping ASCII punctuation, we strip all unicode characters with a punctuation General Category. [Cite_Footnote_6] When a language has stand-alone articles (English, Spanish, German and Vietnamese) we strip them. We use whitespace tokenization for all MLQA lan-guages other than Chinese, where we use the mixed segmentation method from Cui et al. (2019b)."
  },
  {
    "id": 1661,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/BYVoid/OpenCC",
    "section_title": "A Appendices A.5 Additional preprocessing Details",
    "add_info": null,
    "text": "OpenCC (  https://github.com/BYVoid/OpenCC ) is used to convert all Chinese contexts to Simplified Chinese, as wikipedia dumps generally consist of a mixture of simplified and traditional Chinese text."
  },
  {
    "id": 1662,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/tingyaohsu/VIST-Edit",
    "section_title": "References",
    "add_info": "1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.",
    "text": "We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [Cite_Footnote_1] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evalu-ation scores and human ratings, motivating the need for new automatic metrics."
  },
  {
    "id": 1663,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.ncbi.nlm.nih.gov/pubmed/",
    "section_title": "3 Linguistic acceptability study 3.1 Dataset creation",
    "add_info": "1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk",
    "text": "We used equal numbers of sentences from three different genres [Cite_Footnote_1] :"
  },
  {
    "id": 1664,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.natcorp.ox.ac.uk",
    "section_title": "3 Linguistic acceptability study 3.1 Dataset creation",
    "add_info": "1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk",
    "text": "We used equal numbers of sentences from three different genres [Cite_Footnote_1] :"
  },
  {
    "id": 1665,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/uhh-lt/Taxonomy_Refinement_Embeddings",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/uhh-lt/Taxonomy_Refinement_Embeddings",
    "text": "We compare two types of dense vector em-beddings: the standard word2vec CBOW model (Mikolov et al., 2013a,b), that embeds terms in Euclidean space based on distributional similarity, and the more recent Poincare\u0301 embeddings (Nickel and Kiela, 2017), which capture similarity as well as hierarchical relationships in a hyperbolic space. The source code has been published [Cite_Footnote_1] to recreate the employed embedding, to refine taxonomies as well as to enable further research of Poincare\u0301 em-beddings for other semantic tasks."
  },
  {
    "id": 1666,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://jobimtext.org",
    "section_title": "3 Taxonomy Refinement using Hyperbolic Word Embeddings 3.1 Domain-specific Poincare\u0301 Embedding",
    "add_info": "2 http://jobimtext.org: The PattaMaika compo-nent is based on UIMA RUTA (Kluegl et al., 2016).",
    "text": "Training Dataset Construction To create domain-specific Poincare\u0301 embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gi-gaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [Cite_Footnote_2] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016)."
  },
  {
    "id": 1667,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/stanfordnlp/GloVe",
    "section_title": "4 Experiments and Evaluation 4.2 Implementation Details",
    "add_info": "9 We download the pre-trained word embeddings from https://github.com/stanfordnlp/GloVe, and we select the smaller Wikipedia 2014 + Gigaword 5.",
    "text": "We use the validation set (SE7) to find the optimal settings of our framework: the hidden state size n, the number of passes |T M |, the optimizer, etc. We use pre-trained word embeddings with 300 di-mensions [Cite_Footnote_9] , and keep them fixed during the train-ing process. We employ 256 hidden units in both the gloss module and the context module, which means n=256. Orthogonal initialization is used for weights in LSTM and random uniform initializa-tion with range [-0.1, 0.1] is used for others. We assign gloss expansion depth K the value of 4. We also experiment with the number of passes |T M | from 1 to 5 in our framework, finding |T M | = 3 performs best. We use Adam optimizer (Kingma and Ba, 2014) in the training process with 0.001 initial learning rate. In order to avoid overfitting, we use dropout regularization and set drop rate to 0.5. Training runs for up to 100 epochs with early stopping if the validation loss doesn\u2019t im-prove within the last 10 epochs."
  },
  {
    "id": 1668,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.nlm.nih.gov/bsd/pmresources.html",
    "section_title": "1 Introduction",
    "add_info": "2 http://www.nlm.nih.gov/bsd/pmresources.html",
    "text": "Demonstrative nouns, along with pronouns like both and either, are referred to as sortal anaphors (Castan\u0303o et al., 2002; Lin and Liang, 2004; Torii and Vijay-Shanker, 2007). Castan\u0303o et al. observed that sortal anaphors are prevalent in the biomedi-cal literature. They noted that among 100 distinct anaphors derived from a corpus of 70 Medline ab-stracts, 60% were sortal anaphors. But how often do demonstrative nouns refer to abstract objects? We observed that from a corpus of 74,000 randomly chosen Medline [Cite_Footnote_2] abstracts, of the first 150 most fre-quently occurring distinct demonstrative nouns (fre-quency > 30), 51.3% were abstract, 41.3% were concrete, and 7.3% were discourse deictic. This shows that abstract anaphora resolution is an impor-tant component of general anaphora resolution in the biomedical domain. However, automatic resolution of this type of anaphora has not attracted much atten-tion and the previous work for this task is limited."
  },
  {
    "id": 1669,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.stanford.edu/software/lex-parser.shtml",
    "section_title": "4 Resolution Algorithm 4.1 Candidate Extraction",
    "add_info": "7 http://nlp.stanford.edu/software/lex-parser.shtml",
    "text": "For correct resolution, the set of extracted candidates must contain the correct antecedent in the first place. The problem of candidate extraction is non-trivial in abstract anaphora resolution because the antecedents are of many different types of syntactic constituents such as clauses, sentences, and nominalizations. Drawing on our observation that the mixed type an-tecedents are generally a combination of different well-defined syntactic constituents, we extract the set of candidate antecedents as follows. First, we create a set of candidate sentences which contains the sentence containing the this-issue anaphor and the two preceding sentences. Then, we parse every candidate sentence with the Stanford Parser [Cite_Footnote_7] . Ini-tially, the set of candidate constituents contains a list of well-defined syntactic constituents. We re-quire that the node type of these constituents be in the set {S, SBAR, NP, SQ, SBARQ, S+V}. This set was empirically derived from our data. To each constituent, there is associated a set of mixed type constituents. These are created by concatenating the original constituent with its sister constituents. For example, in (4), the set of well-defined eligible can-didate constituents is {NP, NP 1 } and so NP 1 PP 1 is a mixed type candidate."
  },
  {
    "id": 1670,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cogcomp.cs.illinois.edu/page/software_view/SRL",
    "section_title": "4 Resolution Algorithm 4.2 Features",
    "add_info": "9 http://cogcomp.cs.illinois.edu/page/software_view/SRL",
    "text": "We explored the effect of including 43 automati-cally extracted features (12 feature classes), which are summarized in Table 2. The features can also be broadly divided into two groups: issue-specific fea-tures and general abstract-anaphora features. Issue-specific features are based on our common-sense knowledge of the concept of issue and the different semantic forms it can take; e.g., controversy (X is controversial), hypothesis (It has been hypothesized X), or lack of knowledge (X is unknown), where X is the issue. In our data, we observed certain syn-tactic patterns of issues such as whether X or not and that X and the IP feature class encodes this in-formation. Other issue-specific features are IVERB and IHEAD. The feature IVERB checks whether the governing verb of the candidate is an issue verb (e.g., speculate, hypothesize, argue, debate), whereas IHEAD checks whether the candidate head in the dependency tree is an issue word (e.g., contro-versy, uncertain, unknown). The general abstract-anaphora resolution features do not make use of the semantic properties of the word issue. Some of these features are derived empirically from the training data (e.g., ST, L, D). The EL feature is bor-rowed from Mu\u0308ller (2008) and encodes the embed-ding level of the candidate within the candidate sen-tence. The MC feature tries to capture the idea of the THIS-NPs hypothesis (Gundel et al., 1993; Poesio and Modjeska, 2002) that the antecedents of this- NP anaphors are not the center of the previous utter-ance. The general abstract-anaphora features in the SR feature class capture the semantic role of the can-didate in the candidate sentence. We used the Illinois Semantic Role Labeler [Cite_Footnote_9] for SR features. The gen-eral abstract-anaphora features also contain a few lexical features (e.g., M, SC). But these features are independent of the semantic properties of the word issue. The general abstract-anaphora resolution fea-tures also contain dependency-tree features, lexical-overlap features, and context features."
  },
  {
    "id": 1671,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://www2.parc.com/isl/groups/nltt/xle/",
    "section_title": "9 Related Work",
    "add_info": "3 http://www2.parc.com/isl/groups/nltt/xle/",
    "text": "One area of current research which has similarities with this work is on Lexical Functional Grammars (LFGs). Both approaches attempt to abstract away from the surface level syntax of the sentence (e.g., the XLE system [Cite_Footnote_3] ). The most obvious difference be-tween the approaches is that we use SRL data to train our system, avoiding the need to have labeled data specific to our simplification scheme."
  },
  {
    "id": 1672,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/yftah89/PBLM-Cross-language-Cross-domain",
    "section_title": "References",
    "add_info": "1 Our code is publicly available at https://github.com/yftah89/PBLM-Cross-language-Cross-domain",
    "text": "While cross-domain and cross-language trans-fer have long been prominent topics in NLP re-search, their combination has hardly been ex-plored. In this work we consider this problem, and propose a framework that builds on pivot-based learning, structure-aware Deep Neu-ral Networks (particularly LSTMs and CNNs) and bilingual word embeddings, with the goal of training a model on labeled data from one (language, domain) pair so that it can be effec-tively applied to another (language, domain) pair. We consider two setups, differing with re-spect to the unlabeled data available for model training. In the full setup the model has ac-cess to unlabeled data from both pairs, while in the lazy setup, which is more realistic for truly resource-poor languages, unlabeled data is available for both domains but only for the source language. We design our model for the lazy setup so that for a given target domain, it can train once on the source language and then be applied to any target language without re-training. In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms pre-vious models even when it is trained in the lazy setup and previous models are trained in the full setup. [Cite_Footnote_1]"
  },
  {
    "id": 1673,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/quankiquanki/",
    "section_title": "6 Experiments",
    "add_info": "Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.",
    "text": "Following ZR18 we also consider a more chal-lenging setup where the English source domain consists of user airline (A) reviews (Nguyen, 2015)  . We use the dataset of ZR18, consisting of 1000 positive and 1000 negative reviews in the la-beled set, and 39396 reviews as the unlabeled set."
  },
  {
    "id": 1674,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/quankiquanki/",
    "section_title": "6 Experiments",
    "add_info": "Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.",
    "text": "Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), CLCD English-German S-T D-B M-B B-D M-D B-M D-M English-French All D-B M-B B-D M-D B-M D-M All PBLM Models P+BE 78.7 78.6 80.6 79.2 81.7 78.5 PBLM 70.9 62.9 74.5 66.5 75.0 75.5 Lazy 74.8 74.0 75.1 72.8 73.3 73.7 79.5 81.1 74.7 76.3 75.0 75.1 76.8 76.5 71.0 76.0 67.9 70.3 69.9 67.3 70.4 70.3 73.9 74.2 73.1 75.3 74.4 74.1 72.4 73.9 Autoencoder+pivot Models A-S-SR 68.3 62.5 69.4 69.9 70.2 69 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015)  ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target A-B A-D A-M All A-B A-D A-M All PBLM Models P+BE 64 6. 65.167.9 62.5 63.6 63.5 66.9 64.8 PBLM 60.9 59.6 60.1 .60 2 60.9 61.9 58.9 60.5 Lazy 66.3 65.0 66.6 66 0. 65.7 65.6 69.0 66.8 Autoencoder+pivot Models A-S-SR 55.8 57.5 60.8 58 55.9 56.2 58.2 55.8 52.9 56.3 55.7 A-SCL 56 8. 55.8 52.9 56.4 55.0 Pivot-based (no DNN) Models C-SCL 56.6 52.6 53.7 54 3. 52.7 54.5 53.1 53.4 DCI 55.9 52.1 54.5 54 1. 53.1 53.7 53.9 53.5 CLCD without CD Learning CNN 59.4 61.2 61.3 . 57.9 55.3 56.260 6 56.5 Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), Within Language German-German S-T D-B M-B B-D M-D B-M D-M French-French All D-B M-B B-D M-D B-M D-M All In-language cross-domain learning (no CD technique is employed) IL 81.5 78.9 77.8 76.7 77.6 79.8 78.7 80.2 78.2 79.2 79.7 78.5 79.7 79.3 In-language, In-domain learning S-T B-B \u2013 D-D \u2013 M-M \u2013 ILID 84.2 \u2013 81.5 \u2013 83.3 \u2013 All B-B \u2013 D-D \u2013 M-M \u2013 All 83 84.1 \u2013 79.2 \u2013 85.8 \u2013 83"
  },
  {
    "id": 1675,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md",
    "section_title": "References",
    "add_info": null,
    "text": "\u2022 The bilingual embeddings are based on the fastText Facebook embeddings (Bojanowski et al., 2017):  https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"
  },
  {
    "id": 1676,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://scikit-learn.org/stable/",
    "section_title": "References",
    "add_info": null,
    "text": "\u2022 Logistic regression classifier:  http://scikit-learn.org/stable/"
  },
  {
    "id": 1677,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/yftah89/Neural-SCLDomain-Adaptation",
    "section_title": "References",
    "add_info": null,
    "text": "\u2022 AE-SCL and AE-SCL-SR: We use the code from the author\u2019s github:  https://github.com/yftah89/Neural-SCLDomain-Adaptation."
  },
  {
    "id": 1678,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers/blob/master/examples/run",
    "section_title": "3 Experiments and Discussion 3.2 Language Model Setup",
    "add_info": "1 https://github.com/huggingface/transformers/blob/master/examples/run lm finetuning.py",
    "text": "We use pretrained GPT2-base (12 layers, 117M parameters), GPT2-large (24 layers, 345M params), BART-base (6 layers encoder and 6 layers decoder, 139M params) and BART-large (12 layers encoder and 12 layers decoder, 406M params) models in our experiments. We adopted the implementation and pretrained models from Wolf et al. (2019). We fine-tune GPT2 and BART on each training dataset separately. We perform a maximum of 10 fine-tuning epochs and adopt early stopping using the validation sets. Most of the hyperparmeters used for fine-tuning are the default ones from Wolf et al. (2019) [Cite_Footnote_1] , except for learning rate for BART, which we set to 1e \u2212 5."
  },
  {
    "id": 1679,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Ryuto10/pzero-improves-zar",
    "section_title": "1 Introduction",
    "add_info": "2 Our code is publicly available: https://github.com/Ryuto10/pzero-improves-zar",
    "text": "\u2022 We design a new ZAR model [Cite_Footnote_2] that makes full use of pretrained MLMs with minimal architectural modifications;"
  },
  {
    "id": 1680,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13",
    "section_title": "1 Introduction",
    "add_info": "3 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13",
    "text": "Obviously, the quality of this measure depends on the quality of estimating q(w), i.e. the general En-glish word distribution, which is usually estimated over a large text collection. The larger the collec-tion is, the better would be the estimation. Recently, Google has released the Web 1T dataset [Cite_Footnote_3] that pro-vides q(w) estimated on a text collection of one tril-lion tokens. We use it in our experimentation."
  },
  {
    "id": 1681,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.umass.edu/\u02dcronb/name_disambiguation.html",
    "section_title": "5 Experimentation 5.1 Web appearance disambiguation",
    "add_info": "7 http://www.cs.umass.edu/\u02dcronb/name_disambiguation.html",
    "text": "We test our models on the WAD dataset, [Cite_Footnote_7] which consists of 1085 Web pages that mention 12 people names of AI researchers, such as Tom Mitchell and Leslie Kaelbling. Out of the 1085 pages, 420 are on-topic, so we apply our algorithms with k = 420. At a preprocessing step, we binarize document vec-tors and remove low frequent words (both in terms of p(w) and q(w)). The results are summarized in the middle column of Table 1. We can see that both OCCC and LTB dramatically outperform their com-petitors, while showing practically indistinguishable results compared to each other. Note that when the size of the word cluster in OCCC is unfairly set to its optimal value, m r = 2200, the OCCC method is able to gain a 2% boost. However, for obvious reasons, the optimal value of m r may not always be obtained in practice."
  },
  {
    "id": 1682,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://projects.ldc.upenn.edu/TDT5/",
    "section_title": "5 Experimentation 5.2 Detecting the topic of the week",
    "add_info": "8 http://projects.ldc.upenn.edu/TDT5/",
    "text": "We evaluate the TW detection task on the bench-mark TDT-5 dataset [Cite_Footnote_8] , which consists of 250 news events spread over a time period of half a year, and 9,812 documents in English, Arabic and Chinese (translated to English), annotated by their relation-ship to those events. The largest event in TDT-5 dataset (#55106, titled \u201cBombing in Riyadh, Saudi Arabia\u201d) has 1,144 documents, while 66 out of the 250 events have only one document each. We split the dataset to 26 weekly chunks (to have 26 full weeks, we delete all the documents dated with the last day in the dataset, which decreases the dataset\u2019s size to 9,781 documents). Each chunk contains from 138 to 1292 documents."
  },
  {
    "id": 1683,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.google.com/apis",
    "section_title": "3 The U-SVM 3.1 Clustering Web Search Results",
    "add_info": null,
    "text": "Here, we give an example illustrating the prin-ciple of clustering web search results in the FGS. In submitting TREC 2004 test question 1.1 \u201dwhen was the first Crip gang started?\u201d to Google (  http://www.google.com/apis), we extract n(= 8) different candidates from the top m(= 30) Google snippets. The Google snippets containing the same candidates are aligned snippets, and thus the 12 re-tained snippets are grouped into 8 clusters, as listed in Table 2. This table roughly indicates that the snip-pets with the same candidate answers contain the same sub-meanings, so these snippets are considered as aligned snippets. For example, all Google snip-pets that contain the candidate answer 1969 express the time of establishment of \u201dthe first Crip gang\u201d."
  },
  {
    "id": 1684,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.csie.ntu.edu.tw/cjlin/libsvm/",
    "section_title": "3 The U-SVM 3.2 Classifying Question",
    "add_info": "1 http://www.csie.ntu.edu.tw/cjlin/libsvm/",
    "text": "This paper selects LIBSVM toolkit [Cite_Footnote_1] to implement the SVM classifier. The kernel is the radical basis function with the parameter \u03b3 = 0.001 in the exper-iments."
  },
  {
    "id": 1685,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Deep Memory Network for Aspect Level Sentiment Classification 3.6 Aspect Level Sentiment Classification",
    "add_info": "4 Available at: http://nlp.stanford.edu/projects/glove/.",
    "text": "P c (s, a) is the probability of predicting (s, a) as cat-egory c produced by our system. P cg (s,a) is 1 or 0, indicating whether the correct answer is c. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent. We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.9M [Cite_Footnote_4] . We randomize other pa-rameters with uniform distribution U(\u22120.01, 0.01), and set the learning rate as 0.01."
  },
  {
    "id": 1686,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://flowacldemo.appspot.com",
    "section_title": "1. Introduction",
    "add_info": "1 FLOW: http://flowacldemo.appspot.com",
    "text": "This paper presents FLOW [Cite_Footnote_1] (Figure 1), an interactive system for assisting EFL writers in composing and revising writing. Different from existing tools, its context-sensitive and first-language-oriented features enable EFL writers to concentrate on their ideas and thoughts without being hampered by the limited lexical resources. Based on the studies that first language use can positively affect second language composing, FLOW attempts to meet such needs. Given any L1 input, FLOW displays appropriate suggestions including translation, paraphrases, and n-grams during composing and revising processes. We use the following example sentences to illustrate these two functionalities."
  },
  {
    "id": 1687,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://ruscorpora.ru/",
    "section_title": "5 Figurative Language Uses as Outliers",
    "add_info": null,
    "text": "For each Russian word shown in Table 3, we extracted from the Russian National Cor-pora (  http://ruscorpora.ru/) several lit-eral and non-literal occurences. Some of these words have more than one meaning in Russian, e.g. \u043a\u043b\u044e\u0447 can be translated as a key or water spring and the word \u043a\u043e\u0441\u0430 as a plait, scythe or spit."
  },
  {
    "id": 1688,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://snowball.tartarus.org/",
    "section_title": "5 Figurative Language Uses as Outliers",
    "add_info": null,
    "text": "All the documents are stemmed and all stop-words are removed with the SnowBall Stem-mer (  http://snowball.tartarus.org/) for the Russian language."
  },
  {
    "id": 1689,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/DancingSoul/NQ_BERT-DM",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We will release our code and models at  https://github.com/DancingSoul/NQ_BERT-DM . Figure 2: System overview. The document fragments of one document are fed into our model independently. The outputs of graph encoders are merged and sent into the answer selection module, which generates a long answer and a short answer."
  },
  {
    "id": 1690,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://bit.ly/2w7nUQK",
    "section_title": "4 Experiments 4.2 Experimental Settings",
    "add_info": "2 This model can be downloaded at https://bit.ly/2w7nUQK.",
    "text": "We use three BERT encoders to initialize our token node representation: 1) BERT-base: a BERT-base-uncased model finetuned on SQuAD 2.0; 2) BERT-large: a BERT-large-uncased model finetuned on SQuAD 2.0; 3) BERT-syn: Google\u2019s BERT-large-uncased model pre-trained on SQuAD2.0 with N-Gram Masking and Syn-thetic Self-Training. [Cite_Footnote_2] Since the Natural Question dataset does not provide sentence-level informa-tion, we additionally use spacy (Honnibal and Mon-tani, 2017) as the sentence segmentor to get the boundaries of sentences."
  },
  {
    "id": 1691,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.cs.cmu.edu/\u02dcnlao/",
    "section_title": "5 Experiments 5.1 Knowledge Base Completion",
    "add_info": "3 http://www.cs.cmu.edu/\u02dcnlao/",
    "text": "We evaluate our approach on a knowledge base generated by the CMU Never Ending Language Learning (NELL) project (Carlson et al., 2010). NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training [Cite_Footnote_3] , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set . The data statistics of the training set are summarized in Table 1. The numbers of triples in the development and test sets are 19,665 and 117,889, respectively. Notice that this dataset is substantially larger than the datasets used in recent work. For example, the Freebase data used in (Socher et al., 2013) and (Bordes et al., 2013a) have 316k and 483k 5 triples, respec-tively, compared to 1.8M in this dataset."
  },
  {
    "id": 1692,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://bit.ly/trescal",
    "section_title": "5 Experiments 5.1 Knowledge Base Completion",
    "add_info": "4 http://bit.ly/trescal",
    "text": "We evaluate our approach on a knowledge base generated by the CMU Never Ending Language Learning (NELL) project (Carlson et al., 2010). NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set [Cite_Footnote_4] . The data statistics of the training set are summarized in Table 1. The numbers of triples in the development and test sets are 19,665 and 117,889, respectively. Notice that this dataset is substantially larger than the datasets used in recent work. For example, the Freebase data used in (Socher et al., 2013) and (Bordes et al., 2013a) have 316k and 483k 5 triples, respec-tively, compared to 1.8M in this dataset."
  },
  {
    "id": 1693,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://statnlp.org/statnlp-framework",
    "section_title": "-",
    "add_info": "1 http://statnlp.org/statnlp-framework",
    "text": "In this tutorial, we will be discussing how such a wide spectrum of existing structured predic-tion models can all be implemented under a uni-fied framework [Cite_Footnote_1] that involves some basic building blocks. Based on such a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conve-niently and quickly. Furthermore, we also show that the framework can be used to solve certain structured prediction problems that otherwise can-not be easily handled by conventional structured prediction models. Specifically, we show how to use such a framework to construct models that are capable of predicting non-conventional structures, such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will also discuss how to make use of the framework to build other related models such as topic models and highlight its po-tential applications in some recent popular tasks (e.g., AMR parsing (Flanigan et al., 2014))."
  },
  {
    "id": 1694,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://research.microsoft.com/users/silviu/WebAssistant/TestData",
    "section_title": "6 Evaluation",
    "add_info": null,
    "text": "We evaluated the system in two ways: on a set of Wikipedia articles, by comparing the system out-put with the references created by human contribu-tors, and on a set of news stories, by doing a post-hoc evaluation of the system output. The evalua-tion data can be downloaded from  http://research.microsoft.com/users/silviu/WebAssistant/TestData. In both settings, we computed a disambiguation baseline in the following manner: for each surface form, if there was an entity page or redirect page whose title matches exactly the surface form then we chose the corresponding entity as the baseline disambiguation; otherwise, we chose the entity most frequently mentioned in Wikipedia using that surface form."
  },
  {
    "id": 1695,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.kaggle.com/c/asap-aes/",
    "section_title": "1 Introduction",
    "add_info": "3 https://www.kaggle.com/c/asap-aes/",
    "text": "At the outset, our goal is to develop a framework that strengthens the validity of state-of-the-art neu-ral AES approaches with respect to adversarial in-put related to local aspects of coherence. For our experiments, we use the Automated Student As-sessment Prize (ASAP) dataset, [Cite_Footnote_3] which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4)."
  },
  {
    "id": 1696,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/Youmna-H/Coherence_AES",
    "section_title": "5 Model Parameters and Baselines",
    "add_info": "12 Our implementation is available at https://github.com/Youmna-H/Coherence_AES",
    "text": "Coherence models We train and test the LC model described in Section 3.1 on the synthetic dataset and evaluate it using PRA and TPRA. Dur-ing pre-processing, words are lowercased and ini-tialized with pre-trained word embeddings (Zou et al., 2013). Words that occur only once in the training set are mapped to a special UNK embed-ding. All network weights are initialized to values drawn randomly from a uniform distribution with scale = 0.05, and biases are initialized to zeros. We apply a learning rate of 0.001 and RMSProp (Tieleman and Hinton, 2012) for optimization. A size of 100 is chosen for the hidden layers (d lstm and d cnn ), and the convolutional window size (m) is set to 3. Dropout (Srivastava et al., 2014) is ap-plied for regularization to the output of the convo-lutional operation with probability 0.3. The net-work is trained for 60 epochs and performance is monitored on the development sets \u2013 we select the model that yields the highest PRA value. [Cite_Footnote_12]"
  },
  {
    "id": 1697,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://bitbucket.org/melsner/browncoherence",
    "section_title": "5 Model Parameters and Baselines",
    "add_info": "13 https://bitbucket.org/melsner/browncoherence",
    "text": "We use as a baseline the LC model that is based on the multiplication of the clique scores (simi-larly to Li and Hovy (2014)), and compare the results (LC mul ) to our averaged approach. As another baseline, we use the entity grid (EGrid) (Barzilay and Lapata, 2008) that models transi-tions between sentences based on sequences of en-tity mentions labeled with their grammatical role. EGrid has been shown to give competitive re-sults on similar coherence tasks in other domains. Using the Brown Coherence Toolkit (Eisner and Charniak, 2011), [Cite_Footnote_13] we construct the entity transi-tion probabilities with length = 3 and salience = 2. The transition probabilities are then used as fea-tures that are fed as input to an SVM classifier with an RBF kernel and penalty parameter C = 1.5 to predict a coherence score."
  },
  {
    "id": 1698,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/nusnlp/nea",
    "section_title": "5 Model Parameters and Baselines",
    "add_info": "14 https://github.com/nusnlp/nea",
    "text": "LSTM T&N model We replicate and evaluate the LSTM model of Taghipour and Ng (2016) [Cite_Footnote_14] on ASAP and our synthetic data."
  },
  {
    "id": 1699,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://bit.ly/mt-para",
    "section_title": "9 Experimental Setup and Results 9.3 Paraphrase Identification Task",
    "add_info": null,
    "text": "We evaluate paraphrase identification (PI) on the PAN corpus (  http://bit.ly/mt-para, (Madnani et al., 2012)), consisting of training and test sets of 10,000 and 3000 sentence pairs, respectively. Sen-tences are about 40 words long on average."
  },
  {
    "id": 1700,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/dguo98/seqmix",
    "section_title": "3 Method",
    "add_info": "1 Our implementation can be found at https://github.com/dguo98/seqmix, and pseudocode can be found in supplementary materials.",
    "text": "To summarize, this results in a simple algorithm where we sample \u03bb \u223c Beta(\u03b1,\u03b1) and train on these expected samples. [Cite_Footnote_1] Relationship to Existing Methods Table 1 shows that we can recover existing data augmenta-tion methods such as SwitchOut and word dropout under the above framework. In particular, these methods approximate a version of the \u201chard\u201d latent variable objective in Eq. 2 by considering different swap distributions p(m) and sampling distributions D 0 . Compared to other approaches, SeqMix is es-sentially a relaxed variant of the same objective, similar to the difference between soft vs. hard at-tention (Xu et al., 2015; Deng et al., 2018; Wu et al., 2018; Shankar et al., 2018). SeqMix is also more efficient than more sophisticated augmenta-tion strategies such as GECA which requires a com-putationally expensive validation check for swaps."
  },
  {
    "id": 1701,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.cninfo.com.cn/new/index",
    "section_title": "6 Experiments 6.1 Experimental Setup",
    "add_info": "4 Crawling from http://www.cninfo.com.cn/new/index",
    "text": "Data Collection with Event Labeling. We uti-lize ten years (2008-2018) ChFinAnn [Cite_Footnote_4] documents and human-summarized event knowledge bases to conduct the DS-based event labeling. We focus on five event types: Equity Freeze (EF), Equity Re-purchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), which belong to major events required to be disclosed by the regulator and may have a huge impact on the company value. To ensure the labeling quality, we set constraints for matched document-record pairs as Section 4 describes. Moreover, we directly use the character tokenization to avoid error propaga-tions from Chinese word segmentation tools."
  },
  {
    "id": 1702,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/tticoin/AntonymDetection",
    "section_title": "3 Experimental Setup",
    "add_info": "5 https://github.com/tticoin/AntonymDetection",
    "text": "Linguistic Constraints We use three groups of linguistic constraints in the LEAR specialisation model, covering three different relation types which are all beneficial to the specialisation process: di-rected 1) lexical entailment ( LE ) pairs; 2) syn-onymy pairs; and 3) antonymy pairs. Synonyms are included as symmetric ATTRACT pairs (i.e., the B A pairs) since they can be seen as defining a trivial symmetric IS - A relation (Rei and Briscoe, 2014; Vulic\u0301 et al., 2017). For a similar reason, antonyms are clear REPEL constraints as they anti-correlate with the LE relation. Synonymy and antonymy constraints are taken from prior work (Zhang et al., 2014; Ono et al., 2015): they are ex-tracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009). In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on aver-age) and 380,873 antonymy pairs (6.5 per word). [Cite_Footnote_5] As in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (panga-sius, animal)). We retained only noun-noun and verb-verb pairs, while the rest were discarded: the final number of LE constraints is 1,545,630."
  },
  {
    "id": 1703,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Extend",
    "url": "http://www.cl.cam.ac.uk/\u223cdk427/generality.html",
    "section_title": "4 Results and Discussion 4.1 LE Directionality and Detection",
    "add_info": "7 http://www.cl.cam.ac.uk/\u223cdk427/generality.html",
    "text": "The first evaluation uses three classification-style tasks with increased levels of difficulty. The tasks are evaluated on three datasets used extensively in the LE literature (Roller et al., 2014; Santus et al., 2014; Weeds et al., 2014; Shwartz et al., 2017; Nguyen et al., 2017), compiled into an integrated evaluation set by Kiela et al. (2015b). [Cite_Footnote_7]"
  },
  {
    "id": 1704,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://people.ds.cam.ac.uk/iv250/hyperlex.html",
    "section_title": "4 Results and Discussion 4.2 Graded Lexical Entailment",
    "add_info": "10 For further details concerning HyperLex, we refer the reader to the resource paper (Vulic\u0301 et al., 2017). The dataset is available at: http://people.ds.cam.ac.uk/iv250/hyperlex.html",
    "text": "As shown by the high inter-annotator agreement on HyperLex (0.85), humans are able to consis-tently reason about graded LE . [Cite_Footnote_10] However, current state-of-the-art representation architectures are far from this ceiling. For instance, Vulic\u0301 et al. (2017) evaluate a plethora of architectures and report a high-score of only 0.320 (see the summary table in Figure 3). Two recent representation models (Nickel and Kiela, 2017; Nguyen et al., 2017) fo-cused on the LE relation in particular (and employ-ing the same set of WordNet-based constraints as LEAR ) report the highest score of 0.540 (on the entire dataset) and 0.512 (on the noun subset)."
  },
  {
    "id": 1705,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/baoguangsheng/g-transformer",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We evaluate our model on three commonly used document-level MT datasets for English-German translation, covering domains of TED talks, News, and Europarl from small to large. Experiments show that G-Transformer converges faster and more stably than Transformer on dif-ferent settings, obtaining the state-of-the-art re-sults under both non-pretraining and pre-training settings. To our knowledge, we are the first to realize a truly document-by-document transla-tion model. We release our code and model at  https://github.com/baoguangsheng/g-transformer."
  },
  {
    "id": 1706,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu.2002",
    "section_title": "5 Automated Classification",
    "add_info": "McCallum, A. K. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.2002.",
    "text": "We use a Maximum Entropy (Berger et al., 1996) classifier with a large number of boolean features, some of which are novel (e.g., the inclusion of words from WordNet definitions). Maximum En-tropy classifiers have been effective on a variety of NLP problems including preposition sense disam-biguation (Ye and Baldwin, 2007), which is some-what similar to noun compound interpretation. We use the implementation provided in the MALLET machine learning toolkit (McCallum, 2002)  ."
  },
  {
    "id": 1707,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://registry.opendata.aws/amazon-reviews-ml/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "The Multilingual Amazon Reviews Corpus (MARC) can be found at  https://registry.opendata.aws/amazon-reviews-ml/. The dataset description, code snippets, and li-cense agreement can be retrieved at https: //docs.opendata.aws/amazon-reviews-ml/ readme.html ."
  },
  {
    "id": 1708,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/fxsjy/jieba",
    "section_title": "2 Data preparation 2.2 Data Processing",
    "add_info": "1 https://github.com/fxsjy/jieba 2 http://www.phontron.com/kytea",
    "text": "We also applied a vocabulary-based filter on the reviews. If a review contains a token that doesn\u2019t occur in at least 20 other reviews, then the review is excluded from the dataset. We used Jieba [Cite_Footnote_1] for Chi-nese and KyTea 2 for Japanese word segmentation. The segmenters were only used during the filtering process, and the text provided in the dataset is not segmented or tokenized."
  },
  {
    "id": 1709,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.phontron.com/kytea",
    "section_title": "2 Data preparation 2.2 Data Processing",
    "add_info": "1 https://github.com/fxsjy/jieba 2 http://www.phontron.com/kytea",
    "text": "We also applied a vocabulary-based filter on the reviews. If a review contains a token that doesn\u2019t occur in at least 20 other reviews, then the review is excluded from the dataset. We used Jieba [Cite_Footnote_1] for Chi-nese and KyTea 2 for Japanese word segmentation. The segmenters were only used during the filtering process, and the text provided in the dataset is not segmented or tokenized."
  },
  {
    "id": 1710,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://lynx.invisible-island.net",
    "section_title": "2 Data preparation 2.2 Data Processing",
    "add_info": "3 https://lynx.invisible-island.net",
    "text": "Some Amazon reviews contain HTML markup. We used Lynx [Cite_Footnote_3] to render the reviews as UTF-8 plain-text."
  },
  {
    "id": 1711,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/Adaxry/GCDT",
    "section_title": "References",
    "add_info": "1 Code is available at: https://github.com/Adaxry/GCDT.",
    "text": "Current state-of-the-art systems for the se-quence labeling tasks are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global informa-tion restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context en-hanced Deep Transition architecture for se-quence labeling named GCDT. We deepen the state transition path at each position in a sen-tence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard se-quence labeling tasks show that, given only training data and the ubiquitous word embed-dings (Glove), our GCDT achieves 91.96 F 1 on the CoNLL03 NER task and 95.43 F 1 on the CoNLL2000 Chunking task, which outper-forms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F 1 on NER and 97.30 F 1 on Chunking [Cite_Footnote_1] ."
  },
  {
    "id": 1712,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt",
    "section_title": "4 Experiments 4.2 Implementation Details",
    "add_info": "3 https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt",
    "text": "All trainable parameters in our model are initial-ized by the method described by Glorot and Ben-gio (2010). We apply dropout (Srivastava et al., 2014) to embeddings and hidden states with a rate of 0.5 and 0.3 respectively. All models are opti-mized by the Adam optimizer (Kingma and Ba, 2014) with gradient clipping of 5 (Pascanu et al., 2013). The initial learning rate \u03b1 is set to 0.008, and decrease with the growth of training steps. We monitor the training process on the develop-ment set and report the final result on the test set. One layer CNN with a filter of size [Cite_Footnote_3] is utilized to generate 128-dimension word embeddings by max pooling. The cased, 300d Glove is adapted to initialize word embeddings, which is frozen in all models. In the auxiliary experiments, the out-put hidden states of BERT are taken as additional word embeddings and kept fixed all the time."
  },
  {
    "id": 1713,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/JasperGuo/Unimer",
    "section_title": "References",
    "add_info": null,
    "text": "Meaning representation is an important com-ponent of semantic parsing. Although re-searchers have designed a lot of meaning rep-resentations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less un-derstood. Furthermore, existing work\u2019s per-formance is often not comprehensively evalu-ated due to the lack of readily-available execu-tion engines. Upon identifying these gaps, we propose U NIMER , a new unified benchmark on meaning representations, by integrating ex-isting semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enu-meration of logical forms and execution en-gines over three datasets \u00d7 four meaning rep-resentations. A thorough experimental study on U NIMER reveals that neural semantic pars-ing approaches exhibit notably different per-formance when they are trained to generate different meaning representations. Also, pro-gram alias and grammar rules heavily impact the performance of different meaning repre-sentations. Our benchmark, execution engines and implementation can be found on:  https://github.com/JasperGuo/Unimer ."
  },
  {
    "id": 1714,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/JasperGuo/Unimer",
    "section_title": "3 Benchmark",
    "add_info": "3 Our benchmark, execution engines and and our im-plementation can be found on: https://github.com/JasperGuo/Unimer",
    "text": "We plan to cover more domains and more MRs in U NIMER . We have made U NIMER along with the execution engines publicly available. [Cite_Footnote_3] We be-lieve that U NIMER can provide fertile soil for ex-ploring MRs and addressing challenges in semantic parsing."
  },
  {
    "id": 1715,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/microsoft/nni",
    "section_title": "4 Experimental Setup 4.2 Implementations",
    "add_info": "6 https://github.com/microsoft/nni",
    "text": "We implement each approach with the Al-lenNLP (Gardner et al., 2018) and PyTorch (Paszke et al., 2019) frameworks. To make a fair compari-son, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform. [Cite_Footnote_6] Due to the limited number of test data in each domain, we run each approach five times and take the average number. Section A.2 in the supplementary material provides the search space of hyper-parameters for each approach and the pre-processing procedures of logical forms."
  },
  {
    "id": 1716,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.cs.utexas.edu/ml/nldata/jobquery.html",
    "section_title": "- A.1 Details of Benchmark Construction",
    "add_info": "9 http://www.cs.utexas.edu/ml/nldata/jobquery.html river_name := \"ohio\" | \"colorado\u201d | \u2026 | \"red\" state_abbrev := \"dc\" | \"sd\" | \u2026 | \"me\" number := \"0\" | \"1.0\"",
    "text": "Job. There are only annotated logical forms for Lambda Calculus and Prolog in Job. We directly use Prolog logical forms provided on the web-site. [Cite_Footnote_9] To provide annotations for FunQL, we semi-copying a source word to a logical form. In ATIS, following (Jia and Liang, 2016), we leverage an ex-ternal lexicon to identify potential copy candidates, e.g., slc:ap can be identified as a potential entity for description \u201csalt lake city airport\u201d in utterance. When we copy a source word that is part of a phrase in the lexicon, we write the entity associated with that lexicon entry to a logical form."
  },
  {
    "id": 1717,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://bitbucket.org/tclup/alto",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "In this paper, we make two contributions. First, we generalize chart constraints to more expressive grammar formalisms by casting them in terms of allowable parse items that should be considered by the parser. The Roark chart constraints are the special case for PCFGs and CKY; our view applies to any grammar formalism for which a parser can be specified in terms of parsing schemata. Second, we present a neural tagger which predicts begin and end constraints with an accuracy around 98%. We show that these chart constraints speed up a PCFG parser by 18x and a TAG chart parser by 4x. Furthermore, chart constraints can be combined effectively with coarse-to-fine parsing for PCFGs (for an overall speedup of 70x) and supertagging for TAG (overall speedup of 124x), all while improving the accuracy over those of the baseline parsers. Our code is part of the Alto parser (Gontrum et al., 2017), available at  http://bitbucket.org/tclup/alto."
  },
  {
    "id": 1718,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.cs.pitt.edu/mpqa/",
    "section_title": "3 Experimental Methodology",
    "add_info": "6 http://www.cs.pitt.edu/mpqa/",
    "text": "For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA) [Cite_Footnote_6] corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phrase-level subjectivity and intensity. We use the expression-level boundary markings in MPQA to extract phrases. We evaluate on positive, negative and neutral opinion expressions that have intensities \u201cmedium\u201d, \u201chigh\u201d or \u201cextreme\u201d. 7 The schematic mapping of phrase polarity and intensity values on ordinal sentimental scale is shown in Table 1."
  },
  {
    "id": 1719,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://github.com/wtimkey/rogue-dimensions",
    "section_title": "1 Introduction",
    "add_info": "1 Our code is publically released at: http://github.com/wtimkey/rogue-dimensions",
    "text": "Finally, we show that these dimensions can be accounted for using a trivially simple transforma-tion of the embedding space: standardization. Once applied, cosine similarity more closely reflects hu-man word similarity judgments, and we see that representational quality is preserved across all lay-ers rather than degrading/becoming task-specific. Taken together, we argue that accounting for rogue dimensions is essential when evaluating representa-tional similarity in transformer language models. [Cite_Footnote_1]"
  },
  {
    "id": 1720,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/fromhuggingface/transformers",
    "section_title": "3 Rogue Dimensions and 3.1.2 Experiment",
    "add_info": "2 All models https://github.com/fromhuggingface/transformers",
    "text": "We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. [Cite_Footnote_2] We then normalize by the total expected cosine similarity A\u0302(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia. All input sequences consisted of 128 tokens. From the resulting rep-resentations we take a random sample S of 500k token pairs. For each model, we report the three dimensions with the largest cosine contributions in the two most anisotropic layers, as well as the overall anisotropy A\u0302( f ` )."
  },
  {
    "id": 1721,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://zenodo.org/record/4421380",
    "section_title": "3 Rogue Dimensions and 3.1.2 Experiment",
    "add_info": "3 https://zenodo.org/record/4421380",
    "text": "We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. We then normalize by the total expected cosine similarity A\u0302(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec [Cite_Footnote_3] and GloVe, for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia. All input sequences consisted of 128 tokens. From the resulting rep-resentations we take a random sample S of 500k token pairs. For each model, we report the three dimensions with the largest cosine contributions in the two most anisotropic layers, as well as the overall anisotropy A\u0302( f ` )."
  },
  {
    "id": 1722,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "3 Rogue Dimensions and 3.1.2 Experiment",
    "add_info": "4 https://nlp.stanford.edu/projects/glove/ (Wikipedia+Gigaword 5, 300d)",
    "text": "We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. We then normalize by the total expected cosine similarity A\u0302(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, [Cite_Footnote_4] for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia. All input sequences consisted of 128 tokens. From the resulting rep-resentations we take a random sample S of 500k token pairs. For each model, we report the three dimensions with the largest cosine contributions in the two most anisotropic layers, as well as the overall anisotropy A\u0302( f ` )."
  },
  {
    "id": 1723,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.fjoch.com/YASMET.html",
    "section_title": "3 Cascading Guidance Technique SVM Classification",
    "add_info": "1 http://www.fjoch.com/YASMET.html",
    "text": "We use the published package YASMET [Cite_Footnote_1] to conduct parameters training and classification. YASMET requires supervised learning for the training of maximum entropy model."
  },
  {
    "id": 1724,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.infoplease.com/people.html",
    "section_title": "4 Experiments and Results 4.1 Experimental Setup",
    "add_info": "2 http://www.infoplease.com/people.html",
    "text": "We download from infoplease.com [Cite_Footnote_2] and biogra-phy.com two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information. To create the data set, we manually check and categorize each person as having multiple spouses, only one spouse, or no spouse. Similarly, we obtained another list of per-sons having multiple children, only one child, and no child. The sizes of data extracted are given in Table 1."
  },
  {
    "id": 1725,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.biography.com/search/index.jsp",
    "section_title": "4 Experiments and Results 4.1 Experimental Setup",
    "add_info": "3 http://www.biography.com/search/index.jsp",
    "text": "We download from infoplease.com and biogra-phy.com [Cite_Footnote_3] two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information. To create the data set, we manually check and categorize each person as having multiple spouses, only one spouse, or no spouse. Similarly, we obtained another list of per-sons having multiple children, only one child, and no child. The sizes of data extracted are given in Table 1."
  },
  {
    "id": 1726,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/dykang/cgraph",
    "section_title": "5 Results 5.1 Data and Metrics",
    "add_info": "1 https://github.com/dykang/cgraph",
    "text": "The news dataset [Cite_Footnote_1] we used for stock price predic-tion contains news crawled from 2010 to 2013 us-ing Google News APIs and New York Times data from 1989 to 2007. We construct PCG from the time series representation of its 12,804 unigrams and 25,909 bigrams over the entire news corpora of more than 23 years, as well as the 10 stock prices from 2010 to 2012 for training and 2013 as test data for prediction. The prediction is done with varying step sizes (1,3,5), which indicates the time lag between the news data and the day of the predicted stock price in days. The results shown in Table 1 is the root mean squared error (RMSE) in predicted stock value calculated on a 30 day win-dow averaged by moving it by 10 days over the pe-riod and directly comparable to our baseline (Kang et al., 2017). To evaluate the time-varying fac-tors over a larger time window, we present average monthly cross validation RMSE % sampled over a 4 year window of 2010-13 in Table 3. Please note that the results in Table 3 are not comparable with (Kang et al., 2017) as we report a cross validation error over a longer time window."
  },
  {
    "id": 1727,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://finance.yahoo.com",
    "section_title": "5 Results 5.1 Data and Metrics",
    "add_info": "2 https://finance.yahoo.com",
    "text": "The news dataset we used for stock price predic-tion contains news crawled from 2010 to 2013 us-ing Google News APIs and New York Times data from 1989 to 2007. We construct PCG from the time series representation of its 12,804 unigrams and 25,909 bigrams over the entire news corpora of more than 23 years, as well as the 10 stock prices [Cite_Footnote_2] from 2010 to 2012 for training and 2013 as test data for prediction. The prediction is done with varying step sizes (1,3,5), which indicates the time lag between the news data and the day of the predicted stock price in days. The results shown in Table 1 is the root mean squared error (RMSE) in predicted stock value calculated on a 30 day win-dow averaged by moving it by 10 days over the pe-riod and directly comparable to our baseline (Kang et al., 2017). To evaluate the time-varying fac-tors over a larger time window, we present average monthly cross validation RMSE % sampled over a 4 year window of 2010-13 in Table 3. Please note that the results in Table 3 are not comparable with (Kang et al., 2017) as we report a cross validation error over a longer time window."
  },
  {
    "id": 1728,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://timesofindia.indiatimes.com/archive.cms",
    "section_title": "6 Interpretation of Predictive Causal Factors",
    "add_info": "3 https://timesofindia.indiatimes.com/archive.cms",
    "text": "In order to qualitatively validate that the latent inter-topic edges learnt from the news stream is also humanly interpretable, we constructed PCG from the online archives of Times of India (TOI) [Cite_Footnote_3] , the most circulated Indian English newspaper. We used this dataset as, unlike the previous dataset which provided just the time series of words, we also have the raw text of the articles, which al-lowed us to perform manual causal signature ver-ification. This dataset contains all the articles published in their online edition between Jan-uary 1, 2006 and December 31, 2015 containing 1,538,932 articles."
  },
  {
    "id": 1729,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/MiuLab/TC-Bot",
    "section_title": "4 Experiments 4.3 Task-Oriented Dialogue Learning",
    "add_info": "2 https://github.com/MiuLab/TC-Bot",
    "text": "Dataset Following the previous work (Peng et al., 2018; Su et al., 2018), we use the same Movie-Ticket Booking dataset collected from Amazon Mechanical Turk for evaluation. The dataset is manually labeled based on a schema de-fined by domain experts consisting of 11 intents and 16 slots in the full domain setting. In total, the dataset has 280 annotated dialogues with an aver-age length of approximately 11 turns. In this sce-nario, the goal of dialogue systems is to help the user complete the tasks through the conversation. Baselines We compare our SSN-based dis-criminator within the state-of-the-art task-oriented dialogue policy learning approach, Discriminative Deep Dyna-Q (D3Q) (Su et al., 2018). At each turn, the D3Q agent takes S planning steps inter-acting with the simulator and store stimulated user experiences based on the scoring of the discrimi-nator. The stimulated user experiences are gener-ated by the world model, which can be viewed as the generator G in our case. We replace the con-ventional discriminator D of D3Q with our SSN . Implementation Details For a fair comparison, we remain most of the parameters in the D3Q al-gorithm the same as in Su et al. (2018). In the self-supervised network, the dimension of the ut-terance embeddings is 80. The hidden size is 128 for utterance encoding bi-LSTM and 512 for triple reasoning bi-LSTM. The MLP has a single hidden layer of size 128. We use the simulator [Cite_Footnote_2] as in Li et al. (2016c) to generate user utterances, and the threshold interval is set to a range between 0.45 and 0.55."
  },
  {
    "id": 1730,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://berouge.com/default.aspx",
    "section_title": "2 Related Work",
    "add_info": "1 http://berouge.com/default.aspx",
    "text": "Both ROUGE and BE have been implemented and included in the ROUGE/BE evaluation toolkit [Cite_Footnote_1] , which has been used as the default evaluation tool in the summarization track in the Document Un-derstanding Conference (DUC) and Text Analysis Conference (TAC). DUC and TAC also manually evaluated machine generated summaries by adopt-ing the Pyramid method. Besides evaluating with ROUGE/BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall re-sponsiveness."
  },
  {
    "id": 1731,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://wing.comp.nus.edu.sg/\u02dclinzihen/parser/",
    "section_title": "4 DICOMER: Evaluating Summary Readability 4.3 Predicting Readability Scores",
    "add_info": "2 http://wing.comp.nus.edu.sg/\u02dclinzihen/parser/",
    "text": "We use the data from AESOP 2009 and 2010 as the training data, and test our metrics on AESOP 2011 data. To obtain the discourse relations of a summary, we use the discourse parser [Cite_Footnote_2] developed in Lin et al. (2010)."
  },
  {
    "id": 1732,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://wing.comp.nus.edu.sg/\u02dclinzihen/summeval/",
    "section_title": "7 Conclusion",
    "add_info": "3 Our metrics are publicly available at http://wing.comp.nus.edu.sg/\u02dclinzihen/summeval/.",
    "text": "We proposed TESLA-S by adapting an MT eval-uation metric to measure summary content cover-age, and introduced DICOMER by applying a dis-course coherence model with newly introduced fea-tures to evaluate summary readability. We com-bined these two metrics in the CREMER metric \u2013 an SVM-trained regression model \u2013 for auto-matic summarization overall responsiveness evalu-ation. Experimental results on AESOP 2011 show that DICOMER significantly outperforms all sub-mitted metrics on both initial and update tasks with large gaps, while TESLA-S and CREMER signifi-cantly outperform all metrics on the initial task. [Cite_Footnote_3]"
  },
  {
    "id": 1733,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/adalmia96/Cluster-Analysis",
    "section_title": "4 Computational Complexity",
    "add_info": "4 https://github.com/adalmia96/Cluster-Analysis",
    "text": "ter (?) without weighting, while the figure on the right shows that after weighting (larger points have higher weight) a hopefully more representative cluster center is found. Note that top words based on distance from the cluster center could still very well be low frequency word types, motivating reranking (\u00a73.3). 3.1 Obtaining top-J words In traditional topic modeling (LDA), the top J words are those with highest probability under each topic-word distribution. For centroid based clus-tering algorithms, the top words of some cluster i are naturally those closest to the cluster center c (i) , or with highest probability under the cluster parameters. Formally, this means choosing the set of types J as 3.2 Weighting while clustering The intuition of weighted clustering is based on the formulation of classical LDA which models the probability of the word type t belonging to a topic i as PN t,i +\u03b2 t , where N t,i refers to the number of t0 N t0i +\u03b2 t0 times word type t has been assigned to topic i, and 4.1 Cost of obtaining Embeddings For readily available pretrained word embeddings such as word2vec, FastText, GloVe and Spherical, ments and therefore do not consider the runtime of training these models from scratch. 5 Experimental Setup Our implementation is freely available online. [Cite_Footnote_4] 5.1 Datasets We use the 20 newsgroup dataset (20NG) which contains around 18000 documents and 20 cate-gories, 5 and a subset of Reuters21578 6 which con-tains around 10000 documents. 5.2 Evaluation (Topic Coherence) We adopt a standard 60-40 train-test split for 20NG and 70-30 for Reuters. NPMI, for k-means (KM) before and after reranking (KM r ): reranking clearly improves NPMI for BERT and Spherical. (p < 0.05). 7 6.3 Reranking duced by term frequency on 20NG. Embeddings are more sensitive to noisy vocabulary (infrequent terms) than LDA, but reweighting ( w ) helps to alleviate this. A k-means (KM) vs k-medoids (KD) To further understand the effect of other centroid Top 10 Word for Each Topic NPMI dollar rate rates exchange currency market dealers central interest point year growth rise government economic economy expected domestic inflation report gold reserves year tons company production exploration ounces feet mine billion year rose dlrs fell marks earlier figures surplus rise year tonnes crop production week grain sugar estimated expected area dlrs company sale agreement unit acquisition assets agreed subsidiary sell bank billion banks money interest market funds credit debt loans tonnes wheat export sugar tonne exports sources shipment sales week plan bill industry farm proposed government administration told proposal change prices production price crude output barrels barrel increase demand industry group company investment stake firm told companies capital chairman president trade countries foreign officials told official world government imports agreement offer company shares share dlrs merger board stock tender shareholders shares stock share common dividend company split shareholders record outstanding dlrs year quarter earnings company share sales reported expects results market analysts time added long analyst term noted high back coffee meeting stock producers prices export buffer quotas market price loss dlrs profit shrs includes year gain share mths excludes"
  },
  {
    "id": 1734,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://qwone.com/\u02dcjason/20Newsgroups/",
    "section_title": "4 Computational Complexity",
    "add_info": "5 http://qwone.com/\u02dcjason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html",
    "text": "For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [Cite_Footnote_5] random seeds."
  },
  {
    "id": 1735,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://www.nltk.org/book/ch02.html",
    "section_title": "4 Computational Complexity",
    "add_info": "5 http://qwone.com/\u02dcjason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html",
    "text": "For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [Cite_Footnote_5] random seeds."
  },
  {
    "id": 1736,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/IBM/twitter-customer-care-document-prediction",
    "section_title": "References",
    "add_info": "1 The Twitter dataset is available at: https://github.com/IBM/twitter-customer-care-document-prediction",
    "text": "A frequent pattern in customer care conversa-tions is the agents responding with appropri-ate webpage URLs that address users\u2019 needs. We study the task of predicting the documents that customer care agents can use to facilitate users\u2019 needs. We also introduce a new pub-lic dataset [Cite_Footnote_1] which supports the aforementioned problem. Using this dataset and two others, we investigate state-of-the-art deep learning (DL) and information retrieval (IR) models for the task. We also analyze the practicality of such systems in terms of inference time complex-ity. Our results show that an hybrid IR+DL approach provides the best of both worlds."
  },
  {
    "id": 1737,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://github.com/IBM/twitter-customer-care-document-prediction",
    "section_title": "2 Data",
    "add_info": "3 The Twitter dataset is available at: https://github.com/IBM/twitter-customer-care-document-prediction based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, tion set of Telco-Support dataset for dialog context se-quence input handling.",
    "text": "We explore the CDP task using three datasets which contain human-to-human conversations between users and CC agents. Two of these datasets are internal: one from an internal customer support service on Mac devices (Mac-Support) and another from an external client in the telecommunication domain (Telco-Support). We also release a new Twitter dataset, containing conversations between users and CC agents in 25 organizations on the Twitter platform [Cite_Footnote_3] . We summarize the statistics of the three datasets in Table 2."
  },
  {
    "id": 1738,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/IBM/MDfromHTML",
    "section_title": "URL documents",
    "add_info": null,
    "text": "For the internal Mac-Support dataset, the docu-ment content for each URL was obtained by API calls to the customer service knowledge base. For the Telco-Support and Twitter datasets, we cap-ture the HTML content using a Selenium Chrome webdriver, which renders the URL document by loading all CSS styling and Javascript. The ex-tracted HTML was cleaned through a Markdown generation pipeline, where we manually identify and filter the DOM tags (using CSS id and/or class) which correspond to header(s), footer, navigation bars etc. This process is repeated for each URL domain in both datasets. The tools for data prepro-cessing are available here:  https://github.com/IBM/MDfromHTML."
  },
  {
    "id": 1739,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/anjalief/unsupervised_gender_bias",
    "section_title": "References",
    "add_info": "1 Code and pre-trained models are available at https://github.com/anjalief/unsupervised_gender_bias",
    "text": "Despite their prevalence in society, social bi-ases are difficult to identify, primarily because human judgements in this domain can be un-reliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other ar-tifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments di-rected towards other female public figures fo-cus on appearance and sexualization. Ulti-mately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. [Cite_Footnote_1]"
  },
  {
    "id": 1740,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/bert",
    "section_title": "4 Experiments 4.1 Datasets and Experimental Settings",
    "add_info": "8 We use Google\u2019s official base models. See: https://github.com/google-research/bert and https://github.com/google-research/albert.",
    "text": "We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [Cite_Footnote_8] . Three sets of NLP tasks are used for evaluation, with the statistics of all the seven public datasets reported in Table 1:"
  },
  {
    "id": 1741,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/albert",
    "section_title": "4 Experiments 4.1 Datasets and Experimental Settings",
    "add_info": "8 We use Google\u2019s official base models. See: https://github.com/google-research/bert and https://github.com/google-research/albert.",
    "text": "We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [Cite_Footnote_8] . Three sets of NLP tasks are used for evaluation, with the statistics of all the seven public datasets reported in Table 1:"
  },
  {
    "id": 1742,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.lemurproject.org/",
    "section_title": "6 Experiments 6.1 Rumor Retrieval 6.1.1 Baselines",
    "add_info": "3 http://www.lemurproject.org/",
    "text": "Finally, using the Lemur Toolkit software [Cite_Footnote_3] , we employ a KL divergence retrieval model with Dirichlet smoothing (KL). In this model, documents are ranked according to the negation of the diver-gence of query and document language models. More formally, given the query language model \u03b8 Q , and the document language model \u03b8 D , the docu-ments are ranked by \u2212D(\u03b8 Q ||\u03b8 D ), where D is the KL-divergence between the two models."
  },
  {
    "id": 1743,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://lcg-www.uia.ac.be/conll2000/chunking",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "In this paper, we compare regularized Winnow and Winnow algorithms on text chunking (Ab-ney, 1991). In order for us to rigorously com-pare our system with others, we use the CoNLL-2000 shared task dataset (Sang and Buchholz, 2000), which is publicly available from  http://lcg-www.uia.ac.be/conll2000/chunking. An advan-tage of using this dataset is that a large number of state of the art statistical natural language pro-cessing methods have already been applied to the data. Therefore we can readily compare our re-sults with other reported results."
  },
  {
    "id": 1744,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://lcg-www.uia.ac.be/conll2000/chunking",
    "section_title": "3 CoNLL-2000 chunking task",
    "add_info": null,
    "text": "A standard software program has been provided (which is available from  http://lcg-www.uia.ac.be/conll2000/chunking) to compute the performance of each algorithm. For each chunk, three figures of merit are computed: precision (the percentage of detected phrases that are correct), recall (the percentage of phrases in the data that are found), and the \u201eL nL% metric the recall. The overall precision, recall and \u201eL nL% which is the harmonic mean of the precision and overall \u201eL nL% metric on all chunks are also computed. The metric gives a single number that can be used to compare different algorithms."
  },
  {
    "id": 1745,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/allenai/iterative-search-semparse",
    "section_title": "6 Experiments 6.2 Experimental setup",
    "add_info": null,
    "text": "Implementation We ourimplemented model and training algorithms within the AllenNLP (Gardner et al., 2018) toolkit. The code and models are publicly available at  https://github.com/allenai/iterative-search-semparse."
  },
  {
    "id": 1746,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/seraphinatarrant/plan-write-revise",
    "section_title": "1 Introduction",
    "add_info": "1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.",
    "text": "Swanson and Gordon (2009) use an informa-tion retrieval based system to write by alternating turns between a human and their system. Clark and Smith (2018) use a similar turn-taking ap-proach to interactivity, but employ a neural model for generation and allow the user to edit the gen-erated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. Roem-mele and Swanson. (2017) experiment with a user-driven setup, where the machine doesn\u2019t gener-ate until the user requests it to, and then the user can edit or delete at will. They leverage user-acceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful gen-eration. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user\u2019s ability to knit a coherent overall story to-gether out of locally related sentences. They also do not allow a user to control the novelty or \u201cun-expectedness\u201d of the generations, which Clark and Smith (2018) find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [Cite_Footnote_1] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged."
  },
  {
    "id": 1747,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.nist.gov/speech/tests/mt",
    "section_title": "4 Data Sets and Measures 4.1 Data sets",
    "add_info": "2 http://www.nist.gov/speech/tests/mt",
    "text": "In this paper, we provide empirical results for MT and AS. For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 [Cite_Footnote_2] . Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005). Thus, human as-sessments are in a 2-10 scale. For AS, we have used the AS test suites developed in the DUC 2005 and DUC 2006 evaluation campaigns . This AS task was to generate a question focused summary of 250 words from a set of 25-50 documents to a complex question. Summaries were evaluated according to several criteria. Here, we will consider the respon-siveness judgements, in which the quality score was an integer between 1 and 5. See Tables 1 and 2 for a brief quantitative description of these test beds."
  },
  {
    "id": 1748,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.lsi.upc.edu/\u02dcnlp/Asiya",
    "section_title": "4 Data Sets and Measures 4.2 Measures",
    "add_info": "5 http://www.lsi.upc.edu/\u02dcnlp/Asiya",
    "text": "As for evaluation measures, for MT we have used a rich set of 64 measures provided within the ASIYA Toolkit (Gime\u0301nez and Ma\u0300rquez, 2010) [Cite_Footnote_5] . This in-cludes measures operating at different linguistic lev-els: lexical, syntactic, and semantic. At the lexical level this set includes variants of 8 measures em-ployed in the state of the art: BLEU, NIST, GTM, METEOR, ROUGE, WER, PER and TER. In addi-tion, we have included a basic measure O l that com-putes the lexical overlap without considering word ordering. All these measures have similar granular-ity. They use n-grams of a varying length as the ba-sic unit with additional information provided by lin-guistic tools. The underlying similarity criteria in-clude precision, recall, overlap, or edit rate, and the decomposition functions include words, dependency tree nodes (DP HWC, DP-Or, etc.), constituency parsing (CP-STM), discourse roles (DR-Or), seman-tic roles (SR-Or), named entities, etc. Further details on the measure set may be found in the ASIYA tech-nical manual (Gime\u0301nez and Ma\u0300rquez, 2010)."
  },
  {
    "id": 1749,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://pytorch.org/",
    "section_title": "4 Experiments",
    "add_info": "2 https://pytorch.org/",
    "text": "In all the experiments, we use PyTorch [Cite_Footnote_2] (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package (Grefenstette et al., 2019) to implement the proposed algorithm."
  },
  {
    "id": 1750,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/facebookresearch/higher",
    "section_title": "4 Experiments",
    "add_info": "3 https://github.com/facebookresearch/higher",
    "text": "In all the experiments, we use PyTorch (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package [Cite_Footnote_3] (Grefenstette et al., 2019) to implement the proposed algorithm."
  },
  {
    "id": 1751,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq",
    "section_title": "4 Experiments 4.2 Neural Machine Translation",
    "add_info": "5 https://github.com/pytorch/fairseq",
    "text": "Implementation. Recall that to generate adversar-ial examples, we perturb the word embeddings. In NMT experiments, we perturb both the source-side and the target-side embeddings. This strategy is empirically demonstrated (Sato et al., 2019) to be more effective than perturbing only one side of the inputs. We use Fairseq [Cite_Footnote_5] (Ott et al., 2019) to imple-ment our algorithms. We adopt the Transformer-base (Vaswani et al., 2017) architecture in all the low-resource experiments, except IWSLT\u201914 De-En. In this dataset, we use a model smaller than Transformer-base by decreasing the hidden dimen-sion size from 2048 to 1024, and decreasing the number of heads from 8 to 4 (while dimension of each head doubles). For the rich-resource experi-ments, we use the Transformer-big (Vaswani et al., 2017) architecture. Training details are presented in Appendix B.1."
  },
  {
    "id": 1752,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/microsoft/MT-DNN",
    "section_title": "4 Experiments 4.2 Neural Machine Translation",
    "add_info": "6 https://github.com/microsoft/MT-DNN",
    "text": "Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [Cite_Footnote_6] . Training details are presented in Appendix B.2."
  },
  {
    "id": 1753,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt",
    "section_title": "References",
    "add_info": "7 https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt",
    "text": "Note that in this paper, we target for models\u2019 generalization performance on the unperturbed test data, therefore we do not want a strong adversary that \u201ctraps\u201d the model parameters to a bad local optima. Most of the existing algorithms achieve this goal by carefully tuning the hyper-parameters and K, i.e., a small usually generates weaker adversaries, so does a small K. However, these heuristics do not work well, and at times \u03b4 K is too strong. Consequently, conventional adversarial training results in undesirable underfitting on the clean data. B.1 Neural Machine Translation For the rich-resource WMT\u201916 En-De dataset, we use the pre-processed data from Ott et al. (2018) [Cite_Footnote_7] . For the low-resource datasets, we use byte-pair encoding (Sennrich et al., 2016) with 10,000 merge operations to build the vocabulary for the IWSLT (\u201914, \u201915, \u201916) datasets. We follow the scripts in Ott et al. (2019) for other pre-processing steps."
  },
  {
    "id": 1754,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.brenbarn.net/werewolf/",
    "section_title": "3 Experiments 3.1 Data",
    "add_info": "Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].",
    "text": "The raw data consists of 86 game transcripts col-lected by Barnwell (Barnwell, 2012)  . The tran-scripts have an average length of 205 messages per transcript, including judge comments."
  },
  {
    "id": 1755,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://crfpp.sourceforge.net/",
    "section_title": "4 From MaxEnt to CRFs",
    "add_info": "5 CRF experiments used the CRF++ package http://crfpp.sourceforge.net/",
    "text": "A CRF models the entire label sequence y as: where F(y, x) is a global feature vector for input sequence x and label sequence y and Z(x) is a nor-malization term. [Cite_Footnote_5]"
  },
  {
    "id": 1756,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://search.cpan.org/~snowhare/Lingua-Stem-0.83",
    "section_title": "6 Local Lexical Context",
    "add_info": "7 To obtain stemmed words, we use the CPAN package: http://search.cpan.org/~snowhare/Lingua-Stem-0.83.",
    "text": "\u2022 All above features and their stems. [Cite_Footnote_7] (All-Words-Stemmed)"
  },
  {
    "id": 1757,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.ibi-square.jp/index.htm",
    "section_title": "4 Experiments 4.1 Experimental Settings",
    "add_info": "1 http://www.ibi-square.jp/index.htm",
    "text": "We used the five-minute chart of Nikkei 225 from March 2013 to October 2016 as numerical time-series data, which were collected from IBI-Square Stocks [Cite_Footnote_1] , and 7,351 descriptions as market com-ments, which are written in Japanese and provided by Nikkei QUICK News. We divided the dataset into three parts: 5,880 for training, 730 for val-idation, and 741 for testing. For a human eval-uation, we randomly selected 100 comments and their time-series data included in the test set."
  },
  {
    "id": 1758,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/IBM/row-column-intersection",
    "section_title": "References",
    "add_info": "1 The source code and the models we built are available at https://github.com/IBM/row-column-intersection.",
    "text": "Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrat-ing that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that indepen-dently classifies rows and columns to iden-tify relevant cells. While this model yields extremely high accuracy at finding cell val-ues on recent benchmarks, a second model we propose, called RCI representation, pro-vides a significant efficiency advantage for on-line QA systems over tables by materializ-ing embeddings for existing tables. Experi-ments on recent benchmarks prove that the pro-posed methods can effectively locate cell val-ues on tables (up to \u223c98% Hit@1 accuracy on WikiSQL lookup questions). Also, the inter-action model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (T A P AS and T A B ERT ), achieving \u223c3.4% and \u223c18.86% additional pre-cision improvement on the standard WikiSQL benchmark [Cite_Footnote_1] ."
  },
  {
    "id": 1759,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/IBM/row-column-intersection",
    "section_title": "5 Evaluation",
    "add_info": "3 https://github.com/IBM/row-column-intersection",
    "text": "To evaluate these three approaches, we adapt three standard TableQA datasets: WikiSQL (Zhong et al., 2017), WikiTableQuestions (Pasupat and Liang, 2015) and TabMCQ (Jauhar et al., 2016). Wik-iSQL and WikiTableQuestions include both lookup questions as well as aggregation questions. As men-tioned in Section 1, our primary focus in this paper is on lookup questions that require selection and projection operations over tables (i.e., identifying the row and column of a table with very high pre-cision for a given natural language question). We are releasing the processing and evaluation code for the datasets to support reproducibility [Cite_Footnote_3] . Table 2 gives a summary of these datasets."
  },
  {
    "id": 1760,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/tapas",
    "section_title": "5 Evaluation",
    "add_info": "4 https://github.com/google-research/tapas",
    "text": "In order to evaluate our proposed approaches on these datasets we built three different systems and also used three existing models: IS-SP, pro-vided by (Dasigi et al., 2019), T A B ERT (Yin et al., 2020) and T A P AS (Herzig et al., 2020). IS-SP is a semantic parsing based model trained on WikiTa-blesQuestions (Pasupat and Liang, 2015) dataset (See Section 2 for the details of this work). For building their model we used the code provided in (Gardner et al., 2020). For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset. For T A P AS we used the trained BASE (reset) models [Cite_Footnote_4] for Wik-iSQL and applied to the lookup subsets of the dev and test sets."
  },
  {
    "id": 1761,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "A Appendix",
    "add_info": "5 https://github.com/huggingface/transformers",
    "text": "Both the MRC and RCI training was carried out using the pytorch transformers toolkit made avail-able by Huggingface [Cite_Footnote_5] . Table 9 gives the number of parameters for each introduced model."
  },
  {
    "id": 1762,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.mpi-sws.org/\u02dccristian/Politeness.html",
    "section_title": "2 Politeness data",
    "add_info": "3 Publicly available at http://www.mpi-sws.org/\u02dccristian/Politeness.html",
    "text": "Politeness annotation Computational studies of politeness, or indeed any aspect of linguistic prag-matics, demand richly labeled data. We there-fore label a large portion of our request data (over 10,000 utterances) using Amazon Mechan-ical Turk (AMT), creating the largest corpus with politeness annotations (see Table 1 for details). [Cite_Footnote_3]"
  },
  {
    "id": 1763,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://ifttt.com/",
    "section_title": "2 Core competencies 2.1 Learning Procedures",
    "add_info": "1 https://ifttt.com/, https://zapier.com/",
    "text": "Here, the condition and the effect (action) are highlighted in green and red respectively. The condition in each example requires a check that has to be grounded in the perception sensors. If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [Cite_Footnote_1] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example). Another advantage of a conversational setting is that LIA can take initiative when certain things are left am-biguous by the user (e.g., ask the user what to do if there is a conflict on the calendar for the last rule in the list above) \u2014 an issue that cannot be coped with in traditional programming environments."
  },
  {
    "id": 1764,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://zapier.com/",
    "section_title": "2 Core competencies 2.1 Learning Procedures",
    "add_info": "1 https://ifttt.com/, https://zapier.com/",
    "text": "Here, the condition and the effect (action) are highlighted in green and red respectively. The condition in each example requires a check that has to be grounded in the perception sensors. If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [Cite_Footnote_1] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example). Another advantage of a conversational setting is that LIA can take initiative when certain things are left am-biguous by the user (e.g., ask the user what to do if there is a conflict on the calendar for the last rule in the list above) \u2014 an issue that cannot be coped with in traditional programming environments."
  },
  {
    "id": 1765,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://wordnet.princeton.edu/",
    "section_title": "1 Introduction",
    "add_info": "1 http://wordnet.princeton.edu/",
    "text": "Pre-compiled taxonomies such as WordNet [Cite_Footnote_1] and text corpora have been used in previous work on se-mantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, seman-tic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, loca-tion names, product names) and the new uses of ex-isting words. For example, apple is frequently asso-ciated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible."
  },
  {
    "id": 1766,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Extend",
    "url": "http://dmoz.org",
    "section_title": "4 Experiments 4.4 Named Entity Clustering",
    "add_info": "7 http://dmoz.org",
    "text": "Measuring semantic similarity between named en-tities is vital in many applications such as query expansion (Sahami and Heilman, 2006) and com-munity mining (Matsuo et al., 2006a). Since most named entities are not covered by WordNet, simi-larity measures based on WordNet alone cannot be used in such tasks. Unlike common English words, named entities are constantly being created. Manu-ally maintaining an up-to-date taxonomy of named entities is costly, if not impossible. The proposed semantic similarity measure is appealing as it does not require pre-compiled taxonomies. In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 categories : tennis players, golfers, actors, politicians and scien-tists, (10 names from each category) from the dmoz directory [Cite_Footnote_7] . For each pair of names in our dataset, we measure the association between the two names using the proposed method and baselines. We use group-average agglomerative hierarchical clustering to cluster the names in our dataset into five clusters. We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. As summarized in Table 5 the proposed method outper-forms all the baselines with a statistically significant (p \u2264 0.01 Tukey HSD) F score of 0.7897."
  },
  {
    "id": 1767,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "https://github.com/commonsense/conceptnet5/wiki/Downloads",
    "section_title": "* Ethical Considerations",
    "add_info": "3 https://github.com/commonsense/conceptnet5/wiki/Downloads",
    "text": "Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [Cite_Footnote_3] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research. Cultural Bias Reduction Like most most mul-tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural dif-ferences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like \u201cwhat do people usually drink in the morning? (cof-fee/tea/milk)\u201d or \u201cwhen does a wedding usually start? (morning/afternoon/evening)\u201d might be an-swered very differently by people from different backgrounds and cultures, not to mention differ-ent languages. The prior English commonsense resources which our datasets are built on are al-ready possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either labeled as non-neutral by a pre-trained sentiment classifier, or contained any keywords that are relevant to social behavior (e.g., weddings). We manually inspect test examples in X-CSQA and X-CODAH in the English and Chi-nese versions and have a strong confidence there is few strongly controversial example. However, we admit that such reduction of cultural differences in common sense has not been systematically mea-sured in this work for other languages."
  },
  {
    "id": 1768,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "https://www.tau-nlp.org/commonsenseqa",
    "section_title": "* Ethical Considerations",
    "add_info": "4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH",
    "text": "Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [Cite_Footnote_4] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research. Cultural Bias Reduction Like most most mul-tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural dif-ferences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like \u201cwhat do people usually drink in the morning? (cof-fee/tea/milk)\u201d or \u201cwhen does a wedding usually start? (morning/afternoon/evening)\u201d might be an-swered very differently by people from different backgrounds and cultures, not to mention differ-ent languages. The prior English commonsense resources which our datasets are built on are al-ready possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either labeled as non-neutral by a pre-trained sentiment classifier, or contained any keywords that are relevant to social behavior (e.g., weddings). We manually inspect test examples in X-CSQA and X-CODAH in the English and Chi-nese versions and have a strong confidence there is few strongly controversial example. However, we admit that such reduction of cultural differences in common sense has not been systematically mea-sured in this work for other languages."
  },
  {
    "id": 1769,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Introduce",
    "url": "https://github.com/Websail-NU/CODAH",
    "section_title": "* Ethical Considerations",
    "add_info": "4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH",
    "text": "Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [Cite_Footnote_4] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research. Cultural Bias Reduction Like most most mul-tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural dif-ferences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like \u201cwhat do people usually drink in the morning? (cof-fee/tea/milk)\u201d or \u201cwhen does a wedding usually start? (morning/afternoon/evening)\u201d might be an-swered very differently by people from different backgrounds and cultures, not to mention differ-ent languages. The prior English commonsense resources which our datasets are built on are al-ready possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either labeled as non-neutral by a pre-trained sentiment classifier, or contained any keywords that are relevant to social behavior (e.g., weddings). We manually inspect test examples in X-CSQA and X-CODAH in the English and Chi-nese versions and have a strong confidence there is few strongly controversial example. However, we admit that such reduction of cultural differences in common sense has not been systematically mea-sured in this work for other languages."
  },
  {
    "id": 1770,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/kentonl/e2e-coref",
    "section_title": "5 Experimental Setup",
    "add_info": "1 https://github.com/kentonl/e2e-coref",
    "text": "We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [Cite_Footnote_1]"
  },
  {
    "id": 1771,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/dwadden/dygiepp",
    "section_title": "References",
    "add_info": null,
    "text": "We examine the capabilities of a unified, multi-task framework for three information extrac-tion tasks: named entity recognition, rela-tion extraction, and event extraction. Our framework (called D Y GIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) con-text. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform ex-periments comparing different techniques to construct span representations. Contextual-ized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagat-ing span representations via predicted coref-erence links can enable the model to disam-biguate challenging entity mentions. Our code is publicly available at  https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets."
  },
  {
    "id": 1772,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/bsalehi/wiktionary_MWE_compositionality",
    "section_title": "7 Conclusion",
    "add_info": null,
    "text": "We have proposed an unsupervised approach for predicting the compositionality of an MWE rel-ative to each of its components, based on lexi-cal overlap using Wiktionary, optionally incorpo-rating synonym and translation data. Our experi-ments showed that the various instantiations of our approach are superior to previous state-of-the-art supervised methods. All code to replicate the re-sults in this paper has been made publicly avail-able at  https://github.com/bsalehi/wiktionary_MWE_compositionality."
  },
  {
    "id": 1773,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google/jax",
    "section_title": "8 Experiments And Results",
    "add_info": "5 https://github.com/google/jax",
    "text": "We have implemented the proposed hierarchical attention using Jax, an open source library [Cite_Footnote_5] for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax. In all our experiments in this section, we use the standard Transformer architecture described in (Vaswani et al., 2017) as the backbone for our H-Transformer-1D model. Unless specified other-wise, the model parameters are: number of lay-ers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implemen-tation so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation. This al-lows for an easy and fair comparison."
  },
  {
    "id": 1774,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google/flax/blob/master/flax/nn",
    "section_title": "8 Experiments And Results",
    "add_info": "6 https://github.com/google/flax/blob/master/flax/nn",
    "text": "We have implemented the proposed hierarchical attention using Jax, an open source library for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax. In all our experiments in this section, we use the standard Transformer architecture described in (Vaswani et al., 2017) as the backbone for our H-Transformer-1D model. Unless specified other-wise, the model parameters are: number of lay-ers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implemen-tation [Cite_Footnote_6] so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation. This al-lows for an easy and fair comparison."
  },
  {
    "id": 1775,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/google-research/long-range-arena",
    "section_title": "8 Experiments And Results 8.1 Long-Range Arena",
    "add_info": "7 https://github.com/google-research/long-range-arena",
    "text": "The open-source Long-Range Arena (LRA) benchmark [Cite_Footnote_7] has been proposed as a standard way to probe and quantify the capabilities of var-ious xformer (long-range Transformer) architec-tures (Tay et al., 2020c). In our case, it also serves to highlight the effectiveness of the inductive bias inspired by the H-Matrix method, as well as the capability of our hierarchical attention to handle long sequences."
  },
  {
    "id": 1776,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google/flax",
    "section_title": "8 Experiments And Results 8.2 Language Models Trained on One-Billion Words",
    "add_info": "8 https://github.com/google/flax",
    "text": "We have used Flax, an open-source library [Cite_Footnote_8] to train neural networks, as the code base for the model training. Our H-Transformer-1D model uses the standard Transformer decoder implemen-tation in Flax as the backbone. Only the atten-tion is replaced with our hierarchical attention. We trained both the Transformer baseline and H-Transformer-1D on the One-Billion Word bench-mark (Chelba et al., 2014). We tried different N r (numerical rank) in our H-Transformer-1D model. These represent different inductive bias. We found that H-Transformer-1D with N r = 16 generated text with quality comparable to that of the base-line Transformer. For both Transformer baseline and H-Transformer-1D, we also tried two sets of model parameters: 1) embedding size is 512 and feed-forward module size is 2048 and hence the parameter count is 53M; 2) embedding size is 1024 and feed-forward module size is 4096 and hence the parameter count is 144M. The test per-plexity results of these four models and various SOTA models are shown in table 2."
  },
  {
    "id": 1777,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://syntagnet.org/",
    "section_title": "References",
    "add_info": null,
    "text": "Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance. We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web in-terface and a RESTful API. SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at  http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/)."
  },
  {
    "id": 1778,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://api.syntagnet.org/",
    "section_title": "References",
    "add_info": null,
    "text": "Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance. We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web in-terface and a RESTful API. SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at  http://api.syntagnet.org/)."
  },
  {
    "id": 1779,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "https://babelnet.org/",
    "section_title": "1 Introduction",
    "add_info": "1 https://babelnet.org/",
    "text": "This challenge has been tackled by exploiting huge amounts of hand-annotated data in a super-vised fashion (Raganato et al., 2017b; Bevilacqua and Navigli, 2019; Vial et al., 2019; Bevilacqua and Navigli, 2020) or, alternatively, by harnessing struc-tured information (Agirre et al., 2014; Moro et al., 2014; Scarlini et al., 2020), such as that available within existing lexical knowledge bases (LKBs) like WordNet (Fellbaum, 1998). Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the wide array of languages covered by LKBs like Ba-belNet [Cite_Footnote_1] (Navigli and Ponzetto, 2012), or the Open Multilingual WordNet (Bond and Foster, 2013). Moreover, it is widely acknowledged that the per-formance of a knowledge-based WSD system is strongly correlated with the structure of the LKB employed (Boyd-Graber et al., 2006; Lemnitzer et al., 2008; Navigli and Lapata, 2010; Ponzetto and Navigli, 2010). In fact, the knowledge avail-able within LKBs reflects the fact that words can be linked via two types of semantic relations: paradig-matic relations \u2013 i.e. the most frequently encoun-tered relations in LKBs \u2013 concern the substitution of lexical units, and determine to which level in a hierarchy a language unit belongs by semantic analogy with units similar to it; conversely, syn-tagmatic relations concern the positioning of such units, by linking elements belonging to the same hierarchical level (e.g., words), which appear in the same context (e.g., a sentence). As a case in point, a paradigmatic relation exists, independently of a given context, between the words farm n and workplace n (where a farm is a type of workplace), whereas a syntagmatic relation is entertained be-tween the words work v and farm n , e.g., in the sen-tence \u2018her husband works in a farm as a labourer.\u2019 In our most recent study (Maru et al., 2019, Syn-tagNet), we provided further evidence that the na-ture of LKBs impacts on system performance: the injection of syntagmatic relations \u2013 in the form of disambiguated pairs of co-occurring words \u2013 into an existing LKB biased towards paradigmatic knowledge enables knowledge-based systems to rival their supervised counterparts."
  },
  {
    "id": 1780,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://wordnetcode.princeton.edu/glosstag.shtml",
    "section_title": "2 Preliminaries 2.1 Lexical Knowledge Bases",
    "add_info": "2 http://wordnetcode.princeton.edu/glosstag.shtml",
    "text": "Princeton WordNet Gloss Corpus (PWNG) is the semantically-annotated gloss corpus made available by WordNet since its 3.0 release. [Cite_Footnote_2] Glosses are short definitions providing proper meanings for synsets, and in PWNG they have been tagged according to the senses in WordNet. Following Agirre et al. (2014), we induce new WordNet relations from PWNG by linking the synset to which the gloss refers to each of the synsets that have been tagged in the gloss itself. In this way, additional contextual relations are provided, inadvertently covering syntagmatic relations, too."
  },
  {
    "id": 1781,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/SapienzaNLP/mwsd-datasets",
    "section_title": "5 Evaluation",
    "add_info": "10 Made available at https://github.com/SapienzaNLP/mwsd-datasets.",
    "text": "In order to assess its performance, we tested Syn-tagRank on the five English all-words WSD evalu-ation datasets standardized according to WordNet 3.0 in the framework of Raganato et al. (2017a), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Nav-igli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). As regards the appraisal of Synta-gRank in a multilingual setting, we used the Ger-man, Spanish, French and Italian annotations avail-able in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets [Cite_Footnote_10] , which is accordant with the BabelNet API 4.0.1 graph and enables testing on a larger number of instances than hitherto."
  },
  {
    "id": 1782,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://api.syntagnet.org/disambiguate_tokens",
    "section_title": "A API Documentation",
    "add_info": null,
    "text": "Token Parameters id (String) word (String) lemma (String) pos (String) isTargetWord (boolean) POST  http://api.syntagnet.org/disambiguate_tokens"
  },
  {
    "id": 1783,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://scikit-learn.org/stable/",
    "section_title": "3 Experiments 3.3 Sentence Evaluation",
    "add_info": "6 http://scikit-learn.org/stable/",
    "text": "Prediction We use E MBED A LIGN to annotate every word in the training set of the benchmarks above with the posterior mean embedding in con-text. We then average embeddings in a sentence and give that as features to a logistic regression classifier trained with 5-fold cross validation. [Cite_Footnote_6]"
  },
  {
    "id": 1784,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/SussexCompSem/learninghypernyms",
    "section_title": "5 Evaluation 5.1 Hyponymy with Word2Vec Vectors",
    "add_info": "2 https://github.com/SussexCompSem/learninghypernyms",
    "text": "For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs [Cite_Footnote_2] from the BLESS dataset (Baroni and Lenci, 2011). These noun-noun word pairs include positive hy-ponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs. Their selection is balanced between positive and negative examples, so that accuracy can be used as the performance measure. For their semi-supervised experiments, ten-fold cross validation is used, where for each test set, items are removed from the associated training set if they contain any word from the test set. Thus, the vocabulary of the training and testing sets are always disjoint, thereby requiring that the models learn about the vector space and not about the words themselves. We had to perform our own 10-fold split, but apply the same procedure to filter the training set."
  },
  {
    "id": 1785,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "5 Evaluation 5.1 Hyponymy with Word2Vec Vectors",
    "add_info": "4 https://code.google.com/archive/p/word2vec/",
    "text": "We could not replicate the word embeddings used in Weeds et al. (2014), so instead we use pub-licly available word embeddings. [Cite_Footnote_4] These vectors were trained with the Word2Vec software applied to about 100 billion words of the Google-News dataset, and have 300 dimensions."
  },
  {
    "id": 1786,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "http://hierplane.allenai.org/explain",
    "section_title": "4 APIs",
    "add_info": null,
    "text": "Our web interface, hosted at  http://hierplane.allenai.org/explain, has the architecture shown in Figure 6. The front-end accepts text input from the user. It forwards this text to the back-end and receives a JSON that contains an annotated tree and styling instructions. Figure 7 shows a simplified version of the JSON that renders the \u201cHe ate pasta with chopsticks\u201d subtree in Figure 5. This JSON is passed to the visualization library, which renders it in HTML and returns this HTML to the front-end."
  },
  {
    "id": 1787,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://hierplane.allenai.org/explain/stanford/",
    "section_title": "4 APIs 4.1 Basic Features",
    "add_info": null,
    "text": "\u2022 We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence \u201creliably great music, good food, and excellent service\u201d). http://hierplane.allenai.org/explain/ with  http://hierplane.allenai.org/explain/stanford/"
  },
  {
    "id": 1788,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://hierplane.allenai.org/explain/stanford/",
    "section_title": "4 APIs 4.1 Basic Features",
    "add_info": null,
    "text": "For instance:  http://hierplane.allenai.org/explain/stanford/ He%20ate%20pasta%20with% 20chopsticks"
  },
  {
    "id": 1789,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://demos.explosion.ai/",
    "section_title": "2 Related Work",
    "add_info": "Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.",
    "text": "Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016)  provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008)."
  },
  {
    "id": 1790,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlpviz.bpodgursky.com",
    "section_title": "2 Related Work",
    "add_info": "Ben Podgursky. 2015. Nlpviz. http://nlpviz.bpodgursky.com. Accessed: 2017-04-27.",
    "text": "Figure 1 was generated from (Podgursky, 2015)  , which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016) provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008)."
  },
  {
    "id": 1791,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://trainomatic.org",
    "section_title": "References",
    "add_info": null,
    "text": "Annotating large numbers of sentences with senses is the heaviest requirement of current Word Sense Disambiguation. We present Train-O-Matic, a language-independent method for generating mil-lions of sense-annotated training instances for virtually all meanings of words in a language\u2019s vocabulary. The approach is fully automatic: no human interven-tion is required and the only type of hu-man knowledge used is a WordNet-like resource. Train-O-Matic achieves consis-tently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the bur-den of manual annotation. All the training data is available for research purposes at  http://trainomatic.org."
  },
  {
    "id": 1792,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://babelnet.org",
    "section_title": "3 Creating a Denser and Multilingual Semantic Network",
    "add_info": "2 http://babelnet.org",
    "text": "To cope with these issues, we exploit Babel-Net, [Cite_Footnote_2] a huge multilingual semantic network ob-tained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Nav-igli and Ponzetto, 2012), and create the Babel-Net subgraph induced by the WordNet vertices. The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to Word-Net (such as hyperlinks in a Wikipedia article con-necting it to other articles). The greatest contribu-tion of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related ar-ticles (e.g., the English Wikipedia Bus article is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more)."
  },
  {
    "id": 1793,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://trainomatic.org",
    "section_title": "8 Conclusion",
    "add_info": null,
    "text": "We believe that the ability of T-O-M to over-come the current paucity of annotated data for WSD, coupled with video games with a pur-pose for validation purposes (Jurgens and Nav-igli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD. All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at  http://trainomatic.org."
  },
  {
    "id": 1794,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://radimrehurek.com/gensim/",
    "section_title": "5 Experiments",
    "add_info": "2 https://radimrehurek.com/gensim/",
    "text": "For all experiments, we use the Word2Vec imple-mentation in Gensim [Cite_Footnote_2] to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250)."
  },
  {
    "id": 1795,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/mfaruqui/crosslingual-cca",
    "section_title": "5 Experiments",
    "add_info": "3 https://github.com/mfaruqui/crosslingual-cca proaches on various languages. Since about one-third of the test mentions are non-trivial, a baseline is 66.67 for all languages, if we pick the most common title given the mention. Bold signi-fies highest score for each column.",
    "text": "For all experiments, we use the Word2Vec imple-mentation in Gensim to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) [Cite_Footnote_3] in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250)."
  },
  {
    "id": 1796,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://bilbo.cs.illinois.edu/\u02dcctsai12/xlwikifier-wikidata.zip",
    "section_title": "5 Experiments 5.1 Wikipedia Dataset",
    "add_info": null,
    "text": "We create this dataset from the documents in Wikipedia by taking the anchors (hyperlinked texts) as the query mentions and the corresponding English Wikipedia titles as the answers. Note that we only keep the mentions for which we can get the corre-sponding English Wikipedia titles by the language links. As observed in previous work (Ratinov et al., 2011), most of the mentions in Wikipedia docu-ments are easy, that is, the baseline of simply choos-ing the title that maximizes P r(title|mention), the most frequent title given the mention surface string, performs quite well. In order to create a more chal-lenging dataset, we randomly select mentions such that the number of easy mentions is about twice the number of hard mentions (those mentions for which the most common title is not the correct title). This generation process is inspired by (and close to) the distribution generated in the TAC KBP2015 Entity Linking Track. Another problem that occurs when creating a dataset from Wikipedia documents is that even though training documents are different from test documents, many mentions and titles actually overlap. To test that the algorithms really general-ize from training examples, we ensure that no (men-tion, title) pair in the test set appear in the training set. Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at  http://bilbo.cs.illinois.edu/\u02dcctsai12/xlwikifier-wikidata.zip."
  },
  {
    "id": 1797,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://sweaglesw.org/linguistics/ace/",
    "section_title": "3 Literal versus Intended Meaning 3.3.2 SemBanking with ERG",
    "add_info": "2 http://sweaglesw.org/linguistics/ace/",
    "text": "As there is no gold semantics-annotated corpus for learner English and building such a corpus from scratch is tedious and time-consuming, we exploit ERG to establish a large-scale sembank-ing with informative semantic representations. To be specific, for each input sentence S, we gener-ate K-best semantic graphs G 1 , G 2 , ..., G K with an ERG-based processor, i.e. ACE [Cite_Footnote_2] . The created grammar-licensed analyses contain both a deriva-tion tree recording the used grammar rules and lex-ical entries, and the associated semantic represen-tation constructed compositionally via this deriva-tion (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can bene-fit many NLP systems by incorporating domain-specific knowledge and reasoning."
  },
  {
    "id": 1798,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://sites.google.com/site/naistlang8corpora/",
    "section_title": "6 Parsing to Intended Meanings",
    "add_info": "3 https://sites.google.com/site/naistlang8corpora/",
    "text": "We train the factorization-based model on DeepBank and examine the performance on L2 and L1 sentences as well as the revised sentences by two GEC models. The produced graphs are compared with I-silver which represents the in-tended meaning. We notice that during the compu-tation of SMATCH , some disagreements of nodes result from the discrepancy of morphological vari-ation or different collocations between the input and the standard sentence. Hence the node score may be underestimated. Therefore, we relax the standards of matching nodes. We establish a para-phrase table based on the statistical machine trans-lation between a parallel learner corpus [Cite_Footnote_3] . As long as the labels of two aligned nodes have the same stem or they form a paraphrase pair in our ta-ble, then the two nodes can be considered \u201cmatch-ing\u201d. We call the new evaluation metric as \u201cnode-relaxed SMATCH \u201d."
  },
  {
    "id": 1799,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/ibm/loa",
    "section_title": "4 Conclusion",
    "add_info": null,
    "text": "We propose a novel demonstration (URL: https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent. We also extend more complicated LNN for other difficult games on the demo site. At the same time, we open the source code for the demonstration (URL:  https://github.com/ibm/loa)."
  },
  {
    "id": 1800,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://diamt.limsi.fr/eval.html",
    "section_title": "2 Evaluating contextual phenomena 2.1 Our contrastive discursive test sets",
    "add_info": "2 The test sets are freely available at https://diamt.limsi.fr/eval.html.",
    "text": "We created two contrastive test sets to help com-pare how well different contextual MT mod-els handle (i) anaphoric pronoun translation and (ii) coherence and cohesion. [Cite_Footnote_2] For each test set, models are assessed on their ability to rank the cor-rect translation of an ambiguous sentence higher than the incorrect translation, using the disam-biguating context provided in the previous source and/or target sentence. All examples in the test sets are hand-crafted but inspired by real examples from OpenSubtitles2016 (Lison and Tiedemann, 2016) to ensure that they are credible and that vo-cabulary and syntactic structures are varied. The method can be used to evaluate any NMT model, by making it produce a score for a given source sentence and reference translation."
  },
  {
    "id": 1801,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://www.msxiaoice.com",
    "section_title": "References",
    "add_info": "2 http://www.msxiaoice.com",
    "text": "Most current chatbot engines are designed to reply to user utterances based on exist-ing utterance-response (or Q-R) pairs. In this paper, we present DocChat, a novel information retrieval approach for chat-bot engines that can leverage unstructured documents, instead of Q-R pairs, to re-spond to utterances. A learning to rank model with features designed at different levels of granularity is proposed to mea-sure the relevance between utterances and responses directly. We evaluate our pro-posed approach in both English and Chi-nese: (i) For English, we evaluate Doc-Chat on WikiQA and QASent, two answer sentence selection tasks, and compare it with state-of-the-art methods. Reasonable improvements and good adaptability are observed. (ii) For Chinese, we compare DocChat with XiaoIce [Cite_Footnote_2] , a famous chitchat engine in China, and side-by-side evalua-tion shows that DocChat is a perfect com-plement for chatbot engines using Q-R pairs as main source of responses."
  },
  {
    "id": 1802,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://www.freebase.com/",
    "section_title": "4 Response Ranking 4.5 Relation-level Feature",
    "add_info": "5 http://www.freebase.com/",
    "text": "Given a structured knowledge base, such as Free-base [Cite_Footnote_5] , a single relation question Q (in natural language) with its answer can be first parsed into a fact formatted as he sbj ,rel,e obj i, where e sbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, e obj denotes an object entity found from the knowledge base based on e sbj and rel. Then we can get hQ,reli pairs. This rel can help for modeling semantic relationships between Q and R. For example, the Q-A pair hWhat does Jimmy Neutron do? \u2212 inventori can be parsed into hJimmy Neutron, fiction-al character occupation, inventori where the rel is fictional character occupation."
  },
  {
    "id": 1803,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://aka.ms/WikiQA",
    "section_title": "7 Experiments 7.1 Evaluation on QA (English) 7.1.1 Experiment Setup",
    "add_info": "6 http://aka.ms/WikiQA",
    "text": "We select WikiQA [Cite_Footnote_6] as the evaluation data, as it is precisely constructed based on natural language questions and Wikipedia documents, which con-tains 2,118 \u2018question-document\u2019 pairs in the train-ing set, 296 \u2018question-document\u2019 pairs in devel-opment set, and 633 \u2018question-document\u2019 pairs in testing set. Each sentence in the document of a given question is labeled as 1 or 0, where 1 de-notes the current sentence is a correct answer sen-tence, and 0 denotes the opposite meaning. Given a question, the task of WikiQA is to select answer sentences from all sentences in a question\u2019s corre-sponding document. The training data settings of response ranking features are described below."
  },
  {
    "id": 1804,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://wiki.answers.com",
    "section_title": "7 Experiments 7.1 Evaluation on QA (English) 7.1.1 Experiment Setup",
    "add_info": "7 http://wiki.answers.com",
    "text": "F w denotes 3 word-level features, h WM , h W2W and h W2V . For h W2W , GIZA++ is used to train word alignments on 11.6M \u2018question-related question\u2019 pairs (Fader et al., 2013) crawled from WikiAnswers. [Cite_Footnote_7] . For h W2V , Word2Vec (Mikolov et al., 2013) is used to train word embedding on sentences from Wikipedia in English."
  },
  {
    "id": 1805,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://answers.yahoo.com",
    "section_title": "7 Experiments 7.1 Evaluation on QA (English) 7.1.1 Experiment Setup",
    "add_info": "10 https://answers.yahoo.com",
    "text": "F p denotes 2 phrase-level features, h PP and h PT . For h PP , bilingual data is used to extrac-t a phrase-based translation table (Koehn et al., 2003), from which paraphrases are extracted (Sec-tion 4.2.1). For h PT , GIZA++ trains word align-ments on 4M \u2018question-answer\u2019 pairs crawled from Yahoo Answers [Cite_Footnote_10] , and then a phrase ta-ble is extracted from word alignments using the intersect-diag-grow refinement."
  },
  {
    "id": 1806,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://research.facebook.com/research/-babi/",
    "section_title": "7 Experiments 7.1 Evaluation on QA (English) 7.1.1 Experiment Setup",
    "add_info": "11 https://research.facebook.com/research/-babi/",
    "text": "F r and F ty denote relation-level feature h RE and type-level feature h TE . Bordes et al. (2015) released the SimpleQuestions data set [Cite_Footnote_11] , which consists of 108,442 English questions. Each ques-tion (e.g., What does Jimmy Neutron do?) is written by human annotators based on a triple in Freebase which formatted as he sbj , rel, e obj i (e.g., hJimmy Neutron, fictional character occupation, inventori) Here, as described in Section 4.5 and 4.6, \u2018question-relation\u2019 pairs and \u2018question-type\u2019 pairs based upon SimpleQuestions data set are used to train h RE and h TE ."
  },
  {
    "id": 1807,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://msnews.github.io/index.html",
    "section_title": "4 Experiment 4.1 Experimental Datasets and Settings",
    "add_info": "1 We use the small version of MIND for quick experiments. This dataset is at https://msnews.github.io/index.html",
    "text": "We conduct extensive experiments on two real-world datasets to evaluate the effectiveness of Hi-eRec. The first one is the public MIND dataset (Wu et al., 2020d) [Cite_Footnote_1] . It is constructed by user behavior data collected from Microsoft News from October 12 to November 22, 2019 (six weeks), where user data in the first four weeks was used to construct users\u2019 reading history, user data in the penultimate week was used for model training and user data in the last week was used for evaluation. Besides, MIND contains off-the-shelf topic and subtopic la-bel for each news. The second one (named Feeds) is constructed by user behavior data sampled from a commercial news feeds app in Microsoft from Jan-uary 23 to April 01, 2020 (13 weeks). We randomly sample 100,000 and 10,000 impressions from the first ten weeks to construct training and validation set, and 100,000 impressions from the last three weeks to construct test data. Since Feeds only con-tains topic label of news, we implement a simpli-fied version of HieRec with only user- and topic-level interest representations on Feeds. Besides, following Wu et al. (2020d), users in Feeds were anonymized via hash algorithms and de-linked from the production system to protect user privacy. Detailed information is summarized in Table 1."
  },
  {
    "id": 1808,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/JulySinceAndrew/HieRec",
    "section_title": "4 Experiment 4.1 Experimental Datasets and Settings",
    "add_info": "2 https://github.com/JulySinceAndrew/HieRec",
    "text": "Next, we introduce experimental settings and hyper-parameters of HieRec. We use the first 30 words and 5 entities of news titles and users\u2019 re-cent 50 clicked news in experiments. We adopt pre-trained glove (Pennington et al., 2014) word embeddings and TransE entity embeddings (Bor-des et al., 2013) for initialization. In HieRec, the word and entity self-attention network output 400-and 100-dimensional vectors, respectively. Besides, the unified news representation is 400-dimensional. Attention networks (i.e., \u03c6 s (\u00b7), \u03c6 t (\u00b7), and \u03c6 g (\u00b7)) are implemented by single-layer dense networks. Besides, dimensions of topic and subtopic embed-dings are 400, both of which are randomly ini-tialized and fine-tuned. The hyper-parameters for combining different interest scores, i.e. \u03bb t and \u03bb s , are set to 0.15 and 0.7 respectively. Moreover, we utilize dropout technique (Srivastava et al., 2014) and Adam optimizer (Kingma and Ba, 2015) for training. HieRec is trained for 5 epochs with 0.0001 learning rate. All hyper-parameters of HieRec and baseline methods are manually tuned on the valida-tion set. [Cite_Footnote_2] Following Wu et al. (2019e), we use four ranking metrics, i.e., AUC, MRR, nDCG@5, and nDCG@10, for performance evaluation."
  },
  {
    "id": 1809,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/ygraham/significance-williams",
    "section_title": "4 Evaluation and Discussion",
    "add_info": null,
    "text": "As part of this research, we have made avail-able an open-source implementation of statis-tical tests tailored to the assessment of MT metrics available at  https://github.com/ygraham/significance-williams."
  },
  {
    "id": 1810,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "5 Experiments 5.1 Datasets",
    "add_info": "2 We fine-tune GPT2 language model using https://github.com/huggingface/transformers.",
    "text": "Data Preparation. We split the stories from both datasets into two subsets for training generation and evaluation models, respectively. We use 70 percent of stories in ROC (ROC_LM) and WP (WP_LM) for fine-tuning GPT2 (Radford et al., 2019) lan-guage model with batch size of 4. [Cite_Footnote_2] After 3 epochs of fine-tuning, the perplexity on the validation set of ROC and WP datasets are 8.28 and 25.04, re-spectively."
  },
  {
    "id": 1811,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "5 Experiments 5.3 Experimental Setup",
    "add_info": "3 We fine-tune RoBERTa and Longformer mod-els using https://github.com/huggingface/transformers.",
    "text": "In our experiments, we have SENTAVG as the baseline model. We compare SENTAVG across more powerful classifiers \u2013 RoBERTa for ROC sto-ries and Longformer for WP stories (FT_LM). We fine-tune pretrained RoBERTa-base model with the learning rate of 2e-5 and batch size 8 for three epochs and process the ROC stories with a maxi-mum of 128 tokens. To evaluate WP with lengthy stories, we fine-tune pretrained Longformer-base model with the learning rate of 2e-5 and batch size 3 by encoding texts with at most 1024 tokens for three epochs. [Cite_Footnote_3]"
  },
  {
    "id": 1812,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/prdwb/bert_hae",
    "section_title": "4 A more Robust Protocol 4.1 Reproducing the Regular Evaluation in the Semi-automatic Scenario",
    "add_info": "1 https://github.com/prdwb/bert_hae",
    "text": "To evaluate the baseline performance (semi-automatic), we train BERT-HAE and BERT-PHAE on QuAC using the protocol described by the au-thors (Qu et al., 2019a) and the same hyperparame-ters: history markers from up to 6 turns, and spe-cific optimization parameters (12 as batch size, 3e- 5 as learning rate with a linear decrease to 0 over 24k training steps). We implement our own train-ing script on the basis of codes pieces from the transformers library (Wolf et al., 2019) and BERT-HAE\u2019s authors [Cite_Footnote_1] . Experiments are run with a Nvidia Tesla V100 GPU."
  },
  {
    "id": 1813,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/",
    "section_title": "3 Method 3.6 Training Procedure",
    "add_info": "2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.",
    "text": "Our model is initialized using a pre-trained BERT model [Cite_Footnote_2] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task."
  },
  {
    "id": 1814,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2011T13",
    "section_title": "4 Experiments 4.2 Experimental Settings",
    "add_info": "6 https://catalog.ldc.upenn.edu/LDC2011T13",
    "text": "LSTM baselines. We compare character-level B I L STM (Lample et al., 2016) and B I L STM +E NT , which concatenates the character embeddings and its corresponding entity embeddings as inputs. We also compare a gazetteer based method L ATTICE (Zhang and Yang, 2018) and L ATTICE (R E E NT ), which replaces the word gazetteer of L ATTICE with our entity dictionary for fair comparison. We use the same embeddings as (Zhang and Yang, 2018), which are pre-trained on Giga-Word [Cite_Footnote_6] us-ing Word2vec (Mikolov et al., 2013). The entity embeddings are randomly initialized and fine-tuned during training."
  },
  {
    "id": 1815,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://babelnovel.com/",
    "section_title": "B Details of the Datasets B.2 Novel Dataset",
    "add_info": "9 https://babelnovel.com/",
    "text": "Data collection. We construct our corpus from a professional Chinese novel reading site named Babel Novel [Cite_Footnote_9] . Unlike news, the novel dataset cov-ers a mixture of literary style including historical novels, and martial arts novels in the genre of fan-tasy, mystery, romance, military, etc. Therefore, unique characteristics of this dataset such as novel-specific types of named entities present challenges for NER."
  },
  {
    "id": 1816,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/CongBao/AutoencodedMetaEmbedding",
    "section_title": "4 Experiments",
    "add_info": "1 We used the implementation available at: https://github.com/CongBao/AutoencodedMetaEmbedding",
    "text": "2. How do these compare with just fusing Src and Tgt via recent meta-embedding methods like AAEME (Bollegala and Bao, 2018) [Cite_Footnote_1] ?"
  },
  {
    "id": 1817,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://nlp.cis.unimelb.edu.au/resources/cqadupstack/",
    "section_title": "Topics and tasks",
    "add_info": "3 http://nlp.cis.unimelb.edu.au/resources/cqadupstack/",
    "text": "StackExchange topics We pick four topics (Physics, Gaming, Android and Unix) from the CQADupStack [Cite_Footnote_3] dataset of questions and re-sponses. For each topic, the available response text is divided into D T , used for training/adapt-ing embeddings, and Df T , the evaluation fold used to measure perplexity. In each topic, the target corpus D T has 2000 responses totalling roughly 1 MB. We also report results with changing sizes of D T . Depending on the method we use D T , D S , or u S to train topic-specific embeddings and eval-uate them as-is on two tasks that train task-specific layers on top of these fixed embeddings. The first is an unsupervised language modeling task where we train a LSTM on the adapted embed-dings (which are pinned) and report perplexity on Df T . The second is a Duplicate question detec-tion task. Available in each topic are human an-notated duplicate questions (statistics in Table 10 of Appendix) which we partition across train, test and dev as 50%, 40%, 10%. For contrastive train-ing, we add four times as much randomly chosen non-duplicate pairs. The goal is to predict dupli-cate/not for a question pair, for which we use word mover distance (Kusner et al., 2015, WMD) over adapted word embeddings. We found WMD more accurate than BiMPM (Wang et al., 2017). We use three splits of the target corpus, and for each re-sultant embedding, measure AUC on three random (train-)dev-test splits of question pairs, for a total of nine runs. For reporting AUC, WMD does not need the train fold."
  },
  {
    "id": 1818,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/vihari/we_adapt_datasets",
    "section_title": "Topics and tasks",
    "add_info": null,
    "text": "Pretrained embeddings E are trained on Wikipedia using the default settings of word2vec\u2019s CBOW model. All our data splits are made publicly available at  https://github.com/vihari/we_adapt_datasets."
  },
  {
    "id": 1819,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://qwone.com/\u02dcjason/20Newsgroups/",
    "section_title": "Topics and tasks",
    "add_info": "6 http://qwone.com/\u02dcjason/20Newsgroups/",
    "text": "Topics from 20 newsgroup We choose the five top-level classes in the 20 newsgroup dataset [Cite_Footnote_6] as topics; viz.: Computer, Recreation, Science, Pol-itics, Religion. The corresponding five down-stream tasks are text classification over the 3\u2013 fine-grained classes under each top-level class. Train, test, dev splits were 50%, 40%, 10%. We average over nine splits. The body text is used as D T and subject text is used for classification."
  },
  {
    "id": 1820,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "Topics and tasks 4.2 Epochs vs. regularization results",
    "add_info": "7 https://code.google.com/archive/p/word2vec/",
    "text": "In Figure 2 we show perplexity and AUC against training epochs. Here we focus on four meth-ods: Tgt, SrcTune, RegFreq, and RegSense. First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code [Cite_Footnote_7] and left unchanged in RegFreq (Yang et al., 2017)). In contrast, Src-Tune, RegSense, and RegFreq are much better than Tgt at five epochs, saturating quickly. With respect to perplexity, SrcTune starts getting worse around 20 iterations and becomes identical to Tgt, showing catastrophic forgetting. Regularizers in RegFreq and RegSense are able to reduce such for-getting, with RegSense being more effective than RegFreq. These experiments show that any com-parison that chooses a fixed number of training epochs across all methods is likely to be unfair. Henceforth we will use a validation set for the stopping criteria. While this is standard practice for supervised tasks, most word embedding code we downloaded ran for a fixed number of epochs, making comparisons unreliable. We conclude that validation-based stopping is critical for fair evalu-ation."
  },
  {
    "id": 1821,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/Victor0118/cross_domain_embedding/",
    "section_title": "Topics and tasks 4.2 Epochs vs. regularization results",
    "add_info": "8 https://github.com/Victor0118/cross_domain_embedding/",
    "text": "In Figure 2 we show perplexity and AUC against training epochs. Here we focus on four meth-ods: Tgt, SrcTune, RegFreq, and RegSense. First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code and left unchanged in RegFreq [Cite_Footnote_8] (Yang et al., 2017)). In contrast, Src-Tune, RegSense, and RegFreq are much better than Tgt at five epochs, saturating quickly. With respect to perplexity, SrcTune starts getting worse around 20 iterations and becomes identical to Tgt, showing catastrophic forgetting. Regularizers in RegFreq and RegSense are able to reduce such for-getting, with RegSense being more effective than RegFreq. These experiments show that any com-parison that chooses a fixed number of training epochs across all methods is likely to be unfair. Henceforth we will use a validation set for the stopping criteria. While this is standard practice for supervised tasks, most word embedding code we downloaded ran for a fixed number of epochs, making comparisons unreliable. We conclude that validation-based stopping is critical for fair evalu-ation."
  },
  {
    "id": 1822,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://www.ark.cs.cmu.edu/MT/",
    "section_title": "4 Experiments 4.2 Experimental results",
    "add_info": "3 We used the code available at http://www.ark.cs.cmu.edu/MT/.",
    "text": "We also performed the paired bootstrap re-sampling test (Koehn, 2004). [Cite_Footnote_3] We sampled 2000 samples for each significance test."
  },
  {
    "id": 1823,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/rtmaww/SENT",
    "section_title": "1 Introduction",
    "add_info": "1 https://github.com/rtmaww/SENT",
    "text": "In this work, we propose the use of negative training (NT) (Kim et al., 2019) for distant RE. Different from positive training (PT), NT trains a model by selecting the complementary labels of the given label, regarding that \u201cthe input sentence does not belong to this complementary label\u201d. Since the probability of selecting a true label as a complementary label is low, NT decreases the risk of providing noisy information and prevents the model from overfitting the noisy data. Moreover, the model trained with NT is able to separate the noisy data from the training data (a histogram in Fig.3 shows the separated data distribution during NT). Based on NT, we propose SENT, a sentence-level framework for distant RE. During SENT training, the noisy instances are not only filtered with a noise-filtering strategy, but also transformed into useful training data with a re-labeling method. We further design an iterative training algorithm to take full advantage of these data-refining processes, which significantly boost performance. Our codes are publicly available at Github [Cite_Footnote_1] ."
  },
  {
    "id": 1824,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR",
    "section_title": "4 Experiments",
    "add_info": "4 https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR",
    "text": "The first part is the effectiveness study on sentence-level evaluation for distant RE. Different from bag-level evaluation, a sentence-level evalua-tion compute Precision (Prec.), Recall (Rec.) and F1 metric directly on all of the individual instances in the dataset. In this part, we adopt the NYT-10 data set for sentence-level training, following the setting of Jia et al. (2019), who publishes a manually labeled sentence-level test set. [Cite_Footnote_4] Besides, they also publish a test set for evaluating noise-filtering ability. Details of the adopted dataset are shown in Table 1."
  },
  {
    "id": 1825,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Extend",
    "url": "https://github.com/yuhaozhang/tacred-relation",
    "section_title": "4 Experiments",
    "add_info": "5 https://github.com/yuhaozhang/tacred-relation",
    "text": "We construct the second part of experiments (Sec.4.4) to better understand SENT\u2019s behaviors. Since no labeled training data are available in the distant supervision setting, we construct a noisy dataset with 30% noise from a labeled dataset, TACRED (Zhang et al., 2017) [Cite_Footnote_5] . We regard this constructed dataset as noisy-TACRED. The reason we choose this dataset is that 80% instances in the training data are \u201cno relation\u201d. This \u201cNA\u201d rate is similar to the NYT data which contains 70% \u201cNA\u201d relation type, thus analysis on this dataset is more credible."
  },
  {
    "id": 1826,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://nlp.cs.nyu.edu/evalb",
    "section_title": "4 Indicative Current Results 4.2 Comparative Acccuracy",
    "add_info": "Satoshi Sekine and Michael Collins. 1997. EvalB. Available at http://nlp.cs.nyu.edu/evalb",
    "text": "Table 2 primarily compares the accuracy of the Collins model 3 and RH parsers. The entries show the proportion of fully accurate parses, the f-score average of bracket precision and recall, and average crossing brackets, as obtained by EVALB (Sekine and Collins, 1997)  . The RH f-score is currently somewhat lower, but the proportion of fully correct parses is significantly higher."
  },
  {
    "id": 1827,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.jaist.ac.jp/\u02dch-yamada/",
    "section_title": "3 Minimally Lexicalized Dependency Parsing 3.1 Base Dependency Parser",
    "add_info": "2 http://www.jaist.ac.jp/\u02dch-yamada/",
    "text": "We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [Cite_Footnote_2] . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default."
  },
  {
    "id": 1828,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www-tsujii.is.s.u-tokyo.ac.jp/\u02dctsuruoka/postagger/",
    "section_title": "3 Minimally Lexicalized Dependency Parsing 3.1 Base Dependency Parser",
    "add_info": "4 http://www-tsujii.is.s.u-tokyo.ac.jp/\u02dctsuruoka/postagger/",
    "text": "We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger [Cite_Footnote_4] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default."
  },
  {
    "id": 1829,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.org/\u02dctaku-ku/software/yamcha/",
    "section_title": "3 Minimally Lexicalized Dependency Parsing 3.1 Base Dependency Parser",
    "add_info": "5 http://chasen.org/\u02dctaku-ku/software/yamcha/",
    "text": "We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [Cite_Footnote_5] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default."
  },
  {
    "id": 1830,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://chasen.org/\u02dctaku-ku/software/TinySVM/",
    "section_title": "3 Minimally Lexicalized Dependency Parsing 3.1 Base Dependency Parser",
    "add_info": "6 http://chasen.org/\u02dctaku-ku/software/TinySVM/",
    "text": "We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM [Cite_Footnote_6] ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default."
  },
  {
    "id": 1831,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/SapienzaNLP/sgl",
    "section_title": "References",
    "add_info": null,
    "text": "Graph-based semantic parsing aims to repre-sent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several di-verse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multi-ple formalisms as Multilingual Neural Ma-chine Translation ( MNMT ), and propose SGL , a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora com-ing from Machine Translation: we report com-petitive performances on AMR and UCCA pars-ing, especially once paired with pre-trained ar-chitectures. Furthermore, we find that mod-els trained under this configuration scale re-markably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its com-petitors by a large margin without even explic-itly seeing non-English to AMR examples at training time and, once these examples are in-cluded as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at  https://github.com/SapienzaNLP/sgl."
  },
  {
    "id": 1832,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/RikVN/AMR",
    "section_title": "3 Speak the Graph Languages ( SGL ) 3.1 Graph Linearizations",
    "add_info": "6 https://github.com/RikVN/AMR",
    "text": "For AMR parsing, as in van Noord and Bos (2017), we first simplify AMR graphs by remov-ing variables and wiki links. We then convert these stripped AMR graphs into trees by duplicating co-referring nodes. At this point, in order to obtain the final linearized version of a given AMR , we concatenate all the lines of its PENMAN notation (Goodman, 2020) together, replacing newlines and multiple spaces with single spaces (Figure 1a and 1b). Conversely, delinearization is performed by assigning a variable to each predicted concept, per-forming Wikification, restoring co-referring nodes and, where possible, repairing any syntactically malformed subgraph. For both phases, we use the scripts released by van Noord and Bos (2017). [Cite_Footnote_6]"
  },
  {
    "id": 1833,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/berniey/hanziconv",
    "section_title": "4 Experimental Setup 4.2 Datasets and Preprocessing",
    "add_info": "12 We use the hanziconv library (https://github.com/berniey/hanziconv).",
    "text": "UCCA We replicate the setting of the CoNLL 2019 Shared Task (Oepen et al., 2019). We train our models using the freely available 10 UCCA portion of the training data; this corpus amounts to 6 572 sentence-graph pairs, drawn from the English Web Treebank (2012T13) and English Wikipedia arti-cles on celebrities. As no official development set was included in the data release, following Hersh-covich and Arviv (2019), we reserve 500 instances and use them as the validation set. To the best of our knowledge, the full evaluation data have not been released yet and, therefore, we compare with state-of-the-art alternatives and report results only on The Little Prince, a released subset consisting of 100 manually-tagged sentences sampled from the homonymous novel. Parallel Data We use English-centric paral-lel corpora in four languages, namely, Chinese, German, Italian and Spanish; we employ Mul-tiUN (Tiedemann, 2012) for Chinese and Spanish, ParaCrawl (Espl\u00e0 et al., 2019) for German, and Europarl (Tiedemann, 2012) for Italian. We per-form a mild filtering over all the available parallel sentences and then take the first 5M out of these. Preprocessing We do not perform any prepro-cessing or tokenization, except for the graph lin-earizations explained in \u00a73.1 and Chinese simpli-fication. [Cite_Footnote_12] Instead, we directly apply subword to-kenization with a Unigram Model (Kudo, 2018). When working with Cross in a single-task setting on AMR or UCCA , we follow Ge et al. (2019) and use a vocabulary size of 20k subwords; instead, when working in the multilingual setting, we in-crease this value to 50k so as to better accom-modate the increased amount of languages. Con-versely, when using mBART, we always use the original vocabulary consisting of 250k subwords."
  },
  {
    "id": 1834,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/snowblink14/smatch",
    "section_title": "4 Experimental Setup 4.3 Evaluation",
    "add_info": "13 https://github.com/snowblink14/smatch",
    "text": "We evaluate AMR and cross-lingual AMR parsing by using the Smatch score [Cite_Footnote_13] (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems\u2019 performances, we also re-port the fine-grained scores as computed by the evaluation toolkit 14 of Damonte et al. (2017). For UCCA parsing, we employ the official evaluation metric 15 of the shared task, conceptually similar to the Smatch score."
  },
  {
    "id": 1835,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/mdtux89/amr-evaluation",
    "section_title": "4 Experimental Setup 4.3 Evaluation",
    "add_info": "14 https://github.com/mdtux89/amr-evaluation",
    "text": "We evaluate AMR and cross-lingual AMR parsing by using the Smatch score 13 (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems\u2019 performances, we also re-port the fine-grained scores as computed by the evaluation toolkit [Cite_Footnote_14] of Damonte et al. (2017). For UCCA parsing, we employ the official evaluation metric of the shared task, conceptually similar to the Smatch score."
  },
  {
    "id": 1836,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/cfmrp/mtool",
    "section_title": "4 Experimental Setup 4.3 Evaluation",
    "add_info": "15 https://github.com/cfmrp/mtool",
    "text": "We evaluate AMR and cross-lingual AMR parsing by using the Smatch score 13 (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems\u2019 performances, we also re-port the fine-grained scores as computed by the evaluation toolkit of Damonte et al. (2017). For UCCA parsing, we employ the official evaluation metric [Cite_Footnote_15] of the shared task, conceptually similar to the Smatch score."
  },
  {
    "id": 1837,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset",
    "section_title": "References",
    "add_info": "1 The Chinese literature text corpus for relation classi-fication, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset",
    "text": "Relation classification is an important seman-tic processing task in the field of natural lan-guage processing. In this paper, we propose the task of relation classification for Chinese literature text. A new dataset of Chinese liter-ature text is constructed to facilitate the study in this task. We present a novel model, named Structure Regularized Bidirectional Recurrent Convolutional Neural Network (SR-BRCNN), to identify the relation between entities. The proposed model learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized de-pendency tree, which has the benefits of re-ducing the complexity of the whole model. Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art ap-proaches on Chinese literature text [Cite_Footnote_1] ."
  },
  {
    "id": 1838,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://math-qa.github.io/math-QA/",
    "section_title": "References",
    "add_info": null,
    "text": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map prob-lems to operation programs. Due to an-notation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational an-notations over diverse problem types. We introduce a new representation language to model precise operation programs correspond-ing to each math problem that aim to im-prove both the performance and the inter-pretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational pro-grams. We additionally introduce a neu-ral sequence-to-program model enhanced with automatic problem categorization. Our exper-iments show improvements over competitive baselines in our MathQA as well as the AQuA datasets. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future re-search. Our dataset is available at:  https://math-qa.github.io/math-QA/."
  },
  {
    "id": 1839,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://math-qa.github.io/math-QA/",
    "section_title": "1 Introduction",
    "add_info": "1 The dataset is available at: https://math-qa.github.io/math-QA/",
    "text": "In this paper, we introduce a new operation-based representation language for solving math word problems. We use this representation lan-guage to construct MathQA [Cite_Footnote_1] , a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math do-main categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017). We introduce a neu-ral model for mapping problems to operation pro-grams with domain categorization."
  },
  {
    "id": 1840,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://ipsc.jrc.ec.europa.eu/index.php?id=198",
    "section_title": "3 Experiment",
    "add_info": "4 http://ipsc.jrc.ec.europa.eu/index.php?id=198",
    "text": "Table 1 shows a summary of our datasets. The EN\u2013ZH dataset is a translation memory from Symantec. Our EN\u2013FR dataset is from the pub-licly available JRC-Acquis corpus. [Cite_Footnote_4] Word align-ment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and."
  },
  {
    "id": 1841,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/decompositional-semantics-initiative/improved-ParaBank-rewriter",
    "section_title": "E Rule-Based Perturbations",
    "add_info": "6 https://github.com/decompositional-semantics-initiative/improved-ParaBank-rewriter",
    "text": "Here we\u2019re showing examples of perturbations generated via a recent machine paraphraser sys-tem. [Cite_Footnote_6]"
  },
  {
    "id": 1842,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://paracrawl.eu/",
    "section_title": "1 Introduction",
    "add_info": "1 https://paracrawl.eu/",
    "text": "NMT systems have been shown to be sensitive to noise in the training data (Khayrallah and Koehn, 2018), where noise is defined as segments that de-crease output quality of systems trained on the data. It is, therefore, important to be able to ac-curately align multilingual texts and precisely fil-ter out misalignments and bad translations that ad-versely affect performance. In the study, conducted on the impact of various types of noise on MT quality, untranslated and misaligned segments had the most detrimental effect. Misaligned segments were by far the most prevalent type of noise in the ParaCrawl [Cite_Footnote_1] parallel corpus they used, twice as common as accepted segments. However, misalign-ments vary; a segment can have one extraneous word, it can have twice the content its counterpart has, or anything in between. It can be very useful to understand the intricacies of the effects different types and levels of noise have, why it is important not to have noise and whether some kinds of noise are more acceptable than others. This leads us to our first research question:"
  },
  {
    "id": 1843,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://spacy.io/",
    "section_title": "3 Experimental Framework 3.3 Tools and Models",
    "add_info": "2 https://spacy.io/",
    "text": "In Section 4, we will discuss some of the methods we will be experimenting with. These include ap-plying a variety of available tools and models as well as developing our own. ABLTagger (Stein-gr\u00edmsson et al., 2019) will be used for PoS-tagging Icelandic texts. The tagger employs biLSTMs and an external morphological lexicon (Bjarnad\u00f3ttir et al., 2019). Lemmatising will be carried out using Nefnir (Ing\u00f3lfsd\u00f3ttir et al., 2019). For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy. [Cite_Footnote_2]"
  },
  {
    "id": 1844,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://acube.di.unipi.it/tmn-dataset/",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "5 http://acube.di.unipi.it/tmn-dataset/",
    "text": "TagMyNews. We use the news titles as in-stances from the benchmark classification dataset released by Vitale et al. (2012). [Cite_Footnote_5] This dataset con-tains English news from really simple syndication (RSS) feeds. Each news feed (with its title) is an-notated with one from seven labels, e.g., sci-tech. Twitter. This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track. Follow-ing previous settings (Yan et al., 2013; Li et al., 2016a), hashtags, i.e., user-annotated topic la-bels in each tweet such as \u201c#Trump\u201d and \u201c#Su-perBowl\u201d, serve as our ground-truth class labels."
  },
  {
    "id": 1845,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://trec.nist.gov/data/tweets",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "6 http://trec.nist.gov/data/tweets",
    "text": "TagMyNews. We use the news titles as in-stances from the benchmark classification dataset released by Vitale et al. (2012). This dataset con-tains English news from really simple syndication (RSS) feeds. Each news feed (with its title) is an-notated with one from seven labels, e.g., sci-tech. Twitter. This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track. [Cite_Footnote_6] Follow-ing previous settings (Yan et al., 2013; Li et al., 2016a), hashtags, i.e., user-annotated topic la-bels in each tweet such as \u201c#Trump\u201d and \u201c#Su-perBowl\u201d, serve as our ground-truth class labels."
  },
  {
    "id": 1846,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://radimrehurek.com/gensim/utils.html",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "8 https://radimrehurek.com/gensim/utils.html",
    "text": "Table 2 shows the statistic information of the four datasets. Each dataset is randomly split into 80% for training and 20% for test. 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer [Cite_Footnote_8] for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016). Differently, we use the raw word se-quence (without words removal) for the inputs of classification x Seq as is done in previous work of text classification (Kim, 2014; Liu et al., 2017)."
  },
  {
    "id": 1847,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/FudanNLP/fnlp",
    "section_title": "3 Experiment Setup 3.1 Datasets",
    "add_info": "9 https://github.com/FudanNLP/fnlp",
    "text": "Table 2 shows the statistic information of the four datasets. Each dataset is randomly split into 80% for training and 20% for test. 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [Cite_Footnote_9] for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016). Differently, we use the raw word se-quence (without words removal) for the inputs of classification x Seq as is done in previous work of text classification (Kim, 2014; Liu et al., 2017)."
  },
  {
    "id": 1848,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://nlp.stanford.edu/data/glove.6B.zip",
    "section_title": "3 Experiment Setup 3.2 Model Settings",
    "add_info": "11 http://nlp.stanford.edu/data/glove.6B.zip (200d)",
    "text": "We use pre-trained embeddings to initialize all word embeddings. For Snippets and TagMyNews datasets, we use pre-trained GloVe embed-dings (Pennington et al., 2014) [Cite_Footnote_11] . For Twitter and Weibo datasets, we pre-train embeddings on large-scale external data with 99M tweets and 467M Weibo messages, respectively. For the number of topics, we follow previous settings (Yan et al., 2013; Das et al., 2015; Dieng et al., 2016) to set K = 50. For all the other hyperparame-ters, we tune them on the development set by grid search. For our classifier, we employ CNN in experiment because of its better performance in short text classification than its counterparts such as RNN (Wang et al., 2017a). The hidden size of CNN is set as 500. The dimension of word em-bedding E = 200. \u03b3 = 0.8 for trading off \u03b8 and P , and \u03bb = 1.0 for controlling the effects of topic model and classification. In the learning process, we run our model for 800 epochs with early-stop strategy applied (Caruana et al., 2000)."
  },
  {
    "id": 1849,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/dice-group/Palmetto",
    "section_title": "4 Experimental Results 4.2 Topic Coherence Comparison",
    "add_info": "12 https://github.com/dice-group/Palmetto",
    "text": "In Section 4.1, we find that TMN can significantly outperform comparison models on short text clas-sification. In this section, we study whether jointly learning topic models and classification can be helpful in producing coherent and meaningful top-ics. We use the C V metric (Ro\u0308der et al., 2015) computed by Palmetto toolkit [Cite_Footnote_12] to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI (Bouma, 2009). Table 4 shows the compar-ison results of LDA, BTM, NTM, and TMN on the three English datasets. Note that we do not re-port C V scores for Chinese Weibo dataset as the Palmetto toolkit cannot process Chinese topics."
  },
  {
    "id": 1850,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.seggu.net/ccl/",
    "section_title": "3 Method 3.3.1 Constituency structure induction",
    "add_info": "2 http://www.seggu.net/ccl/",
    "text": "To induce constituency structures, we compare two different techniques: the pre-neural statistical com-mon cover link parser (CCL, Seginer, 2007) and the neural parser Deep Inside-Outside Recursive Auto-encoder (DIORA, Drozdov et al., 2019). CCL While proposed in 2007, CCL [Cite_Footnote_2] is still con-sidered a state-of-the-art unsupervised parser. Con-trary to other popular parsers from the 2000s (e.g. Klein and Manning, 2004, 2005; Ponvert et al., 2011; Reichart and Rappoport, 2010), it does not require POS-annotation of the words in the corpus, making it appropriate for our setup."
  },
  {
    "id": 1851,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/iesl/diora",
    "section_title": "3 Method 3.3.1 Constituency structure induction",
    "add_info": "3 https://github.com/iesl/diora",
    "text": "CCL is an incremental and greedy parser, that aims to incrementally add cover links to all words in a sentence. From these sets of cover links, con-stituency trees can be constructed. To limit the search space, CCL incorporates a few assumptions based on knowledge about natural language, such as the fact that constituency trees are generally skewed and the word distribution zipfian. In our experiments, we use the default settings for CCL. DIORA In addition to CCL, we also experiment with the more recent neural unsupervised parser DIORA [Cite_Footnote_3] . As the name suggests, DIORA is built on the application of recursive auto-encoders."
  },
  {
    "id": 1852,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://nlp.stanford.edu/software/GloVe-1.2.zip",
    "section_title": "3 Method 3.3.1 Constituency structure induction",
    "add_info": "4 https://nlp.stanford.edu/software/GloVe-1.2.zip",
    "text": "In our experiments with DIORA, we use a tree- LSTM with a hidden dimension of 50, and train for a maximum of 5 epochs with a batch size of 128. We use the GloVe framework [Cite_Footnote_4] (Pennington et al., 2014) to pretrain word-embeddings for our corpus; using an embedding size of 16."
  },
  {
    "id": 1853,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/pld/BMM_labels/",
    "section_title": "3 Method 3.3.2 Constituency labelling",
    "add_info": "5 https://github.com/pld/BMM_labels/",
    "text": "The BMM algorithm starts from a set of con-stituency trees in which each constituent is given its own unique label. It defines an iterative search procedure that merges labels to reduce the joint de-scription length of the data (DDL) and the grammar that can be inferred from the labelling (GDL). To find the next best merge step, the algorithm com-putes the effect of merging two labels on the sum of the GDL and DDL after doing the merge, where the GDL is defined as the number of bits to encode the grammar that can be inferred from the current labelled treebank with relative frequency estima-tion, and the DDL as the negative log-likelihood of the corpus given this grammar. To facilitate the search and avoid local minima, several heuris-tics and a look-ahead procedure are used to im-prove the performance of the algorithm. We use the BMM implementation provided by Borensztajn and Zuidema (2007) [Cite_Footnote_5] ."
  },
  {
    "id": 1854,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/i-machine-think/emergent_grammar_induction",
    "section_title": "6 Conclusion",
    "add_info": "9 https://github.com/i-machine-think/emergent_grammar_induction",
    "text": "We argue that while the extent to which syntax develops in different types of referential games is an interesting question in its own right, a better understanding of the syntactic structure of emer-gent languages could also provide pivotal in better understanding their semantics, especially if this is considered from a compositional point of view. To facilitate such analysis, we bundled our tests in a comprehensive and easily usable evaluation frame-work. [Cite_Footnote_9] We hope to have inspired other researchers to apply syntactic analysis techniques and encour-age them to use our code to evaluate new emergent languages trained in other scenarios."
  },
  {
    "id": 1855,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://gutenberg.org/",
    "section_title": "6 Experiments 6.1 Corpora",
    "add_info": "7 http://gutenberg.org/",
    "text": "We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kuc\u030cera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg [Cite_Footnote_7] as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). 10 The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora."
  },
  {
    "id": 1856,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/",
    "section_title": "6 Experiments 6.1 Corpora",
    "add_info": "8 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/",
    "text": "We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kuc\u030cera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank [Cite_Footnote_8] (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). 10 The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora."
  },
  {
    "id": 1857,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.ncbi.nlm.nih.gov/PubMed/",
    "section_title": "6 Experiments 6.1 Corpora",
    "add_info": "9 http://www.ncbi.nlm.nih.gov/PubMed/",
    "text": "We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kuc\u030cera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. [Cite_Footnote_9] We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). 10 The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora."
  },
  {
    "id": 1858,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://nclt.computing.dcu.ie/\u02dcjfoster/resources/",
    "section_title": "6 Experiments 6.1 Corpora",
    "add_info": "10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.",
    "text": "We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation 6 of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kuc\u030cera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg 7 as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank 8 (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. 9 We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [Cite_Footnote_10] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora."
  },
  {
    "id": 1859,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://wapiti.limsi.fr",
    "section_title": "3 Reranking Framework 3.1 Conditional Random Fields",
    "add_info": "1 available at http://wapiti.limsi.fr",
    "text": "Two particular effective implementations of CRFs have been recently proposed. One is described in (Hahn et al., 2009) and uses a margin based criterion for probabilities estimation. The other is described in (Lavergne et al., 2010) and has been implemented in the software wapiti [Cite_Footnote_1] . The latter solution in partic-ular trains the model using two different regulariza-tion factors at the same time:"
  },
  {
    "id": 1860,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://wapiti.limsi.fr",
    "section_title": "5 Experiments",
    "add_info": "2 available at http://wapiti.limsi.fr",
    "text": "For our CRF models, both Automatic Concept An-notation and Attribute Value Extraction SLU phases, we used wapiti 2 (Lavergne et al., 2010). The CRF model for the first SLU phase integrates a tradi-tional set of features like word prefixes and suffixes (of length up to 5), plus some Yes/No features like \u201cDoes the word start with capital letter ?\u201d, \u201cDoes the word contain non alphanumeric characters ?\u201d, \u201cIs the word preceded by non alphanumeric char-acteris ?\u201d etc. The CRF model for AVE integrates only words, prefixes and suffixes (length 3 and 4) concatenated with concepts. Since in this case la-bels are attribute values, which are a huge set with respect to concepts (7\u030300 VS 99), using a lot of fea-tures would make model training problematic. De-spite the reduced set of features, training error rate at both token and sentence level is under 1%. We didn\u2019t carry out optimization for parameters \u03c1 1 and \u03c1 [Cite_Footnote_2] of the elastic net (see section 3.1), default values lead in most cases to very accurate models."
  },
  {
    "id": 1861,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://disi.unitn.it/moschitti/Tree-Kernel.htm",
    "section_title": "5 Experiments",
    "add_info": "3 available at http://disi.unitn.it/moschitti/Tree-Kernel.htm",
    "text": "Reranking models based on SVM and PTK have been trained with \u201cSVM-Light-TK\u201d [Cite_Footnote_3] . Kernel param-eters M and SVM parameter C have been optimized on the development set, as well as thresholds for the WRR (see section 4.2)."
  },
  {
    "id": 1862,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/facebookresearch/fairseq-py",
    "section_title": "5 Experimental evaluation: improving WMT\u201914 En-De NMT systems 5.1 Baseline NMT systems",
    "add_info": "3 https://github.com/facebookresearch/fairseq-py",
    "text": "We have performed all our experiments with the freely available Sequence-to-Sequence Py-Torch toolkit from Facebook AI Research, [Cite_Footnote_3] called fairseq-py. It implements a convolutional model which achieves very competitive results (Gehring et al., 2017). We use this system to show the improvements obtained by filtering the stan-dard training data and by integrating additional mined data. We will freely share this data so that it can be used to train different NMT architectures."
  },
  {
    "id": 1863,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://fasttext.cc/docs/en/language-identification.html",
    "section_title": "5 Experimental evaluation: improving WMT\u201914 En-De NMT systems 5.2 Filtering Common Crawl",
    "add_info": "5 LID itself may also commit errors, we used https://fasttext.cc/docs/en/language-identification.html",
    "text": "After some initial experiments, it turned out that some additional steps are needed before cal-culating the distances (see Table 5): 1) remove sentences with more than 3 commas. Those are indeed often enumerations of names, cities, etc. While such sentences maybe useful to train NMT systems, the multilingual distance is not very reli-able to distinguish list of named entities; 2) limit to sentences with less than 50 words; 3) perform LID on source and target sentences; These steps discarded overall 19% of the data. It is surpris-ing that almost 6% of the data seems to have the wrong source or target language. [Cite_Footnote_5]"
  },
  {
    "id": 1864,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://paracrawl.eu/download.html",
    "section_title": "6 Conclusion",
    "add_info": "6 http://paracrawl.eu/download.html",
    "text": "There are many directions to extend this re-search, in particular to scale-up to larger corpora. We will apply it to the data mined by the European ParaCrawl project. [Cite_Footnote_6] The proposed multilingual sentence distance could be also used in MT con-fidence estimation, or to filter back-translations of monolingual data (Sennrich et al., 2016a)."
  },
  {
    "id": 1865,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://cogcomp.cs.illinois.edu/page/software_view/Wikifier",
    "section_title": "3 Wikipedia as Knowledge 3.2 Injecting Knowledge Attributes",
    "add_info": "7 Available at: http://cogcomp.cs.illinois.edu/page/software_view/Wikifier",
    "text": "Additionally, while (Rahman and Ng, 2011) uses the union of all possible meanings a mention may have in Wikipedia, we deploy GLOW (Ratinov et al., 2011) [Cite_Footnote_7] , a context-sensitive system for disam-biguation to Wikipedia. Using context-sensitive dis-ambiguation to Wikipedia as well as high-precision set of knowledge attributes allows us to inject the knowledge to more mention pairs when compared to (Rahman and Ng, 2011). Our exact heuristic for injecting knowledge attributes to mentions is as fol-lows:"
  },
  {
    "id": 1866,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/abisee/pointer-generator",
    "section_title": "4 Experimental Setup",
    "add_info": "2 We used the code available at https://github.com/abisee/pointer-generator.",
    "text": "For S EQ 2S EQ , P T G EN and P T G EN -C OVG , we used the best settings reported on the CNN and DailyMail data (See et al., 2017). [Cite_Footnote_2] All three mod-els had 256 dimensional hidden states and 128 di-mensional word embeddings. They were trained using Adagrad (Duchi et al., 2011) with learning rate 0.15 and an initial accumulator value of 0.1. We used gradient clipping with a maximum gradi-ent norm of 2, but did not use any form of regular-ization. We used the loss on the validation set to implement early stopping."
  },
  {
    "id": 1867,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/facebookresearch/fairseq-py",
    "section_title": "4 Experimental Setup",
    "add_info": "3 We used the code available at https://github.com/facebookresearch/fairseq-py.",
    "text": "For C ONV S2S [Cite_Footnote_3] and T-C ONV S2S, we used 512 dimensional hidden states and 512 dimensional word and position embeddings. We trained our convolutional models with Nesterov\u2019s accelerated gradient method (Sutskever et al., 2013) using a momentum value of 0.99 and renormalized gra-dients if their norm exceeded 0.1 (Pascanu et al., 2013). We used a learning rate of 0.10 and once the validation perplexity stopped improving, we reduced the learning rate by an order of magnitude after each epoch until it fell below 10 \u22124 . We also applied a dropout of 0.2 to the embeddings, the decoder outputs and the input of the convolutional blocks. Gradients were normalized by the number of non-padding tokens per mini-batch. We also used weight normalization for all layers except for lookup tables."
  },
  {
    "id": 1868,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/dr97531/SparseSCVB0",
    "section_title": "6 Experiments",
    "add_info": "1 Code implementing SparseSCVB0 can be found at https://github.com/dr97531/SparseSCVB0.",
    "text": "In this section we study the performance of our SparseSCVB0 [Cite_Footnote_1] algorithm, on small as well as large corpora to validate the proposed method for topic models such as LDA and MMSGTM, and to compare with other state-of-the-art algorithms."
  },
  {
    "id": 1869,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/huggingface/transformers",
    "section_title": "4 Experiments 4.1 Settings",
    "add_info": "1 https://github.com/huggingface/transformers",
    "text": "Network Training We implement our proposed method based on huggingface Transformers [Cite_Footnote_1] . Fol-lowing Wolf et al. (2019), we use a batch size of 32, and 3 training epochs to ensure convergence of op-timization. Following Wu and Dredze (2019), we freeze the parameters of the embedding layer and the bottom three layers of BERT BASE . For the op-timizers, we use AdamW (Loshchilov and Hutter, 2017) with learning rate of 5e \u2212 5 for teacher mod-els (Wolf et al., 2019), and 1e \u2212 4 for the student model (Yang et al., 2019) to converge faster. As for language similarity measuring (i.e., Eq. 10), we set \u03b3 = 0.01 following Pinheiro (2018). Besides, we use a low-rank approximation for the bilinear operator M, i.e., M = U T V where U, V \u2208 R d\u00d7m with d m, and we empirically set d = 64."
  },
  {
    "id": 1870,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://www.mpi-inf.mpg.de/",
    "section_title": "3 KG Completion 3.1 Rule Inference and Transfer",
    "add_info": "1 https://www.mpi-inf.mpg.de/",
    "text": "Since the acquirement of rule knowledge is not our focus in this paper, we utilize AMIE+ (Gala\u0301rraga et al., 2015), a modern rule mining system, to effi-ciently find Horn rules from large-scale KG, such as marriedTo(x, y) \u2227 liveIn(x, z) \u21d2 liveIn(y, z). Its source code is available online [Cite_Footnote_1] ."
  },
  {
    "id": 1871,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://wordnet.princeton.edu",
    "section_title": "5 Parameterization 5.2 Features",
    "add_info": "4 http://wordnet.princeton.edu",
    "text": "The first feature is an indicator invoked only at specific values. On the other hand, the rest of the features are invoked across multiple values, allow-ing general patterns to be learned. The second fea-ture is invoked if two heads are identical or a head is a substring of another. The third feature is in-voked if two heads are synonyms or derivations that are extracted from the WordNet [Cite_Footnote_4] . The fourth feature is invoked if the cosine similarity between word embeddings of two heads is larger than a threshold. The fifth feature is invoked when the heads are both prepositions to capture their differ-ent natures from the content words. The last two features are for categories; the sixth one is invoked at each category pair, while the seventh feature is invoked if the input categories are the same."
  },
  {
    "id": 1872,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "http://ilk.uvt.nl/conll/software.html",
    "section_title": "6 Evaluation 6.3 Evaluation Metric",
    "add_info": "6 http://ilk.uvt.nl/conll/software.html",
    "text": "Parsing Quality The parsing quality was evalu-ated using the CONLL-X (Buchholz and Marsi, 2006) standard. Dependencies were extracted from the output HPSG trees, and evaluated using the official script [Cite_Footnote_6] . Due to this conversion, the accuracy on the relation labels is less important. Thus, we reported only the unlabeled attachment score (UAS) . The development and test sets pro-vide 2, 371 and 6, 957 dependencies, respectively."
  },
  {
    "id": 1873,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://opus.nlpl.eu/OpenSubtitles-v2018.php",
    "section_title": "7 Ethical considerations",
    "add_info": "2 https://opus.nlpl.eu/OpenSubtitles-v2018.php",
    "text": "EDOS contains dialogues derived from the Open-Subtitles corpus (Lison et al., 2019), which is pub-licly available. [Cite_Footnote_2] It is part of the OPUS (Open Par-allel corpUS), which is based on open source prod-ucts and is delivered as an open content package. The workers annotating the dataset were compen-sated with $0.4 per HIT, which takes 4.12 minutes on average to complete (excluding the time taken by workers who took an unusually long time to complete the task) and a bonus of $0.1 if they com-pleted at least 3 out of 5 quiz questions correctly. Fair compensation was determined based on the US minimum wage of $7.12 per hour. Since the dataset is in English, the annotators recruited from AMT were restricted the majority native English-speaking countries: US; UK; Canada; Australia; and New Zealand. The fact that the dataset is English-only potentially perpetuates an English bias in NLP systems."
  },
  {
    "id": 1874,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://nlp.stanford.edu/data/glove.6B.zip",
    "section_title": "4 Experiments and Results 4.1 Obtaining Word-Pair Embedding",
    "add_info": "1 http://nlp.stanford.edu/data/glove.6B.zip",
    "text": "To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) \u2208 T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [Cite_Footnote_1] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 \u00b7 10 \u22127 , following Joshi et al. (2019)."
  },
  {
    "id": 1875,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://code.google.com/archive/p/word2vec/",
    "section_title": "4 Experiments and Results 4.2 Definition Embedding",
    "add_info": "4 https://code.google.com/archive/p/word2vec/",
    "text": "We implemented the CPAE (Section 2.1) as a baseline and compared it to CPAE with the word-pair embeddings (Section 3.1), which is our pro-posed method. The word embeddings in the def-inition encoder were initialized by the Google Word2Vec vectors [Cite_Footnote_4] . Google Word2Vec vectors and GloVe were used as the other baselines."
  },
  {
    "id": 1876,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://nlp.stanford.edu/projects/glove/",
    "section_title": "4 Evaluation 4.2 Baselines",
    "add_info": "1 https://nlp.stanford.edu/projects/glove/",
    "text": "VecOff. We used the 300-dimensional pre-trained GloVe (Pennington et al., 2014) [Cite_Footnote_1] and represented word pairs as described in Section 2.1."
  },
  {
    "id": 1877,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/sinantie/NeuralAmr",
    "section_title": "5 Experiments",
    "add_info": "1 We used the evaluation script available at https://github.com/sinantie/NeuralAmr.",
    "text": "We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [Cite_Footnote_1] We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017). Hyperparameters of each model were tuned on the development set of LDC2015E86. For the GCN components, we use two layers, ReLU activations, and tanh highway layers. We use single layer LSTMs. We train with SGD with the initial learning rate set to 1 and decay to 0.8. Batch size is set to 100."
  },
  {
    "id": 1878,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/mdtux89/OpenNMT-py-AMR-to-text",
    "section_title": "5 Experiments",
    "add_info": "2 Our code is available at https://github.com/mdtux89/OpenNMT-py-AMR-to-text.",
    "text": "We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017). Hyperparameters of each model were tuned on the development set of LDC2015E86. For the GCN components, we use two layers, ReLU activations, and tanh highway layers. We use single layer LSTMs. We train with SGD with the initial learning rate set to 1 and decay to 0.8. Batch size is set to 100. [Cite_Footnote_2]"
  },
  {
    "id": 1879,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/tensorflow/tensor2tensor",
    "section_title": "3 Experiments 3.1 Neural Machine Translation",
    "add_info": "1 https://github.com/tensorflow/tensor2tensor",
    "text": "For neural machine translation (NMT), we evaluate on both the subword-level and character-level tasks. Experimental Setup For subword-level NMT, we evaluate our models on seven NMT datasets us-ing the Tensor2Tensor [Cite_Footnote_1] framework (Vaswani et al., 2018), namely IWSLT\u201914 De\u2192En, IWSLT\u201914 Ro\u2192En, IWSLT\u201915 En\u2192Vi, IWSLT\u201917 En\u2192Id, WMT\u201917 En\u2192Et, SETIMES En\u2192Mk, and the well-established large-scale WMT\u201916 En\u2192De."
  },
  {
    "id": 1880,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/osmanio2/multi-domain-belief-tracking",
    "section_title": "5 MultiWOZ as a New Benchmark 5.1 Dialogue State Tracking",
    "add_info": "4 The model is publicly available at https://github.com/osmanio2/multi-domain-belief-tracking",
    "text": "A robust natural language understanding and dia-logue state tracking is the first step towards build-ing a good conversational system. Since multi-domain dialogue state tracking is still in its infancy and there are not many comparable approaches available (Rastogi et al., 2017), we instead report our state-of-the-art result on the restaurant subset of the MultiWOZ corpus as the reference baseline. The proposed method (Ramadan et al., 2018) ex-ploits the semantic similarity between dialogue ut-terances and the ontology terms which allows the information to be shared across domains. Further-more, the model parameters are independent of the ontology and belief states, therefore the number of the parameters does not increase with the size of the domain itself. [Cite_Footnote_4]"
  },
  {
    "id": 1881,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/budzianowski/multiwoz",
    "section_title": "5 MultiWOZ as a New Benchmark 5.2 Dialogue-Context-to-Text Generation",
    "add_info": "5 The model is publicly available at https://github.com/budzianowski/multiwoz",
    "text": "After a robust dialogue state tracking module is built, the next challenge becomes the dia-logue management and response generation com-ponents. These problems can either be addressed separately (Young et al., 2013), or jointly in an end-to-end fashion (Bordes et al., 2017; Wen et al., 2017; Li et al., 2017). In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [Cite_Footnote_5]"
  },
  {
    "id": 1882,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/andy194673/nlg-sclstm-multiwoz",
    "section_title": "5 MultiWOZ as a New Benchmark 5.3 Dialogue-Act-to-Text Generation",
    "add_info": "6 The model is publicly available at https://github.com/andy194673/nlg-sclstm-multiwoz",
    "text": "Table 4 : Performance comparison of two different model architectures using a corpus-based evaluation. logue turns. To give more statistics about the two datasets: the SFX corpus has 9 different act types with 12 slots comparing to 12 acts and 14 slots in our corpus. The best model for both datasets was found through a grid search over a set of hyper-parameters such as the size of embeddings, learn-ing rate, and number of LSTM layers. [Cite_Footnote_6]"
  },
  {
    "id": 1883,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/",
    "section_title": "References",
    "add_info": null,
    "text": "This work was funded by a Google Faculty Re-search Award (RG91111), an EPSRC studentship (RG80792), an EPSRC grant (EP/M018946/1) and by Toshiba Research Europe Ltd, Cam-bridge Research Laboratory (RG85875). The authors thank many excellent Mechanical Turk contributors for building this dataset. The au-thors would also like to thank Thang Minh Luong for his support for this project ad Nikola Mrks\u030cic\u0301 and anonymous reviewers for their constructive feedback. The data is avail-able at  http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/."
  },
  {
    "id": 1884,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "http://www.purl.com/net/ArabicSentiment",
    "section_title": "1 Introduction",
    "add_info": "1 http://www.purl.com/net/ArabicSentiment",
    "text": "In the process of developing these experiments to study how translation alters sentiment, we created a state-of-the-art Arabic sentiment analysis system by porting NRC-Canada\u2019s competition winning system (Kiritchenko et al., 2014) to Arabic. We also cre-ated a substantial amount of sentiment labeled data pertaining to Arabic social media texts and their En-glish translations which is made freely available. [Cite_Footnote_1] This is the first such resource where text in one language and its translations into another language (both manually and automatically produced) are each manually labeled for sentiment."
  },
  {
    "id": 1885,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://catalog.ldc.upenn.edu/LDC2012T09",
    "section_title": "3 Method for Determining Sentiment Predictability on Translation",
    "add_info": "2 https://catalog.ldc.upenn.edu/LDC2012T09",
    "text": "DATA: Since manual translation of text from Ara-bic to English is a costly exercise, we chose, for our experiments, an existing Arabic social media dataset that has already been translated \u2013 the BBN Arabic-Dialect/English Parallel Text (Zbib et al., 2012). [Cite_Footnote_2] It contains about 3.5 million tokens of Arabic dialect sentences and their English translations. We use a randomly chosen subset of 1200 Levantine dialectal sentences, which we will refer to as the BBN posts or BBN dataset, in our experiments. Additionally, we also conduct experiments on a dataset of 2000 tweets originating from Syria (a country where Levantine dialectal Arabic is commonly spoken). These tweets were collected in May 2014 by polling the Twitter API. We will refer to this dataset as the Syrian tweets or Syrian dataset. Note, however, that manual trans-lations of the Syrian dataset are not available."
  },
  {
    "id": 1886,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://www.purl.com/net/lexicons",
    "section_title": "6 English Sentiment Analysis",
    "add_info": "4 http://www.purl.com/net/lexicons",
    "text": "A linear-kernel Support Vector Machine (Chang and Lin, 2011) classifier is trained on the avail-able training data. The classifier leverages a vari-ety of surface-form, semantic, and sentiment lexi-con features described below. The sentiment lex-icon features are derived from existing, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Moham-mad and Turney, 2013), Bing Liu\u2019s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically gen-erated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014). [Cite_Footnote_4]"
  },
  {
    "id": 1887,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://pilehvar.github.io/wic/",
    "section_title": "References",
    "add_info": null,
    "text": "By design, word embeddings are unable to model the dynamic nature of words\u2019 seman-tics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized mean-ing representation techniques such as sense or contextualized embeddings have been pro-posed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contex-tual Word Similarity, and highlight its short-comings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on anno-tations curated by experts, for generic evalua-tion of context-sensitive representations. WiC is released in  https://pilehvar.github.io/wic/."
  },
  {
    "id": 1888,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://www.wiktionary.org/",
    "section_title": "2 WiC: the Word-in-Context dataset 2.1 Construction",
    "add_info": "3 https://www.wiktionary.org/",
    "text": "Contextual sentences in WiC were extracted from example usages provided for words in three lexi-cal resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) Verb-Net (Kipper-Schuler, 2005), the largest domain-independent verb-based resource; and (3) Wik-tionary [Cite_Footnote_3] , a large collaborative-constructed online dictionary. We used WordNet as our core re-source, exploiting BabelNet\u2019s mappings (Navigli and Ponzetto, 2012) as a bridge between Wik-tionary and VerbNet to WordNet. Lexicographer examples constitute a reliable base for the con-struction of the dataset, as they are curated in a way to be clearly distinguishable across different senses of a word."
  },
  {
    "id": 1889,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "https://github.com/orenmel/context2vec",
    "section_title": "3 Experiments",
    "add_info": "7 https://github.com/orenmel/context2vec",
    "text": "Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models [Cite_Footnote_7] . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, 12 layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). 9 Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings."
  },
  {
    "id": 1890,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://www.tensorflow.org/hub/modules/google/elmo/1",
    "section_title": "3 Experiments",
    "add_info": "8 https://www.tensorflow.org/hub/modules/google/elmo/1",
    "text": "Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models [Cite_Footnote_8] for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, 12 layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). 9 Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings."
  },
  {
    "id": 1891,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/google-research/bert/blob/master/",
    "section_title": "3 Experiments",
    "add_info": "9 https://github.com/google-research/bert/blob/master/",
    "text": "Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models 7 . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models 8 for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). [Cite_Footnote_9] Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings."
  },
  {
    "id": 1892,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "https://github.com/uhh-lt/sensegram",
    "section_title": "3 Experiments",
    "add_info": "11 https://github.com/uhh-lt/sensegram",
    "text": "Multi-prototype embeddings. We experiment with three recent techniques that release 300-d pre-trained multi-prototype embeddings . JBT [Cite_Footnote_11] (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embed-dings and computes embedding for each cluster (sense). DeConf 12 (Pilehvar and Collier, 2016) exploits the knowledge encoded in WordNet. For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelev-ina et al. (2016): for each example we retrieve the closest sense embedding to the context vec-tor, which is computed by averaging its contained words\u2019 embeddings."
  },
  {
    "id": 1893,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://pilehvar.github.io/deconf/",
    "section_title": "3 Experiments",
    "add_info": "12 https://pilehvar.github.io/deconf/",
    "text": "Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models 7 . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models 8 for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, [Cite_Footnote_12] layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings."
  },
  {
    "id": 1894,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Introduce",
    "url": "http://lcl.uniroma1.it/sw2v",
    "section_title": "3 Experiments",
    "add_info": "13 http://lcl.uniroma1.it/sw2v",
    "text": "Multi-prototype embeddings. We experiment with three recent techniques that release 300-d pre-trained multi-prototype embeddings . JBT (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embed-dings and computes embedding for each cluster (sense). DeConf 12 (Pilehvar and Collier, 2016) exploits the knowledge encoded in WordNet. For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V [Cite_Footnote_13] (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelev-ina et al. (2016): for each example we retrieve the closest sense embedding to the context vec-tor, which is computed by averaging its contained words\u2019 embeddings."
  },
  {
    "id": 1895,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://evalai.cloudcv.org/web/challenges/challenge-page/689",
    "section_title": "3 Tasks",
    "add_info": "5 available at https://evalai.cloudcv.org/web/challenges/challenge-page/689.",
    "text": "We consider five tasks that use Wikipedia as a knowledge source for KILT: fact checking, open domain question answering, slot filling, entity link-ing, and dialogue. The diversity of these tasks challenge models to represent knowledge flexibly. Some tasks require a discrete prediction (e.g., an entity), others, such as extractive question answer-ing, can copy the output directly from a Wikipedia page, while still other tasks must synthesize mul-tiple pieces of knowledge in an abstractive way to produce an output. KILT also provides a variety of ways to seek knowledge, from a claim to verify to a text chunk to annotate, from a structured or natural question to a conversation (see Table 1 for details). We are able to include the test set for all datasets in KILT, either because the test set is public, or because we were able to obtain the test set from the authors of the original dataset. These test sets are not publicly released, but are used for the KILT challenge on EvalAI (Yadav et al., 2019) where participants can upload their models\u2019 predictions and be listed on the public leaderboard. [Cite_Footnote_5]"
  },
  {
    "id": 1896,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://lemurproject.org/clueweb12",
    "section_title": "3 Tasks 3.2 Entity Linking",
    "add_info": "7 http://lemurproject.org/clueweb12",
    "text": "WNED-CWEB (Guo and Barbosa, 2018) is a dataset created with the same strategy as WNED-WIKI, but sampling from the ClueWeb 2012 cor-pora annotated with the FACC1 system. [Cite_Footnote_7] Similarly, we randomly split into dev and test."
  },
  {
    "id": 1897,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://yjernite.github.io/lfqa.html",
    "section_title": "3 Tasks 3.4 Open Domain Question Answering",
    "add_info": "8 https://yjernite.github.io/lfqa.html",
    "text": "ELI5 (Fan et al., 2019b) [Cite_Footnote_8] is a collection of question-answer-evidence triples where the ques-tions are complex, and the answers are long, ex-planatory, and free-form. For dev and test, we col-lect annotations using Amazon Mechanical Turk, asking evaluators to select which supporting docu-ments from Wikipedia can be used to answer the question. We treat these as gold provenance anno-tations for evaluation (details in section 4)."
  },
  {
    "id": 1898,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://www.wikidata.org",
    "section_title": "8 Discussion",
    "add_info": "13 https://www.wikidata.org",
    "text": "There are custom solutions that can easily simplify the slot filling task. For instance, subject entities can be used for lookups by title in Wikipedia to retrieve knowledge (this heuristic will always work for zsRE), and structured human-curated resources (such as Wikidata [Cite_Footnote_13] ) could be used to get all an-swers right. Nevertheless, we are interested in test-ing if a general model can extract attributes about specific entities from a large body of text."
  },
  {
    "id": 1899,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/easonnie/combine-FEVER-NSMN",
    "section_title": "7:453\u2013466.",
    "add_info": "22 available at https://github.com/easonnie/combine-FEVER-NSMN",
    "text": "For fact checking, we consider NSMN (Nie et al., 2019), the highest scoring system from the FEVER shared task (Thorne et al., 2018b). We use the public model [Cite_Footnote_22] pre-trained on FEVER, and con-sider not enough information predictions as false. Moreover, we develop a fact checking baseline that combines a BERT-base classifier with passages re-turned from DPR where the claim and retrieved passage are input. The classifier is trained to label the claim-passage pair as supported or refuted with an additional neutral class for negative-sampled unrelated passages. Unrelated passages are sam-pled from two sources: (1) DPR-retrieved passages from pages that are not in the list of pages in the instance\u2019s provenance and (2) passages sampled uniformly at random from pages in the instance\u2019s provenance. At inference, we classify the first sen-tence of the Wikipedia pages retrieved by the top- 100 DPR passages against the claim. Using pages labelled as supported or refuted, we label the claim through majority voting. For claim provenance, we re-rank passages by probability according to this label."
  },
  {
    "id": 1900,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://kaldi.sf.net",
    "section_title": "4 Experiments 4.1 Baseline Systems",
    "add_info": "2 http://kaldi.sf.net",
    "text": "First, we build an HMM-GMM system using the Kaldi open-source toolkit [Cite_Footnote_2] (Povey et al., 2011). The baseline recognizer has 8,986 sub-phone states and 200K Gaussians trained using maximum likelihood. Input features are speaker-adapted MFCCs. Overall, the baseline GMM system setup largely follows the existing s5b Kaldi recipe, and we defer to previous work for details (Vesely et al., 2013)."
  },
  {
    "id": 1901,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/Lucien-qiang/Rhetoric-Generator",
    "section_title": "4 Experiments 4.1 Datasets and Setups",
    "add_info": "1 https://github.com/Lucien-qiang/Rhetoric-Generator",
    "text": "We conduct all experiments on two datasets [Cite_Footnote_1] . One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training. To this end, we built a clas-sifier to predict the rhetoric label automatically. We sampled about 15,000 sentences from the orig-inal poetry dataset and annotated the data manu-ally with three categories, i.e., metaphor, personi-fication, and other. This dataset was divided into a training set, validation set, and test set. Three clas-sifiers, including LSTM, Bi-LSTM, and Bi-LSTM with a self-attention model, were trained on this dataset. The Bi-LSTM with self-attention classi-fier (Yang et al., 2016) outperforms the other mod-els and achieves the best accuracy of 0.83 on the test set. In this classifier, the sizes of word embed-ding, hidden state and the attention size are set to 128, 256, 30 respectively, and a two-layer LSTM is used. The results for different classes are given in Table 2."
  },
  {
    "id": 1902,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "http://www.shigeku.com/",
    "section_title": "4 Experiments 4.1 Datasets and Setups",
    "add_info": "2 http://www.shigeku.com/",
    "text": "We conduct all experiments on two datasets . One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website [Cite_Footnote_2] and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training. To this end, we built a clas-sifier to predict the rhetoric label automatically. We sampled about 15,000 sentences from the orig-inal poetry dataset and annotated the data manu-ally with three categories, i.e., metaphor, personi-fication, and other. This dataset was divided into a training set, validation set, and test set. Three clas-sifiers, including LSTM, Bi-LSTM, and Bi-LSTM with a self-attention model, were trained on this dataset. The Bi-LSTM with self-attention classi-fier (Yang et al., 2016) outperforms the other mod-els and achieves the best accuracy of 0.83 on the test set. In this classifier, the sizes of word embed-ding, hidden state and the attention size are set to 128, 256, 30 respectively, and a two-layer LSTM is used. The results for different classes are given in Table 2."
  },
  {
    "id": 1903,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://gitlab.freedesktop.org/poppler/poppler",
    "section_title": "2 Data",
    "add_info": "2 https://gitlab.freedesktop.org/poppler/poppler",
    "text": "Anthology statistics Figure 1 shows the distri-bution of the articles in the corpus: the number of articles published in these venues steadily in-creases from 2010\u20132019. The CL and TACL jour-nals publish articles at a steady rate; the ACL con-ference fluctuates in size, depending on whether it is co-located with NAACL; and the EACL confer-ence nearly doubles in size each time it takes place. In terms of whether the field is rapidly growing, we note that there was a year-on-year increase of 42% between in 2017\u20132018 due to the increase in the number of papers published at NAACL and EMNLP, and a 34% increase between 2018\u20132019. Extracting citations To extract a list of refer-ences from an article, we first extract the text stream from the PDF file via pdftotext, [Cite_Footnote_2] then feed it into ParsCit (Councill et al., 2008) to obtain the references. For each reference in this list, we then extract and keep the parsed \u201cdate\u201d, \u201cauthor\u201d, and \u201ctitle\u201d entries. For 1.4% of the input files, this pipeline fails to extract any references; spot-checking reveals that many of those are not regular papers (but, e.g., book reviews or front matter), some PDFs have no embedded text, and others silently fail to parse."
  },
  {
    "id": 1904,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/un33k/python-slugify",
    "section_title": "A Fuzzy paper matching",
    "add_info": "10 We achieve this by using https://github.com/un33k/python-slugify.",
    "text": "Figure [Cite_Footnote_10] shows the average number of citations per paper, analogous to Figure 7, but for a larger number of citation ages. citations of 40 Number citations of 40 Number"
  },
  {
    "id": 1905,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/seatgeek/fuzzywuzzy",
    "section_title": "A Fuzzy paper matching",
    "add_info": "12 As implemented by https://github.com/seatgeek/fuzzywuzzy.",
    "text": "Figure 7: Average number of citations per paper that are 15 years or older, by venue of publication. tance ratio [Cite_Footnote_12] is \u2264 95%."
  },
  {
    "id": 1906,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://qwone.com/\u02dcjason/20Newsgroups/",
    "section_title": "3 High Water Mark for Tandem Anchors 3.1 Experimental Setup",
    "add_info": "1 http://qwone.com/\u02dcjason/20Newsgroups/",
    "text": "We use the well-known 20 Newsgroups dataset (20 NEWS ) used in previous interactive topic mod-eling work: 18,846 Usenet postings from 20 dif-ferent newgroups in the early 1990s. [Cite_Footnote_1] We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes. We then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents."
  },
  {
    "id": 1907,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://hunch.net/\u02dcvw/",
    "section_title": "3 High Water Mark for Tandem Anchors 3.2 Experimental Results",
    "add_info": "4 http://hunch.net/\u02dcvw/",
    "text": "Our first evaluation is a classification task to pre-dict documents\u2019 newsgroup membership. Thus, we do not aim for state-of-the-art accuracy, but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than Gram-Schmidt anchors. After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tan-dem anchors and the Gram-Schmidt single word anchors. For multiword anchors, we use each of the combiner functions from Section 2.2. The anchor algorithm only gives the topic-word dis-tributions and not word-level topic assignments, so we infer token-level topic assignments using LDA Latent Dirichlet Allocation (Blei et al., 2003) with fixed topics discovered by the anchor method. We use our own implementation of Gibbs sam-pling with fixed topics and a symmetric document-topic Dirichlet prior with concentration \u03b1 = .01. Since the topics are fixed, this inference is very fast and can be parallelized on a per-document ba-sis. We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit [Cite_Footnote_4] with topic-word pairs as features. Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features. For both training and test, we exclude words outside the LDA vocabulary."
  },
  {
    "id": 1908,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download",
    "section_title": "6 Experiments 6.1 Development Tests",
    "add_info": "1 http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download.",
    "text": "In practice, it is often unnecessary to leave lexi-cal category disambiguation completely to the gram-maticality improvement system. When it is reason-able to assume that the input sentence for the gram-maticality improvement system is sufficiently fluent, a list of candidate lexical categories can be assigned automatically to each word via supertagging (Clark and Curran, 2007) on the input sequence. We use the C&C supertagger [Cite_Footnote_1] to assign a set of probable lexical categories to each input word using the gold-standard order. When the input is noisy, the accuracy of a supertagger tends to be lower than when the in-put is grammatical. One way to address this problem is to allow the supertagger to produce a larger list of possible supertags for each input word, and leave the ambiguity to the grammatical improvement sys-tem. We simulate the noisy input situation by using a small probability cutoff (\u03b2) value in the supertag-ger, and supertag correctly ordered input sentences before breaking them into bags of words. With a \u03b2 value of 0.0001, there are 5.4 lexical categories for each input word in the development test (which is smaller than the dictionary case)."
  },
  {
    "id": 1909,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.statmt.org/wmt17/",
    "section_title": "4 Experiments 4.3 Neural Language Generation",
    "add_info": "3 http://www.statmt.org/wmt17/",
    "text": "Following recent unconditional long text genera-tion work (Caccia et al., 2018), we perform exper-iments on EMNLP2017 WMT News dataset [Cite_Footnote_3] . All sentences are longer than 20, making the dataset appropriate for testing the exposure bias problem."
  },
  {
    "id": 1910,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://statmt.org/wmt16",
    "section_title": "References",
    "add_info": "5 http://statmt.org/wmt16",
    "text": "Dataset Two standard datasets are tested for NMT tasks. The first one is a small-scale English-Vietnamese corpus from the IWSLT 2015 Evalu-ation Campaign (Cettolo et al., 2015), which is a parallel corpus of TED-talks and contains 133K sentence pairs. We follow the pre-processing pro-cedure in (Luong and Manning, 2015) by replac-ing words with frequencies less than 5 with hunki. As a result, our vocabulary reduces to 17K for English and 7.7K for Vietnamese. We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [Cite_Footnote_5] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set. We conduct the sub-word tokenization on the corpus using the Byte Pair Encoding (BPE) method (Sen-nrich et al., 2015). Following Klein et al. (2017), we set the vocabulary size of both English and German to 32K."
  },
  {
    "id": 1911,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "https://github.com/tensorflow/nmt",
    "section_title": "References",
    "add_info": "6 https://github.com/tensorflow/nmt",
    "text": "Setup We use Google\u2019s Neural Machine Trans-lation (GNMT) system (Wu et al., 2016) as our baseline MLE model, which follows the standard architecture and hyper-parameters [Cite_Footnote_6] for fair com-parison. All other models are built on top of with same network structure. We evaluate the model performance using BLEU scores (Papineni et al., 2002). We set OT weighting parameter \u03bb = 0.1 and order-preserving penalty weighting parameter \u03b2 = 0.1."
  },
  {
    "id": 1912,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "http://www.rulequest.com/see5-info.html",
    "section_title": "6 Comparison with other Models",
    "add_info": null,
    "text": "To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0,  http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004. LIB-SVM: a Library for Support Vector Machines, http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features."
  },
  {
    "id": 1913,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "http://jbnc.sourceforge.net",
    "section_title": "6 Comparison with other Models",
    "add_info": null,
    "text": "To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox,  http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004. LIB-SVM: a Library for Support Vector Machines, http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features."
  },
  {
    "id": 1914,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Compare",
    "url": "http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf",
    "section_title": "6 Comparison with other Models",
    "add_info": null,
    "text": "To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004. LIB-SVM: a Library for Support Vector Machines,  http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features."
  },
  {
    "id": 1915,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Introduce",
    "url": "http://www.ukp.tu-darmstadt.de/fnwkde/",
    "section_title": "10 Conclusion",
    "add_info": null,
    "text": "We make both resources publicly available in the standardized format UBY-LMF (Eckle-Kohler et al., 2012), which supports automatic processing of the resources via the UBY Java API, see  http://www.ukp.tu-darmstadt.de/fnwkde/."
  },
  {
    "id": 1916,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.tartarus.org/\u02dcmartin/PorterStemmer",
    "section_title": "4 Generation of Domain Model 4.2 Taxonomy Generation",
    "add_info": "1 http://www.tartarus.org/\u02dcmartin/PorterStemmer",
    "text": "The Clusterer generates individual levels of the taxonomy by using text clustering. We used CLUTO package for doing text clustering. We experimented with all the available clustering functions in CLUTO but no one clustering al-gorithm consistently outperformed others. Also, there was not much difference between various algorithms based on the available goodness met-rics. Hence, we used the default repeated bisec-tion technique with cosine function as the similar-ity metric. We ran this algorithm on a collection of 2000 transcriptions multiple times. First we generate 5 clusters from the 2000 transcriptions. Next we generate 10 clusters from the same set of transcriptions and so on. At the finest level we split them into 100 clusters. To generate the topic taxonomy, these sets containing 5 to 100 clusters are passed through the Taxonomy Builder compo-nent. This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v [Cite_Footnote_1] . Now v 1 and v 2 become nodes in adjacent layers in the taxonomy. Here we found the taxonomy to be a tree but in general it can be a DAG. Now onwards, each node in the taxonomy will be referred to as a topic."
  },
  {
    "id": 1917,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://glaros.dtc.umn.edu/gkhome/views/cluto",
    "section_title": "4 Generation of Domain Model 4.2 Taxonomy Generation",
    "add_info": "2 http://glaros.dtc.umn.edu/gkhome/views/cluto",
    "text": "The Clusterer generates individual levels of the taxonomy by using text clustering. We used CLUTO package [Cite_Footnote_2] for doing text clustering. We experimented with all the available clustering functions in CLUTO but no one clustering al-gorithm consistently outperformed others. Also, there was not much difference between various algorithms based on the available goodness met-rics. Hence, we used the default repeated bisec-tion technique with cosine function as the similar-ity metric. We ran this algorithm on a collection of 2000 transcriptions multiple times. First we generate 5 clusters from the 2000 transcriptions. Next we generate 10 clusters from the same set of transcriptions and so on. At the finest level we split them into 100 clusters. To generate the topic taxonomy, these sets containing 5 to 100 clusters are passed through the Taxonomy Builder compo-nent. This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v . Now v 1 and v 2 become nodes in adjacent layers in the taxonomy. Here we found the taxonomy to be a tree but in general it can be a DAG. Now onwards, each node in the taxonomy will be referred to as a topic."
  },
  {
    "id": 1918,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://rajpurkar.github.io/SQuAD-explorer/",
    "section_title": "1 Introduction",
    "add_info": "1 https://rajpurkar.github.io/SQuAD-explorer/",
    "text": "High-performance deep neural language models such as BERT (Devlin et al., 2018), XLNet (Z. Yang et al., 2019), and GPT-2 (Radford et al., 2019) have brought breakthroughs to a wide range of Natural Language Processing (NLP) tasks including text classification, sentiment analysis, textual entailment, natural language inference, machine translation, and question answering. Their immense ability in capturing various linguistic properties has led these state-of-the-art language models to master different NLP tasks, even surpassing human accuracy on some benchmarks such as SQuAD [Cite_Footnote_1] ."
  },
  {
    "id": 1919,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003",
    "section_title": "2 NLP tasks",
    "add_info": "2 https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003",
    "text": "CoNLL-2003 [Cite_Footnote_2] is a Named Entity Recognition (NER) dataset containing news stories from the Reuters corpus with more than 200K tokens annotated as Person, Organization, Location, Miscellaneous, or Other."
  },
  {
    "id": 1920,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/mmoradi-iut/NLP-perturbation",
    "section_title": "4 Perturbation methods",
    "add_info": null,
    "text": "We designed and implemented various character-level and word-level perturbation methods that simulate different types of noise an NLP system may encounter in real-world situations. The perturbations can be produced for every dataset regardless of the underlying language model or NLP system being tested. Table 2 presents an example for every perturbation method. The perturbation methods were implemented in Python using the NLTK library. The source code is available at  https://github.com/mmoradi-iut/NLP-perturbation."
  },
  {
    "id": 1921,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://github.com/deepset-ai/FARM",
    "section_title": "3 Language models",
    "add_info": "3 https://github.com/deepset-ai/FARM",
    "text": "We retrieved the pretrained models, fine-tuned them separately on each downstream task using the training and development sets, and tested them on the test sets. We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [Cite_Footnote_3] libraries to implement the transformer-based models. A complete list of hyperparameter values is presented in Appendix A."
  },
  {
    "id": 1922,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://mat-annotation.sourceforge.net/",
    "section_title": "2 Related Work",
    "add_info": "1 http://mat-annotation.sourceforge.net/",
    "text": "Neves and \u0160eva (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT [Cite_Footnote_1] (Bayer, 2015) is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources."
  },
  {
    "id": 1923,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://www.tagtog.net/",
    "section_title": "2 Related Work",
    "add_info": "2 https://www.tagtog.net/ 3 https://prodi.gy/",
    "text": "Neves and \u0160eva (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog [Cite_Footnote_2] (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources."
  },
  {
    "id": 1924,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://prodi.gy/",
    "section_title": "2 Related Work",
    "add_info": "2 https://www.tagtog.net/ 3 https://prodi.gy/",
    "text": "Neves and \u0160eva (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog [Cite_Footnote_2] (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources."
  },
  {
    "id": 1925,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://www.lighttag.io/",
    "section_title": "2 Related Work",
    "add_info": "4 https://www.lighttag.io/ Demonstrations, pages 99\u2013105",
    "text": "Neves and \u0160eva (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag [Cite_Footnote_4] are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources."
  },
  {
    "id": 1926,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/doccano/doccano",
    "section_title": "2 Related Work",
    "add_info": "5 https://github.com/doccano/doccano",
    "text": "Another annotation tool with limited sup-port for document-level annotations is Doccano [Cite_Footnote_5] (Nakayama et al., 2018), though it is not mentioned in the evaluation of Neves and \u0160eva. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks."
  },
  {
    "id": 1927,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://mat-annotation.sourceforge.net/",
    "section_title": "2 Related Work",
    "add_info": "Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.",
    "text": "Neves and \u0160eva (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015)  is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources."
  },
  {
    "id": 1928,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "http://mat-annotation.sourceforge.net/",
    "section_title": "4 Annotation Generator 4.2 Active Learning Process",
    "add_info": "Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.",
    "text": "This process can then be repeated until the ma-chine learning model is performing well enough to be partly or fully automated. This is similar to the \u201ctag-a-little, learn-a-little loop\u201d from MAT (Bayer, 2015)  . If combined with enabling pre-annotations, it is also very similar to the annotation process of RapTAT (Gobbel et al., 2014). To partly automate the process, the project has to be configured to treat the generator as an annotator and to require one annotator per document. Additionally, the configu-ration option of the finalize condition has to be set to some confidence threshold, for example, 80%. Then, only the documents with a confidence value below 80% will be required to be annotated further. To fully automate the process, the finalize condition should be set to always to accept the annotations automatically without additional conditions."
  },
  {
    "id": 1929,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Compare",
    "url": "https://github.com/doccano/doccano",
    "section_title": "2 Related Work",
    "add_info": "Hiroki Nakayama, Takahiro Kubo, Junya Kamura, Ya-sufumi Taniguchi, and Xu Liang. 2018. doccano: Text annotation tool for human. Software available from https://github.com/doccano/doccano.",
    "text": "Another annotation tool with limited sup-port for document-level annotations is Doccano (Nakayama et al., 2018)  , though it is not mentioned in the evaluation of Neves and \u0160eva. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks."
  },
  {
    "id": 1930,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://ls6-www.informatik.uni-dortmund.de/ir/projects/freeWAIS-sf/",
    "section_title": "4 Virtual Examples for Text Classification We assume on text classification the following:",
    "add_info": "2 Available at http://ls6-www.informatik.uni-dortmund.de/ir/projects/freeWAIS-sf/",
    "text": "Before describing our methods, we describe text representation which we used in this study. We to-kenize a document to words, downcase them and then remove stopwords, where the stopword list of freeWAIS-sf [Cite_Footnote_2] is used. Stemming is not performed. We adopt binary feature vectors where word fre-quency is not used."
  },
  {
    "id": 1931,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.daviddlewis.com/resources/testcollections/reuters21578/",
    "section_title": "5 Experimental Results and Discussion 5.1 Test Set Collection",
    "add_info": "3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/",
    "text": "We used the Reuters-21578 dataset [Cite_Footnote_3] to evaluate the proposed methods. The dataset has several splits of a training set and a test set. We used here \u201cModApte\u201d split, which is most widely used in the literature on text classification. This split has 9,603 training ex-amples and 3,299 test examples. More than 100 cat-egories are in the dataset. We use, however, only the most frequent 10 categories. Table 2 shows the 10 categories and the number of training and test exam-ples in each of the categories."
  },
  {
    "id": 1932,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://mastarpj.nict.go.jp/IWSLT2008/downloads/",
    "section_title": "3 Hidden Event Language Model",
    "add_info": "1 http://mastarpj.nict.go.jp/IWSLT2008/downloads/ case+punc tool using SRILM.instructions.txt",
    "text": "Thus, in practice, special techniques are usually required on top of using a hidden event language model in order to overcome long range dependen-cies. Examples include relocating or duplicating punctuation symbols to different positions of a sen-tence such that they appear closer to the indicative words (e.g., \u201chow much\u201d indicates a question sen-tence). One such technique was introduced by the organizers of the IWSLT evaluation campaign, who suggested duplicating the ending punctuation sym-bol to the beginning of each sentence before training the language model [Cite_Footnote_1] . Empirically, the technique has demonstrated its effectiveness in predicting question marks in English, since most of the indicative words for English question sentences appear at the begin-ning of a question. However, such a technique is specially designed and may not be widely applica-ble in general or to languages other than English. Furthermore, a direct application of such a method may fail in the event of multiple sentences per utter-ance without clearly annotated sentence boundaries within an utterance."
  },
  {
    "id": 1933,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.cis.upenn.edu/\u223ctreebank/tokenization.html",
    "section_title": "6 Experiments",
    "add_info": "2 http://www.cis.upenn.edu/\u223ctreebank/tokenization.html",
    "text": "We perform experiments on part of the corpus of the IWSLT09 evaluation campaign (Paul, 2009), where both Chinese and English conversational speech texts are used. Two multilingual datasets are consid-ered, the BTEC (Basic Travel Expression Corpus) dataset and the CT (Challenge Task) dataset. The former consists of tourism-related sentences, and the latter consists of human-mediated cross-lingual di-alogs in travel domain. The official IWSLT09 BTEC training set consists of 19,972 Chinese-English ut-terance pairs, and the CT training set consists of 10,061 such pairs. We randomly split each of the two datasets into two portions, where 90% of the ut-terances are used for training the punctuation predic-tion models, and the remaining 10% for evaluating the prediction performance. For all the experiments, we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with the Penn Treebank tokenizer [Cite_Footnote_2] . We list the statistics of the two datasets after processing in Table 3. The proportions of sentence types in the two datasets are listed. The majority of the sentences are declarative sentences. However, question sentences are more frequent in the BTEC dataset compared to the CT dataset. Exclamatory sentences contribute less than 1% for all datasets and are not listed. We also count how often each utterance consists of multiple sen-tences. The utterances from the CT dataset are much longer (with more words per utterance), and there-fore more CT utterances actually consist of multiple sentences."
  },
  {
    "id": 1934,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://code.google.com/p/berkeleyaligner/",
    "section_title": "6 Experiments 6.3 Performance in Translation",
    "add_info": "3 http://code.google.com/p/berkeleyaligner/",
    "text": "In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for train-ing the translation system. The state-of-the-art un-supervised Berkeley aligner [Cite_Footnote_3] (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexi-calized reordering model enabled. This is because lexicalized reordering gives better performance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reorder-ing model (msd-bidirectional-fe) is used."
  },
  {
    "id": 1935,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://mallet.cs.umass.edu/grmm/",
    "section_title": "5 Factorial Conditional Random Fields",
    "add_info": "C. Sutton. 2006. GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/.",
    "text": "In this work, we use the G RMM package (Sutton, 2006)  for building both the linear-chain CRF (L-C RF ) and factorial CRF (F-C RF ). The tree-based reparameterization (TRP) schedule for belief propa-gation (Wainwright et al., 2001) is used for approxi-mate inference."
  },
  {
    "id": 1936,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://bit.ly/30sGEbi",
    "section_title": "2 Methods",
    "add_info": "2 nvidia-smi: https://bit.ly/30sGEbi",
    "text": "We measure energy use as follows. We train the models described in \u00a72.1 using the default settings provided, and sample GPU and CPU power con-sumption during training. Each model was trained for a maximum of 1 day. We train all models on a single NVIDIA Titan X GPU, with the excep-tion of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeat-edly query the NVIDIA System Management In-terface [Cite_Footnote_2] to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel\u2019s Running Average Power Limit interface."
  },
  {
    "id": 1937,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://bit.ly/2LObQhV",
    "section_title": "2 Methods",
    "add_info": "3 RAPL power meter: https://bit.ly/2LObQhV",
    "text": "We measure energy use as follows. We train the models described in \u00a72.1 using the default settings provided, and sample GPU and CPU power con-sumption during training. Each model was trained for a maximum of 1 day. We train all models on a single NVIDIA Titan X GPU, with the excep-tion of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeat-edly query the NVIDIA System Management In-terface to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel\u2019s Running Average Power Limit interface. [Cite_Footnote_3]"
  },
  {
    "id": 1938,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "http://www.cs.toronto.edu/~aditya/g2p-tl-rr",
    "section_title": "4 Experiments",
    "add_info": "1 http://www.cs.toronto.edu/~aditya/g2p-tl-rr",
    "text": "Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [Cite_Footnote_1] ."
  },
  {
    "id": 1939,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "https://github.com/VinAIResearch/BERTweet",
    "section_title": "References",
    "add_info": null,
    "text": "We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same ar-chitecture as BERT base (Devlin et al., 2019), is trained using the RoBERTa pre-training pro-cedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa base and XLM-R base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tag-ging, Named-entity recognition and text clas-sification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at:  https://github.com/VinAIResearch/BERTweet."
  },
  {
    "id": 1940,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "https://archive.org/details/twitterstream",
    "section_title": "2 BERTweet Pre-training data",
    "add_info": "1 https://archive.org/details/twitterstream",
    "text": "\u2022 We first download the general Twitter Stream grabbed by the Archive Team, [Cite_Footnote_1] containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter. To identify English Tweets, we employ the language identification compo-nent of fastText (Joulin et al., 2017). We to-kenize those English Tweets using \u201cTweetTo-kenizer\u201d from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). We also normal-ize the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. We filter out retweeted Tweets and the ones shorter than 10 or longer than 64 word tokens. This pre-process results in the first corpus of 845M English Tweets."
  },
  {
    "id": 1941,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://pypi.org/project/emoji",
    "section_title": "2 BERTweet Pre-training data",
    "add_info": "2 https://pypi.org/project/emoji",
    "text": "\u2022 We first download the general Twitter Stream grabbed by the Archive Team, containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter. To identify English Tweets, we employ the language identification compo-nent of fastText (Joulin et al., 2017). We to-kenize those English Tweets using \u201cTweetTo-kenizer\u201d from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). [Cite_Footnote_2] We also normal-ize the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. We filter out retweeted Tweets and the ones shorter than 10 or longer than 64 word tokens. This pre-process results in the first corpus of 845M English Tweets."
  },
  {
    "id": 1942,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz",
    "section_title": "3 Experimental setup Downstream task datasets",
    "add_info": "4 https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz)",
    "text": "For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter [Cite_Footnote_4] (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018)."
  },
  {
    "id": 1943,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/Oneplus/Tweebank",
    "section_title": "3 Experimental setup Downstream task datasets",
    "add_info": "5 https://github.com/Oneplus/Tweebank",
    "text": "For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 [Cite_Footnote_5] (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018)."
  },
  {
    "id": 1944,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "https://github.com/guitaowufeng/TPANN",
    "section_title": "3 Experimental setup Downstream task datasets",
    "add_info": "6 https://github.com/guitaowufeng/TPANN",
    "text": "For Ritter11-T-POS, we employ a 70/15/15 training/validation/test pre-split available from Gui et al. (2017). [Cite_Footnote_6] ARK-Twitter contains two files daily547.conll and oct27.conll in which oct27.conll is further split into files oct27.traindev and oct27.test. Fol-lowing Owoputi et al. (2013) and Gui et al. (2017), we employ daily547.conll as a test set. In addition, we use oct27.traindev and oct27.test as training and validation sets, re-spectively. For the T WEEBANK - V 2, WNUT16 and WNUT17 datasets, we use their existing training/validation/test split. The SemEval2017- Task4A and SemEval2018-Task3A datasets are provided with training and test sets only (i.e. there is not a standard split for validation), thus we sam-ple 10% of the training set for validation and use the remaining 90% for training."
  },
  {
    "id": 1945,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Use",
    "url": "http://luululu.com/tweet/",
    "section_title": "3 Experimental setup Downstream task datasets",
    "add_info": "Eiji Aramaki. 2010. TYPO CORPUS. http://luululu.com/tweet/.",
    "text": "We use a \u201csoft\u201d normalization strategy to all of the experimental datasets by translating word to-kens of user mentions and web/url links into spe-cial tokens @USER and HTTPURL, respectively, and converting emotion icon tokens into corre-sponding strings. We also apply a \u201chard\u201d strategy by further applying lexical normalization dictio-naries (Aramaki, 2010  ; Liu et al., 2012; Han et al., 2012) to normalize word tokens in Tweets."
  },
  {
    "id": 1946,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/vzhong/e3",
    "section_title": "References",
    "add_info": null,
    "text": "Conversational machine reading systems help users answer high-level questions (e.g. deter-mine if they qualify for particular govern-ment benefits) when they do not know the ex-act rules by which the determination is made (e.g. whether they need certain income levels or veteran status). The key challenge is that these rules are only provided in the form of a procedural text (e.g. guidelines from govern-ment website) which the system must read to figure out what to ask the user. We present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational his-tory and which still need to be edited to create questions for the user. On the recently intro-duced ShARC conversational machine read-ing dataset, our Entailment-driven Extract and Edit network (E 3 ) achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In addition, by explicitly highlighting which information still needs to be gathered, E 3 provides a more ex-plainable alternative to prior work. We release source code for our models and experiments at  https://github.com/vzhong/e3."
  },
  {
    "id": 1947,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/vzhong/e3",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "We compare E 3 to the previous-best systems as well as a new, strong, BERT-based extrac-tive question answering model (BERTQA) on the recently proposed ShARC CMR dataset (Saeidi et al., 2018). Our results show that E 3 is more accurate in its decisions and generates more rele-vant inquiries. In particular, E 3 outperforms the previous-best model by 5.7% in micro-averaged decision accuracy and 4.3 in inquiry BLEU4. Similarly, E 3 outperforms the BERTQA base-line by 4.0% micro-averaged decision accuracy and 2.4 in inquiry BLEU4. In addition to out-performing previous methods, E 3 is explainable in the sense that one can visualize what rules the model extracted and how previous interactions and inquiries ground to the extracted rules. We re-lease source code for E 3 and the BERTQA model at  https://github.com/vzhong/e3."
  },
  {
    "id": 1948,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/jekbradbury/revtok",
    "section_title": "4 Experiment 4.1 Experimental setup",
    "add_info": "1 https://github.com/jekbradbury/revtok",
    "text": "We tokenize using revtok [Cite_Footnote_1] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al. (2019) (e.g. bert-base-uncased). We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4."
  },
  {
    "id": 1949,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/huggingface/pytorch-pretrained-BERT",
    "section_title": "4 Experiment 4.1 Experimental setup",
    "add_info": "2 We use the BERT implementation from https://github.com/huggingface/pytorch-pretrained-BERT",
    "text": "We tokenize using revtok and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al. (2019) (e.g. bert-base-uncased). [Cite_Footnote_2] We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4."
  },
  {
    "id": 1950,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://www.research.att.com/sw/tools/fsm/",
    "section_title": "5 Speech Translation Models 5.1 Acoustic and Language Model",
    "add_info": "M. Mohri, F. Pereira, and M. Riley. 1997. At&t general-purpose finite-state machine software tools, http://www.research.att.com/sw/tools/fsm/.",
    "text": "The English language model was built using the permissible data in the IWSLT 2011 evaluation. The texts were normalized using a variety of cleanup, number and spelling normalization techniques and filtered by restricting the vocabulary to the top 375000 types; i.e., any sentence containing a to-ken outside the vocabulary was discarded. First, we removed extraneous characters beyond the ASCII range followed by removal of punctuations. Sub-sequently, we normalized hyphenated words and re-moved words with more than 25 characters. The re-sultant text was normalized using a variety of num-ber conversion routines and each corpus was fil-tered by restricting the vocabulary to the top 150000 types; i.e., any sentence containing a token outside the vocabulary was discarded. The vocabulary from all the corpora was then consolidated and another round of filtering to the top 375000 most frequent types was performed. The OOV rate on the TED dev2010 set is 1.1%. We used the AT&T FSM toolkit (Mohri et al., 1997)  to train a trigram lan-guage model (LM) for each component (corpus). Fi-nally, the component language models were interpo-lated by minimizing the perplexity on the dev2010 set. The results are shown in Table 2."
  },
  {
    "id": 1951,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://github.com/tylin/coco-caption",
    "section_title": "4 Experiments 4.4 Quantitative Results",
    "add_info": "4 https://github.com/tylin/coco-caption",
    "text": "We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image cap-tioning evaluation tool [Cite_Footnote_4] which provides evalua-tion on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedan-tam et al., 2015)."
  },
  {
    "id": 1952,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Use",
    "url": "https://sjmielke.com/papers/syncretism",
    "section_title": "6 Conclusion",
    "add_info": null,
    "text": "We have presented a novel generative latent-variable model for resolving ambiguity in unigram counts, notably due to syncretism. Given a lexicon, an unsupervised model partitions the corpus count for each ambiguous form among its analyses listed in a lexicon. We empirically evaluated our method on 5 languages under two evaluation metrics. The code is availabile at  https://sjmielke.com/papers/syncretism, along with type-disambiguated unigram counts for all lexicons pro-vided by the UniMorph project (100+ languages)."
  },
  {
    "id": 1953,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Extend",
    "url": "https://github.com/duyvuleo/Transformer-DyNet",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "7 https://github.com/duyvuleo/Transformer-DyNet",
    "text": "All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based NMT implementation in mantis (Cohn et al., 2016). The encoder is a sin-gle layer bidirectional GRU (Cho et al., 2014) and the decoder is a 2-layer GRU with embeddings and hidden dimensions set to 512. The dropout rate for the output layer is set to 0.2. For the Transformer, we use Transformer-DyNet [Cite_Footnote_7] implementation and extend it for our context-aware NMT model. The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1. We also employ all four types of dropouts as in the original Transformer with a rate of 0.1 for the sentence-based model and 0.2 for our context-aware model."
  },
  {
    "id": 1954,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/sameenmaruf/selective-attn",
    "section_title": "4 Experiments 4.1 Setup",
    "add_info": "8 The code is available at https://github.com/sameenmaruf/selective-attn",
    "text": "All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based NMT implementation in mantis (Cohn et al., 2016). The encoder is a sin-gle layer bidirectional GRU (Cho et al., 2014) and the decoder is a 2-layer GRU with embeddings and hidden dimensions set to 512. The dropout rate for the output layer is set to 0.2. For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [Cite_Footnote_8] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1. We also employ all four types of dropouts as in the original Transformer with a rate of 0.1 for the sentence-based model and 0.2 for our context-aware model."
  },
  {
    "id": 1955,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://www.lexalytics.com/webdemo",
    "section_title": "2 Analyzing Sentiment in Texts",
    "add_info": "1 http://www.lexalytics.com/webdemo",
    "text": "In the internet, we can find many systems and companies related with sentiment analysis. For example, the company Lexalytics has in its website an available demo [Cite_Footnote_1] for sentiment detection. This demo shows an interface which highlights positive and negative words in the text. The interface also shows entities, categories associated, a summary and the top terms."
  },
  {
    "id": 1956,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Use",
    "url": "http://www.epinions.com",
    "section_title": "3 The System and its Interface",
    "add_info": null,
    "text": "The tool accepts as input pure text files or xml files. The xml files follow a specific format which allows the system to retrieve metadata information. It is also possible to retrieve web pages from the web. The tool offers the possibility to retrieve a single webpage, given the URL, or a collection of pages by crawling. To crawl, for example, reviews webpages, the user need to setup some crawling and information extraction rules defined by a template in the configuration file. The files retrieved from the web are converted in xml format, which allows preserving the metadata information. As an example, Figure 1 shows the organization of this xml file from a review retrieved from the website epinions.com (  http://www.epinions.com)."
  },
  {
    "id": 1957,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "https://pytorch.org/",
    "section_title": "3 Methodology 4.1 Experimental Setup",
    "add_info": "1 https://pytorch.org/",
    "text": "Hardware Details We trained all models using a single NVIDIA V100 GPU. The batch size was set to 64. We used mixed-precision training (Micike-vicius et al., 2018) to expedite the training proce-dure. All experiments were run using the PyTorch [Cite_Footnote_1] framework."
  },
  {
    "id": 1958,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Other",
    "url": "https://www.mindspore.cn/",
    "section_title": "Acknowledgments",
    "add_info": "3 https://www.mindspore.cn/",
    "text": "We thank Mindspore [Cite_Footnote_3] which is a new deep learning computing framework for the partial support of this work."
  },
  {
    "id": 1959,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Produce",
    "url": "https://github.com/Jess1ca/CoordinationExtPTB",
    "section_title": "References",
    "add_info": "1 The data is available in: https://github.com/Jess1ca/CoordinationExtPTB",
    "text": "Coordination is an important and common syntactic construction which is not han-dled well by state of the art parsers. Co-ordinations in the Penn Treebank are miss-ing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsisten-cies. In this work, we initiated manual an-notation process for solving these issues. We identify the different elements in a co-ordination phrase and label each element with its function. We add phrase bound-aries when these are missing, unify incon-sistencies, and fix errors. The outcome is an extension of the PTB that includes con-sistent and detailed structures for coordi-nations. We make the coordination anno-tation publicly available, in hope that they will facilitate further research into coordi-nation disambiguation. [Cite_Footnote_1]"
  },
  {
    "id": 1960,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://crisisnlp.qcri.org/",
    "section_title": "3 Aggregated Dataset",
    "add_info": "2 https://crisisnlp.qcri.org/",
    "text": "To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, [Cite_Footnote_2] to-gether with two resources from CrisisLex. Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex. The original classes in each resource, together with the mapping to the new classes included in our data set, can be seen in Table 1. Some examples from the dataset are shown in Table 2. For the dataset construction, the following classes were included:"
  },
  {
    "id": 1961,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "DataSource",
    "func": "Extend",
    "url": "https://crisislex.org/data-collections.html",
    "section_title": "3 Aggregated Dataset",
    "add_info": "3 https://crisislex.org/data-collections.html",
    "text": "To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, to-gether with two resources from CrisisLex. [Cite_Footnote_3] Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex. The original classes in each resource, together with the mapping to the new classes included in our data set, can be seen in Table 1. Some examples from the dataset are shown in Table 2. For the dataset construction, the following classes were included:"
  },
  {
    "id": 1962,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Produce",
    "url": "https://github.com/JRC1995/Multilingual-BERT-Disaster",
    "section_title": "4 Methods 5.1 Experimental Setup",
    "add_info": "5 https://github.com/JRC1995/Multilingual-BERT-Disaster",
    "text": "We use four datasets for testing: Russia Meteor, Cy-clone Pam, Philippines Flood, and Mixed disasters. To demonstrate the generalization capabilities of our models, we ensured that the first three datasets are from disasters that are absent in the training set. For M-BERT-based models, we use a mini batch size of 32, a learning rate of 10 \u22123 for non-BERT parameters, and a fine-tuning rate of 2 \u00d7 10 \u22125 for M-BERT parameters. We set the parameter \u03b1 of the Beta distribution for the Mixup equation to 2. We run each model five times and report the mean and standard deviation of the results obtained in the 5 runs. For the other models, we import the parame-ter settings from their corresponding paper and then perform light manual tuning. The exact hyperpa-rameters are available on Github. [Cite_Footnote_5] . For significance testing, we used the paired t-test (p \u2264 0.05) (Dror et al., 2018) Note that the CNN baseline is also similar to the model used by Nguyen et al. (2017) which was demonstrated to be a strong performer in disaster-related classification."
  },
  {
    "id": 1963,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://lil.nlp.cornell.edu/nlvr/",
    "section_title": "1 Introduction",
    "add_info": null,
    "text": "This paper includes four main contributions: (1) a procedure for collecting visually rich im-ages paired with semantically-diverse language descriptions; (2) NLVR2, which contains 107,292 examples of captions and image pairs, includ-ing 29,680 unique sentences and 127,502 im-ages; (3) a qualitative linguistically-driven data analysis showing that our process achieves a broader representation of linguistic phenomena compared to other resources; and (4) an evalu-ation with several baselines and state-of-the-art visual reasoning methods on NLVR2. The rel-atively low performance we observe shows that NLVR2 presents a significant challenge, even for methods that perform well on existing vi-sual reasoning tasks. NLVR2 is available at  http://lil.nlp.cornell.edu/nlvr/."
  },
  {
    "id": 1964,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://groups.csail.mit.edu/sls/downloads/",
    "section_title": "3 The Corpus",
    "add_info": "7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/",
    "text": "Table 1 shows the distribution over the stance labels, [Cite_Footnote_7] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance."
  },
  {
    "id": 1965,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Knowledge",
    "func": "Produce",
    "url": "http://alt.qcri.org/resources/",
    "section_title": "3 The Corpus",
    "add_info": "7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/",
    "text": "Table 1 shows the distribution over the stance labels, [Cite_Footnote_7] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance."
  },
  {
    "id": 1966,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Produce",
    "url": "https://github.com/hiaoxui/D2T-Grounding",
    "section_title": "References",
    "add_info": "1 Our implementation is available at https://github.com/hiaoxui/D2T-Grounding.",
    "text": "Previous work on grounded language learn-ing did not fully capture the semantics under-lying the correspondences between structured world state representations and texts, espe-cially those between numerical values and lex-ical terms. In this paper, we attempt at learn-ing explicit latent semantic annotations from paired structured tables and texts, establishing correspondences between various types of val-ues and texts. We model the joint probabil-ity of data fields, texts, phrasal spans, and la-tent annotations with an adapted semi-hidden Markov model, and impose a soft statistical constraint to further improve the performance. As a by-product, we leverage the induced an-notations to extract templates for language generation. Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. [Cite_Footnote_1]"
  },
  {
    "id": 1967,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Use",
    "url": "http://www.itl.nist.gov/iad/mig/tests/mt/",
    "section_title": "References",
    "add_info": "1 http://www.itl.nist.gov/iad/mig/tests/mt/",
    "text": "We carried out a study that involved monolin-gual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 2008 [Cite_Footnote_1] test sets, being assisted by statistical machine translation systems trained on data created under the GALE research program."
  },
  {
    "id": 1968,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Material",
    "type": "Dataset",
    "func": "Introduce",
    "url": "http://www.trados.com/",
    "section_title": "2 Human Translation 2.1 Translation Tools",
    "add_info": "3 for instance: Trados, http://www.trados.com/",
    "text": "The use of computers has also led to the adoption of tools such as translation memories [Cite_Footnote_3] (databases of translated material that are queried for fuzzy matches, i.e. translated sentences similar to the one to be processed), monolingual and bilingual concor-dances (showing words used in context, and their translations), terminology databases, online dictio-naries and thesauri, and basic editing tools such as word processors and spell checkers (Desilets, 2009)."
  },
  {
    "id": 1969,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Introduce",
    "url": "http://translate.google.com/toolkit/",
    "section_title": "2 Human Translation 2.2 Translation Skills",
    "add_info": "8 http://translate.google.com/toolkit/",
    "text": "Research has shown that less qualified transla-tors are able to increase their productivity and qual-ity disproportionally when given automatic assis-tance (Koehn and Haddow, 2009). Assistance may be as limited as offering machine translation in a post-editing environment, as for instance provided by Google Translator Toolkit [Cite_Footnote_8] (Galvez and Bhansali, 2009) which provides a special function to translate Wikipedia articles."
  },
  {
    "id": 1970,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Tool",
    "func": "Use",
    "url": "http://anserini.io/",
    "section_title": "3 System Architecture 3.1 Anserini Retriever",
    "add_info": "1 http://anserini.io/",
    "text": "At inference time, we retrieve k text segments (one of the above conditions) using the question as a \u201cbag of words\u201d query. We use a post-v0.3.0 branch of Anserini, [Cite_Footnote_1] with BM25 as the ranking function (Anserini\u2019s default parameters)."
  },
  {
    "id": 1971,
    "name": "N/A",
    "fullname": "N/A",
    "genericmention": [
      "N/A"
    ],
    "description": [
      "N/A"
    ],
    "citationtag": [
      "N/A"
    ],
    "role": "Method",
    "type": "Code",
    "func": "Extend",
    "url": "https://github.com/google-research/bert",
    "section_title": "3 System Architecture 3.2 BERT Reader",
    "add_info": "2 https://github.com/google-research/bert",
    "text": "Our BERT reader is based on Google\u2019s refer-ence implementation [Cite_Footnote_2] (TensorFlow 1.12.0). For training, we begin with the BERT-Base model (un-cased, 12-layer, 768-hidden, 12-heads, 110M pa-rameters) and then fine tune the model on the train-ing set of SQuAD (v1.1). All inputs to the reader are padded to 384 tokens; the learning rate is set to 3 \u00d7 10 \u22125 and all other defaults settings are used."
  }
]
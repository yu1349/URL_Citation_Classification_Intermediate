id,id,url,passage-title,citation-sentense,citation-info,citation-paragraph,role,type,type-confident,function,function-confident,citation-id,year,paper-title,citation-type
0,10001," https://github.com/mdelhoneux/uuparser-composition"," ['4 Composition in a K&G Parser']","Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [Cite_Footnote_4] .",4 The code can be found at https://github.com/mdelhoneux/uuparser-composition,"Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [Cite_Footnote_4] . The S WAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarse-grained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below the best system, which uses an ensemble system as well as ELMo embed-dings, as introduced by Peters et al. (2018). Note, however, that we use a slightly impoverished ver-sion of the model used for the shared task which is described in Smith et al. (2018a): we use a less ac-curate POS tagger (UDPipe) and we do not make use of multi-treebank models. In addition, Smith et al. (2018a) use the three top items of the stack as well as the first item of the buffer to represent the configuration, while we only use the two top items of the stack and the first item of the buffer. Smith et al. (2018a) also use an extended feature set as introduced by Kiperwasser and Goldberg (2016b) where they also use the rightmost and left-most children of the items of the stack and buffer that they consider. We do not use that extended feature set. This is to keep the parser settings as simple as possible and avoid adding confounding factors. It is still a near-SOTA model. We evaluate parsing models on the development sets and report the average of the 5 best results in 30 epochs and 5 runs with different random seeds.",Method,Code,True,Use（引用目的）,True,N19-1159_0_0,2019,Recursive Subtree Composition in LSTM-Based Dependency Parsing,Footnote
1,10002," http://hdl.handle.net/11234/1-2364"," ['5 What Correlates with Difficulty?']","We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017) [Cite_Ref] .","Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88–99. Documented models at http: //hdl.handle.net/11234/1-2364.","Head-POS Entropy Dehouck and Denis (2018) propose an alternative measure of morphosyntactic complexity. Given a corpus of dependency graphs, they estimate the conditional entropy of the POS tag of a random token’s parent, conditioned on the token’s type. In a language where this HPE-mean metric is low, most tokens can predict the POS of their parent even without context. We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017) [Cite_Ref] .",Material,Knowledge,True,Use（引用目的）,True,P19-1491_0_0,2019,What Kind of Language Is Hard to Language-Model?,Reference
2,10003," http://sjmielke.com/papers/tokenize/"," ['D Data selection: Europarl']","We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [Cite_Footnote_31] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set.",31 http://sjmielke.com/papers/tokenize/,"Finally, it should be said that the text in CoStEP itself contains some markup, marking reports, el-lipses, etc., but we strip this additional markup to obtain the raw text. We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [Cite_Footnote_31] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set. This way we ensure uniform division over sessions of the parliament and sizes of 2 / 3 , 1 / 6 , and 1 / 6 , respectively.",Method,Tool,True,Use（引用目的）,True,P19-1491_1_0,2019,What Kind of Language Is Hard to Language-Model?,Footnote
3,10004," https://help.twitter.com/en/rules-and-policies/twitter-limits"," ['2 Problem Formulation', '2.1 Methodology']","Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy ( [Cite] https://help.twitter.com/en/rules-and-policies/twitter-limits).",,"Challenges: In training a classifier to distinguish between user and non-user tweets, we should ide-ally have an equal amount of examples of both classes. User tweets are simply all the tweets from that user’s Twitter account, and measure perhaps in the thousands. Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy ( [Cite] https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day (https://business.twitter.com). Thus there is an ex-treme class imbalance between user and non-user tweets. Moreover, the nature of language used on Twitter does not conform to formal syntactic or se-mantic rules. The sentences tend to be highly un-structured, and the vocabulary is not restricted to a particular dictionary.",補足資料,Document,True,Introduce（引用目的）,True,D19-1175_0_0,2019,The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets,Body
4,10005," https://business.twitter.com"," ['2 Problem Formulation', '2.1 Methodology']","The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day ( [Cite] https://business.twitter.com).",,"Challenges: In training a classifier to distinguish between user and non-user tweets, we should ide-ally have an equal amount of examples of both classes. User tweets are simply all the tweets from that user’s Twitter account, and measure perhaps in the thousands. Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy (https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day ( [Cite] https://business.twitter.com). Thus there is an ex-treme class imbalance between user and non-user tweets. Moreover, the nature of language used on Twitter does not conform to formal syntactic or se-mantic rules. The sentences tend to be highly un-structured, and the vocabulary is not restricted to a particular dictionary.",補足資料,Document,True,Introduce（引用目的）,True,D19-1175_1_0,2019,The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets,Body
5,10006," https://developer.twitter.com/en/docs.html"," ['2 Problem Formulation', '2.2 Data']",• Negative examples: We have col-lected 1% of tweets from Twitter’s daily feed using the Twitter API ( [Cite] https://developer.twitter.com/en/docs.html) to use as negative examples.,,• Negative examples: We have col-lected 1% of tweets from Twitter’s daily feed using the Twitter API ( [Cite] https://developer.twitter.com/en/docs.html) to use as negative examples.,Method,Tool,True,Use（引用目的）,True,D19-1175_2_0,2019,The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets,Body
6,10007," https://www.mturk.com/"," ['5 User study']","To verify whether human evaluators are in agree-ment with our characterization model, we con-ducted a user study using MTurk (Amazon, 2005) [Cite_Ref] .",Amazon. 2005. MTurk. (https://www.mturk.com/).,"To verify whether human evaluators are in agree-ment with our characterization model, we con-ducted a user study using MTurk (Amazon, 2005) [Cite_Ref] .",Method,Tool,True,Use（引用目的）,True,D19-1175_3_0,2019,The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets,Reference
7,10008," https://research.fb.com/fasttext/"," ['4 Approaches to authorship verification', '4.4 Approach 4: Document embeddings']","We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016) [Cite_Ref] (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768).",Facebook-Research. 2016. FastText. (https://research.fb.com/fasttext/).,"1. We obtain representations of tweets as doc-ument embeddings. We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016) [Cite_Ref] (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768).",Method,Code,True,Use（引用目的）,True,D19-1175_4_0,2019,The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets,Reference
8,10009," https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/"," ['1 Introduction']","Social media platforms, particularly microblog-ging services such as Twitter, have become in-creasingly popular (Statista, 2019) [Cite_Ref] as a means to express thoughts and opinions.",Statista. 2019. Twitter: Num-ber 2010-2018.of active users (https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/).,"Social media platforms, particularly microblog-ging services such as Twitter, have become in-creasingly popular (Statista, 2019) [Cite_Ref] as a means to express thoughts and opinions. Twitter users emit tweets about a wide variety of topics, which vary in the extent to which they reflect a user’s person-ality, brand and interests. This observation mo-tivates the question we consider here, of how to quantify the degree to which tweets are character-istic of their author?",補足資料,Document,True,Introduce（引用目的）,True,D19-1175_5_0,2019,The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets,Reference
9,10010," http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20"," ['3 Related work', '3.2 Predicting tweet popularity']","Zhang et al. (2018) [Cite_Ref] approach retweet prediction as a multi-class classification problem, and present a feature-weighted model, where weights are com-puted using information gain.","Yang Zhang, Zhiheng Xu, and Qing Yang. 2018. Predicting Popularity of Messages in Twitter using a Feature-weighted Model. http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20.","Suh et al. (2010) leverages features such as URL, number of hashtags, number of followers and fol-lowees etc. in a generalized linear model, to predict the number of retweets. Naveed et al. (2011) extend this approach to perform content-based retweet prediction using several features in-cluding sentiments, emoticons, punctuations etc. Bandari et al. (2012) apply the same approach for regression as well as classification, to predict the number of retweets specifically for news ar-ticles. Zaman et al. (2014) present a Bayesian model for retweet prediction using early retweet times, retweets of other tweets, and the user’s fol-lower graph. Tan et al. (2014) analyze whether different wording of a tweet by the same author af-fects its popularity. SEISMIC (Zhao et al., 2015) and PSEISMIC (Chen and Li, 2017) are statistical methods to predict the final number of retweets. Zhang et al. (2018) [Cite_Ref] approach retweet prediction as a multi-class classification problem, and present a feature-weighted model, where weights are com-puted using information gain.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1175_6_0,2019,The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets,Reference
10,10011," http://books.google.com/ngrams"," ['3 The Proposed Method', '3.1 Automatic Seed Generation']","Let λ(t) denotes the LRT score of a product feature candidate t, where k 1 and k 2 are the frequencies of t in the review corpus R and a background corpus [Cite_Footnote_1] B, n 1 and n 2 are the total number of terms in R and B, p",1 Google-n-Gram (http://books.google.com/ngrams) is used as the background corpus.,"The seed set consists of positive labeled examples (i.e. product features) and negative labeled exam-ples (i.e. noise terms). Intuitively, popular product features are frequently mentioned in reviews, so they can be extracted by simply mining frequently occurring nouns (Hu and Liu, 2004). However, this strategy will also find many noise terms (e.g., commonly used nouns like thing, one, etc.). To produce high quality seeds, we employ a Domain Relevance Measure (DRM) (Jiang and Tan, 2010), which combines term frequency with a domain-specific measuring metric called Likelihood Ratio Test (LRT) (Dunning, 1993). Let λ(t) denotes the LRT score of a product feature candidate t, where k 1 and k 2 are the frequencies of t in the review corpus R and a background corpus [Cite_Footnote_1] B, n 1 and n 2 are the total number of terms in R and B, p = (k 1 + k 2 )/(n 1 + n 2 ), p 1 = k 1 /n 1 and p 2 = k 2 /n 2 . Then a modified DRM 2 is proposed, where tf(t) is the frequency of t in R and df(t) is the frequency of t in B.",Material,Knowledge,True,Use（引用目的）,True,P14-1032_0_0,2014,Product Feature Mining: Semantic Clues versus Syntactic Constituents,Footnote
11,10012," http://www.wikipedia.org"," ['3 The Proposed Method', '3.2 Capturing Lexical Semantic Clue in a Semantic Similarity Graph', '3.2.1 Learning Word Embedding for']","To alleviate the data sparsity problem, EB is first trained on a very large corpus [Cite_Footnote_3] (denoted by C), and then fine-tuned on the target review cor-pus R. Particularly, for phrasal product features, a statistic-based method in (Zhu et al., 2009) is used to detect noun phrases in R.",3 Wikipedia(http://www.wikipedia.org) is used in practice.,"To alleviate the data sparsity problem, EB is first trained on a very large corpus [Cite_Footnote_3] (denoted by C), and then fine-tuned on the target review cor-pus R. Particularly, for phrasal product features, a statistic-based method in (Zhu et al., 2009) is used to detect noun phrases in R. Then, an Unfold-ing Recursive Autoencoder (Socher et al., 2011) is trained on C to obtain embedding vectors for noun phrases. In this way, semantics of infrequent terms in R can be well captured. Finally, the phrase-based Skip-gram model in (Mikolov et al., 2013) is applied on R.",Material,Knowledge,True,Use（引用目的）,True,P14-1032_1_0,2014,Product Feature Mining: Semantic Clues versus Syntactic Constituents,Footnote
12,10013," http://timan.cs.uiuc.edu/downloads.html"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']",[Cite_Footnote_5] .,5 http://timan.cs.uiuc.edu/downloads.html,"Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) [Cite_Footnote_5] . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper.",Method,Code,True,Use（引用目的）,True,P14-1032_2_0,2014,Product Feature Mining: Semantic Clues versus Syntactic Constituents,Footnote
13,10014," http://ir-china.org.cn/coae2008.html"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']","[Cite_Footnote_6] , where two review sets (Camera and Car) are selected.",6 http://ir-china.org.cn/coae2008.html,"Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) [Cite_Footnote_6] , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper.",Method,Code,True,Use（引用目的）,True,P14-1032_3_0,2014,Product Feature Mining: Semantic Clues versus Syntactic Constituents,Footnote
14,10015," http://www.d.umn.edu/∼tpederse/data.html"," ['4 Experiments and Results', '4.1 Experiment Design']","For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - “interest”, “line” [Cite_Footnote_1] and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) .",1 Available at http://www.d.umn.edu/∼tpederse/data.html,"For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - “interest”, “line” [Cite_Footnote_1] and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3.",Material,Dataset,True,Compare（引用目的）,True,P05-1049_0_0,2005,Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning,Footnote
15,10016," http://www.senseval.org/senseval3"," ['4 Experiments and Results', '4.1 Experiment Design']",[Cite_Footnote_2] .,2 Available at http://www.senseval.org/senseval3,"For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - “interest”, “line” and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) [Cite_Footnote_2] . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3.",Material,Dataset,True,Compare（引用目的）,True,P05-1049_1_0,2005,Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning,Footnote
16,10017," http://svmlight.joachims.org/"," ['4 Experiments and Results', '4.2 Experiment 1: LP vs. SVM']",It also lists the official results of baseline method and top [Cite_Footnote_3] systems in ELS task of SENSEVAL-3.,"3 we SV M light ,used linear available at http://svmlight.joachims.org/.",Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [Cite_Footnote_3] systems in ELS task of SENSEVAL-3.,Method,Tool,True,Introduce（引用目的）,True,P05-1049_2_0,2005,Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning,Footnote
17,10018," http://isomap.stanford.edu/"," ['4 Experiments and Results', '4.4 An Example: Word “use”']","For data visualization, we conducted unsupervised nonlinear dimensionality reduction [Cite_Footnote_5] on these 40 feature vec-tors with 210 dimensions.","5 We used Isomap to perform dimensionality reduction by computing two-dimensional, 39-nearest-neighbor-preserving embedding of 210-dimensional input. Isomap is available at http://isomap.stanford.edu/.","For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word “use” in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set). For data visualization, we conducted unsupervised nonlinear dimensionality reduction [Cite_Footnote_5] on these 40 feature vec-tors with 210 dimensions. Figure 3 (a) shows the dimensionality reduced vectors in two-dimensional space. We randomly sampled only one labeled ex-ample for each sense of word “use” as labeled data. The remaining data in training set and test set served as unlabeled data for bootstrapping and LP. All of these three algorithms are evaluated using accuracy on test set.",Method,Code,True,Use（引用目的）,True,P05-1049_3_0,2005,Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning,Footnote
18,10019," https://osf.io/z89vn/"," ['1 Introduction']","Results on reading and fMRI measures show substantial generalization im-provements from CDRNN over baselines, along with detailed insights about the underlying dynam-ics that cannot easily be obtained from existing methods. [Cite_Footnote_1]","1 Because of page constraints, additional replication details and synthetic results are provided in an external supplement, available here: https://osf.io/z89vn/.","This study proposes an attempt to leverage the flexibility of DNNs for psycholinguistic data anal-ysis. The continuous-time deconvolutional regres-sive neural network (CDRNN) is an extension of CDR that reimplements the impulse response function as a DNN describing the expected in-fluence of preceding events (e.g. words) on fu-ture responses (e.g. reading times) as a function of their properties and timing. CDRNN retains the deconvolutional design of CDR while relax-ing many of its simplifying assumptions (linear-ity, additivity, homosketasticity, stationarity, and context-independence, see Section 2), resulting in a highly flexible model. Nevertheless, CDRNN is interpretable and can shed light on the underlying data generating process. Results on reading and fMRI measures show substantial generalization im-provements from CDRNN over baselines, along with detailed insights about the underlying dynam-ics that cannot easily be obtained from existing methods. [Cite_Footnote_1]",補足資料,Document,True,Introduce（引用目的）,True,2021.acl-long.288_0_0,2021,CDRNN: Discovering Complex Dynamics in Human Language Processing,Footnote
19,10020," https://github.com/coryshain/cdr"," ['4 Methods']",Code and documentation are avail-able at [Cite] https://github.com/coryshain/cdr.,,"Two CDRNN variants are considered in all ex-periments: the full model (CDRNN-RNN) contain-ing an RNN over the predictor sequence, and a feed-forward only model (CDRNN-FF) with the RNN ablated (gray arrows removed in Figure 1). This manipulation is of interest because CDRNN-FF is both more parsimonious (fewer parameters) and faster to train, and may therefore be preferred in the absence of prior expectation that the IRF is sensi-tive to context. All plots show means and 95% cred-ible intervals. Code and documentation are avail-able at [Cite] https://github.com/coryshain/cdr.",Mixed,Mixed,True,Use（引用目的）,True,2021.acl-long.288_1_0,2021,CDRNN: Discovering Complex Dynamics in Human Language Processing,Body
20,10021," https://github.com/GeneZC/ASGCN"," ['References']","Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models [Cite_Footnote_1] , and further demon-strate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.",1 Code and preprocessed datasets are available at https://github.com/GeneZC/ASGCN.,"Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neu-ral Networks (CNNs) are widely applied for aspect-based sentiment classification. How-ever, these models lack a mechanism to ac-count for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we pro-pose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models [Cite_Footnote_1] , and further demon-strate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.",Material,Dataset,True,Use（引用目的）,True,D19-1464_0_0,2019,Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks,Footnote
21,10022," http://svmlight.joachims.org"," ['3 Problem Formulation']","SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments. [Cite_Footnote_2]",2 Available at http://svmlight.joachims.org.,"The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Schölkopf and Smola (2002). SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments. [Cite_Footnote_2]",Method,Tool,True,Use（引用目的）,True,N09-1031_0_0,2009,Predicting Risk from Financial Reports with Regression,Footnote
22,10023," http://www.sec.gov/edgar.shtml"," ['4 Dataset']",These reports are available to the public and pub-lished on the SEC’s web site. [Cite_Footnote_3],3 http://www.sec.gov/edgar.shtml,"In the United States, the Securities Exchange Com-mission mandates that all publicly-traded corpora-tions produce annual reports known as “Form 10- K.” The report typically includes information about the history and organization of the company, equity and subsidiaries, as well as financial information. These reports are available to the public and pub-lished on the SEC’s web site. [Cite_Footnote_3] The structure of the 10-K is specified in detail in the legislation. We have collected 54,379 reports published over the period 1996–2006 from 10,492 different companies. Each report comes with a date of publication, which is im-portant for tying the text back to the financial vari-ables we seek to predict.",補足資料,Website,True,Introduce（引用目的）,True,N09-1031_1_0,2009,Predicting Risk from Financial Reports with Regression,Footnote
23,10024," http://www.ark.cs.cmu.edu/10K"," ['4 Dataset']",US Stocks Database to obtain the price return series along with other firm characteristics. [Cite_Footnote_4],4 The text and volatility data are publicly available at http: //www.ark.cs.cmu.edu/10K.,"In addition to the reports, we used the Center for Research in Security Prices (CRSP) US Stocks Database to obtain the price return series along with other firm characteristics. [Cite_Footnote_4] We proceeded to calcu-late two volatilities for each firm/report observation: the twelve months prior to the report (v (−12) ) and the twelve months after the report (v (+12) ).",Material,Dataset,True,Use（引用目的）,True,N09-1031_2_0,2009,Predicting Risk from Financial Reports with Regression,Footnote
24,10025," http://nlp.cs.nyu.edu/pubs/"," ['1 Introduction']","Lastly, we propose a synchronous generalization of Chom-sky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKY-style algorithm (Younger, 1967; Melamed, 2004 [Cite_Ref] ).","I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.","This paper begins with an informal description of GMTG. It continues with an investigation of this formalism’s generative capacity. Next, we prove that in GMTG each component grammar retains its generative power, a requirement for synchronous formalisms that Rambow and Satta (1996) called the “weak language preservation property.” Lastly, we propose a synchronous generalization of Chom-sky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKY-style algorithm (Younger, 1967; Melamed, 2004 [Cite_Ref] ).",Method,Code,True,Use（引用目的）,True,P04-1084_0_0,2004,Generalized Multitext Grammars,Reference
25,10026," http://nlp.cs.nyu.edu/pubs/"," ['6 Generalized Chomsky Normal Form', '6.2 Step 4: Eliminate ’s']","The absence of ’s simplifies parsers for GMTG (Melamed, 2004) [Cite_Ref] .","I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.","Grammars in GCNF cannot have ’s in their productions. Thus, GCNF is a more restrictive normal form than those used by Wu (1997) and Melamed (2003). The absence of ’s simplifies parsers for GMTG (Melamed, 2004) [Cite_Ref] . Given a GMTG u with in some productions, we give the construction of a weakly equivalent gram-mar u9O without any ’s. First, determine all nullable links and associated - strings in u . - A link * Z Z is nullable if < y is an ITV where at least one y•bhg is . We say the link is nullable and the string at address in is nullable. For each nullable link, we create versions of the link, where is the number of nullable strings of that link. There is one version for each of the possible combinations of the nullable strings being present or absent. The version of the link with all strings present is its original version. Each non-original version of the link (except in the case of start links) gets a unique subscript, which is applied to all the nonterminals in the link, so that each link is unique in the grammar. We construct a new grammar u O whose set of productions w O is determined as follows: for each production, we identify the nullable links on the RHS and replace them with each combination of the non-original versions found earlier. If a string is left empty during this process, that string is removed from the RHS and the fan-out of the production component is reduced by one. The link on the LHS is replaced with its appropriate matching non-original link. There is one exception to the replacements. If a production consists of all nullable strings, do not include this case. Lastly, we remove all strings on the RHS of productions that have ’s, and reduce the fan-out of the productions accordingly. Once again, we replace the LHS link with the appropriate version. case and are nullable - so we create 54 a new version of both links: and . We then alter the productions. Pro-duction (31) gets replaced by (40). A new produc-tion based on (30) is Production (38). Lastly, Pro-duction (29) has two nullable strings on the RHS, so it gets altered to add three new productions, (34), (35) and (36). The altered set of productions are the following:",Material,Knowledge,False,Use（引用目的）,False,P04-1084_0_1,2004,Generalized Multitext Grammars,Reference
26,10027," http://nlp.cs.nyu.edu/pubs/"," ['4 Generative Capacity']","For lack of space, some proofs are only sketched, or entirely omitted when relatively intuitive: Melamed et al. (2004) [Cite_Ref] provide more details.","I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.","In this section we compare the generative capac-ity of GMTG with that of mildly context-sensitive grammars. We focus on LCFRS, using the no-tational variant introduced by Rambow and Satta (1999), briefly summarized v4 below. Throughout this section , strings =:% < and vectors of the form will be identified. For lack of space, some proofs are only sketched, or entirely omitted when relatively intuitive: Melamed et al. (2004) [Cite_Ref] provide more details. where ( 0( represents some grouping into strings of all and only the variables appearing in the left-hand side, possibly with some additional termi-nal symbols. (Symbols ‘ , q and are overloaded below.)",補足資料,Document,True,Introduce（引用目的）,True,P04-1084_0_2,2004,Generalized Multitext Grammars,Reference
27,10028," http://nlp.cs.nyu.edu/pubs/"," ['6 Generalized Chomsky Normal Form', '6.2 Step 4: Eliminate ’s']","Melamed et al. (2004) [Cite_Ref] give more details about conversion to GCNF, as well as the full proof of our final theorem:","I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.","Melamed et al. (2004) [Cite_Ref] give more details about conversion to GCNF, as well as the full proof of our final theorem:",補足資料,Document,True,Introduce（引用目的）,True,P04-1084_0_3,2004,Generalized Multitext Grammars,Reference
28,10029," http://hohocode.github.io/textSimilarityConvNet/"," ['6 Experiments and Results']",Everything necessary to replicate our experimen-tal results can be found in our open-source code repository. [Cite_Footnote_4],4 http://hohocode.github.io/textSimilarityConvNet/,Everything necessary to replicate our experimen-tal results can be found in our open-source code repository. [Cite_Footnote_4],Method,Tool,True,Introduce（引用目的）,True,D15-1181_0_0,2015,Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks,Footnote
29,10031," https://www.irit.fr/STAC/"," ['1 Introduction']","After training a supervised deep learning algorithm to predict attachments on the STAC corpus [Cite_Footnote_1] , we then constructed a weakly supervised learning system in which we used 10% of the corpus as a develop-ment set.",1 https://www.irit.fr/STAC/,"In our study, we restrict the structure learning problem to predicting edges or attachments be-tween DU pairs in the dependency graph. After training a supervised deep learning algorithm to predict attachments on the STAC corpus [Cite_Footnote_1] , we then constructed a weakly supervised learning system in which we used 10% of the corpus as a develop-ment set. Experts on discourse structure wrote a set of attachment rules, Labeling Functions (LFs), with reference to this development set. Although the whole of the STAC corpus is annotated, we treated the remainder of the corpus as unseen/u-nannotated data in order to simulate the conditions in which the snorkel framework is meant to be used, i.e. where there is a large amount of unla-beled data but where it is only feasible to hand la-bel a relatively small portion of it. Accordingly, we applied the completed LFs to our “unseen” training set, 80% of the corpus, and used the final 10% as our test set.",Material,Knowledge,True,Use（引用目的）,True,P19-1061_0_0,2019,Data Programming for Learning Discourse Structure,Footnote
30,10032," https://tizirinagh.github.io/acl2019/"," ['4 Data Programming Experiments', '4.1 Candidates and Labeling Functions']",[Cite] https://tizirinagh.github.io/acl2019/.,,"To predict dialogue attachment, our LFs exploit information about candidates including whether they are linguistic or non-linguistic DUs, the di-alogue acts they express, their speaker identities, lexical content and grammatical category, as well as the distance between DUs: all features also used in supervised learning methods (Perret et al., 2016; Afantenos et al., 2015; Muller et al., 2012). Fur-thermore, our LFs take into account the particu-lar behavior of each relation type, information that expert annotators consider when deciding whether two DUs are attached. Thus the LFs were di-vided among the 9 relation types as well as the combination of DU endpoints for each type, e.g. linguistic/non-linguistic. We also fix the order in which each LF “sees” the candidates such that it considers adjacent DUs before distant DUs. This allows LFs to exploit information about previously predicted attachments and dialogue history in new predictions. Our complete rule set, along with de-scriptions of each of the relation types, is available here: [Cite] https://tizirinagh.github.io/acl2019/.",Mixed,Mixed,True,Produce（引用目的）,True,P19-1061_1_0,2019,Data Programming for Learning Discourse Structure,Body
31,10033," https://tizirinagh.github.io/acl2019/"," ['5 Results and Analysis']","We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [Cite_Footnote_3] .",3 https://tizirinagh.github.io/acl2019/,"We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [Cite_Footnote_3] . We then evaluated the generative model and the generative + discriminative model with the Snorkel architecture on the test set with the results in Table 2.",Material,Knowledge,True,Use（引用目的）,True,P19-1061_2_0,2019,Data Programming for Learning Discourse Structure,Footnote
32,10034," http://dx.doi.org/10.3765/sp.11.10"," ['3 The STAC Annotated Corpus', '3.1 Overview']","While earlier versions only included linguistic moves by players, the latest version of STAC is a multi-modal corpus of multi-party chats between players of an online game (Asher et al., 2016; Hunter et al., 2018 [Cite_Ref] ).","Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.","While earlier versions only included linguistic moves by players, the latest version of STAC is a multi-modal corpus of multi-party chats between players of an online game (Asher et al., 2016; Hunter et al., 2018 [Cite_Ref] ). It includes 2,593 dialogues (each with a weakly connected DAG discourse structure), 12,588 “linguistic” DUs, 31,811 “non-linguistic” DUs and 31,251 semantic relations. A dialogue begins at the beginning of a player’s turn, and ends at the end of that player’s turn. In the interim, players can bargain with each other or make spontaneous conversation. These player ut-terances are the “linguistic” turns. In addition the corpus contains information given visually in the game interface but transcribed in the corpus into Server or interface messages, “non-linguistic” turns (Hunter et al., 2018). All turns are seg-mented into DUs, and these units are then con-nected by semantic relations.",補足資料,Paper,True,Introduce（引用目的）,True,P19-1061_3_0,2019,Data Programming for Learning Discourse Structure,Reference
33,10035," http://dx.doi.org/10.3765/sp.11.10"," ['3 The STAC Annotated Corpus', '3.1 Overview']","In addition the corpus contains information given visually in the game interface but transcribed in the corpus into Server or interface messages, “non-linguistic” turns (Hunter et al., 2018) [Cite_Ref] .","Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.","While earlier versions only included linguistic moves by players, the latest version of STAC is a multi-modal corpus of multi-party chats between players of an online game (Asher et al., 2016; Hunter et al., 2018). It includes 2,593 dialogues (each with a weakly connected DAG discourse structure), 12,588 “linguistic” DUs, 31,811 “non-linguistic” DUs and 31,251 semantic relations. A dialogue begins at the beginning of a player’s turn, and ends at the end of that player’s turn. In the interim, players can bargain with each other or make spontaneous conversation. These player ut-terances are the “linguistic” turns. In addition the corpus contains information given visually in the game interface but transcribed in the corpus into Server or interface messages, “non-linguistic” turns (Hunter et al., 2018) [Cite_Ref] . All turns are seg-mented into DUs, and these units are then con-nected by semantic relations.",補足資料,Paper,True,Introduce（引用目的）,False,P19-1061_3_1,2019,Data Programming for Learning Discourse Structure,Reference
34,10036," https://catalog.ldc.upenn.edu/LDC2006T06"," ['1 Introduction']",We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [Cite_Footnote_1] .,1 https://catalog.ldc.upenn.edu/LDC2006T06,"We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [Cite_Footnote_1] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.",Material,Dataset,True,Compare（引用目的）,True,N19-1080_0_0,2019,Event Detection without Triggers,Footnote
35,10037," https://github.com/liushulinle/event"," ['1 Introduction']",We publish our code for further study by the NLP community. [Cite_Footnote_2],2 https://github.com/liushulinle/event detection without triggers,"We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community. [Cite_Footnote_2]",Method,Code,True,Produce（引用目的）,True,N19-1080_1_0,2019,Event Detection without Triggers,Footnote
36,10038," http://stanfordnlp.github.io/CoreNLP"," ['3 Methodology', '3.1 Input Tokens']","Given a sentence, we use Stanford CoreNLP tool-s [Cite_Footnote_3] (Manning et al., 2014) to convert texts into to-kens.",3 http://stanfordnlp.github.io/CoreNLP,"Given a sentence, we use Stanford CoreNLP tool-s [Cite_Footnote_3] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).",Method,Tool,True,Use（引用目的）,True,N19-1080_2_0,2019,Event Detection without Triggers,Footnote
37,10039," https://catalog.ldc.upenn.edu/LDC2008T19"," ['3 Methodology', '3.2 Word/Entity Embeddings']","In this work, we use the Skip-gram mod-el(Mikolov et al., 2013) to learn word embeddings on the NYT corpus [Cite_Footnote_4] .",4 https://catalog.ldc.upenn.edu/LDC2008T19,"In this work, we use the Skip-gram mod-el(Mikolov et al., 2013) to learn word embeddings on the NYT corpus [Cite_Footnote_4] . Furthermore, we random-ly initialized an embedding table for each entity tags. All the input word tokens and entity tags will be transformed into low-dimensional vectors by looking up these embedding tables. In this work, we denote the dimension of word embeddings by d w , and that of entity embeddings by d e .",Method,Code,True,Use（引用目的）,True,N19-1080_3_0,2019,Event Detection without Triggers,Footnote
38,10040," http://www.kde.cs.uni-kassel.de/bibsonomy/dumps"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']","The second dataset, denoted as BIBTEX, is obtained from an English online bibliography web-site www.bibsonomy.org [Cite_Footnote_2] .",2 The dataset can be obtained from http://www.kde.cs.uni-kassel.de/bibsonomy/dumps,"The first dataset, denoted as BOOK, is obtained from a popular Chinese book review website www. douban.com, which contains the descriptions of books and the tags collaboratively annotated by users. The second dataset, denoted as BIBTEX, is obtained from an English online bibliography web-site www.bibsonomy.org [Cite_Footnote_2] . The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users. As shown in Table 2, the average length of descriptions in the BIBTEX dataset is much shorter than the BOOK dataset. Moreover, the BIBTEX dataset does not provide how many times each tag is annotated to a resource.",Material,Dataset,True,Use（引用目的）,False,D11-1146_0_0,2011,A Simple Word Trigger Method for Social Tag Suggestion,Footnote
39,10041," https://idir.uta.edu/claimportal"," ['1 Introduction']",ClaimPortal is available at [Cite] https://idir.uta.edu/claimportal.,,"In this paper we present ClaimPortal, a web-based platform for monitoring, searching, check-ing, and analytics of factual claims on Twit-ter. ClaimPortal is available at [Cite] https://idir.uta.edu/claimportal. ClaimPortal con-tinuously collects tweets and monitors factual claims embedded in tweets. It is integrated with fact-checking tools, including a claim matcher which finds known fact-checks matching any given tweet, a claim spotter which scores each claim and the corresponding tweet based on their check-worthiness, i.e., how important it is to fact-check them. ClaimPortal provides an intuitive and convenient search interface that assists its users to sift through these factual claims in tweets us-ing filtering conditions on dates, twitter accounts, content, hashtags, check-worthiness scores, and types of claims. ClaimPortal also provides simple analytics and visualization tools for discovering patterns pertinent to how certain twitter accounts make claims, how different types of claims are dis-tributed, and so on.",補足資料,Website,True,Produce（引用目的）,True,P19-3026_0_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Body
40,10042," https://mashable.com/article/snopes-stops-fact-checking-for-facebook/"," ['1 Introduction']","On the one hand, it is un-likely fact-checkers are able to check every social media post, due to limited resources and the sheer volume of data. [Cite_Footnote_1]",1 https://mashable.com/article/snopes-stops-fact-checking-for-facebook/,"This problem of unchecked claims is exacer-bated on social media. On the one hand, it is un-likely fact-checkers are able to check every social media post, due to limited resources and the sheer volume of data. [Cite_Footnote_1] On the other hand, a large num-ber of false claims, likely much more than those in traditional media, are being spread through so-cial media. This can be due to the compounded effect of several factors: social media platforms have become increasingly important to public fig-ures and organizations in engaging with voters and citizens; mobile devices have brought an age in which sharing and disseminating information is easy for anyone, including both malicious and un-intentional creators of falsehoods; the falsehoods are further replicated and amplified by social me-dia bots and clickbait articles. The consequence can be devastating. For instance, a recent study reports that a sample of 140,000 Twitter users in the battleground state of Michigan shared as many junk news items as professional news during the final ten days of the 2016 election, each constitut-ing 23% of the web links they shared on Twitter in that period.",補足資料,Document,True,Introduce（引用目的）,True,P19-3026_1_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
41,10043," http://politicalbots.org/?p=1064"," ['1 Introduction']","For instance, a recent study reports that a sample of 140,000 Twitter users in the battleground state of Michigan shared as many junk news items as professional news during the final ten days of the 2016 election, each constitut-ing 23% of the web links they shared on Twitter in that period. [Cite_Footnote_2]",2 http://politicalbots.org/?p=1064,"This problem of unchecked claims is exacer-bated on social media. On the one hand, it is un-likely fact-checkers are able to check every social media post, due to limited resources and the sheer volume of data. On the other hand, a large num-ber of false claims, likely much more than those in traditional media, are being spread through so-cial media. This can be due to the compounded effect of several factors: social media platforms have become increasingly important to public fig-ures and organizations in engaging with voters and citizens; mobile devices have brought an age in which sharing and disseminating information is easy for anyone, including both malicious and un-intentional creators of falsehoods; the falsehoods are further replicated and amplified by social me-dia bots and clickbait articles. The consequence can be devastating. For instance, a recent study reports that a sample of 140,000 Twitter users in the battleground state of Michigan shared as many junk news items as professional news during the final ten days of the 2016 election, each constitut-ing 23% of the web links they shared on Twitter in that period. [Cite_Footnote_2]",補足資料,Document,False,Introduce（引用目的）,True,P19-3026_2_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
42,10044," https://www.elastic.co/products/elasticsearch"," ['2 System Architecture and Components', '2.1 System Architecture']","ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [Cite_Footnote_3] search engine, an API, and several decoupled batch data processing components (Figure 1).",3 https://www.elastic.co/products/ elasticsearch,"ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [Cite_Footnote_3] search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers. The front-end presentation layer allows users to narrow down search results by applying multiple filters. Keyword search on tweets is powered by Elasticsearch which is cou-pled with querying the database to provide addi-tional filters. Additionally, it provides numerous visualized graphs. The back-end data collection and computation layer performs pre-processing of tweets, computing check-worthiness scores of tweets using the public ClaimBuster API (Hassan et al., 2017a), Elasticsearch batch insertion, de-tecting claim types of tweets, and finding similar fact-checked claims for each tweet, using Claim-Buster API. ClaimPortal stays up-to-date with current tweets by periodically calling the Twitter REST API.",Method,Tool,True,Use（引用目的）,True,P19-3026_3_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
43,10045," http://flask.pocoo.org"," ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']",The API is a web service designed using Python and the Flask [Cite_Footnote_4] micro-framework.,4 http://flask.pocoo.org,"ClaimPortal’s back-end layer focuses on data processing and storage. The Twitter REST API provides us with the necessary data. However, the system does not require all of it. In fact, a lot of the API’s response is discarded to keep our database small and yet sufficient enough to pro-vide all necessary information for the portal. This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask [Cite_Footnote_4] micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters. Based on the keyword search and filters requested by a user, the API queries the database to find the resulting list of tweet IDs and returns the list as a JSON response. A tweet ID is a unique number assigned to a tweet by Twitter. By using Twitter’s card API the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID.",補足資料,Website,True,Use（引用目的）,True,P19-3026_4_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
44,10046," https://developer.twitter.com/en/docs/tweets/optimize-with-cards"," ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']","By using Twitter’s card API [Cite_Footnote_5] the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID.",5 https://developer.twitter.com/en/docs/tweets/optimize-with-cards,"ClaimPortal’s back-end layer focuses on data processing and storage. The Twitter REST API provides us with the necessary data. However, the system does not require all of it. In fact, a lot of the API’s response is discarded to keep our database small and yet sufficient enough to pro-vide all necessary information for the portal. This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters. Based on the keyword search and filters requested by a user, the API queries the database to find the resulting list of tweet IDs and returns the list as a JSON response. A tweet ID is a unique number assigned to a tweet by Twitter. By using Twitter’s card API [Cite_Footnote_5] the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID.",Method,Code,True,Introduce（引用目的）,True,P19-3026_5_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
45,10047," https://idir.uta.edu/claimbuster/"," ['2 System Architecture and Components', '2.3 Claim Spotter']","This score is obtained by probing the ClaimBuster API, [Cite_Footnote_6] a well-known fact-checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019).",6 https://idir.uta.edu/claimbuster/,"In ClaimPortal, each tweet is given a check-worthiness score which denotes whether the tweet has a factual claim of which the truthfulness is im-portant to the public. This score is obtained by probing the ClaimBuster API, [Cite_Footnote_6] a well-known fact-checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019). Claim-Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates. The Claim-Buster API returns a check-worthiness score for any given text. The score is on a scale from 0 to 1, ranging from least check-worthy to most check-worthy. The background task of probing Claim-Buster API for getting scores for tweets is another batch process, in parallel with the tweet collection and the Elasticsearch indexing processes.",Method,Code,True,Use（引用目的）,True,P19-3026_6_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
46,10048," https://www.politifact.com"," ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']",We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [Cite_Footnote_7] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks.,7 https://www.politifact.com,"We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [Cite_Footnote_7] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it. Details of these purposely created new frames can be found in (Arslan et al., 2019). The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available.",Material,DataSource,True,Use（引用目的）,True,P19-3026_7_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
47,10049," https://github.com/idirlab/factframe"," ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']",The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available. [Cite_Footnote_8],8 https://github.com/idirlab/factframe,"We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it. Details of these purposely created new frames can be found in (Arslan et al., 2019). The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available. [Cite_Footnote_8]",Material,Knowledge,True,Produce（引用目的）,True,P19-3026_8_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
48,10050," https://wapo.st/2rucTq8"," ['2 System Architecture and Components', '2.5 Claim Matcher']","While politicians may refrain themselves from making outright false claims to avoid being fact-checked, oftentimes they even double down after their false claims are debunked. [Cite_Footnote_9]",9 https://wapo.st/2rucTq8,"Claim matching is an important step in the work-flow of fact-checking. Given a factual claim, it aims at finding identical or similar claims from a repository of existing fact-checks. The premise is that public figures keep making the same false claims. While politicians may refrain themselves from making outright false claims to avoid being fact-checked, oftentimes they even double down after their false claims are debunked. [Cite_Footnote_9]",補足資料,Document,True,Introduce（引用目的）,True,P19-3026_9_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
49,10051," http://www.sharethefacts.org/"," ['2 System Architecture and Components', '2.5 Claim Matcher']","The fact-check repository is composed of the Share-the-facts [Cite_Footnote_10] fact checks as well as fact checks collected from several fact-checking organizations like PolitiFact, Snopes, factcheck.org, Washington Post, etc.",10 http://www.sharethefacts.org/,"ClaimPortal leverages the claim matching func-tion in the ClaimBuster API. The fact-check repository is composed of the Share-the-facts [Cite_Footnote_10] fact checks as well as fact checks collected from several fact-checking organizations like PolitiFact, Snopes, factcheck.org, Washington Post, etc. The system measures the similarity between a claim and a fact-check based on the similarity of their tokens. An Elasticsearch server is deployed for searching the repository based on token similarity.",Method,Tool,False,Introduce（引用目的）,True,P19-3026_10_0,2019,"ClaimPortal: Integrated Monitoring, Searching, Checking, and Analytics of Factual Claims on Twitter",Footnote
50,10052," https://github.com/AntNLP/gnn-dep-parsing"," ['4 Experiments']",The default setting for our final parser is a 2-layer GNN model that uses hd ▷ h (Equation 8) aggregating function and “H-first” asynchronous update method (Equation 9). [Cite_Footnote_6],6 Our implementation is publicly available at: https: //github.com/AntNLP/gnn-dep-parsing,The default setting for our final parser is a 2-layer GNN model that uses hd ▷ h (Equation 8) aggregating function and “H-first” asynchronous update method (Equation 9). [Cite_Footnote_6],Method,Code,True,Produce（引用目的）,True,P19-1237_0_0,2019,Graph-based Dependency Parsing with Graph Neural Networks,Footnote
51,10053," https://github.com/facebookresearch/fastText"," ['4 Experiments', '4.1 Main Results']",We use FastText multilingual pretrained vectors instead of Glove vectors. [Cite_Footnote_8],8 https://github.com/facebookresearch/ fastText,"Finally, we report the results of our model on partial UD treebanks on the CoNLL 2018 shared task (Table 5). Our model uses only word and XPOS tag (predict by UDPipe), without any cross lingual features. We use FastText multilingual pretrained vectors instead of Glove vectors. [Cite_Footnote_8] The results show that our GNN parser performs better on 10 UD 2.2 treebanks. For bg, our parser does not improve performance. For nl, our parser im-proves 0.22 UAS, although LAS is slightly lower than the baseline parser. For average performance, it achieves 0.24 percent UAS and 0.28 percent LAS improvement over the baseline parser.",Method,Code,True,Use（引用目的）,True,P19-1237_1_0,2019,Graph-based Dependency Parsing with Graph Neural Networks,Footnote
52,10054," http://hdl.handle.net/11234/1-1983xxx"," ['4 Experiments']","We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [Cite_Ref] tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018).","Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.","We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [Cite_Ref] tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=§2-21, dev=§22, test=§23), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy ≈ 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task’s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016).",Material,Knowledge,True,Use（引用目的）,True,P19-1237_2_0,2019,Graph-based Dependency Parsing with Graph Neural Networks,Reference
53,10055," https://github.com/JiaweiSheng/FAAN"," ['References']",The source code is available at [Cite] https://github.com/JiaweiSheng/FAAN.,,"Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a rela-tion given its few-shot reference entity pairs. Recent attempts solve this problem by learn-ing static representations of entities and refer-ences, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, en-tities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mecha-nism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acqui-sition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at [Cite] https://github.com/JiaweiSheng/FAAN.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.131_0_0,2020,Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,Body
54,10056," https://github.com/xwhan/One-shot-Relational-Learning"," ['5 Experiments', '5.1 Datasets']",We conduct experiments on two public benchmark datasets: NELL and Wiki [Cite_Footnote_1] .,1 https://github.com/xwhan/ One-shot-Relational-Learning,"We conduct experiments on two public benchmark datasets: NELL and Wiki [Cite_Footnote_1] . In both datasets, re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks. There are 67 and 183 tasks in NELL and Wiki, re-spectively. We use original 51/5/11 and 133/16/34 relations in NELL and Wiki, respectively, for train-ing/validation/testing as defined in Section 3. More-over, for each task relation, both datasets also pro-vide candidate entities, which are constructed based on the entity type constraint (Xiong et al., 2018). More details are shown in Table 1.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.131_1_0,2020,Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,Footnote
55,10057," https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch"," ['5 Experiments', '5.3 Implementation Details']","Our implementation for KG embedding baselines is based on OpenKE [Cite_Footnote_2] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature.",2 https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch,"We perform 5-shot KG completion task for all the methods. Our implementation for KG embedding baselines is based on OpenKE [Cite_Footnote_2] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature. During training, all triples in back-ground KG G 0 and training set, as well as few-shot reference triples of validation and testing set are used to train models. For few-shot relational learn-ing baselines, we extend GMatching from original one-shot scenario to few-shot scenario by three set-tings: obtaining general reference representation by mean/max pooling (denoted as MeanP/MaxP) over references, or taking the reference that leads to the maximal similarity score to the query (denoted as Max). Because FSRL was reported in completely different experimental settings, we reimplement the model to make a fair comparison. We directly re-port the original results of MetaR with pre-trained embeddings to avoid re-implementation bias.",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.131_2_0,2020,Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,Footnote
56,10058," http://www.weibo.com/"," ['4 Experiments', '4.1 Dataset']","To evaluate the effectiveness of our proposed per-sonalized WAE model (PersonaWAE), we collect a dataset from an open online chatting forum, i.e., Weibo [Cite_Footnote_2] , which contains massive multi-turn con-versation sessions and user identification informa-tion.",2 http://www.weibo.com/,"To evaluate the effectiveness of our proposed per-sonalized WAE model (PersonaWAE), we collect a dataset from an open online chatting forum, i.e., Weibo [Cite_Footnote_2] , which contains massive multi-turn con-versation sessions and user identification informa-tion. Overall, there are 31,128,520 utterances in the raw dataset with corresponded user identifica-tions. To construct the personalized conversation systems, we retrieve users with more than 14 utter-ances from the raw Weibo corpus. We also filtrate conversation sessions with less than 2 turns for training multi-turn conversation systems. We use a sliding window with a size of 3 to construct each dialogue session and there are 3 utterances in each dialogue session. By doing so, there are 336,342 conversation sessions in the cleaned corpus. We remove emojis in utterances and utilize NLTK for tokenization. Then, we randomly split the Weibo corpus into 335,342/5,000/5,000 sessions as train-ing/validation/testing sets. For each session, the last utterance is the target response for generation while other utterances are treated as context.",Material,DataSource,True,Produce（引用目的）,True,D19-1201_0_0,2019,Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders,Footnote
57,10059," https://github.com/tmikolov/word2vec"," ['4 Experiments', '4.3 Settings']","The dimension of word embeddings is set to 200, which is initialized with pre-trained word2vec vectors [Cite_Footnote_3] .",3 https://github.com/tmikolov/word2vec,"The dimension of word embeddings is set to 200, which is initialized with pre-trained word2vec vectors [Cite_Footnote_3] . The vocabulary is comprised of the most frequent 31,000 words. The sentence encoder and the context encoder in our PersonaWAE model are two bi-directional RNN with the GRU cells, re-spectively. The decoder consists of a one-layer RNN with GRUs. The hidden state sizes of both GRU encoder and decoder are set to 256. Each user is allocated a user-level vector representation with dimension size 512. We set the mini-batch size to 100. The SGD optimizer is used to train the autoencoder module with the initial learning rate 1.0, and the learning rate decay strategy is employed. We use RMSprop optimizer (Hinton et al., 2012) to update the parameters of the gener-ator and the discriminator, where the initial learn-ing rates are set to 5e-5 and 1e-5, respectively. The gradient penalty is used for training discriminator (Gulrajani et al., 2017). The value of τ in Gumbel softmax is set to 0.1.",Method,Code,True,Use（引用目的）,True,D19-1201_1_0,2019,Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders,Footnote
58,10060," http://www.nltk.org/_modules/nltk/translate/bleu_score.html"," ['4 Experiments', '4.4 Evaluation Metrics']","Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [Cite_Footnote_4] .",4 http://www.nltk.org/_modules/nltk/translate/bleu_score.html,"Overlap-based Metric. We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated re-sponse. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [Cite_Footnote_4] . For each testing context, we sam-ple 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEU-Precision), n-gram recall (BLEU-Recall), and n-gram F1 (BLEU-F1).",Method,Code,True,Use（引用目的）,True,D19-1201_2_0,2019,Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders,Footnote
59,10061," http://nlp.stanford.edu/software/lex-parser.shtml"," ['4 Experimentation', '4.2 Experimental Settings']","In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [Cite_Footnote_2] .",2 http://nlp.stanford.edu/software/lex-parser.shtml,"In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [Cite_Footnote_2] . Specially, we train the parser on the GENIA Treebank 1.0 (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.",Method,Tool,True,Use（引用目的）,True,D16-1078_0_0,2016,Speculation and Negation Scope Detection via Convolutional Neural Networks,Footnote
60,10062," http://www.geniaproject.org/genia-corpus/treebank"," ['4 Experimentation', '4.2 Experimental Settings']","Specially, we train the parser on the GENIA Treebank 1.0 [Cite_Footnote_3] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus.",3 http://www.geniaproject.org/genia-corpus/treebank,"In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser . Specially, we train the parser on the GENIA Treebank 1.0 [Cite_Footnote_3] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.",Material,Knowledge,True,Use（引用目的）,True,D16-1078_1_0,2016,Speculation and Negation Scope Detection via Convolutional Neural Networks,Footnote
61,10063," https://code.google.com/archive/p/word2vec/"," ['4 Experimentation', '4.2 Experimental Settings']","The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [Cite_Footnote_4] (Mikolov et al., 2013).",4 https://code.google.com/archive/p/word2vec/,"For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, λ=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [Cite_Footnote_4] (Mikolov et al., 2013).",Method,Code,True,Use（引用目的）,True,D16-1078_2_0,2016,Speculation and Negation Scope Detection via Convolutional Neural Networks,Footnote
62,10064," http://mallet.cs.umass.edu/"," ['4 Experimentation', '4.3 Experimental Results on Abstracts']",The imbalance between positive and negative instances has negative effects on both the baseline and the 5 [Cite] http://mallet.cs.umass.edu/ CNN-based models for negation scope detection.,,"Table 2 illustrates that the performance of spec-ulation scope detection is higher than that of nega-tion (Best PCS: 85.75% vs 77.14%). It is mainly attributed to the shorter scopes of negation cues. Under the circumstances that the average length of negation sentences is almost as long as that of speculation ones (29.28 vs 29.77), shorter negation scopes mean that more tokens do not belong to the scopes, indicating more negative instances. The imbalance between positive and negative instances has negative effects on both the baseline and the 5 [Cite] http://mallet.cs.umass.edu/ CNN-based models for negation scope detection.",Material,Knowledge,True,Use（引用目的）,True,D16-1078_3_0,2016,Speculation and Negation Scope Detection via Convolutional Neural Networks,Body
63,10065," https://github.com/doug919/entity_based_narrative_graph"," ['1 Introduction']",Our code and trained models are pub-licly available [Cite_Footnote_1] .,1 https://github.com/doug919/entity_ based_narrative_graph,"The evaluated downstream tasks include two challenging narrative analysis tasks, predicting characters’ psychological states (Rashkin et al., 2018) and desire fulfilment (Rahimtoroghi et al., 2017). Results show that our model can outperform competitive transformer-based representations of the narrative text, suggesting that explicitly model-ing the relational structure of entities and events is beneficial. Our code and trained models are pub-licly available [Cite_Footnote_1] .",Material,Knowledge,True,Produce（引用目的）,True,2021.naacl-main.391_0_0,2021,Modeling Human Mental States with an Entity-based Narrative Graph,Footnote
64,10066," https://dev.twitter.com/"," ['3 Experiments and Evaluations']",The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [Cite_Footnote_3] .,3 https://dev.twitter.com/,The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [Cite_Footnote_3] . The statistics are shown in Table 1.,Method,Code,True,Use（引用目的）,True,P13-2101_0_0,2013,Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics,Footnote
65,10067," https://github.com/tagoyal/sow-reap-paraphrasing"," ['References']","Our evaluation, both au-tomatic and human, shows that the proposed system retains the quality of the baseline ap-proaches while giving a substantial increase in the diversity of the generated paraphrases. [Cite_Footnote_1]",1 Data and code are available at https://github.com/tagoyal/sow-reap-paraphrasing,"Paraphrasing natural language sentences is a multifaceted process: it might involve replac-ing individual words or short phrases, local re-arrangement of content, or high-level restruc-turing like topicalization or passivization. Past approaches struggle to cover this space of para-phrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic trans-formations to softly “reorder” the source sen-tence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both au-tomatic and human, shows that the proposed system retains the quality of the baseline ap-proaches while giving a substantial increase in the diversity of the generated paraphrases. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.22_0_0,2020,Neural Syntactic Preordering for Controlled Paraphrase Generation,Footnote
66,10068," https://github.com/luyaojie/text2event"," ['1 Introduction']","We conducted experiments [Cite_Footnote_1] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings.",1 Our source codes are openly available at https://github.com/luyaojie/text2event,"We conducted experiments [Cite_Footnote_1] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings. In summary, the contri-butions are as follows:",Method,Code,True,Produce（引用目的）,True,2021.acl-long.217_0_0,2021,T EXT 2E VENT : Controllable Sequence-to-Structure Generation for End-to-end Event Extraction,Footnote
67,10069," https://archive.org/download/"," ['3 Analysis Setup', '3.1 Experiment Procedure']","All intermediate pre-training is done with input sequence length of 128, batch size of 2048, learning rate of 0.0001, up to a total num-ber of 100, 000 updates, using Wikipedia snapshot from December 20, 2018 [Cite_Footnote_1] .",1 The snapshot is available at https://archive.org/download/enwiki-20181220. Wikipedia is licensed under CC BY-SA 3.0.,"Our goal is to analyze the influence in downstream task performance brought by different masking policies g(.;φ) during intermediate pre-training. Towards this goal, we ensure that the only vari-able is the masking policy, while all other aspects are controlled, so that the downstream performance reveal the influence we aim to study. We first initial-ize with a BART-base model (Lewis et al., 2020); then for each masking policy, we conduct experi-ments following a two-stage pipeline: Stage 1. Intermediate Pre-training. We per-form intermediate pre-training with a given mask-ing policy g(.; φ). All intermediate pre-training is done with input sequence length of 128, batch size of 2048, learning rate of 0.0001, up to a total num-ber of 100, 000 updates, using Wikipedia snapshot from December 20, 2018 [Cite_Footnote_1] .",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.573_0_0,2021,On the Influence of Masking Policies in Intermediate Pre-training,Footnote
68,10070," http://commoncrawl.org/2016/10/newsdatasetavailable"," ['mixture of corpus 5 used to pre-train BART.']","5 Similar to RoBERTa, BART uses the combination of BookCorpus (Zhu et al., 2015), CC-News (Nagel, 2016) [Cite_Ref] , OpenWebText (Gokaslan and Cohen, 2019), and Stories (Trinh and Le, 2018) as pre-training corpus.",Sebastian Nagel. 2016. Cc-news. URL: http://web.archive. org/save/http://commoncrawl.org/2016/10/newsdatasetavailable.,"5 Similar to RoBERTa, BART uses the combination of BookCorpus (Zhu et al., 2015), CC-News (Nagel, 2016) [Cite_Ref] , OpenWebText (Gokaslan and Cohen, 2019), and Stories (Trinh and Le, 2018) as pre-training corpus.",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.573_1_0,2021,On the Influence of Masking Policies in Intermediate Pre-training,Reference
69,10071," https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py"," ['D Reproducibility', 'D.1 Dataset Details']","We obtain closed-book QA datasets from [Cite] https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py.",,"We obtain closed-book QA datasets from [Cite] https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set.",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.573_2_0,2021,On the Influence of Masking Policies in Intermediate Pre-training,Body
70,10072," https://github.com/facebookresearch/KILT/blob/master/scripts/donwload"," ['D Reproducibility', 'D.1 Dataset Details']","We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from [Cite] https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py.",,"We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from [Cite] https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets (https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set.",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.573_3_0,2021,On the Influence of Masking Policies in Intermediate Pre-training,Body
71,10073," https://huggingface.co/datasets"," ['D Reproducibility', 'D.1 Dataset Details']","We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets ( [Cite] https://huggingface.co/datasets).",,"We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets ( [Cite] https://huggingface.co/datasets). For more details, see Table 6. KILT hosts the test set evaluation on its leaderboard and the test set annotations are not publicly available; therefore we report performance on dev set in Table 2. The test set annotations for ROPES is not publicly available, so we take 50% of original dev set as the new dev set, and the other 50% as the new test set.",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.573_4_0,2021,On the Influence of Masking Policies in Intermediate Pre-training,Body
72,10074," http://www.allenai.org/paper-appendix/emnlp2016-p3"," ['5 Evaluation']","Code, data and supplementary material for this paper are available at: [Cite] http://www.allenai.",,"Code, data and supplementary material for this paper are available at: [Cite] http://www.allenai.org/paper-appendix/emnlp2016-p3",Mixed,Mixed,True,Use（引用目的）,False,D16-1016_0_0,2016,Semantic Parsing to Probabilistic Programs for Situated Question Answering,Body
73,10075," https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py"," ['3 Experiments']","We present complete effectiveness graphs and details of human evaluation in Appendix B and C. BAE is implemented [Cite_Footnote_1] in TextAttack (Morris et al., 2020), a popular suite of NLP adversarial attacks.",1 https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py,"We present complete effectiveness graphs and details of human evaluation in Appendix B and C. BAE is implemented [Cite_Footnote_1] in TextAttack (Morris et al., 2020), a popular suite of NLP adversarial attacks.",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.498_0_0,2020,BAE: BERT-based Adversarial Examples for Text Classification,Footnote
74,10076," https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"," ['A Experimental Reproducibility']",• SUBJ: A dataset for classifying a sentence as objective or subjective. [Cite_Footnote_2],2 https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences,• SUBJ: A dataset for classifying a sentence as objective or subjective. [Cite_Footnote_2],Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.498_1_0,2020,BAE: BERT-based Adversarial Examples for Text Classification,Footnote
75,10077," https://www.cs.cornell.edu/people/pabo/movie-review-data/"," ['A Experimental Reproducibility']",• MR: A movie reviews dataset based on sub-jective rating and sentiment polarity [Cite_Footnote_3] .,3 https://www.cs.cornell.edu/people/pabo/movie-review-data/,• MR: A movie reviews dataset based on sub-jective rating and sentiment polarity [Cite_Footnote_3] .,Material,Dataset,True,Use（引用目的）,False,2020.emnlp-main.498_2_0,2020,BAE: BERT-based Adversarial Examples for Text Classification,Footnote
76,10078," http://mpqa.cs.pitt.edu/"," ['A Experimental Reproducibility']",An unbalanced dataset for polarity detection of opinions [Cite_Footnote_4] .,4 http://mpqa.cs.pitt.edu/,• MPQA: An unbalanced dataset for polarity detection of opinions [Cite_Footnote_4] .,Material,Dataset,True,Use（引用目的）,False,2020.emnlp-main.498_3_0,2020,BAE: BERT-based Adversarial Examples for Text Classification,Footnote
77,10079," http://cogcomp.org/Data/QA/QC/"," ['A Experimental Reproducibility']",A dataset for classifying types of ques-tions with 6 classes [Cite_Footnote_5] .,5 http://cogcomp.org/Data/QA/QC/,• TREC: A dataset for classifying types of ques-tions with 6 classes [Cite_Footnote_5] .,Material,Dataset,True,Use（引用目的）,False,2020.emnlp-main.498_4_0,2020,BAE: BERT-based Adversarial Examples for Text Classification,Footnote
78,10080," https://github.com/decomp-sem/neural-sprl"," ['1 Introduction']",achieves state-of-the-art performance for SPRL. [Cite_Footnote_2],2 Implementation available at https://github.com/decomp-sem/neural-sprl.,"Figure 1 : BiLSTM sentence encoder with SPR de-coder. Semantic proto-role labeling is with respect to a specific predicate and argument within a sen-tence, so the decoder receives the two correspond-ing hidden states. achieves state-of-the-art performance for SPRL. [Cite_Footnote_2] As depicted in Figure 1, our model’s architecture is an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training. We include a thorough quanti-tative analysis highlighting the contrasting errors between the proposed model and previous (non-neural) state-of-the-art.",Method,Code,True,Produce（引用目的）,False,D18-1114_0_0,2018,Neural-Davidsonian Semantic Proto-role Labeling,Footnote
79,10081," https://nlp.stanford.edu/projects/glove/"," ['3 “Neural-Davidsonian” Model']","As an adapted BiLSTM, our model easily ex- 2017) trained on the [Cite_Footnote_10]","10 300-dimensional, uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/; 15,533 out-of-vocabulary words across all datasets were assigned a random embedding (uniformly from [− 01 01]).. , . Embeddings remained fixed during training.","There are a few noteworthy differences between our neural model and the CRF of prior work. As an adapted BiLSTM, our model easily ex- 2017) trained on the [Cite_Footnote_10] Fr-En corpus (Callison-Burch et al., 2009) (Appendix A). ploits the benefits of large-scale pretraining, in the form of GloVe embeddings and MT pretrain-ing, both absent in the CRF. Ablation experiments (Appendix A) show the advantages conferred by these features. In contrast, the discrete-featured CRF model makes use of gold dependency labels, as well as joint modeling of SPR attribute pairs with explicit joint factors, both absent in our neu-ral model. Future SPRL work could explore the use of models like the LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016) to combine the advan-tages of both paradigms.",Material,Knowledge,False,Use（引用目的）,True,D18-1114_1_0,2018,Neural-Davidsonian Semantic Proto-role Labeling,Footnote
80,10082," http://www.kaggle.com/c/asap-sas"," ['1 Introduction']","To our knowledge, there is little if any research investigat-ing the value of stacking for NLP applications such as automated scoring. [Cite_Footnote_1]","1 Some applications have used stacking but not analyzed its value. For example, many participants used stacking in the ASAP2 competition http://http://www.kaggle.com/c/asap-sas. Also, Heilman and Madnani (2013) used stacking for Task 7 of SemEval 2013.","Therefore, we apply stacked generalization (i.e. stacking) (Wolpert, 1992; Sakkis et al., 2001; Tor-res Martins et al., 2008) to build an ensemble of the response- and reference-based approaches. To our knowledge, there is little if any research investigat-ing the value of stacking for NLP applications such as automated scoring. [Cite_Footnote_1]",補足資料,Website,False,Introduce（引用目的）,True,N15-1111_0_0,2015,Effective Feature Integration for Automated Short Answer Scoring ∗,Footnote
81,10083," https://github.com/EducationalTestingService/skll"," ['3 Models for Short Answer Scoring']","All models use support vector regression (SVR) (Smola and Schölkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [Cite_Footnote_2]","2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.","Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Schölkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [Cite_Footnote_2]",Method,Tool,True,Use（引用目的）,True,N15-1111_1_0,2015,Effective Feature Integration for Automated Short Answer Scoring ∗,Footnote
82,10084," http://verbs.colorado.edu/~mpalmer/projects/ace.html"," ['3 Models for Short Answer Scoring', '3.1 Response-based']",• semantic roles in the form of PropBank [Cite_Footnote_3] style (e.g. say.01-A0-boy for “(the) boy said”),3 http://verbs.colorado.edu/˜mpalmer/projects/ace.html,• semantic roles in the form of PropBank [Cite_Footnote_3] style (e.g. say.01-A0-boy for “(the) boy said”),Method,Code,True,Use（引用目的）,False,N15-1111_2_0,2015,Effective Feature Integration for Automated Short Answer Scoring ∗,Footnote
83,10085," http://www.clearnlp.com,v2.0.2"," ['3 Models for Short Answer Scoring', '3.1 Response-based']",The syntactic and semantic features were extracted using the ClearNLP parser. [Cite_Footnote_4],"4 http://www.clearnlp.com,v2.0.2",The syntactic and semantic features were extracted using the ClearNLP parser. [Cite_Footnote_4] We used the default models and options for the parser. We treat this model as a strong baseline to which we will add reference-based features.,Method,Tool,True,Use（引用目的）,True,N15-1111_3_0,2015,Effective Feature Integration for Automated Short Answer Scoring ∗,Footnote
84,10086," http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt"," ['5 Evaluation', '5.1 Methodology']","The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [Cite_Footnote_6]",6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt,"In order to evaluate the performance of our tensor-based factorization model of compositionality, we make use of the sentence similarity task for transi-tive sentences, defined in Grefenstette and Sadrzadeh (2011a). This is an extension of the similarity task for compositional models developed by Mitchell and Lapata (2008), and constructed according to the same guidelines. The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [Cite_Footnote_6]",Material,Dataset,True,Produce（引用目的）,False,N13-1134_0_0,2013,A Tensor-based Factorization Model of Semantic Compositionality,Footnote
85,10087," https://github.com/marziehf/DataAugmentationNMT"," ['5 Experiments', '5.5 Translation Result']",We follow the official implementation [Cite_Footnote_3] of TDA.,3 https://github.com/marziehf/ DataAugmentationNMT,"We compare TCWR with six baselines, Word-Dropout, BPEDropout, SwitchOut, SCDA, TDA and DADA. For WordDropout and BPEDropout, we perform a range search on its dropout proba-bility from 0 to 1 and select the best one on de-velopment sets. Similarly, we choose the temper-ature with the highest score on development sets for SwitchOut. For SCDA, we search the replacing probability and set it to 0.15. We follow the official implementation [Cite_Footnote_3] of TDA. We reuse the hyperpa-rameters from Cheng et al. (2019) for DADA.",Method,Code,False,Use（引用目的）,True,2021.naacl-main.18_1_0,2021,Counterfactual Data Augmentation for Neural Machine Translation,Footnote
86,10088," https://github.com/duyvuleo/VNTC"," ['5 Experiments', '5.6 Backtranslation Result']","To perform backtransla-tion, we use the monolingual sequences from News Crawl 2017, News Crawl 2010 and VNTC [Cite_Footnote_4] for En-Tr, En-De and En-Vi, respectively.",4 https://github.com/duyvuleo/VNTC,"As backtranslation is a widely-used data augmenta-tion method by utilizing monolingual data to gener-ate new parallel pairs, we show how TCWR can be used with backtranslation. To perform backtransla-tion, we use the monolingual sequences from News Crawl 2017, News Crawl 2010 and VNTC [Cite_Footnote_4] for En-Tr, En-De and En-Vi, respectively. Then we per-form data augmentation on both training data and backtranslated data. As shown in Table 5, TCWR improves upon backtranslation, demonstrating that TCWR and backtranslation are not mutually exclu-sive, and TCWR can enhance the performance of backtranslation.",Material,DataSource,True,Use（引用目的）,True,2021.naacl-main.18_2_0,2021,Counterfactual Data Augmentation for Neural Machine Translation,Footnote
87,10089," https://github.com/fxsjy/jieba"," ['3 Embeddings for Chinese Text']",We run a Chinese word segmentation system [Cite_Footnote_7] over the raw corpus of Weibo messages.,7 We use Jieba for segmentation: https://github.com/fxsjy/jieba,"Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation system [Cite_Footnote_7] over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word. Since the NER system tags char-acters, we add the same word embedding features to each character in the word.",Material,Knowledge,True,Use（引用目的）,True,D15-1064_0_0,2015,Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings,Footnote
88,10090," https://github.com/hltcoe/golden-horse"," ['4 Weibo NER Corpus']",We make our code and the annotated corpus available. [Cite_Footnote_10],10 https://github.com/hltcoe/golden-horse,"Table 1 shows statistics of the final corpus. We divided the corpus into 7 folds, each with 127 mes-sages, where each message corresponds to a single instance. We use the first 5 folds for train, the 6th for development, and the 7th for test. We make our code and the annotated corpus available. [Cite_Footnote_10]",Mixed,Mixed,True,Produce（引用目的）,True,D15-1064_1_0,2015,Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings,Footnote
89,10091," https://github.com/abisee/cnn-dailymail"," ['4 Experiments', '4.1 Dataset']","For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [Cite_Footnote_1] and fairseq .",1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail,"We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [Cite_Footnote_1] and fairseq .",Material,DataSource,True,Use（引用目的）,True,2021.naacl-srw.20_0_0,2021,Hie-BART: Document Summarization with Hierarchical BART,Footnote
90,10092," https://github.com/pytorch/fairseq/tree/master/examples/bart"," ['4 Experiments', '4.1 Dataset']","For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [Cite_Footnote_2] .",2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart,"We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [Cite_Footnote_2] .",Method,Code,True,Use（引用目的）,True,2021.naacl-srw.20_1_0,2021,Hie-BART: Document Summarization with Hierarchical BART,Footnote
91,10093," https://github.com/pltrdy/files2rouge"," ['4 Experiments', '[Gold Summary]']",used files2rouge [Cite_Footnote_3] .,3 files2rouge usage : https://github.com/pltrdy/files2rouge,"About a dozen Native American actors walk off set of Adam Sandler comedy, says report . Actors say satirical Western’s script is insulting to Native Americans and women . used files2rouge [Cite_Footnote_3] . Hie-BART was compared with LEAD-3 (Nallapati et al., 2017), PTGEN, PT-GEN+COV (See et al., 2017), B ERT S UM E XT A BS (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART with our environment, and BART with Lewis et al. (2020). The LEAD-3 method uses the first three sentences of the source document as a summary. PTGEN is a sequence-to-sequence model that incorporates a pointer generator net-work. PTGEN+COV introduces the coverage mechanism into PTGEN. B ERT S UM E XT A BS is a pre-training model that adapts BERT for sum-marization tasks. T5 is a generalized pre-training model for sequence-to-sequence tasks based on the Transformer model. The statistical signifi-cance test was performed by the Wilcoxon-Mann-Whitney test. In Table 1, * and ** indicate that the comparisons with BART (ours) are statistically significant at 5% significance level and 10% sig-nificance level, respectively.",Method,Tool,True,Use（引用目的）,True,2021.naacl-srw.20_2_0,2021,Hie-BART: Document Summarization with Hierarchical BART,Footnote
92,10094," https://github.com/jerbarnes/sentiment_graphs"," ['1 Introduction']","Finally, we release the code and datasets [Cite_Footnote_3] to enable future work on this problem.",3 Code and datasets available at https://github.com/jerbarnes/sentiment_graphs.,"This perspective also allows us to unify a num-ber of approaches, including targeted, and opinion tuple mining. We aim to answer RQ1: whether graph-based approaches to structured sentiment outperform state-of-the-art sequence labeling ap-proaches, and RQ2: how to best encode structured sentiment as parsing graphs. We perform experi-ments on five standard datasets in four languages (English, Norwegian, Basque, Catalan) and show that graph-based approaches outperform state-of-the-art baselines on all datasets on several standard metrics, as well as our proposed novel (unlabeled and labeled) sentiment graph metrics. We further propose methods to inject linguistic structure into the sentiment graphs using syntactic dependencies. Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance, and 3) extensive experimental results that outper-form state-of-the-art baselines. Finally, we release the code and datasets [Cite_Footnote_3] to enable future work on this problem.",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.263_0_0,2021,Structured Sentiment Analysis as Dependency Graph Parsing,Footnote
93,10095," https://github.com/ruidan/IMN-E2E-ABSA"," ['4 Modeling', '4.3 Baselines']","We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN [Cite_Footnote_6] , RACL , as well as RACL-BERT, which also incorporates contextualized em-beddings.",6 IMN code available at https://github.com/ruidan/IMN-E2E-ABSA.,"We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN [Cite_Footnote_6] , RACL , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models. Note, however, that our model does not update the mBERT representa-tions, putting it at a disadvantage to RACL-BERT. We also compare with previously reported extrac-tion results from Barnes et al. (2018) and Øvrelid et al. (2020).",Method,Code,True,Compare（引用目的）,True,2021.acl-long.263_1_0,2021,Structured Sentiment Analysis as Dependency Graph Parsing,Footnote
94,10096," https://github.com/NLPWM-WHU/RACL"," ['4 Modeling', '4.3 Baselines']","We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN , RACL [Cite_Footnote_7] , as well as RACL-BERT, which also incorporates contextualized em-beddings.",7 https://github.com/NLPWM-WHU/RACL.,"We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN , RACL [Cite_Footnote_7] , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models. Note, however, that our model does not update the mBERT representa-tions, putting it at a disadvantage to RACL-BERT. We also compare with previously reported extrac-tion results from Barnes et al. (2018) and Øvrelid et al. (2020).",Method,Code,True,Compare（引用目的）,True,2021.acl-long.263_2_0,2021,Structured Sentiment Analysis as Dependency Graph Parsing,Footnote
95,10097," http://vectors.nlpl.eu/repository/"," ['6 Experiments']","All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [Cite_Footnote_8] (Fares et al., 2017).","8 Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokmål (id 58).","All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [Cite_Footnote_8] (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set (Targeted F 1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Ap-pendix) and run all of our models five times with different random seeds and report the mean (stan-dard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference be-tween the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3 shows the results for all datasets.",Material,DataSource,True,Use（引用目的）,True,2021.acl-long.263_3_0,2021,Structured Sentiment Analysis as Dependency Graph Parsing,Footnote
96,10098," https://github.com/jerbarnes/sentiment_graphs/src"," ['8 Conclusion']","GPU Infrastructure NVIDIA P100, 16 GiB RAM CPU Infrastructure Intel Xeon-Gold 6138 2.0 GHz Training duration 00:31:43 (MultiB EU ) – 07:40:54 (NoReC Fine ) Model implementation [Cite] https://github.com/jerbarnes/sentiment_graphs/src Hyperparameter Best assignment embedding Word2Vec",,"The computations were performed on resources scheme (y-axis) on the evaluation metrics (x-axis) for MultiB EU . percentage points for MultiB CA . GPU Infrastructure NVIDIA P100, 16 GiB RAM CPU Infrastructure Intel Xeon-Gold 6138 2.0 GHz Training duration 00:31:43 (MultiB EU ) – 07:40:54 (NoReC Fine ) Model implementation [Cite] https://github.com/jerbarnes/sentiment_graphs/src Hyperparameter Best assignment embedding Word2Vec SkipGram 100D contexualized embedding mBERT embeddings trainable False number of epochs 100 batch size 50 beta1 0 beta2 0.95 l2 3e-09 hidden lstm 200 hidden char lstm 100 layers lstm 3 dim mlp 200 dim embedding 100 dim char embedding 80 early stopping 0 pos style xpos attention bilinear model interpolation 0.5 loss interpolation 0.025 lstm implementation drop connect char implementation convolved emb dropout type replace bridge dpa+ dropout embedding 0.2 dropout edge 0.2 dropout label 0.3 dropout main recurrent 0.2 dropout recurrent char 0.3 dropout main ff 0.4 dropout char ff 0.3 dropout char linear 0.3",Method,Code,False,Use（引用目的）,True,2021.acl-long.263_4_0,2021,Structured Sentiment Analysis as Dependency Graph Parsing,Body
97,10099," http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking"," ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']","This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article’s text. [Cite_Footnote_2]",2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking,"For (ii), we apply entity linking to provide links from the sentence to additional entities (Milne and Witten, 2008). This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article’s text. [Cite_Footnote_2] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity. We restrict our search space to entities that are linked from within the source article of the sentence and from within articles to which the source article links. This way, our entity linking method achieves high precision as almost no disambiguation is necessary.",補足資料,Paper,True,Introduce（引用目的）,True,P15-1055_0_0,2015,Learning to Explain Entity Relationships in Knowledge Graphs,Footnote
98,10100," https://github.com/nickvosk/acl2015-"," ['5 Experimental setup', '5.1 Dataset']","Five human annotators provided relevance judg-ments, manually judging sentences based on how well they describe the relationship for an entity pair, for which we use a five-level graded rele-vance scale (perfect, excellent, good, fair, bad). [Cite_Footnote_5]",5 https://github.com/nickvosk/acl2015- dataset-learning-to-explain-entity- relationships,"Five human annotators provided relevance judg-ments, manually judging sentences based on how well they describe the relationship for an entity pair, for which we use a five-level graded rele-vance scale (perfect, excellent, good, fair, bad). [Cite_Footnote_5] Of all relevance grades 8.1% is perfect, 15.69% excellent, 19.98% good, 8.05% fair, and 48.15% bad. Out of 1 476 entity pairs, 1 093 have at least one sentence annotated as fair. As is common in information retrieval evaluation, we discard entity pairs that have only “bad” sentences. We examine the difficulty of the task for human annotators by measuring inter-annotator agreement on a subset of 105 sentences that are judged by 3 annotators. Fleiss’ kappa is k = 0.449, which is considered to be moderate agreement.",Material,Knowledge,True,Use（引用目的）,True,P15-1055_1_0,2015,Learning to Explain Entity Relationships in Knowledge Graphs,Footnote
99,10101," https://github.com/FilippoC/diffdp"," ['1 Introduction']","This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [Cite_Footnote_1]",1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.,"We study properties of our approach on a syn-thetic structure induction task and experiment on sentiment classification (Socher et al., 2013) and natural language inference (Bowman et al., 2015). Our experiments confirm that the structural bias encoded in our approach is beneficial. For ex-ample, our approach achieves a 4.9% improve-ment on multi-genre natural language inference (MultiNLI) over a structure-agnostic baseline. We show that stochastisticity and higher-order statis-tics given by the global inference are both impor-tant. In ablation experiments, we also observe that forcing the structures to be projective dependency trees rather than permitting any general graphs yields substantial improvements without sacrific-ing execution time. This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [Cite_Footnote_1] Our main contributions can be summarized as follows:",Method,Code,True,Use（引用目的）,False,P19-1551_0_0,2019,Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming,Footnote
100,10102," https://github.com/QAML/S3QACoreFramework"," ['5 Software Package']",Our cQA pipeline is available for download [Cite_Footnote_1] and is distributed under the terms of the Apache 2.0 Li-cense.,1 https://github.com/QAML/ S3QACoreFramework,"Our cQA pipeline is available for download [Cite_Footnote_1] and is distributed under the terms of the Apache 2.0 Li-cense. By taking advantage of the Apache Maven project management tool, most dependencies are automatically handled. The only exception is the UIMA framework toolkit. Still, its installation is straightforward. The pipeline is able to process natural language texts and metadata information associated with them and offers three main func-tionalities:",Method,Tool,False,Produce（引用目的）,True,P18-4023_0_0,2018,"A Flexible, Efficient and Accurate Framework for Community Question Answering Pipelines",Footnote
101,10103," http://www.kelp-ml.org"," ['5 Software Package']","Currently KeLP (Filice et al., 2018) [Cite_Footnote_2] is integrated in the pipeline.",2 http://www.kelp-ml.org,"Learning and classification allow to apply a variety of learning algorithms on vectorial or structured data. Currently KeLP (Filice et al., 2018) [Cite_Footnote_2] is integrated in the pipeline. KeLP allows to apply a growing number of kernel-based algo-rithms and kernel functions to perform unsuper-vised, online and batch supervised kernel meth-ods. We opt for integrating KeLP because the kernel-based cQA systems relying on it perform at state-of-the-art level (see Section 2). Our pipeline is able to reproduce the state-of-the-art models for SemEval cQA tasks 3-A and 3-B.",Method,Tool,True,Use（引用目的）,False,P18-4023_1_0,2018,"A Flexible, Efficient and Accurate Framework for Community Question Answering Pipelines",Footnote
102,10104," http://www.qatarliving.com/betasearch"," ['5 Software Package']",We imple-mented the technology described in Section 3 both for question and comment re-ranking [Cite_Footnote_3] .,3 http://www.qatarliving.com/betasearch,"Our pipeline has been used in a number of pro-totypes. Qatarliving.com is a forum where expats in Qatar may ask questions on a variety of dif-ferent topics and comment on them. We imple-mented the technology described in Section 3 both for question and comment re-ranking [Cite_Footnote_3] . Fig. 1 shows an example of usage: the user asks the question “Where can I buy a bike in Doha?”, the systems returns similar questions in the forum to-gether with the best overall comment. By click-ing on a question, the right panel shows the corre-sponding thread of comments with their relevance.",補足資料,Media,False,Introduce（引用目的）,False,P18-4023_2_0,2018,"A Flexible, Efficient and Accurate Framework for Community Question Answering Pipelines",Footnote
103,10105," http://cqa.iyas.qcri.org/cQA-Arabic-Demo"," ['5 Software Package']","A second example is a cQA demo [Cite_Footnote_4] in Arabic, which retrieves data from multiple medical forums from middle-east.",4 http://cqa.iyas.qcri.org/ cQA-Arabic-Demo,"A second example is a cQA demo [Cite_Footnote_4] in Arabic, which retrieves data from multiple medical forums from middle-east. In this case physicians answer to patients’ questions: the left panel shows a ques-tion from a user and the right panel similar ques-tions with the answers from the expert. In general there is only one (good) answer from the doctor, so this is mostly a question re-ranking task.",補足資料,Website,False,Introduce（引用目的）,True,P18-4023_3_0,2018,"A Flexible, Efficient and Accurate Framework for Community Question Answering Pipelines",Footnote
104,10106," https://badripatro.github.io/MDN-VQG/"," ['4 Method', '4.3 Cost function']",The code for MDN-VQG model is provided [Cite_Footnote_1] .,1 The project page for MDN-VQG Model is https://badripatro.github.io/MDN-VQG/,"Our objective is to minimize the total loss, that is the sum of cross entropy loss and triplet loss over all training examples. The total loss is: where M is the total number of samples,γ is a con-stant, which controls both the loss. L triplet is the triplet loss function 5. L cross is the cross entropy loss between the predicted and ground truth ques-tions and is given by: where, N is the total number of question tokens, y t is the ground truth label. The code for MDN-VQG model is provided [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,D18-1434_0_0,2018,Multimodal Differential Network for Visual Question Generation,Footnote
105,10107," http://nlp.cs.berkeley.edu"," ['6 Statistical Parsing Experiments']",We used the English and Chinese parsers in Petrov and Klein (2007) [Cite_Footnote_5] to generate all k-best lists and as our evaluation baseline.,5 Available at http://nlp.cs.berkeley.edu.,"We used the English and Chinese parsers in Petrov and Klein (2007) [Cite_Footnote_5] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270). This modified grammar was used to generate the k-best lists that we trained our model on. However, as we tested on the same set of articles used for monolingual Chi-nese parser evaluation, there was no need to use a modified grammar to generate k-best lists at test time, and so we used a regularly trained Chinese parser for this purpose.",Material,DataSource,False,Use（引用目的）,True,D08-1092_0_0,2008,Two Languages are Better than One (for Syntactic Parsing),Footnote
106,10108," http://nlp.cs.berkeley.edu"," ['6 Statistical Parsing Experiments']","Posterior word alignment probabilities were ob-tained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [Cite_Footnote_6] , trained on approxi-mately 1.7 million sentence pairs.",6 Available at http://nlp.cs.berkeley.edu.,"Posterior word alignment probabilities were ob-tained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [Cite_Footnote_6] , trained on approxi-mately 1.7 million sentence pairs. For our alignment model we used an HMM in each direction, trained to agree (Liang et al., 2006), and we combined the pos-teriors using DeNero and Klein’s (2007) soft union method.",Material,DataSource,False,Use（引用目的）,True,D08-1092_1_0,2008,Two Languages are Better than One (for Syntactic Parsing),Footnote
107,10109," http://www.phontron.com/kytea/"," ['4 Experiments', '4.1 Setup']","Specifically, among the [Cite_Footnote_2] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).",2 http://www.phontron.com/kytea/,"Preprocessings For Japanese sentences, we per-formed tokenization using KyTea 0.4.7 2 (Neu-big et al., 2011). Then we performed bunsetsu-chunking with J.DepP 2015.10.05 3 (Yoshinaga and Kitsuregawa, 2009, 2010, 2014). Special end-of-chunk tokens were inserted at the end of the chunks. Our word-level decoders described in § will stop generating words after each end-of-chunk token. For English sentences, we per-formed the same preprocessings described on the WAT ’16 Website. To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the [Cite_Footnote_2] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data.",Material,DataSource,True,Use（引用目的）,True,P17-1174_0_0,2017,Chunk-based Decoder for Neural Machine Translation,Footnote
108,10110," http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jdepp/"," ['4 Experiments', '4.1 Setup']",Our word-level decoders described in § [Cite_Footnote_3] will stop generating words after each end-of-chunk token.,3 http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/jdepp/,"Preprocessings For Japanese sentences, we per-formed tokenization using KyTea 0.4.7 2 (Neu-big et al., 2011). Then we performed bunsetsu-chunking with J.DepP 2015.10.05 3 (Yoshinaga and Kitsuregawa, 2009, 2010, 2014). Special end-of-chunk tokens were inserted at the end of the chunks. Our word-level decoders described in § [Cite_Footnote_3] will stop generating words after each end-of-chunk token. For English sentences, we per-formed the same preprocessings described on the WAT ’16 Website. To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1174_1_0,2017,Chunk-based Decoder for Neural Machine Translation,Footnote
109,10111," http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html"," ['4 Experiments', '4.1 Setup']","For English sentences, we per-formed the same preprocessings described on the WAT ’16 Website. [Cite_Footnote_4]",4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html,"Preprocessings For Japanese sentences, we per-formed tokenization using KyTea 0.4.7 2 (Neu-big et al., 2011). Then we performed bunsetsu-chunking with J.DepP 2015.10.05 3 (Yoshinaga and Kitsuregawa, 2009, 2010, 2014). Special end-of-chunk tokens were inserted at the end of the chunks. Our word-level decoders described in § will stop generating words after each end-of-chunk token. For English sentences, we per-formed the same preprocessings described on the WAT ’16 Website. [Cite_Footnote_4] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data.",補足資料,Website,True,Introduce（引用目的）,True,P17-1174_2_0,2017,Chunk-based Decoder for Neural Machine Translation,Footnote
110,10112," https://github.com/moses-smt/mgiza"," ['4 Experiments', '4.1 Setup']","The dictionary was extracted with the MGIZA++ 0.7.0 [Cite_Footnote_5] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.",5 https://github.com/moses-smt/mgiza,"Postprocessing To perform unknown word re-placement (Luong et al., 2015a), we built a bilin-gual English-Japanese dictionary from all of the three million translation pairs. The dictionary was extracted with the MGIZA++ 0.7.0 [Cite_Footnote_5] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.",Method,Tool,True,Use（引用目的）,True,P17-1174_3_0,2017,Chunk-based Decoder for Neural Machine Translation,Footnote
111,10113," http://www.statmt.org/moses/"," ['4 Experiments', '4.1 Setup']","The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 [Cite_Footnote_6] (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 (Isozaki et al., 2010).",6 http://www.statmt.org/moses/,"Evaluation Following the WAT ’16 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 [Cite_Footnote_6] (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 (Isozaki et al., 2010). Follow-ing Cho et al. (2014a), we performed beam search with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set dur-ing training and used them to evaluate the systems with the test set.",Method,Tool,True,Use（引用目的）,True,P17-1174_4_0,2017,Chunk-based Decoder for Neural Machine Translation,Footnote
112,10114," http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html"," ['4 Experiments', '4.1 Setup']","The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [Cite_Footnote_7] (Isozaki et al., 2010).",7 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html,"Evaluation Following the WAT ’16 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [Cite_Footnote_7] (Isozaki et al., 2010). Follow-ing Cho et al. (2014a), we performed beam search with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set dur-ing training and used them to evaluate the systems with the test set.",Method,Tool,True,Use（引用目的）,True,P17-1174_5_0,2017,Chunk-based Decoder for Neural Machine Translation,Footnote
113,10115," http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation"," ['4 Experiments', '4.1 Setup']",Full results are available on the WAT ’16 Website. [Cite_Footnote_10],10 http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation,"Table 3: The settings and results of the baseline systems and our systems. |V src | and |V trg | denote the vocabulary size of the source language and the target language, respectively. d emb and d hid are the dimension size of the word embeddings and hidden states, respectively. Only single NMT models (w/o ensembling) reported in WAT ’16 are listed here. Full results are available on the WAT ’16 Website. [Cite_Footnote_10]",補足資料,Website,True,Introduce（引用目的）,True,P17-1174_6_0,2017,Chunk-based Decoder for Neural Machine Translation,Footnote
114,10116," https://github.com/pstadler1990/nmt_paper21_appendix"," ['4 Experiment setup', '4.1 Test suite setup']","The raw test items, as well as the translations eval-uated can be found in our repository [Cite_Footnote_1] .",1 https://github.com/pstadler1990/nmt_ paper21_appendix,"For the development and application of the test suite we used the tool TQ-AutoTest (Macketanz et al., 2018a). We created 10 sentences per phe-nomenon, resulting in a total of 585 sentences, ex-amining 49 phenomena organised in 13 categories. The raw test items, as well as the translations eval-uated can be found in our repository [Cite_Footnote_1] . The phe-nomena selected for this experiment are a subset of the ones of German→English MT, as described in Macketanz et al. (2018b) and Avramidis et al. (2020), adapted to the opposite language direction. An extract of the used sentences can be found in table 5.",補足資料,Website,True,Introduce（引用目的）,True,2021.acl-srw.20_0_0,2021,Observing the Learning Curve of Neural Machine Translation with regard to Linguistic Phenomena,Footnote
115,10117," https://github.com/marian-nmt/marian-examples/blob/master/transformer"," ['4 Experiment setup', '4.3 Training setup']","Transformer We used the same training, dev and test sets as in the big RNN model, and the exam-ple configuration for a transformer model from Marian [Cite_Footnote_2] adapted to our needs as shown in Table 1.",2 https://github.com/marian-nmt/marian-examples/blob/master/transformer,"Transformer We used the same training, dev and test sets as in the big RNN model, and the exam-ple configuration for a transformer model from Marian [Cite_Footnote_2] adapted to our needs as shown in Table 1. This configuration utilises a six-layer deep encoder and decoder, learning rate warm-up and tied em-beddings for source, target and output layer. As suggested by Karita et al. (2019), we increased the minibatch size for the transformer model from 5,000 to 10,000.",Method,Code,False,Extend（引用目的）,True,2021.acl-srw.20_1_0,2021,Observing the Learning Curve of Neural Machine Translation with regard to Linguistic Phenomena,Footnote
116,10118," https://github.com/pstadler1990/nmt_paper21_appendix"," ['5 Results', '5.1.1 Snapshot selection']",• Epoch 39 (iteration 45): Highest BLEU score The complete dataset can be found online in the repository [Cite_Footnote_3] .,3 https://github.com/pstadler1990/nmt_ paper21_appendix Computational Linguistics (ACL).,• Epoch 39 (iteration 45): Highest BLEU score The complete dataset can be found online in the repository [Cite_Footnote_3] .,補足資料,Website,True,Introduce（引用目的）,True,2021.acl-srw.20_2_0,2021,Observing the Learning Curve of Neural Machine Translation with regard to Linguistic Phenomena,Footnote
117,10119," https://www.gsk.or.jp/en/catalog//"," ['4 Data Development']",We have released a part of it to the public on the web [Cite_Footnote_4] .,4 The corpus data are available at https://www.gsk.or.jp/en/catalog//.,We created a dataset for feedback comment gen-eration for preposition use. We used it in the eval-uation as described in Sect. 6. We have released a part of it to the public on the web [Cite_Footnote_4] .,補足資料,Website,True,Introduce（引用目的）,True,D19-1316_0_0,2019,Toward a Task of Feedback Comment Generation for Writing Learning,Footnote
118,10120," http://taku910.github.io/mecab/"," ['6 Evaluation', '6.1 Conditions and Procedures']",We used MeCab [Cite_Footnote_10] to tokenize the feedback comments.,10 http://taku910.github.io/mecab/,"We implemented and trained the baseline meth-ods with the created dataset. We first obtain word embeddings for learner sentences from the cor-pora as shown in Appendix A. We also used the word embeddings for English words in the LSTMLMs to encode feedback comments. For the rest (i.e., Japanese words), we initialized them using random-valued vectors. We used MeCab [Cite_Footnote_10] to tokenize the feedback comments. With these word embeddings, we trained the networks on the training set of the respective subsets (PART-TIME JOB and SMOKING). We implemented the case frame-based method with the following cor-pora: British National Corpus (BNC) (Burnard, 1995), the EDR corpus (Japan electronic dictio-nary research institute Ltd, 1993) as a native cor-pus and the training and development set of the corresponding dataset as a learner corpus. As a result, we obtained two versions of each method. We determined the hyperparameters by using the corresponding development set 12 . We tested the resulting models on the corresponding test set.",Method,Tool,True,Use（引用目的）,True,D19-1316_1_0,2019,Toward a Task of Feedback Comment Generation for Writing Learning,Footnote
119,10121," http://language.sakura.ne.jp/s/doc/projects/CEEAUS.pdf"," ['References']","[Cite_Footnote_15] , ETS Corpus of non-native written English (Daniel Blanchard et al., 2014), The International Corpus of Learner English (ICLE) (Granger, 1993), Cambridge Learner Corpus (CLC) First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and Nagoya Interlanguage Corpus of English (NICE)",15 http://language.sakura.ne.jp/s/doc/projects/CEEAUS.pdf,"Learner corpora: Corpus of English Essays Written by Japanese University Students (CEEEJUS) [Cite_Footnote_15] , ETS Corpus of non-native written English (Daniel Blanchard et al., 2014), The International Corpus of Learner English (ICLE) (Granger, 1993), Cambridge Learner Corpus (CLC) First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and Nagoya Interlanguage Corpus of English (NICE) (Sug-iura et al., 2007). Native corpus: English Web Treebank (EWT) (Bies, Ann, et al., 2012).",Material,Dataset,True,Introduce（引用目的）,False,D19-1316_2_0,2019,Toward a Task of Feedback Comment Generation for Writing Learning,Footnote
120,10122," http://www.gutenberg.org"," ['2 A Parallel Corpus of Literary Texts', '2.1 Data Selection']","We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1]",1 http://www.gutenberg.org and http://gutenberg.spiegel.de/,"We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1] Examples include Dickens’ David Copper-field or Tolstoy’s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align.",Material,DataSource,True,Use（引用目的）,True,P11-2082_0_0,2011,"“I Thou Thee, Thou Traitor”: Predicting Formal vs. Informal Address in English Literature",Footnote
121,10123," http://gutenberg.spiegel.de/"," ['2 A Parallel Corpus of Literary Texts', '2.1 Data Selection']","We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1]",1 http://www.gutenberg.org and http://gutenberg.spiegel.de/,"We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [Cite_Footnote_1] Examples include Dickens’ David Copper-field or Tolstoy’s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align.",Material,DataSource,True,Use（引用目的）,True,P11-2082_1_0,2011,"“I Thou Thee, Thou Traitor”: Predicting Formal vs. Informal Address in English Literature",Footnote
122,10124," http://www.nlpado.de/~sebastian/data.shtml"," ['2 A Parallel Corpus of Literary Texts', '2.3 T/V Gold Labels for English Utterances']","Our projection on the English side results in 53K V and 35K T sentences, of which 731 are labeled as both T and V. Finally, from the English labeled sentences we ex-tracted a training set with 72 novels (63K sentences) and a test set with 21 novels (15K sentences). [Cite_Footnote_4]",4 The corpus can be downloaded for research purposes from http://www.nlpado.de/~sebastian/data.shtml.,"Choice of English units to label. On the German side, we assign the T/V labels to pronouns, and the most straightforward way of setting up annotation projection would be to label their word-aligned En-glish pronouns as T/V. However, pronouns are not necessarily translated into pronouns; additionally, we found word alignment accuracy for pronouns, as a function of word class, to be far from perfect. For these reasons, we decided to treat complete sentences as either T or V. This means that sentence alignment is sufficient for projection, but English sentences can receive conflicting labels, if a German sentence con-tains both a T and a V label. However, this occurs very rarely: of the 76K German sentences with T or V pronouns, only 515, or less than 1%, contain both. Our projection on the English side results in 53K V and 35K T sentences, of which 731 are labeled as both T and V. Finally, from the English labeled sentences we ex-tracted a training set with 72 novels (63K sentences) and a test set with 21 novels (15K sentences). [Cite_Footnote_4]",Material,Dataset,True,Produce（引用目的）,True,P11-2082_2_0,2011,"“I Thou Thee, Thou Traitor”: Predicting Formal vs. Informal Address in English Literature",Footnote
123,10125," https://github.com/dhruvramani/C2AE-Multilabel-Classification"," ['2 The Proposed Method (Rank-AE)', '2.3 L h and L ae Loss Functions']","C2AE attempts to minimize the number of misclassified pairs between relevant and irrelevant labels, as a result its computational complexity is quadratic with number of labels in the worst case; also it fails to scale well on large number of in-put features or labels due to its inefficient imple-mentation [Cite_Footnote_1] .",1 https://github.com/dhruvramani/ C2AE-Multilabel-Classification,"Reconstructing Output (L ae ). Unlike L h with small space, L ae loss usually involves a large num-ber of labels. Moreover, L ae also directly affects the classification performance significantly since different loss functions lead to their own proper-ties (Hajiabadi et al., 2017). Accordingly, solving such problems with large scale and desirable prop-erties presents open challenges in three aspects: 1) how to improve time efficiency, 2) how to produce comparable labels scores and 3) how to deal with noise labels. Unfortunately, most of the related deep learning methods only target one or two as-pects. C2AE attempts to minimize the number of misclassified pairs between relevant and irrelevant labels, as a result its computational complexity is quadratic with number of labels in the worst case; also it fails to scale well on large number of in-put features or labels due to its inefficient imple-mentation [Cite_Footnote_1] . XML-CNN (Liu et al., 2017) achieves computational efficiency by training a deep neural network with hidden layers much smaller than the output layer with binary cross-entropy loss (BCE), which has linear complexity in number of labels. Despite this, BCE loss could neither capture la-bel dependencies nor produce directly compara-ble label scores, since each label is treated inde-pendently. Moreover, BCE loss tends to be sensi-tive to label noise, which is frequently observed in XML data (Reed et al., 2014; Ghosh et al., 2017).",Material,Knowledge,False,Use（引用目的）,True,N19-1289_0_0,2019,Ranking-Based Autoencoder for Extreme Multi-label Classification,Footnote
124,10126," https://www.imdb.com/"," ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']","Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database [Cite_Footnote_2] and the rest five datasets are downloaded from the extreme classification repository .",2 https://www.imdb.com/,"Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database [Cite_Footnote_2] and the rest five datasets are downloaded from the extreme classification repository . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set. For all datasets, we reserve another 20% of train-ing data as validation for tuning hyper-parameters. After tuning, all models are trained on the entire training set.",Material,DataSource,True,Use（引用目的）,True,N19-1289_1_0,2019,Ranking-Based Autoencoder for Extreme Multi-label Classification,Footnote
125,10127," http://manikvarma.org/downloads/XC/XMLRepository.html"," ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']","Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [Cite_Footnote_3] .",3 http://manikvarma.org/downloads/XC/XMLRepository.html,"Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [Cite_Footnote_3] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set. For all datasets, we reserve another 20% of train-ing data as validation for tuning hyper-parameters. After tuning, all models are trained on the entire training set.",Material,DataSource,True,Use（引用目的）,True,N19-1289_2_0,2019,Ranking-Based Autoencoder for Extreme Multi-label Classification,Footnote
126,10128," https://nlp.stanford.edu/projects/glove/"," ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']","Embedding component is initialized by Glove [Cite_Footnote_4] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed.",4 https://nlp.stanford.edu/projects/glove/,"Hyper-parameters. In Rank-AE, we use the fixed neural network architecture, with two fully con-nected layers in both Encoder and Decoder, and one fully connected layer following Embedding & Atten network in Feature Embedding. We also fix most of the hyper-parameters, including hidden dimension h (100 for small number of la-bels data and 200 for large ones), word embed-ding size C = 100, and reduction ratio r = 4. The remaining hyper-parameters, such as balance λ between L h and L ae , margin m in L ae , and oth-ers (decay, learning rate) in the optimization algo-rithms, are tuned on validation set. In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [Cite_Footnote_4] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed.",Material,Knowledge,True,Use（引用目的）,True,N19-1289_3_0,2019,Ranking-Based Autoencoder for Extreme Multi-label Classification,Footnote
127,10129," https://github.com/kaushalshetty/Structured-Self-Attention"," ['3 Experiments & Analysis', '3.4 More Analysis in Rank-AE']","By summing up the attention weights of each word embedding vector, we can visualize the overall attention for that word with the visualiza-tion tool [Cite_Footnote_5] .",5 The visualization tool is provided by https://github.com/kaushalshetty/ Structured-Self-Attention,". employ the visualization tool (Lin et al., 2017) to highlight important words based on the attention output. Specifically, we run our method on IMDb dataset, wherein each instance is a movie story as-sociated with relevant genres as labels. Instead of extracting V 0 matrix using the proposed spatial-wise attention, we obtain a fixed size embeddings from a bidirectional LSTM on variable length of sentence, fed to our channel-attention network. Through the channel-attention network, we can observe the attention matrix A for each input doc-ument. By summing up the attention weights of each word embedding vector, we can visualize the overall attention for that word with the visualiza-tion tool [Cite_Footnote_5] . We randomly select three movies from IMDb testing set (See Figure 5). By looking at the highlighted regions, we can see that the pro-posed channel-attention is able to focus more on the words that are highly related to the topics.",Method,Tool,True,Use（引用目的）,False,N19-1289_4_0,2019,Ranking-Based Autoencoder for Extreme Multi-label Classification,Footnote
128,10130,https://github.com/facebookresearch/Zero-Shot-DST,"['3 Methodology', '3.1 T5DST']","Then all the utterances and slot names s i are concatenated into a single sequence, i.e., user:U [Cite_Footnote_1] . .",1 Source code is available in https://github.com/facebookresearch/Zero-Shot-DST,"The design of our model follows the basis of gen-erative question answering models. As illustrated in Figure 1, given a dialogue history which con-sists of an alternating set of utterances from two speakers, denoted as C t = {U 1 , R 1 , . . . , R t−1 , U t }, we add the ""user:"" and ""system:"" prefixes to the user and system utterance respectively. Then all the utterances and slot names s i are concatenated into a single sequence, i.e., user:U [Cite_Footnote_1] . . .system:R t−1 user:U t [sep] s i . The sequence is used as the in-put to the encoder, and the decoder generates the corresponding slot value v i :",Method,Code,True,Use（引用目的）,True,2021.naacl-main.448_0_0,2021,Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue State Tracking,Footnote
129,10131," http://mallet.cs.umass.edu/"," ['3 The Proposed Approach', '3.1 Text Representation']","The maximum en-tropy (ME) classifier is implemented with the public tool, Mallet Toolkits [Cite_Footnote_3] .",3 http://mallet.cs.umass.edu/,"Classification algorithm: The maximum en-tropy (ME) classifier is implemented with the public tool, Mallet Toolkits [Cite_Footnote_3] .",Method,Tool,True,Use（引用目的）,True,P14-2136_0_0,2014,[unextracted],Footnote
130,10132," http://www.im2.ch"," ['1 Introduction']","In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [Cite_Footnote_1] .","1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html","In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [Cite_Footnote_1] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.",補足資料,Media,True,Introduce（引用目的）,True,P06-4013_0_0,2006,Archivus: A multimodal system for multimedia meeting browsing and retrieval,Footnote
131,10133," http://www.is.cs.cmu.edu/mie"," ['1 Introduction']","In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [Cite_Footnote_1] .","1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html","In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [Cite_Footnote_1] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.",補足資料,Media,True,Introduce（引用目的）,True,P06-4013_1_0,2006,Archivus: A multimodal system for multimedia meeting browsing and retrieval,Footnote
132,10134," http://www.icsi.berkeley.edu/Speech/EARS/rt.html"," ['1 Introduction']","In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [Cite_Footnote_1] .","1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html","In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [Cite_Footnote_1] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.",補足資料,Media,True,Introduce（引用目的）,True,P06-4013_2_0,2006,Archivus: A multimodal system for multimedia meeting browsing and retrieval,Footnote
133,10135," http://webdatacommons.org/webtables"," ['A Pretraining Details', 'A.1 Training Data']","WDC WebTable Corpus (Lehmberg et al., 2016) is a large collection of Web tables extracted from the Common Crawl Web scrape [Cite_Footnote_10] .",10 http://webdatacommons.org/webtables,"WDC WebTable Corpus (Lehmberg et al., 2016) is a large collection of Web tables extracted from the Common Crawl Web scrape [Cite_Footnote_10] . We use its 2015 English-language relational subset, which consists of 50.8 million relational tables and their surrounding NL contexts.",Material,DataSource,True,Use（引用目的）,True,2020.acl-main.745_0_0,2020,T A B ERT : Pretraining for Joint Understanding of Textual and Tabular Data,Footnote
134,10136," https://news.google.com/"," ['1 Introduction']","Online news platforms such as Google News [Cite_Footnote_1] and MSN News have attracted many users to read news online (Das et al., 2007).",1 https://news.google.com/,"Online news platforms such as Google News [Cite_Footnote_1] and MSN News have attracted many users to read news online (Das et al., 2007). Massive news ar-ticles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is very important for online news platforms to target user interests and allevi-ate information overload (IJntema et al., 2010).",補足資料,Website,True,Introduce（引用目的）,True,D19-1671_0_0,2019,Neural News Recommendation with Multi-Head Self-Attention,Footnote
135,10137," https://www.msn.com/en-us/news"," ['1 Introduction']","Online news platforms such as Google News and MSN News [Cite_Footnote_2] have attracted many users to read news online (Das et al., 2007).",2 https://www.msn.com/en-us/news,"Online news platforms such as Google News and MSN News [Cite_Footnote_2] have attracted many users to read news online (Das et al., 2007). Massive news ar-ticles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is very important for online news platforms to target user interests and allevi-ate information overload (IJntema et al., 2010).",補足資料,Website,True,Introduce（引用目的）,True,D19-1671_1_0,2019,Neural News Recommendation with Multi-Head Self-Attention,Footnote
136,10138," https://www.msn.com/en-us/news"," ['3 Experiments', '3.1 Datasets and Experimental Settings']","We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_3] logs in one month (Dec. 13, 2018 to Jan. 12, 2019).",3 https://www.msn.com/en-us/news,"We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_3] logs in one month (Dec. 13, 2018 to Jan. 12, 2019). The detailed statistics are shown in Ta-ble 1. The logs in the last week were used for test, and the rest were used for training. We randomly sampled 10% of training data for validation.",Material,DataSource,True,Use（引用目的）,True,D19-1671_2_0,2019,Neural News Recommendation with Multi-Head Self-Attention,Footnote
137,10139," http://www.statmt.org/wmt16/multimodal-task.html"," ['1 Introduction']","To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [Cite_Footnote_1] .",1 http://www.statmt.org/wmt16/multimodal-task.html,"To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [Cite_Footnote_1] . The best results of NMT model were those of Huang et al. (2016) who used LSTM fed with global visual features or multiple regional visual features fol-lowed by rescoring. Recently, Calixto et al. (2017) proposed a doubly-attentive decoder that outper-formed this baseline with less data and without rescoring.",補足資料,Website,True,Introduce（引用目的）,True,D17-1095_0_0,2017,An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,Footnote
138,10140," http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf"," ['3 Attention-based Models', '3.2 Hard Stochastic attention']","Hard attention has previously been used in the context of object recognition (Mnih et al., 2014 [Cite_Ref]","Volodymyr Mnih, Nicolas Heess, Alex Graves, and koray kavukcuoglu. 2014. Recurrent models of visual attention. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2204– 2212. http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf.","This model is a stochastic and sampling-based process where, at every timestep t, we are making a hard choice to attend only one annotation. This corresponds to one spatial location in the image. Hard attention has previously been used in the context of object recognition (Mnih et al., 2014 [Cite_Ref] ; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al. (2015) because both our models involve the same process on images.",補足資料,Paper,True,Introduce（引用目的）,True,D17-1095_8_0,2017,An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,Reference
139,10141," http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf"," ['3 Attention-based Models', '3.1 Soft attention']",Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) [Cite_Ref] but has been widely used for translation tasks ever since.,"Oriol Vinyals, Ł ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-mar as a foreign language. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 2773– 2781. http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf.","Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) [Cite_Ref] but has been widely used for translation tasks ever since. One should note that it slightly differs from Bahdanau et al. (2014) where their attention takes as input the previous decoder hidden state instead of the current (intermediate) one as shown in equation 7. This mechanism has also been successfully investigated for the task of image description generation (Xu et al., 2015) where a model generates an image’s description in natural language. It has been used in multimodal translation as well (Calixto et al., 2017), for which it constitutes a state-of-the-art.",補足資料,Paper,True,Introduce（引用目的）,True,D17-1095_12_0,2017,An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,Reference
140,10142," http://code.google.com/p/jacana/"," ['1 Introduction']","We then describe how we cou-ple TED features to a linear-chain CRF for answer extraction, providing the set of features used, and fi-nally experimental results on an extraction dataset we make public (together with the software) to the community. [Cite_Footnote_3]",3 http://code.google.com/p/jacana/,"In the following we first provide background on the TED model, going on to evaluate our implemen-tation against prior work in the context of question answer sentence ranking (QASR), achieving state of the art in that task. We then describe how we cou-ple TED features to a linear-chain CRF for answer extraction, providing the set of features used, and fi-nally experimental results on an extraction dataset we make public (together with the software) to the community. [Cite_Footnote_3] Related prior work is interspersed throughout the paper.",Mixed,Mixed,False,Produce（引用目的）,False,N13-1106_0_0,2013,Answer Extraction as Sequence Tagging with Tree Edit Distance,Footnote
141,10143," https://github.com/IBM/WordMoversEmbeddings"," ['1 Introduction']",Our code and data is available at [Cite] https://github.com/IBM/WordMoversEmbeddings.,,"The proposed embedding is more efficient and flexible than WMD in many situations. As an example, WME with a simple linear classifier reduces the computational cost of WMD-based KNN from cubic to linear in document length and from quadratic to linear in number of samples, while simultaneously improving accuracy. WME is extremely easy to implement, fully paralleliz-able, and highly extensible, since its two build-ing blocks, Word2Vec and WMD, can be replaced by other techniques such as GloVe (Pennington et al., 2014; Wieting et al., 2015b) or S-WMD (Huang et al., 2016). We evaluate WME on 9 real-world text classification tasks and 22 textual similarity tasks, and demonstrate that it consis-tently matches or outperforms other state-of-the-art techniques. Moreover, WME often achieves orders of magnitude speed-up compared to KNN-WMD while obtaining the same testing accuracy. Our code and data is available at [Cite] https://github.com/IBM/WordMoversEmbeddings.",Mixed,Mixed,True,Produce（引用目的）,True,D18-1482_0_0,2018,Word Mover’s Embedding: From Word2Vec to Document Embedding,Body
142,10144," https://github.com/Makwen1995/LDGNMLTC"," ['3 Experiment', '3.1 Experimental Setup']",Our code is available on GitHub [Cite_Footnote_1] .,1 https://github.com/Makwen1995/LDGN MLTC,"The word embeddings in the proposed network are initialized with the 300-dimensional word vec-tors, which are trained on the datasets by Skip-gram (Mikolov et al., 2013) algorithm. The hid-den sizes of Bi-LSTM and GCNs are set to 300 and 512, respectively. We use the Adam optimiza-tion method (Kingma and Ba, 2014) to minimize the cross-entropy loss, the learning rate is initial-ized to 1e-3 and gradually decreased during the process of training. We select the best parameter configuration based on performance on the valida-tion set and evaluate the configuration on the test set. Our code is available on GitHub [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2021.acl-long.298_0_0,2021,Label-Specific Dual Graph Neural Network for Multi-Label Text Classification,Footnote
143,10145," https://github.com/yizhen20133868/Retriever-Dialogue"," ['4 Training the KB-Retriever', '4.3 Experimental Settings']","To justify the gen-eralization of the proposed model, we also use an-other public CamRest dataset (Wen et al., 2017b) and partition the datasets into training, validation and testing set in the ratio 3:1:1. [Cite_Footnote_3]",3 The dataset can be available at: https://github.com/yizhen20133868/Retriever-Dialogue,"We choose the InCar Assistant dataset (Eric et al., 2017) including three distinct domains: naviga-tion, weather and calendar domain. For weather domain, we follow Wen et al. (2018) to separate the highest temperature, lowest temperature and weather attribute into three different columns. For calendar domain, there are some dialogues with-out a KB or incomplete KB. In this case, we padding a special token “-” in these incomplete KBs. Our framework is trained separately in these three domains, using the same train/validation/test split sets as Eric et al. (2017). To justify the gen-eralization of the proposed model, we also use an-other public CamRest dataset (Wen et al., 2017b) and partition the datasets into training, validation and testing set in the ratio 3:1:1. [Cite_Footnote_3] Especially, we hired some human experts to format the CamRest dataset by equipping the corresponding KB to ev-ery dialogues.",Material,Dataset,True,Use（引用目的）,True,D19-1013_0_0,2019,Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever,Footnote
144,10146," http://www.stackoverflow.com"," ['1 Introduction']","StackOverflow [Cite_Footnote_1] , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally .",1 http://www.stackoverflow.com,"Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow [Cite_Footnote_1] , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 . These typically start with a registered user posting a question/problem 6 to which other users respond. Typical response posts include so-lutions or clarification requests, whereas feedback posts form another major category of forum posts. As is the case with any community of humans, discussion forums have their share of inflamma-tory remarks too. Mining problem-solution pairs from discussion forums has attracted much atten-tion from the scholarly community in the recent past. Since the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g., (Gandhe et al., 2012; Hong and Davison, 2009)). Extract-ing problem-solution pairs from forums enables the usage of such knowledge in knowledge reuse frameworks such as case-based reasoning (Kolod-ner, 1992) that use problem-solution pairs as raw material. In this paper, we address the problem of unsupervised solution post identification 7 from discussion forums.",補足資料,Website,True,Introduce（引用目的）,True,P14-1015_0_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
145,10147," http://www.alexa.com/siteinfo/stackoverflow.com"," ['1 Introduction']","StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [Cite_Footnote_2] .",2 http://www.alexa.com/siteinfo/stackoverflow.com,"Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [Cite_Footnote_2] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 . These typically start with a registered user posting a question/problem 6 to which other users respond. Typical response posts include so-lutions or clarification requests, whereas feedback posts form another major category of forum posts. As is the case with any community of humans, discussion forums have their share of inflamma-tory remarks too. Mining problem-solution pairs from discussion forums has attracted much atten-tion from the scholarly community in the recent past. Since the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g., (Gandhe et al., 2012; Hong and Davison, 2009)). Extract-ing problem-solution pairs from forums enables the usage of such knowledge in knowledge reuse frameworks such as case-based reasoning (Kolod-ner, 1992) that use problem-solution pairs as raw material. In this paper, we address the problem of unsupervised solution post identification 7 from discussion forums.",補足資料,Document,True,Introduce（引用目的）,True,P14-1015_1_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
146,10148," http://www.cadillacforums.com/"," ['1 Introduction']","Now, there are discussion fo-rums for almost every major product ranging from automobiles [Cite_Footnote_3] to gadgets such as those of Mac or Samsung .",3 http://www.cadillacforums.com/,"Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles [Cite_Footnote_3] to gadgets such as those of Mac or Samsung . These typically start with a registered user posting a question/problem to which other users respond. Typical response posts include so-lutions or clarification requests, whereas feedback posts form another major category of forum posts. As is the case with any community of humans, discussion forums have their share of inflamma-tory remarks too. Mining problem-solution pairs from discussion forums has attracted much atten-tion from the scholarly community in the recent past. Since the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g., (Gandhe et al., 2012; Hong and Davison, 2009)). Extract-ing problem-solution pairs from forums enables the usage of such knowledge in knowledge reuse frameworks such as case-based reasoning (Kolod-ner, 1992) that use problem-solution pairs as raw material. In this paper, we address the problem of unsupervised solution post identification from discussion forums.",補足資料,Website,True,Introduce（引用目的）,True,P14-1015_2_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
147,10149," https://discussions.apple.com/"," ['1 Introduction']","Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac [Cite_Footnote_4] or Samsung .",4 https://discussions.apple.com/,"Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac [Cite_Footnote_4] or Samsung . These typically start with a registered user posting a question/problem to which other users respond. Typical response posts include so-lutions or clarification requests, whereas feedback posts form another major category of forum posts. As is the case with any community of humans, discussion forums have their share of inflamma-tory remarks too. Mining problem-solution pairs from discussion forums has attracted much atten-tion from the scholarly community in the recent past. Since the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g., (Gandhe et al., 2012; Hong and Davison, 2009)). Extract-ing problem-solution pairs from forums enables the usage of such knowledge in knowledge reuse frameworks such as case-based reasoning (Kolod-ner, 1992) that use problem-solution pairs as raw material. In this paper, we address the problem of unsupervised solution post identification from discussion forums.",補足資料,Website,True,Introduce（引用目的）,True,P14-1015_3_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
148,10150," http://www.galaxyforums.net/"," ['1 Introduction']","Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac or Samsung [Cite_Footnote_5] .",5 http://www.galaxyforums.net/,"Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac or Samsung [Cite_Footnote_5] . These typically start with a registered user posting a question/problem to which other users respond. Typical response posts include so-lutions or clarification requests, whereas feedback posts form another major category of forum posts. As is the case with any community of humans, discussion forums have their share of inflamma-tory remarks too. Mining problem-solution pairs from discussion forums has attracted much atten-tion from the scholarly community in the recent past. Since the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g., (Gandhe et al., 2012; Hong and Davison, 2009)). Extract-ing problem-solution pairs from forums enables the usage of such knowledge in knowledge reuse frameworks such as case-based reasoning (Kolod-ner, 1992) that use problem-solution pairs as raw material. In this paper, we address the problem of unsupervised solution post identification from discussion forums.",補足資料,Website,True,Introduce（引用目的）,True,P14-1015_4_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
149,10151," http://en.wikipedia.org/wiki/F1score"," ['3 Problem Definition']","In short, we would like to find problem-solution pairs from C such that the F-measure [Cite_Footnote_8] for solution identification is maximized.",8 http://en.wikipedia.org/wiki/F1 score,"Let a thread T from a discussion forum be made up of t posts. Since we assume, much like many other earlier papers, that the first post is the problem post, the task is to identify which among the remaining t − 1 posts are solutions. There could be multiple (most likely, different) solutions within the same thread. We may now model the thread T as t − 1 post pairs, each pair having the problem post as the first element, and one of the t − 1 remaining posts (i.e., re-ply posts in T ) as the second element. Let C = {(p 1 , r 1 ), (p 2 , r 2 ), . . . , (p n , r n )} be the set of such problem-reply pairs from across threads in the dis-cussion forum. We are interested in finding a sub-set C 0 of C such that most of the pairs in C 0 are problem-solution pairs, and most of those in C−C 0 are not so. In short, we would like to find problem-solution pairs from C such that the F-measure [Cite_Footnote_8] for solution identification is maximized.",補足資料,Document,True,Introduce（引用目的）,True,P14-1015_5_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
150,10152," http://discussions.apple.com"," ['5 Experimental Evaluation']",We use a crawl of 140k threads from Apple Dis-cussion forums [Cite_Footnote_10] .,10 http://discussions.apple.com,"We use a crawl of 140k threads from Apple Dis-cussion forums [Cite_Footnote_10] . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement of 0.71. On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure 12 for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set λ = 0.5 and τ = 0.4 unless other-wise mentioned. Since we have only 300 labeled threads, accuracy measures are reported on those (like in (Catherine et al., 2013)). We pre-process the post data by stemming words (Porter, 1980).",Material,DataSource,True,Use（引用目的）,True,P14-1015_6_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
151,10153," http://en.wikipedia.org/wiki/Cohen’skappa"," ['5 Experimental Evaluation']","Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement [Cite_Footnote_11] of 0.71.",11 http://en.wikipedia.org/wiki/Cohen’s kappa,"We use a crawl of 140k threads from Apple Dis-cussion forums . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement [Cite_Footnote_11] of 0.71. On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure 12 for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set λ = 0.5 and τ = 0.4 unless other-wise mentioned. Since we have only 300 labeled threads, accuracy measures are reported on those (like in (Catherine et al., 2013)). We pre-process the post data by stemming words (Porter, 1980).",補足資料,Document,True,Introduce（引用目的）,True,P14-1015_7_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
152,10154," http://en.wikipedia.org/wiki/F1score"," ['5 Experimental Evaluation']","We use the F-measure [Cite_Footnote_12] for solu-tion identification, as the primary evaluation mea-sure.",12 http://en.wikipedia.org/wiki/F1 score,"We use a crawl of 140k threads from Apple Dis-cussion forums 10 . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement 11 of 0.71. On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure [Cite_Footnote_12] for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set λ = 0.5 and τ = 0.4 unless other-wise mentioned. Since we have only 300 labeled threads, accuracy measures are reported on those (like in (Catherine et al., 2013)). We pre-process the post data by stemming words (Porter, 1980).",Method,Code,True,Use（引用目的）,True,P14-1015_8_0,2014,Unsupervised Solution Post Identification from Discussion Forums,Footnote
153,10155," https://github.com/mrlyk423/relation_extraction"," ['References']",The source code of this paper can be obtained from [Cite] https://github.com/mrlyk423/relation_extraction.,,"Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns be-tween entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learn-ing, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allo-cation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as com-pared with baselines, our model achieves significant and consistent improvements on knowledge base completion and re-lation extraction from text. The source code of this paper can be obtained from [Cite] https://github.com/mrlyk423/relation_extraction.",Method,Code,True,Produce（引用目的）,True,D15-1082_0_0,2015,Modeling Relation Paths for Representation Learning of Knowledge Bases,Body
154,10156," http://hub.microsofttranslator.com"," ['1 Introduction']","To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [Cite_Footnote_2] to train a cus-tom MT engine for English to French translations.",2 http://hub.microsofttranslator.com,"To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [Cite_Footnote_2] to train a cus-tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida-tion process, first using crowd-sourced evaluations with Amazon’s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior.",Method,Tool,True,Use（引用目的）,True,D16-1251_0_0,2016,Measuring the behavioral impact of machine translation quality improvements with A/B testing,Footnote
155,10157," https://github.com/ElliottYan/Multi"," ['References']","These results demonstrate the effectiveness of the MUTE, as well as its ef-ficiency in both the inference process and pa-rameter usage. [Cite_Footnote_1]",1 Code is available at https://github.com/ElliottYan/Multi Unit Transformer,"Transformer models (Vaswani et al., 2017) achieve remarkable success in Neural Ma-chine Translation. Many efforts have been de-voted to deepening the Transformer by stack-ing several units (i.e., a combination of Multi-head Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we pro-pose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use sev-eral parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit set-ting, we design biased module and sequen-tial dependency that guide and encourage com-plementariness among different units. Exper-imental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to- English, show that the MUTE models signif-icantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its ef-ficiency in both the inference process and pa-rameter usage. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.77_0_0,2020,Multi-Unit Transformers for Neural Machine Translation,Footnote
156,10158," https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl"," ['4 Experimental Settings']","We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [Cite_Footnote_4] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT’14 En-De, respectively.",4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl,"Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [Cite_Footnote_4] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT’14 En-De, respectively. For WMT’18 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.77_1_0,2020,Multi-Unit Transformers for Neural Machine Translation,Footnote
157,10159," https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/boots"," ['4 Experimental Settings']","We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [Cite_Footnote_5] for NIST Zh-En and WMT’14 En-De, respectively.",5 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance. pl,"Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [Cite_Footnote_5] for NIST Zh-En and WMT’14 En-De, respectively. For WMT’18 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention.",Method,Code,False,Use（引用目的）,True,2020.emnlp-main.77_2_0,2020,Multi-Unit Transformers for Neural Machine Translation,Footnote
158,10160," https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl"," ['4 Experimental Settings']","For WMT’18 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [Cite_Footnote_6] .",6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl,"Evaluation. For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT’14 En-De, respectively. For WMT’18 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [Cite_Footnote_6] . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention.",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.77_3_0,2020,Multi-Unit Transformers for Neural Machine Translation,Footnote
159,10161," https://talkbank.org/DementiaBank"," ['1 Introduction']","For example, Fraser et al. (2015) achieved up to 82% accuracy on Demen-tiaBank [Cite_Footnote_1] , the largest publicly available dataset on detecting cognitive impairments from speech, and Weissenbacher et al. (2016) achieved up to 86% accuracy on a corpus of 500 subjects.",1 https://talkbank.org/DementiaBank,"This insight makes automatic detection possi-ble. Machine learning classifiers can detect cogni-tive impairments given descriptive linguistic fea-tures. In recent work, linguistic features includ-ing pronoun-noun-ratios, pauses, and so on, are used to train classifiers to detect cognitive dis-eases in various tasks. For example, Fraser et al. (2015) achieved up to 82% accuracy on Demen-tiaBank [Cite_Footnote_1] , the largest publicly available dataset on detecting cognitive impairments from speech, and Weissenbacher et al. (2016) achieved up to 86% accuracy on a corpus of 500 subjects. Yancheva et al. (2015) estimated Mini-Mental State Estima-tion scores (MMSEs), describing the cognitive sta-tus and characterizing the extent of cognitive im-pairment.",補足資料,Document,True,Introduce（引用目的）,True,N19-1146_0_0,2019,Detecting Cognitive Impairments by Agreeing on Interpretations of Linguistic Features,Footnote
160,10162," https://www.cs.toronto.edu/talk2me/"," ['1 Introduction']","Tak-ing the first approach, Noorian et al. (2017) in-corporated normative data from Talk2Me [Cite_Footnote_2] and the Wisconsin Longitudinal Study (Herd et al., 2014) in addition to DementiaBank, which in-creased AD:control accuracy up to 93%, and mod-erateAD:mildAD:control three-way classification accuracy to 70%.",2 https://www.cs.toronto.edu/talk2me/,"To improve the accuracy of automated assess-ment using engineered linguistic features, there are usually two approaches: incorporating more clinical data or calculating more features. Tak-ing the first approach, Noorian et al. (2017) in-corporated normative data from Talk2Me [Cite_Footnote_2] and the Wisconsin Longitudinal Study (Herd et al., 2014) in addition to DementiaBank, which in-creased AD:control accuracy up to 93%, and mod-erateAD:mildAD:control three-way classification accuracy to 70%. Taking the second approach, Yancheva and Rudzicz (2016) used 12 features derived from vector space models and reached a .80 F-score on DementiaBank. Santos et al. (2017) calculated features depicting characteristics of co-occurrence graphs of narrative transcripts (e.g., the degree of each vertex in the graph). Their clas-sifiers reached 65% accuracy on DementiaBank (MCI versus a subset of Control).",補足資料,Media,True,Introduce（引用目的）,True,N19-1146_1_0,2019,Detecting Cognitive Impairments by Agreeing on Interpretations of Linguistic Features,Footnote
161,10163," https://spacy.io"," ['A Appendices']",• The number of occurrences of part-of-speech (PoS) tags from Penn-treebank [Cite_Footnote_9] .,9 Using https://spacy.io,• The number of occurrences of part-of-speech (PoS) tags from Penn-treebank [Cite_Footnote_9] .,Material,Dataset,True,Use（引用目的）,True,N19-1146_2_0,2019,Detecting Cognitive Impairments by Agreeing on Interpretations of Linguistic Features,Footnote
162,10164," http://www.cs.jhu.edu/~ozaidan/AOC"," ['3.2. Experiments and Results']","We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. [Cite_Footnote_1]",1 Available from http://www.cs.jhu.edu/~ozaidan/AOC,"We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. [Cite_Footnote_1] This corpus is highly dialectal",Material,DataSource,True,Extend（引用目的）,False,P12-2035_0_0,2012,Transforming Standard Arabic to Colloquial Arabic,Footnote
163,10165," https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md"," ['4 Experiment', '4.1 Settings']","For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [Cite_Footnote_3] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .",3 We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md,"For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [Cite_Footnote_3] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .",Method,Code,True,Use（引用目的）,True,D19-1057_0_0,2019,Syntax-Enhanced Self-Attention-Based Semantic Role Labeling,Footnote
164,10166," https://github.com/google-research/bert"," ['4 Experiment', '4.1 Settings']","For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [Cite_Footnote_4] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .","4 We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the first word piece as the whole word representation.","For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [Cite_Footnote_4] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .",Method,Code,True,Use（引用目的）,True,D19-1057_1_0,2019,Syntax-Enhanced Self-Attention-Based Semantic Role Labeling,Footnote
165,10167," https://github.com/chenyangh/Seq2Emo"," ['References']","In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) ap-proaches in a fair setting. [Cite_Footnote_1]",1 Our code is available at https://github.com/chenyangh/Seq2Emo,"Multi-label emotion classification is an impor-tant task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval’18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) ap-proaches in a fair setting. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.375_0_0,2021,Seq2Emo: A Sequence to Multi-Label Emotion Classification Model,Footnote
166,10168," https://github.com/lancopku/SGM"," ['4 Experimental Setup']",We include it as a baseline by using its publicly released code. [Cite_Footnote_2],2 https://github.com/lancopku/SGM,"Baselines. On SemEval’18, we compare our system with the top submissions from the SemEval-2018 competition and recent development. NTUA-SLP (Baziotis et al., 2018) uses large amount of external emotion-related data to pretrain an LSTM-based model. TCS Research’s system (Meish-eri and Dey, 2018) uses the support vector ma-chine with mannually engineered features: out-put from LSTM models, emotion lexicons (Mo-hammad and Kiritchenko, 2015), and SentiNeural (Radford et al., 2017). PlusEmo2Vec (Park et al., 2018) combines neural network models, which are pretrained by using emojis as labels (Felbo et al., 2017). Apart from the competition, Yu et al. (2018) propose DATN, which introduces sentiment information through dual-attention. These afore-mentioned systems are based on the BR approach. SGM (Yang et al., 2018), however, is a CC-based model for multi-label classification. We include it as a baseline by using its publicly released code. [Cite_Footnote_2]",Method,Code,True,Use（引用目的）,True,2021.naacl-main.375_1_0,2021,Seq2Emo: A Sequence to Multi-Label Emotion Classification Model,Footnote
167,10169," http://nlp.stanford.edu/software/mteval.shtml"," ['4 Experimental Evaluation', '4.4 Combination Metrics']","M T +R TE R uses all M T R and R TE R features, combining matching and entailment evidence. [Cite_Footnote_3]",3 Software for R TE R and M T +R TE R is available from http://nlp.stanford.edu/software/mteval.shtml.,"M T +R TE R uses all M T R and R TE R features, combining matching and entailment evidence. [Cite_Footnote_3]",Method,Tool,True,Use（引用目的）,False,P09-1034_0_0,2009,Robust Machine Translation Evaluation with Entailment Features ∗,Footnote
168,10170," http://www.nist.gov"," ['5 Expt. 1: Predicting Absolute Scores']","Our first experiment evaluates the models we have proposed on a corpus with traditional an-notation on a seven-point scale, namely the NIST OpenMT 2008 corpus. [Cite_Footnote_4]",4 Available from http://www.nist.gov.,"Data. Our first experiment evaluates the models we have proposed on a corpus with traditional an-notation on a seven-point scale, namely the NIST OpenMT 2008 corpus. [Cite_Footnote_4] The corpus contains trans-lations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur)). Each language consists of 1500–2800 sen-tence pairs produced by 7–15 MT systems.",Material,Dataset,True,Use（引用目的）,True,P09-1034_1_0,2009,Robust Machine Translation Evaluation with Entailment Features ∗,Footnote
169,10171," http://www.statmt.org/"," ['6 Expt. 2: Predicting Pairwise Preferences']",This experiment uses the 2006–2008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [Cite_Footnote_7],7 Available from http://www.statmt.org/.,"In this experiment, we predict human pairwise pref-erence judgments (cf. Section 4). We reuse the linear regression framework from Section 2 and predict pairwise preferences by predicting two ab-solute scores (as before) and comparing them. Data. This experiment uses the 2006–2008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [Cite_Footnote_7] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems). The test set is formed by the WMT 2008 relative rank annotation task. As in Experi-ment 1, we set ε so that the incidence of ties in the training and test set is equal (60%).",Material,Dataset,True,Use（引用目的）,True,P09-1034_2_0,2009,Robust Machine Translation Evaluation with Entailment Features ∗,Footnote
170,10172," http://clipdemos.umiacs.umd.edu/catvar/"," ['3 Building the CatVar']",The CatVar is web-browseable at [Cite] http://clipdemos.umiacs.umd.edu/catvar/. Figure 2 shows the CatVar web-based interface with the hunger cluster as an example.,,The CatVar is web-browseable at [Cite] http://clipdemos.umiacs.umd.edu/catvar/. Figure 2 shows the CatVar web-based interface with the hunger cluster as an example. The interface allows searching clusters using regular expressions as well as cluster length restrictions. The database is also available for researchers in perl/C and lisp searchable formats.,Method,Tool,True,Produce（引用目的）,True,N03-1013_0_0,2003,A Categorial Variation,Body
171,10173," http://www.keenage.com"," ['1 Introduction']","Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) and HowNet (Dong, 2000) [Cite_Ref] (among others) have inspired the work of many re-searchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998; Voorhees, 1993).","Zhendong Dong. 2000. HowNet Chinese-English Con-ceptual Database. Technical Report Online Software Database, Released at ACL. http://www.keenage.com.","Natural Language Processing (NLP) applications may only be as good as the resources upon which they rely. Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) and HowNet (Dong, 2000) [Cite_Ref] (among others) have inspired the work of many re-searchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998; Voorhees, 1993).",補足資料,Website,True,Introduce（引用目的）,True,N03-1013_1_0,2003,A Categorial Variation,Reference
172,10174," http://www.umiacs.umd.edu/~bonnie/LCS"," ['3 Building the CatVar']","Verb and Preposition Databases (Dorr, 2001) [Cite_Ref] , the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998), and the Porter stemmer.","Bonnie J. Dorr. 2001. LCS Verb Database. Technical Report Online Software Database, University of Mary-land, College Park, MD. http://www.umiacs.umd.edu/˜bonnie/LCS Database Docmentation.html.","The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001) [Cite_Ref] , the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998), and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4",Material,Dataset,True,Use（引用目的）,True,N03-1013_2_0,2003,A Categorial Variation,Reference
173,10175," http://www.cogsci.princeton.edu/~wn"," ['1 Introduction']","Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) [Cite_Ref] and HowNet (Dong, 2000) (among others) have inspired the work of many re-searchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998; Voorhees, 1993).","Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/˜wn [2000, Septem-ber 7].","Natural Language Processing (NLP) applications may only be as good as the resources upon which they rely. Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) [Cite_Ref] and HowNet (Dong, 2000) (among others) have inspired the work of many re-searchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998; Voorhees, 1993).",補足資料,Website,True,Introduce（引用目的）,True,N03-1013_3_0,2003,A Categorial Variation,Reference
174,10176," http://www.cogsci.princeton.edu/~wn"," ['2 Background']","WordNet is the most well-developed and widely used lexical database of English (Fellbaum, 1998) [Cite_Ref] .","Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/˜wn [2000, Septem-ber 7].","WordNet is the most well-developed and widely used lexical database of English (Fellbaum, 1998) [Cite_Ref] . In Word-Net, both types of lexical relations are specified among words with the same part of speech (verbs, nouns, ad-jectives and adverbs). WordNet has been used by many researchers for different purposes ranging from the con-struction or extension of knowledge bases such as SEN-SUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al., 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000). In the context of these projects, one criticism of WordNet is its lack of cross-categorial links, such as verb-noun or noun-adjective re-lations.",補足資料,Paper,True,Introduce（引用目的）,True,N03-1013_3_1,2003,A Categorial Variation,Reference
175,10177," http://www.cogsci.princeton.edu/~wn"," ['3 Building the CatVar']","Verb and Preposition Databases (Dorr, 2001), the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998) [Cite_Ref] , and the Porter stemmer.","Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/˜wn [2000, Septem-ber 7].","The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001), the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998) [Cite_Ref] , and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4",Material,Dataset,True,Use（引用目的）,True,N03-1013_3_2,2003,A Categorial Variation,Reference
176,10178," http://www.xerox.fr/people/grenoble/hull/papers/sigir96.ps"," ['References']",[Cite] http://www.xerox.fr/people/grenoble/hull,,May not be a Real Word: This word is not known and couldn’t be found it in a dictionary. iments in Multilingual Information Retrieval. In Pro-ceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Informa-tion Retrieval. [Cite] http://www.xerox.fr/people/grenoble/hull/papers/sigir96.ps.,補足資料,Paper,True,Introduce（引用目的）,True,N03-1013_4_0,2003,A Categorial Variation,Body
177,10179," http://lang.ee.washington.edu/MAWPS"," ['1 Introduction']",M AWPS is located at [Cite] http: //lang.ee.washington.edu/MAWPS.,,"In response to the burgeoning interest in this task, we present M AWPS (MAth Word ProblemS, pro-nounced mops), a framework for building an on-line repository of math word problems. Our frame-work includes a collection of varying types of math word problems, their answers, and equation tem-plates, as well as interfaces which allow researchers to dynamically update and add more problem types. Additionally, in light of the problematic features of the current datasets cited above, our framework pro-vides the possibility for customizing a dataset with regard to considerations such as lexical and template overlap or grammaticality, allowing researchers to choose how many of the difficulties of open do-main web-sourced word problem texts they want to tackle. We report the important characteristics of the current available data as well as the perfor-mance of some state-of-the-art systems on various subsets of the data. M AWPS is located at [Cite] http: //lang.ee.washington.edu/MAWPS.",Mixed,Mixed,True,Produce（引用目的）,True,N16-1136_0_0,2016,M AWPS : A Math Word Problem Repository,Body
178,10180," http://www.ark.cs.cmu.edu/movie$-data"," ['5 Conclusion']",The dataset used in this paper has been made available for research at [Cite] http://www.ark.cs.cmu.edu/movie$-data.,,We conclude that text features from pre-release re-views can substitute for and improve over a strong metadata-based first-weekend movie revenue pre-diction. The dataset used in this paper has been made available for research at [Cite] http://www.ark.cs.cmu.edu/movie$-data.,Material,DataSource,True,Use（引用目的）,True,N10-1038_0_0,2010,Movie Reviews and Revenues: An Experiment in Text Regression ∗,Body
179,10181," https://pypi.org/project/textblob/"," ['3 Concept and Implementation', '3.1.1 Pre-Analysis']","First, the MT output from the training set of the WMT 2019 QE shared task is PoS tagged using the TextBlob library [Cite_Footnote_1] , which provides a sim-ple API for common natural language processing tasks .",1 https://pypi.org/project/textblob/,"We use PoS (Part of Speech) information as a way of capturing error types of current word-level QE systems. First, the MT output from the training set of the WMT 2019 QE shared task is PoS tagged using the TextBlob library [Cite_Footnote_1] , which provides a sim-ple API for common natural language processing tasks . The PoS-tagged MT is then fed into the “QEBrain” model (Wang et al., 2018) to generate quality predictions. This QE model was chosen be-cause it was the best performing system according to the assessment carried out by Shterionov et al. (2019). Next, we check how often each PoS is in-correctly labelled by comparing the QE output to the reference annotations. The probability of each PoS and the corresponding conditional error prob-ability (given as (P (P oS), P (error|P oS))) are: nouns are most often wrong (32%, 48%), followed by prepositions (10.9%, 15.6%), pronouns (8.69%, 14.8%), determiners (13.04%, 14.3%), conjunc-tions (7%, 13.9%), interjections (4.5%, 12.9%), verbs (28%, 9.6%), adjectives (4.34%, 7.9%) and adverbs (5%, 2.9%).",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.799_0_0,2021,Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output,Footnote
180,10182," https://github.com/NicoHerbig/MMPE"," ['3 Concept and Implementation', '3.3 QE Integration into CAT Environment']","Our implementation was done within the MMPE CAT tool [Cite_Footnote_4] (Herbig et al., 2019, 2020a,b,c; Jamara et al., 2021), as it is open source and easily extendable.",4 https://github.com/NicoHerbig/MMPE,"A Computer-Aided Translation (CAT) tool or PE environment allows the capture and correction of mistakes, as well as the selection, manipulation, adaptation and recombination of good segments (Herbig et al., 2020c). Our implementation was done within the MMPE CAT tool [Cite_Footnote_4] (Herbig et al., 2019, 2020a,b,c; Jamara et al., 2021), as it is open source and easily extendable.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.799_1_0,2021,Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output,Footnote
181,10183," https://youtu.be/6LgUzia_3pM"," ['4 Evaluation', '4.1 Method']","In order to successfully perform stage two of the experiment, the partici-pants received an explanation of all of the proto-type’s features and pointers regarding the execution of the main experiment in a four minute introduc-tory video [Cite_Footnote_6] .",6 https://youtu.be/6LgUzia_3pM,"Due to the ongoing COVID-19 pandemic the eval-uation was conducted online by 17 participants. The study took approximately one hour per par-ticipant and involved three separate stages. First, participants filled in a questionnaire capturing de-mographics as well as information on their trans-lation skills, post-editing skills, and CAT usage. Stage two was the main experiment, where par-ticipants had to post-edit segments with different quality QE annotations in the two visualizations, as described in detail below. In order to successfully perform stage two of the experiment, the partici-pants received an explanation of all of the proto-type’s features and pointers regarding the execution of the main experiment in a four minute introduc-tory video [Cite_Footnote_6] . After the video, translators were asked to try out MMPE with word-level QE in a trial project to get accustomed to the environment (for which we did not record data) before delving into the main experiment. In the final stage, the par-ticipants filled out another questionnaire capturing their experience during the experiment, pain points, and other feedback.",補足資料,Media,True,Produce（引用目的）,False,2021.emnlp-main.799_2_0,2021,Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output,Footnote
182,10184," https://www.upwork.com/"," ['4 Evaluation', '4.3 Participants']",The prototype was evaluated by non color-blind professional translators and translation students who were working as freelancers on a platform called Upwork Global Inc [Cite_Footnote_8] .,"8 https://www.upwork.com/tings. Machine Translation, 29:49–67.","The prototype was evaluated by non color-blind professional translators and translation students who were working as freelancers on a platform called Upwork Global Inc [Cite_Footnote_8] . We used EN–DE text, and therefore recruited participants that were well-versed in English and German, having either C1 or C2 level of proficiency in both languages.",補足資料,Website,True,Introduce（引用目的）,True,2021.emnlp-main.799_3_0,2021,Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output,Footnote
183,10185," http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/?page=downloads/compounds"," ['5 Conclusions and Future Work']",The complete resource will be made freely available. [Cite_Footnote_3],3 http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/?page=downloads/ compounds,"We presented a multilingual dataset of nominal com-pounds containing human judgments about composi-tionality. It contains 180 compounds for each of the 3 target languages: English, French and Portuguese. An-notations are collected through crowdsourcing. Since the task is performed by native speakers who may not have a background in linguistics, it needs to be appro-priately constrained not to require expert knowledge. The resulting resource can be used for applications and tasks involving some degree of semantic processing, such as lexical substitution and text simplification. For the cases where the numerical judgments alone are not enough for a given task, our dataset also provides sets of paraphrases, which serve as a symbolic counterpart to those scores. The complete resource will be made freely available. [Cite_Footnote_3] As future work, we plan to validate these scores through compositionality prediction (Yaz-dani et al., 2015; Salehi et al., 2015) and by incorporat-ing the scores and paraphrases into a machine transla-tion system. We also envisage extending the dataset for each of the languages and for additional languages.",Material,Dataset,True,Produce（引用目的）,True,P16-2026_0_0,2016,How Naked is the Naked Truth? A Multilingual Lexicon of Nominal Compound Compositionality,Footnote
184,10186," https://github.com/wxue004cs/GCAE"," ['References']",The experiments on SemEval datasets demon-strate the efficiency and effectiveness of our models. [Cite_Footnote_1],1 The code and data is available at https://github.com/wxue004cs/GCAE,"Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term senti-ment analysis (ATSA). Most previous ap-proaches employ long short-term mem-ory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural net-works and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selec-tively output the sentiment features ac-cording to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Sec-ond, the computations of our model could be easily parallelized during training, be-cause convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demon-strate the efficiency and effectiveness of our models. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P18-1234_0_0,2018,Aspect Based Sentiment Analysis with Gated Convolutional Networks,Footnote
185,10187," http://132.70.6.148:at:8080/exploration"," ['5 Application to the Health-care Domain']",The web application of our system is available [Cite] http://132.70.6.148:,,The web application of our system is available [Cite] http://132.70.6.148:at:8080/exploration,補足資料,Website,True,Produce（引用目的）,True,P12-3014_1_0,2012,Entailment-based Text Exploration with Application to the Health-care Domain,Body
186,10188," http://www.cs.tau.ac.il/~jonatha6/homepage_files/resources/HealthcareGraphs.rar"," ['5 Application to the Health-care Domain']",For the entailment graph we used the 23 entail-ment graphs published by Berant et al. [Cite_Footnote_2] .,2 http://www.cs.tau.ac.il/˜jonatha6/homepage_files/resources/HealthcareGraphs. rar,For the entailment graph we used the 23 entail-ment graphs published by Berant et al. [Cite_Footnote_2] . For the ar-gument taxonomy we employed UMLS – a database that maps natural language phrases to over one mil-lion unique concept identifiers (CUIs) in the health-care domain. The CUIs are also mapped in UMLS to a concept taxonomy for the health-care domain.,Material,Knowledge,True,Use（引用目的）,True,P12-3014_2_0,2012,Entailment-based Text Exploration with Application to the Health-care Domain,Footnote
187,10189," http://www.cs.york.ac.uk/semeval-2012/task6/"," ['6 Conclusions']","An example of such an approach, as applied to the task of grammat-ical error detection, can be found in (Madnani et al., 2011). [Cite_Footnote_3]",3 A good approximation is to use an ordinal scale for the human judgments as in the Semantic Textual Similarity task of SemEval 2012. See http://www.cs.york.ac.uk/semeval-2012/task6/ for more details.,"We also have two specific suggestions that we be-lieve can benefit the community. First, we believe that binary indicators of semantic equivalence are not ideal and a continuous value between 0 and 1 indicating the degree to which two pairs are para-phrastic is more suitable for most approaches. How-ever, rather than asking annotators to rate pairs on a scale, a better idea might be to show the sentence pairs to a large number of Turkers (≥ 20) on Ama-zon Mechanical Turk and ask them to classify it as either a paraphrase or a non-paraphrase. A simple estimate of the degree of semantic equivalence of the pair is simply the proportion of the Turkers who classified the pair as paraphrastic. An example of such an approach, as applied to the task of grammat-ical error detection, can be found in (Madnani et al., 2011). [Cite_Footnote_3] Second, we believe that the PAN corpus— with Turker simulated plagiarism—contains much more realistic examples of paraphrase and should be incorporated into future evaluations of paraphrase identification. In order to encourage this, we are re-leasing our PAN dataset containing 13,000 sentence pairs.",補足資料,Paper,True,Introduce（引用目的）,True,N12-1019_0_0,2012,Re-examining Machine Translation Metrics for Paraphrase Identification,Footnote
188,10190," http://bit.ly/mt-para"," ['6 Conclusions']","Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [Cite_Footnote_4]",4 The data is available at http://bit.ly/mt-para.,"We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [Cite_Footnote_4]",Material,Dataset,True,Produce（引用目的）,True,N12-1019_1_0,2012,Re-examining Machine Translation Metrics for Paraphrase Identification,Footnote
189,10191," http://www.itl.nist.gov/iad/mig/tests/metricsmatr/"," ['3 Classifying with MT Metrics', '3.2 MT metrics used']","Our choice of metrics was based on their popular-ity in the MT community, their performance in open competitions such as the NIST MetricsMATR chal-lenge (NIST, 2008) [Cite_Ref] and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity.",NIST. 2008. NIST MetricsMATR Challenge. Informa-tion Access Division. http://www.itl.nist.gov/iad/mig/tests/metricsmatr/.,"Our choice of metrics was based on their popular-ity in the MT community, their performance in open competitions such as the NIST MetricsMATR chal-lenge (NIST, 2008) [Cite_Ref] and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity.",補足資料,Website,True,Introduce（引用目的）,True,N12-1019_2_0,2012,Re-examining Machine Translation Metrics for Paraphrase Identification,Reference
190,10192," http://www.statmt.org/wmt16/translation-task.html"," ['3 Experiments', '3.1 Data and preprocessing']",We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task [Cite_Footnote_1] .,1 http://www.statmt.org/wmt16/translation-task.html,"For the syntax-based NMT, we take syntac-tic trees of source texts as inputs. We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task [Cite_Footnote_1] . Both sides are tokenized and split into subwords using BPE with 8000 merge operations. English text is parsed using SyntaxNet (Alberti et al., 2017). Then we transform the labeled dependency tree into the extended Levi graph as described in Section 2.2. Unlike AMR-to-text generation, in NMT task the input sentence contains significant sequential in-formation. This information is lost when treating the sentence as a graph. Guo et al. (2019) consider this information by adding sequential connections between each word node. In our model, we also add forward and backward edges in the extended Levi graph. Thus, the edge types vocabulary for the extended Levi graph of the dependency tree is T ={default, reverse, self, forward, backward}. So the set of subgraphs for NMT is G sub = {fully-connected, connected, default, reverse, forward, backward}. Note that we do not change the model architecture in the NMT tasks. However, we still get good results, which indicates the effectiveness of our model on Graph2Seq tasks. Except for in-troducing BPE into Levi graph, the above prepro-cessing steps are following Bastings et al. (2017). We refer to them for further information on the preprocessing steps.",Material,DataSource,True,Use（引用目的）,True,2020.acl-main.640_0_0,2020,Heterogeneous Graph Transformer for Graph-to-Sequence Learning,Footnote
191,10193," https://github.com/QAQ-v/HetGT"," ['3 Experiments', '3.2 Parameter Settings']",Our code is available at [Cite] https://github.com/QAQ-v/HetGT.,,"Both our encoder and decoder have 6 layers with 512-dimensional word embeddings and hidden states. We employ 8 heads and dropout with a rate of 0.3. For optimization, we use Adam opti-mizer with β 2 = 0.998 and set batch size to 4096 tokens. Meanwhile, we increase learning rate lin-early for the first warmup steps, and decrease it thereafter proportionally to the inverse square root of the step number. We set warmup steps to 8000. The similar learning rate schedule is adopted in (Vaswani et al., 2017). Our implementa-tion uses the openNMT library (Klein et al., 2017). We train the models for 250K steps on a single GeForce GTX 1080 Ti GPU. Our code is available at [Cite] https://github.com/QAQ-v/HetGT.",Method,Code,True,Produce（引用目的）,True,2020.acl-main.640_1_0,2020,Heterogeneous Graph Transformer for Graph-to-Sequence Learning,Body
192,10194," http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model"," ['3 Experiments', '3.3 Metrics and Baselines']",Our baseline is the original Transformer [Cite_Footnote_2] .,2 Parameters were chosen following the OpenNMT FAQ: http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i- use-the-transformer-model,"Our baseline is the original Transformer [Cite_Footnote_2] . For AMR-to-text generation, Transformer takes lin-earized graphs as inputs. For syntax-based NMT, Transformer is trained on the preprocessed trans-lation dataset without syntactic information. We also compare the performance of HetGT with pre-vious single/ensenmble approaches which can be grouped into three categories: (1) Recurrent neu-ral network (RNN) based methods (GGNN2Seq, GraphLSTM); (2) Graph neural network (GNN) based methods (GCNSEQ, DGCN, G2S-GGNN); (3) The Transformer based methods (Structural Transformer, GTransformer). The ensemble mod-els are denoted by subscripts in Table 2 and Table 3.",Method,Code,True,Use（引用目的）,True,2020.acl-main.640_2_0,2020,Heterogeneous Graph Transformer for Graph-to-Sequence Learning,Footnote
193,10195," http://github.com/ryancotterell/derviational-paradigms"," ['3 Task and Models', '3.1 Data']","The final dataset includes 6,029 derivational samples, which we split into train (70%), development (15%), and test (15%). [Cite_Footnote_5]",5 The dataset is available at http://github.com/ryancotterell/derviational-paradigms.,"We experiment on English derivational triples ex-tracted from NomBank (Meyers et al., 2004). 4 Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., hameliorate, RESULT , ameliorationi. Note that in this task we do not predict whether a slot ex-ists, merely what form it would take given the base and the slot. In terms of current study, we consider the following derivational types: verb nominaliza-tion such as RESULT , AGENT and PATIENT , ad-verbalization and adjective-noun transformations. We intentionally avoid zero-derivations. We also exclude overly orthographically distant pairs by fil-tering out those for which the Levenshtein distance exceeds half the sum of their lengths, which ap-pear to be misannotations in NomBank. The final dataset includes 6,029 derivational samples, which we split into train (70%), development (15%), and test (15%). [Cite_Footnote_5] We also note that NomBank annota-tions are often semantically more coarse-grained.",Material,Dataset,True,Produce（引用目的）,True,D17-1074_0_0,2017,Paradigm Completion for Derivational Morphology Ryan Cotterell @ Ekaterina Vylomova H Huda Khayrallah @ Christo Kirov @ David Yarowsky @,Footnote
194,10196," https://github.com/rsennrich/nematus/"," ['3 Task and Models', '3.4 RNN Encoder-Decoder']","We use the Nematus toolkit (Sennrich et al., 2017). [Cite_Footnote_6]",6 https://github.com/rsennrich/nematus/,"Training. We use the Nematus toolkit (Sennrich et al., 2017). [Cite_Footnote_6] We exactly follow the recipe in Kann and Schütze (2016), the winning submission on the 2016 SIGMORPHON shared task for inflectional morphology. Accordingly, we use a character em-bedding size of 300, 100 hidden units in both the encoder and decoder, Adadelta (Zeiler, 2012) with a minibatch size of 20, and a beam size of 12. We train for 300 epochs and select the test model based on the performance on the development set.",Method,Tool,True,Use（引用目的）,True,D17-1074_1_0,2017,Paradigm Completion for Derivational Morphology Ryan Cotterell @ Ekaterina Vylomova H Huda Khayrallah @ Christo Kirov @ David Yarowsky @,Footnote
195,10197," https://github.com/abisee/cnn-dailymail"," ['4 Experiments', '4.1 Datasets']","For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017) [Cite_Footnote_3] .",3 Scripts publicly available at https://github.com/abisee/cnn-dailymail,"We conducted our summarization experiments on the non-anonymous version CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; See et al., 2017), and the New York Times dataset (Durrett et al., 2016; Xu and Durrett, 2019). For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017) [Cite_Footnote_3] . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we cre-ated the NYT50 dataset by removing the docu-ments whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Dur-rett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive sum-marization, we used a strategy similar to Nallapati et al. (2017). We label the subset of sentences in a document that maximizes R OUGE (Lin, 2004) (against the human summary) as True and all other sentences as False.",Material,DataSource,True,Use（引用目的）,True,P19-1499_0_0,2019,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,Footnote
196,10198," https://catalog.ldc.upenn.edu/LDC2012T21"," ['4 Experiments', '4.1 Datasets']","To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [Cite_Footnote_4] dataset and the training split of the CNNDM dataset.",4 https://catalog.ldc.upenn.edu/LDC2012T21,"To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [Cite_Footnote_4] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al., 2014). To re-duce the vocabulary size, we applied byte pair en-coding (BPE; Sennrich et al. 2016) to all of our datasets. To limit the memory consumption dur-ing training, we limit the length of each sentence to be 50 words (51th word and onwards are re-moved) and split documents with more than 30 sentences into smaller documents with each con-taining at most 30 sentences.",Material,DataSource,True,Use（引用目的）,True,P19-1499_1_0,2019,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,Footnote
197,10199," https://github.com/huggingface/pytorch-pretrained-BERT"," ['4 Experiments', '4.4 Results']","Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [Cite_Footnote_7] and finetuned on the CNNDM dataset.",7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/ pytorch-pretrained-BERT,"Our main results on the CNNDM dataset are shown in Table 1, with abstractive models in the top block and extractive models in the bot-tom block. Pointer+Coverage (See et al., 2017), Abstract-ML+RL (Paulus et al., 2017) and DCA (Celikyilmaz et al., 2018) are all sequence to se-quence learning based models with copy and cov-erage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite (Hsu et al., 2018) and InconsisLoss (Chen and Bansal, 2018) all try to decompose the word by word summary generation into sentence selection from document and “sentence” level summariza-tion (or compression). Bottom-Up (Gehrmann et al., 2018) generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; Nalla-pati et al. 2017 and NeuSum; Cheng and Lapata 2016). They have been extended with reinforce-ment learning (Refresh; Narayan et al. 2018 and BanditSum; Dong et al. 2018), Maximal Marginal Relevance (NeuSum-MMR; Zhou et al. 2018), la-tent variable modeling (LatentSum; Zhang et al. 2018) and syntactic compression (JECS; Xu and Durrett 2019). Lead3 is a baseline which sim-ply selects the first three sentences. Our model H IBERT S (in-domain), which only use one pre-training stage on the in-domain CNNDM training set, outperforms all of them and differences be-tween them are all significant with a 0.95 confi-dence interval (estimated with the ROUGE script). Note that pre-training H IBERT S (in-domain) is very fast and it only takes around 30 minutes for one epoch on the CNNDM training set. Our models with two pre-training stages (H IBERT S ) or larger size (H IBERT M ) perform even better and H IBERT M outperforms BERT by 0.5 ROUGE . We also implemented two baselines. One is the hierarchical transformer summarization model (HeriTransfomer; described in 3.3) without pre-training. Note the setting for HeriTransfomer is (L = 4,H = 300 and A = 4) . We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [Cite_Footnote_7] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1. The posi-tional embedding of BERT supports input length up to 512 words, we therefore split documents with more than 10 sentences into multiple blocks (each block with 10 sentences 8 ). We feed each block (the BOS and EOS tokens of each sentence are replaced with [CLS] and [SEP] tokens) into BERT and use the representation at [CLS] token to classify each sentence. Our model H IBERT S outperforms BERT by 0.4 to 0.5 ROUGE despite with only half the number of model parameters (H IBERT S 54.6M v.s. BERT 110M).",Material,DataSource,True,Extend（引用目的）,False,P19-1499_2_0,2019,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,Footnote
198,10200," https://github.com/bishanyang/EventEntityExtractor"," ['1 Introduction']","In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context. [Cite_Footnote_1]",1 The code for our system is available at https://github.com/bishanyang/EventEntityExtractor.,"In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context. [Cite_Footnote_1] We first decompose the learn-ing problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the co-occurrence relations between events across the doc-ument, and (3) learning for entity extraction. Then we combine the learned models for these subprob-lems into a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document. In summary, our main contribu-tions are:",Material,Knowledge,True,Produce（引用目的）,True,N16-1033_0_0,2016,Joint Extraction of Events and Entities within a Document Context,Footnote
199,10201," http://www.itl.nist.gov/iad/mig/tests/ace/2005/"," ['4 Experiments']",We conduct experiments on the ACE2005 corpus. [Cite_Footnote_6],6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/,"We conduct experiments on the ACE2005 corpus. [Cite_Footnote_6] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al. (2013). Table 2 shows the data statistics.",Material,Dataset,True,Use（引用目的）,True,N16-1033_1_0,2016,Joint Extraction of Events and Entities within a Document Context,Footnote
200,10202," https://github.com/oferbr/BIU-RPI-Event-Extraction-Project"," ['4 Experiments', '4.1 Results']","To obtain results from J OINT B EAM , we ran the ac-tual system [Cite_Footnote_8] used in Li et al. (2013) using the entity mentions output by our CRF-based entity extractor.",8 https://github.com/oferbr/ BIU-RPI-Event-Extraction-Project,"Event Extraction. We compare the proposed models W ITHIN E VENT (in Section 3.1) and J OIN - T E VENT E NTITY (in Section 3.4) with two strong baselines. One is J OINT B EAM (Li et al., 2013), a state-of-the-art event extractor that uses a structured perceptron with beam search for sentence-level joint extraction of event triggers and arguments. The other is S TAGED M AX E NT , a typical two-stage ap-proach that detects event triggers first and then event arguments. We use the same event trigger candidates and entity mention candidates as input to all the comparing models except for J OINT B EAM , because J OINT B EAM only extracts event mentions and as-sumes entity mentions are given. We consider a re-alistic experimental setting where no gold-standard annotations are available for entities during testing. To obtain results from J OINT B EAM , we ran the ac-tual system [Cite_Footnote_8] used in Li et al. (2013) using the entity mentions output by our CRF-based entity extractor.",補足資料,Website,False,Introduce（引用目的）,True,N16-1033_2_0,2016,Joint Extraction of Events and Entities within a Document Context,Footnote
201,10203," https://en.wikipedia.org"," ['2 The Document Grounded Dataset', '2.1 Document Set Creation']",We choose Wikipedia (Wiki) [Cite_Footnote_1] articles to cre-ate a set of documents,1 https://en.wikipedia.org,"We choose Wikipedia (Wiki) [Cite_Footnote_1] articles to cre-ate a set of documents D = {d 1 ,...,d 30 } for grounding of conversations. We randomly select 30 movies, covering various genres like thriller, super-hero, animation, romantic, biopic etc. We extract the key information provided in the Wiki article and divide it into four separate sections. This was done to reduce the load of the users to read, absorb and discuss the information in the document. Hence, each movie document d i con-sists of four sections {s 1 , s 2 , s 3 , s 4 } correspond-ing to basic information and three key scenes of the movie. The basic information section s 1 con-tains data from the Wikipedia article in a stan-dard form such as year, genre, director. It also includes a short introduction about the movie, rat-ings from major review websites, and some crit-ical responses. Each of the key scene sections {s 2 , s 3 , s 4 } contains one short paragraph from the plot of the movie. Each paragraph contains on an average 7 sentences and 143 words. These para-graphs were extracted automatically from the orig-inal articles, and were then lightly edited by hand to make them of consistent size and detail. An ex-ample of the document is attached in Appendix.",Material,DataSource,True,Extend（引用目的）,True,D18-1076_0_0,2018,A Dataset for Document Grounded Conversations,Footnote
202,10204," https://www.github.com/festvox/datasets/CMUDoG"," ['References']",In this paper we introduce a crowd-sourced con-versations dataset that is grounded in a predefined set of documents which is available for download [Cite_Footnote_4] .,4 https://www.github.com/festvox/datasets/CMU DoG,"In this paper we introduce a crowd-sourced con-versations dataset that is grounded in a predefined set of documents which is available for download [Cite_Footnote_4] . We perform multiple automatic and human judgment based analysis to understand the value the information from the document provides to the generation of responses. The SEQS model which uses the information from the section to generate responses outperforms the SEQ model in the eval-uation tasks of engagement, fluency and perplex-ity. as a part of The Conversational Intelligence Chal-lenge (ConvAI, NIPS 2017) and we would like to thank the ConvAI team. We are also grateful to the anonymous reviewers for their constructive feedback and to Carolyn Penstein Rose, Shivani Poddar, Sreecharan Sankaranarayanan, Samridhi Shree Choudhary and Zhou Yu for valuable discussions at earlier stages of this work.",Material,DataSource,True,Extend（引用目的）,True,D18-1076_1_0,2018,A Dataset for Document Grounded Conversations,Footnote
203,10205," http://www.cs.cmu.edu/~cprose/SIDE.html"," ['References']","In this type-II demo, we introduce SIDE [Cite_Footnote_1] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user.","1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043","In this type-II demo, we introduce SIDE [Cite_Footnote_1] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes. Rather, the quality of a summary is subjective, task dependent, and possibly specific to a user. The SIDE frame-work allows users flexibility in determining what they find more useful in a summary, both in terms of structure and content. As an educational tool, it has been successfully user tested by a class of 21 students in a graduate course on Summarization and Personal Infor-mation Management.",補足資料,Website,True,Introduce（引用目的）,True,P08-4007_0_0,2008,SIDE: The Summarization Integrated Development Environment,Footnote
204,10206," http://ankara.lti.cs.cmu.edu/side/video.swf"," ['References']","In this type-II demo, we introduce SIDE [Cite_Footnote_1] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user.","1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043","In this type-II demo, we introduce SIDE [Cite_Footnote_1] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes. Rather, the quality of a summary is subjective, task dependent, and possibly specific to a user. The SIDE frame-work allows users flexibility in determining what they find more useful in a summary, both in terms of structure and content. As an educational tool, it has been successfully user tested by a class of 21 students in a graduate course on Summarization and Personal Infor-mation Management.",補足資料,Media,True,Introduce（引用目的）,True,P08-4007_1_0,2008,SIDE: The Summarization Integrated Development Environment,Footnote
205,10207," http://nlp.cs.nyu.edu/comlex/refman.ps"," ['2 Preliminaries', '2.2 Gold standard data']","Information about noun countability was obtained from two sources: COMLEX 3.0 (Grishman et al., 1998) [Cite_Ref] and the common noun part of ALT-J/E ’s Japanese-to-English semantic transfer dictio-nary (Ikehara et al., 1991).","Ralph Grishman, Catherine Macleod, and Adam Myers, 1998. COMLEX Syntax Reference Manual. Proteus Project, NYU. (http://nlp.cs.nyu.edu/comlex/refman.ps).","Information about noun countability was obtained from two sources: COMLEX 3.0 (Grishman et al., 1998) [Cite_Ref] and the common noun part of ALT-J/E ’s Japanese-to-English semantic transfer dictio-nary (Ikehara et al., 1991). Of the approximately 22,000 noun entries in COMLEX , 13,622 are marked as countable , 710 as uncountable and the remainder are unmarked for countability. ALT-J/E has 56,245 English noun types with distinct countability.",Material,Knowledge,True,Use（引用目的）,True,W03-1010_0_0,2003,A Plethora of Methods for Learning English Countability,Reference
206,10208," http://www.isi.edu/~cyl/ROUGE"," ['4 Experiments on DUC 2004 data', '4.1 DUC 2004 data and ROUGE']","For evaluation, we used the new automatic sum-mary evaluation metric, ROUGE [Cite_Footnote_1] , which was used for the first time in DUC 2004.",1 http://www.isi.edu/˜cyl/ROUGE,"For evaluation, we used the new automatic sum-mary evaluation metric, ROUGE [Cite_Footnote_1] , which was used for the first time in DUC 2004. ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence. It reports separate scores for 1, 2, 3, and 4-gram, and also for longest common subsequence co-occurences. Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgements most (Lin and Hovy, 2003). We show three of the ROUGE metrics in our experiment results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on longest common subsequence weighted by the length).",Method,Code,True,Use（引用目的）,True,W04-3247_0_0,2004,LexPageRank: Prestige in Multi-Document Text Summarization,Footnote
207,10209," http://www.summarization.com"," ['4 Experiments on DUC 2004 data', '4.2 MEAD summarization toolkit']",MEAD [Cite_Footnote_2] is a publicly available toolkit for extractive multi-document summarization.,2 http://www.summarization.com,"MEAD [Cite_Footnote_2] is a publicly available toolkit for extractive multi-document summarization. Although it comes as a centroid-based summarization system by de-fault, its feature set can be extended to implement other methods.",Method,Tool,True,Introduce（引用目的）,True,W04-3247_1_0,2004,LexPageRank: Prestige in Multi-Document Text Summarization,Footnote
208,10210," https://github.com/sheffieldnlp/fever-scorer"," ['4 Experiments', '4.1 Setup']",We use the officially released evaluation scorer [Cite_Footnote_3] .,3 https://github.com/sheffieldnlp/fever-scorer,"This task has three evaluations: (i) N O S CORE E V – accuracy of claim verifica-tion, neglecting the validity of evidence; (ii) S CORE E V – accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for S UPPORTED and R E - FUTED ; (iii) F 1 – between the predicted evidence sentences and the ones chosen by annotators. We use the officially released evaluation scorer [Cite_Footnote_3] .",Material,Knowledge,True,Use（引用目的）,True,D18-1010_0_0,2018,T WO W ING OS: A Two-Wing Optimization Strategy for Evidential Claim Verification,Footnote
209,10211," https://github.com/pushshift/api"," ['4 Data', '4.1 Data collection']",Data was scraped from the two subreddits by querying the Pushshift API. [Cite_Footnote_4],4 https://github.com/pushshift/api model with all features. The numerical results are given in the appendix (Table 6.),"Data was scraped from the two subreddits by querying the Pushshift API. [Cite_Footnote_4] 3 years’ worth of posts, ranging from 1 January 2017 to 31 Decem-ber 2019, were collected for each subreddit. To ensure each post had sufficient linguistic content, I excluded any posts containing less than 101 char-acters.",Method,Code,True,Use（引用目的）,True,2021.acl-srw.10_1_0,2021,Stylistic Approaches to Predicting Reddit Popularity in Diglossia,Footnote
210,10212," https://www.reddit.com/r/singapore/comments/8gfewd/"," ['6 Qualitative evaluation and analysis', '6.3 Lexical closeness']","For ex-ample, one post is written in very eloquent standard English [Cite_Footnote_5] , but includes Singlish quotes as well as specific, appropriate Singlish terms (with English explanations in brackets).",5 https://www.reddit.com/r/singapore/comments/8gfewd/ the singaporean male version of metoo an exguards/,"A reading of the Level 7 texts including Singlish terms confirm that this is indeed the case. For ex-ample, one post is written in very eloquent standard English [Cite_Footnote_5] , but includes Singlish quotes as well as specific, appropriate Singlish terms (with English explanations in brackets).",補足資料,Document,True,Introduce（引用目的）,True,2021.acl-srw.10_2_0,2021,Stylistic Approaches to Predicting Reddit Popularity in Diglossia,Footnote
211,10213," http://www.jnlp.org/SNOW/E4"," ['1 Introduction']",Kajiwara and Ya-mamoto (2015) constructed an evaluation dataset for Japanese lexical simplification [Cite_Footnote_1] in languages other than English.,1 http://www.jnlp.org/SNOW/E4,"Lexical simplification is the task to find and sub-stitute a complex word or phrase in a sentence with its simpler synonymous expression. We de-fine complex word as a word that has lexical and subjective difficulty in a sentence. It can help in reading comprehension for children and language learners (De Belder and Moens, 2010). This task is a rather easier task which prepare a pair of com-plex and simple representations than a challeng-ing task which changes the substitute pair in a given context (Specia et al., 2012; Kajiwara and Yamamoto, 2015). Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation. However, few re-sources are available for the automatic evaluation of lexical simplification. Specia et al. (2012) and De Belder and Moens (2010) created benchmark datasets for evaluating English lexical simplifica-tion. In addition, Horn et al. (2014) extracted sim-plification candidates and constructed an evalua-tion dataset using English Wikipedia and Simple English Wikipedia. In contrast, such a parallel cor-pus does not exist in Japanese. Kajiwara and Ya-mamoto (2015) constructed an evaluation dataset for Japanese lexical simplification [Cite_Footnote_1] in languages other than English.",補足資料,Website,True,Introduce（引用目的）,True,P16-3001_0_0,2016,Controlled and Balanced Dataset for Japanese Lexical Simplification,Footnote
212,10214," https://github.com/KodairaTomonori/EvaluationDataset"," ['1 Introduction']",Our dataset is available at GitHub [Cite_Footnote_2] .,2 https://github.com/KodairaTomonori/EvaluationDataset,• The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators. Our dataset is available at GitHub [Cite_Footnote_2] .,Material,Dataset,True,Produce（引用目的）,True,P16-3001_1_0,2016,Controlled and Balanced Dataset for Japanese Lexical Simplification,Footnote
213,10215," http://www.lancers.jp/"," ['4 Balanced dataset for evaluation of Japanese lexical simplification']","We use a crowdsourcing application, Lancers, [Cite_Footnote_3] to perform substitute extraction, substitute evalua-tion, and substitute ranking.",3 http://www.lancers.jp/,"We use a crowdsourcing application, Lancers, [Cite_Footnote_3] to perform substitute extraction, substitute evalua-tion, and substitute ranking. In each task, we re-quested the annotators to complete at least 95% of their previous assignments correctly. They were native Japanese speakers.",Material,DataSource,False,Use（引用目的）,True,P16-3001_2_0,2016,Controlled and Balanced Dataset for Japanese Lexical Simplification,Footnote
214,10216," http://jhlee.sakura.ne.jp/JEV.html"," ['4 Balanced dataset for evaluation of Japanese lexical simplification', '4.1 Extracting sentences']","Our work defines complex words as “High Level” words in the Lexicon for Japanese Language Edu-cation (Sunakawa et al., 2012). [Cite_Footnote_4]",4 http://jhlee.sakura.ne.jp/JEV.html,"Our work defines complex words as “High Level” words in the Lexicon for Japanese Language Edu-cation (Sunakawa et al., 2012). [Cite_Footnote_4] The word level is calculated by five teachers of Japanese, based on their experience and intuition. There were 7,940 high-level words out of 17,921 words in the lex-icon. In addition, target words of this work com-prised content words (nouns, verbs, adjectives, ad-verbs, adjectival nouns, sahen nouns, and sahen verbs ).",Material,Knowledge,False,Use（引用目的）,True,P16-3001_3_0,2016,Controlled and Balanced Dataset for Japanese Lexical Simplification,Footnote
215,10217," https://github.com/wanqiulong0923/TED-CDB"," ['References']",The dataset and our Chinese annotation guidelines has been made freely available. [Cite_Footnote_1],"1 https://github.com/wanqiulong0923/TED-CDB manifestation of intelligence, but more about our na-ture as a living, breathable organism] 1 . [The difference between consciousness and intelligence is very large. You will feel pain even if you are not smart, but only if you have to live] 2 . [(Therefore) RESULT+SPEECHACT","As different genres are known to differ in their communicative properties and as previ-ously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chi-nese that have been manually annotated ac-cording to the goals and principles of Penn Dis-course Treebank, but adapted to features that are not present in English. It serves as a unique Chinese corpus of spoken discourse. Bench-mark experiments show that TED-CDB poses a challenge for state-of-the-art discourse rela-tion classifiers, whose F1 performance on 4-way classification is <60%. This is a dramatic drop of 35% from performance on the news text in the Chinese Discourse Treebank. Trans-fer learning experiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain cross-language transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insuffi-cient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.223_0_0,2020,TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,Footnote
216,10218," https://github.com/PaddlePaddle/ERNIE"," ['5 Experiments', '5.1 Methods']","[Cite_Footnote_3] , a.k.a Enhanced Represen-tation through Knowledge Integration (Sun et al., 2019), which is trained with not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc.",3 https://github.com/PaddlePaddle/ERNIE,"• ERNIE (115M) [Cite_Footnote_3] , a.k.a Enhanced Represen-tation through Knowledge Integration (Sun et al., 2019), which is trained with not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc.",Material,Knowledge,False,Produce（引用目的）,False,2020.emnlp-main.223_2_0,2020,TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,Footnote
217,10219," https://github.com/ymcui/Chinese-BERT-wwm"," ['5 Experiments', '5.1 Methods']",We used ROBERTa-wwm-est-large. [Cite_Footnote_4],4 https://github.com/ymcui/Chinese-BERT-wwm,"• ROBERTa, a robust BERT (Liu et al., 2019). We used ROBERTa-wwm-est-large. [Cite_Footnote_4]",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.223_3_0,2020,TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,Footnote
218,10220," https://github.com/huggingface/transformers"," ['6 Transfer Learning via TED-CDB', '6.2 Same-Domain Cross-Language Learning']",We used the multilingual BERT implementation from Huggingface. [Cite_Footnote_5],5 https://github.com/huggingface/transformers,"TED-MDB (Zeyrek et al., 2018) corpus annotation follows the PDTB 3.0 framework. It contains man-ual annotation of 6 TED talks in seven languages (English, Turkish, European Potuguese, Polish, German, Russian, and Lithuanian). The sub-corpus for each language is quite small, with about 200 im-plicit discourse relations each, compared with the ∼7.0 K implicit relations in the TED-CDB. There-fore, we can see whether the TED-CDB can help them. For this experiment, the multilingual BERT was used, which is as large as BERT-wwm but the training data is expanded to cover 104 languages. We used the multilingual BERT implementation from Huggingface. [Cite_Footnote_5] The design for these exper-iments is making a comparison between a cross validation within the TED-MDB and a zero-shot transfer learning from TED-CDB to TED-MDB. Due to the unbalanced distribution of senses in TED-MDB, using the method of Easy Ensemble (Liu, 2009), we divided the Expansion data of every language in the TED-MDB into 4 parts and then each part was added into the data of other types to become the training set. Finally, we integrated these training sets from 6 language into one train-ing set, and the left data for one language would be the test set. Therefore, what we used here is 4-fold cross validation where each fold is used as the test set exactly once. The average test set accuracy is then reported.",Material,Knowledge,False,Use（引用目的）,True,2020.emnlp-main.223_4_0,2020,TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,Footnote
219,10221," https://github.com/WalterSimoncini/SeqAttack"," ['5 Results and discussion', '5.2 Adversarial training']","To be Examples Acc Recall F1 SeqAttack [Cite_Footnote_1] , a Python library for conducting ad- 500 examples 91% 1000 examples 90% 1500 examples 90% 2000 examples 90% NER small 91% 90% versarial attacks against token classification models.90% 89%",1 https://github.com/WalterSimoncini/ SeqAttack,"Figure 4: KDE plots for the labels score and modifica-tion rate distributions for NER small and its robust coun-terpart, when attacked with DeepWordBug-I 5 . To be Examples Acc Recall F1 SeqAttack [Cite_Footnote_1] , a Python library for conducting ad- 500 examples 91% 1000 examples 90% 1500 examples 90% 2000 examples 90% NER small 91% 90% versarial attacks against token classification models.90% 89% The library is accompanied by a web application 90% 90% to inspect the generated adversarial examples and90%",Method,Code,False,Use（引用目的）,True,2021.emnlp-demo.35_0_0,2021,SeqAttack: On Adversarial Attacks for Named Entity Recognition,Footnote
220,10222," https://ner-attack.ashita.nl/"," ['5 Results and discussion', '5.2 Adversarial training']",The library is accompanied by a web application [Cite_Footnote_2] 90% 90% to inspect the generated adversarial examples,2 Application available at https://ner-attack.ashita.nl/,"Figure 4: KDE plots for the labels score and modifica-tion rate distributions for NER small and its robust coun-terpart, when attacked with DeepWordBug-I 5 . To be Examples Acc Recall F1 SeqAttack , a Python library for conducting ad- 500 examples 91% 1000 examples 90% 1500 examples 90% 2000 examples 90% NER small 91% 90% versarial attacks against token classification models.90% 89% The library is accompanied by a web application [Cite_Footnote_2] 90% 90% to inspect the generated adversarial examples and90%",Method,Tool,True,Produce（引用目的）,False,2021.emnlp-demo.35_1_0,2021,SeqAttack: On Adversarial Attacks for Named Entity Recognition,Footnote
221,10223," https://github.com/chakki-works/seqeval"," ['4 Experiments', '4.2 Adversarial training']","The metrics were calculated using seqeval (Nakayama, 2018) [Cite_Ref] .",Hiroki Nakayama. 2018. seqeval: A python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval.,"The metrics were calculated using seqeval (Nakayama, 2018) [Cite_Ref] . ↑ (↓) indicate whether the higher (or lower) the better from the attack perspective.",Method,Code,True,Use（引用目的）,True,2021.emnlp-demo.35_2_0,2021,SeqAttack: On Adversarial Attacks for Named Entity Recognition,Reference
222,10224," https://github.com/INK-USC/DIG"," ['References']",We provide the source code of DIG to encour-age reproducible research [Cite_Footnote_1] .,1 https://github.com/INK-USC/DIG,"As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation ax-ioms and the ease of gradient computation. It measures feature importance by averaging the model’s output gradient interpolated along a straight-line path in the input data space. How-ever, such straight-line interpolated points are not representative of text data due to the inher-ent discreteness of the word embedding space. This questions the faithfulness of the gradi-ents computed at the interpolated points and consequently, the quality of the generated ex-planations. Here we propose Discretized In-tegrated Gradients (DIG), which allows effec-tive attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yield-ing more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encour-age reproducible research [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.805_0_0,2021,Discretized Integrated Gradients for Explaining Language Models,Footnote
223,10225," https://en.wikipedia.org/wiki/Riemann_sum"," ['2 Method', '2.1 Discretized integrated gradients']",1 using Riemann summation [Cite_Footnote_2] which requires monotonic paths.,2 https://en.wikipedia.org/wiki/ Riemann_sum,Here m is the total number of steps for interpola-tion. This constraint is essential because it allows approximating the integral in Eq. 1 using Riemann summation [Cite_Footnote_2] which requires monotonic paths. We note that the interpolation points used by IG nat-urally satisfy this constraint since they lie along a straight line joining x and x 0 . The key distinction of our formulation from IG is that DIG is agnostic of any fixed step size parameter α and thus allows non-linear interpolation paths in the embedding space. The integral approximation of DIG is de-fined as follows:,Method,Code,True,Introduce（引用目的）,True,2021.emnlp-main.805_1_0,2021,Discretized Integrated Gradients for Explaining Language Models,Footnote
224,10226," https://github.com/huggingface/datasets"," ['3 Experimental Setup']","We use the processed dataset made available by HuggingFace Dataset library [Cite_Footnote_3] (Wolf et al., 2020b).",3 https://github.com/huggingface/ datasets,"In this section, we describe the datasets and models used for evaluating our proposed algorithm. Datasets. The SST2 (Socher et al., 2013) dataset has 6920/872/1821 example sentences in the train/dev/test sets. The task is binary classifica-tion into positive/negative sentiment. The IMDB (Maas et al., 2011) dataset has 25000/25000 exam-ple reviews in the train/test sets with similar binary labels for positive and negative sentiment. Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library [Cite_Footnote_3] (Wolf et al., 2020b).",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.805_2_0,2021,Discretized Integrated Gradients for Explaining Language Models,Footnote
225,10227," https://github.com/huggingface/datasets"," ['3 Experimental Setup']","We use the processed dataset made available by HuggingFace Dataset library 3 (Wolf et al., 2020b) [Cite_Ref] .","Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, François Lagunas, Lysandre Debut, Morgan Funtow-icz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Muštar, Jeff Boudier, and Anna Tordjmann. 2020b. Datasets. GitHub. Note: https://github.com/huggingface/datasets.","In this section, we describe the datasets and models used for evaluating our proposed algorithm. Datasets. The SST2 (Socher et al., 2013) dataset has 6920/872/1821 example sentences in the train/dev/test sets. The task is binary classifica-tion into positive/negative sentiment. The IMDB (Maas et al., 2011) dataset has 25000/25000 exam-ple reviews in the train/test sets with similar binary labels for positive and negative sentiment. Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library 3 (Wolf et al., 2020b) [Cite_Ref] .",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.805_3_0,2021,Discretized Integrated Gradients for Explaining Language Models,Reference
226,10228," https://github.com/parag1604/A2L"," ['4 Experiments', '4.1 Active Learning Strategies for Sequence Tagging']","The strategy se-lects examples for which M margin ≤ τ [Cite_Footnote_1] , where τ 1 is a hyper-parameter.",1 Codes for the experiments are available at the following github link: https://github.com/parag1604/A2L.,"Margin-based strategy: Let s(y) = P θ (Y = y|X = x) be the score assigned by a model M with parameters θ to output y for a given example x. Margin is defined as the difference in scores ob-tained by the best scoring output y and the second best scoring output y 0 , i.e.: where, y max = arg max y s(y). The strategy se-lects examples for which M margin ≤ τ [Cite_Footnote_1] , where τ 1 is a hyper-parameter. We use Viterbi’s algorithm (Ryan and Nudd, 1993) to compute the scores s(y).",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.159_0_0,2021,Active 2 Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation,Footnote
227,10229," https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs"," ['4 Experiments', '4.2 Active Learning Strategies for Neural Machine Translation 2']","Citations: CoNLL 2003 (Sang and De Meulder, 2003), CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000), SEMCOR , Europarl (Koehn, 2005), SICK (Marelli et al., 2014), Quora Pairs [Cite_Footnote_4] .",4 https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs,"Table 1: Task and dataset descriptions. AUX is the task of training the Siamese network (Section 3.2) or Integrated network C (Section 3.3). Citations: CoNLL 2003 (Sang and De Meulder, 2003), CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000), SEMCOR , Europarl (Koehn, 2005), SICK (Marelli et al., 2014), Quora Pairs [Cite_Footnote_4] .",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-main.159_1_0,2021,Active 2 Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation,Footnote
228,10230," http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz"," ['1 Motivation']","We introduce Simple PPDB, [Cite_Footnote_1] a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules.",1 http://www.seas.upenn.edu/˜nlp/resources/simple-ppdb.tgz,"Recent research in data-driven paraphrasing has produced enormous resources containing millions of meaning-equivalent phrases (Ganitkevitch et al., 2013). Such resources capture a wide range of language variation, including the types of lexical and phrasal simplifications just described. In this work, we apply state-of-the-art machine learned models for lexical simplification in order to iden-tify phrase pairs from the Paraphrase Database (PPDB) applicable to the task of text simplifica-tion. We introduce Simple PPDB, [Cite_Footnote_1] a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules. The large scale of Sim-ple PPDB will support research into increasingly advanced methods for text simplification.",Material,Knowledge,True,Use（引用目的）,True,P16-2024_0_0,2016,Simple PPDB: A Paraphrase Database for Simplification,Footnote
229,10231," http://paraphrase.org/#/download"," ['2 Identifying Simplification Rules', '2.1 Paraphrase Rules']","In this work, we use the PPDB-TLDR [Cite_Footnote_2] dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases, and is intended to give a generally good tradeoff between precision and recall.",2 http://paraphrase.org/#/download,"The Paraphrase Database (PPDB) is currently the largest available collection of paraphrases. Each paraphrase rule in the database has an automatically-assigned quality score between 1 and 5 (Pavlick et al., 2015). In this work, we use the PPDB-TLDR [Cite_Footnote_2] dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases, and is intended to give a generally good tradeoff between precision and recall. To preprocess the data, we lemmatize all of the phrases, and remove rules which differ only by morphology, punctu-ation, or stop words, or which involve phrases longer than 3 words. The resulting list contains 7.5 million paraphrase rules covering 625K unique lemmatized words and phrases.",Material,Dataset,True,Use（引用目的）,True,P16-2024_1_0,2016,Simple PPDB: A Paraphrase Database for Simplification,Footnote
230,10232," http://scikit-learn.org/"," ['2 Identifying Simplification Rules', '2.2 Lexical Simplification Model']","We train a multi-class logistic regression model [Cite_Footnote_4] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output.",4 http://scikit-learn.org/,"We train a multi-class logistic regression model [Cite_Footnote_4] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output.",Material,Knowledge,True,Use（引用目的）,True,P16-2024_2_0,2016,Simple PPDB: A Paraphrase Database for Simplification,Footnote
231,10233," http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz"," ['References']","Simple PPDB, along with the hu-man judgements collected as part of its creation, is freely available with the publication of this paper. [Cite_Footnote_6]",6 http://www.seas.upenn.edu/˜nlp/resources/simple-ppdb.tgz,"We have described Simple PPDB, a subset of the Paraphrase Database adapted for the task of text simplification. Simple PPDB is built by apply-ing state-of-the-art machine learned models for lexical simplification to the largest available re-source of lexical and phrasal paraphrases, result-ing in a web-scale resource capable of supporting research in data-driven methods for text simplifi-cation. We have shown that Simple PPDB offers substantially increased coverage of both words and multiword phrases, while maintaining high quality compared to existing methods for lexical simplification. Simple PPDB, along with the hu-man judgements collected as part of its creation, is freely available with the publication of this paper. [Cite_Footnote_6] rial is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this pub-lication are those of the authors and should not be interpreted as representing official policies or en-dorsements of DARPA and the U.S. Government.",Material,Knowledge,True,Produce（引用目的）,True,P16-2024_3_0,2016,Simple PPDB: A Paraphrase Database for Simplification,Footnote
232,10234," https://github.com/google-research/bert/blob/master/multilingual.md"," ['6 Data']","Multilingual BERT [Cite_Footnote_10] was trained on the 104 languages with the largest Wikipedias —of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.",10 Information about multilingual BERT can be found in: https://github.com/google-research/bert/blob/master/multilingual.md,"We used Wikipedia as the main data source for all our experiments. Multilingual BERT [Cite_Footnote_10] was trained on the 104 languages with the largest Wikipedias —of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.328_0_0,2020,Speakers Fill Lexical Semantic Gaps with Context,Footnote
233,10235," https://meta.wikimedia.org/wiki/List_of_Wikipedias"," ['6 Data']","Multilingual BERT was trained on the 104 languages with the largest Wikipedias [Cite_Footnote_11] —of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.",11 List of Wikipedias can be found in https://meta.wikimedia.org/wiki/List_of_Wikipedias,"We used Wikipedia as the main data source for all our experiments. Multilingual BERT was trained on the 104 languages with the largest Wikipedias [Cite_Footnote_11] —of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.328_1_0,2020,Speakers Fill Lexical Semantic Gaps with Context,Footnote
234,10236," http://www.cs.umn.edu~cluto"," ['References']",Available at [Cite] http://www.cs.umn.edu˜clutoS. Ploux and H. Ji. 2003.,,"Finally, evaluation of induced sense taxonomies is always problematic. First of all, there is no agreed “correct” way to G. Karypis. 2002. CLUTO: A Clustering Toolkit. Tech Report 02-017, Dept. of Computer Science, University of Minnesota. Available at [Cite] http://www.cs.umn.edu˜clutoS. Ploux and H. Ji. 2003. A Model for Matching Semantic Maps Between Languages (French/English, English/French). Computational Linguistics, 29(2):155- 178.",Method,Tool,False,Introduce（引用目的）,True,P04-3007_0_0,2004,Exploiting Aggregate Properties of Bilingual Dictionaries For Distinguishing Senses of English Words and Inducing English Sense Clusters,Body
235,10237," https://github.com/qingsongma/percentage-refBias"," ['2 Background', '2.1 Measures of Central Tendency']","Table 2 shows proportions of all judge pairs with significant differences in Kappa point esti-mates (non-overlapping confidence intervals) for each combination of settings (Revelle, 2014). [Cite_Footnote_7]",7 Our re-analysis code is available at https://github.com/qingsongma/percentage-refBias,"Table 2 shows proportions of all judge pairs with significant differences in Kappa point esti-mates (non-overlapping confidence intervals) for each combination of settings (Revelle, 2014). [Cite_Footnote_7] The number of significant differences in Kappa point estimates for pairs of judges in SAME and DIFF is only 13%, or, in other words, 87% of judge pairs across SAME and DIFF have no significant difference in agreement levels. Table 2 also in-cludes proportions of significant differences for Kappa point estimates resulting from judges be-longing to a single setting (significance testing all Kappa of SAME with respect to all other Kappa be-longing to SAME , for example), revealing that the proportion of significant differences within SAME (12%) to be very similar to that of SAME × DIFF (13%), and similarly for DIFF (12%), with only a single percentage point difference in both cases in proportions of significant differences. Sub-sequently, even after correcting the measure of central tendency error in Fomicheva and Specia (2016), evidence of reference bias can still not be concluded.",Method,Code,True,Produce（引用目的）,True,D17-1262_0_0,2017,Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation,Footnote
236,10238," https://www.mturk.com"," ['3 Alternate Reference Bias Investigation', '3.1 Reference Bias Experiments']","Once post-edits had been created, DA was em-ployed in two separate runs on Amazon Mechani-cal Turk, [Cite_Footnote_11] once for GEN - REF and once for POST - EDIT .",11 https://www.mturk.com,"Once post-edits had been created, DA was em-ployed in two separate runs on Amazon Mechani-cal Turk, [Cite_Footnote_11] once for GEN - REF and once for POST - EDIT . Besides employing distinct reference trans-lations in the assessment, all other set-up crite-ria were identical for both evaluation settings, in-cluding the conventional segment-level DA set-ting, where a minimum of 15 human assessments are combined into a mean DA score for a given translation, after strict quality control measures and score standardization have been applied.",補足資料,Website,True,Use（引用目的）,True,D17-1262_1_0,2017,Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation,Footnote
237,10239," https://github.com/sweetalyssum/Seq2Seq-DU"," ['1 Introduction']","Experimental results on benchmark datasets show that Seq2Seq-DU [Cite_Footnote_1] performs much better than the baselines on SGD, MultiWOZ2.2, and Multi-WOZ2.1 in multi-turn dialogue with schema de-scriptions, is superior to BERT-DST on WOZ2.0, DSTC2, and M2M, in multi-turn dialogue with-out schema descriptions, and works equally well as Joint BERT on ATIS and SNIPS in single turn dialogue (in fact, it degenerates to Joint BERT).",1 The code is available at https://github.com/sweetalyssum/Seq2Seq-DU.,"Experimental results on benchmark datasets show that Seq2Seq-DU [Cite_Footnote_1] performs much better than the baselines on SGD, MultiWOZ2.2, and Multi-WOZ2.1 in multi-turn dialogue with schema de-scriptions, is superior to BERT-DST on WOZ2.0, DSTC2, and M2M, in multi-turn dialogue with-out schema descriptions, and works equally well as Joint BERT on ATIS and SNIPS in single turn dialogue (in fact, it degenerates to Joint BERT).",Method,Code,True,Produce（引用目的）,True,2021.acl-long.135_0_0,2021,A Sequence-to-Sequence Approach to Dialogue State Tracking,Footnote
238,10240," http://www.statmt.org/wmt16/translation-task.html"," ['5 Experiments', '5.1 Setup']","We conducted our experiments on datasets with different scales, translating between Chinese→English using the BTEC corpus, and German→English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [Cite_Footnote_3] cor-pora.",3 http://www.statmt.org/wmt16/translation-task.html,"Datasets. We conducted our experiments on datasets with different scales, translating between Chinese→English using the BTEC corpus, and German→English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [Cite_Footnote_3] cor-pora. The statistics of the datasets can be found in Table 1.",補足資料,Website,True,Use（引用目的）,True,D17-1014_0_0,2017,Towards Decoding as Continuous Optimisation in Neural Machine Translation,Footnote
239,10241," https://github.com/duyvuleo/Mantidae"," ['5 Experiments', '5.1 Setup']","We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [Cite_Footnote_4] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017).",4 https://github.com/duyvuleo/Mantidae,"NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [Cite_Footnote_4] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words. For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs.",Method,Tool,True,Use（引用目的）,True,D17-1014_1_0,2017,Towards Decoding as Continuous Optimisation in Neural Machine Translation,Footnote
240,10242," https://github.com/clab/dynet"," ['5 Experiments', '5.1 Setup']","We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [Cite_Footnote_5] (Neubig et al., 2017).",5 https://github.com/clab/dynet,"NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [Cite_Footnote_5] (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words. For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs.",Method,Code,True,Use（引用目的）,True,D17-1014_2_0,2017,Towards Decoding as Continuous Optimisation in Neural Machine Translation,Footnote
241,10243," https://github.com/McGill-NLP/contextual-nmn"," ['References']",Our dataset is pub-licly available at [Cite] https://github.com/McGill-NLP/contextual-nmn.,,"Neural module networks (NMN) are a pop-ular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit associ-ations between similar textual contexts (e.g. “dark cube on the left” vs. “black cube on the left”). In this work, we address these limitations and evaluate the impact of contex-tual clues in improving the performance of NMN models. First, we address the prob-lem of fixed textual inputs by parameteriz-ing the module arguments. This substan-tially reduce the number of modules in NMN by up to 75% without any loss in perfor-mance. Next we propose a method to con-textualize our parameterized model to enhance the module’s capacity in exploiting the vi-siolinguistic associations. Our model out-performs the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1% improve-ment in accuracy on the single-referent test set and +4.3% on the full test set. Addi-tionally, we demonstrate that contextualization provides +11.2% and +1.7% improvements in accuracy over prior NMN models on CLO-SURE and NLVR2. We further evaluate the impact of our contextualization by construct-ing a contrast set for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the baselines by as much as +10.4% absolute ac-curacy on CC-Ref+, illustrating the generaliza-tion skills of our approach. Our dataset is pub-licly available at [Cite] https://github.com/McGill-NLP/contextual-nmn.",Material,Dataset,True,Produce（引用目的）,True,2021.emnlp-main.516_0_0,2021,Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions,Body
242,10244," https://github.com/ruotianluo/iep-ref"," ['3 Module Parameterization in NMN']","IEP-Ref, a NMN solution based on IEP (Johnson et al., 2017b), is the current state-of-the-art model on CLEVR-Ref+ dataset. [Cite_Footnote_1]",1 We used the IEP-Ref implementation provided at the link https://github.com/ruotianluo/iep-ref,"We propose parametrization as the first step to en-able weight sharing and exploiting associations be-tween similar textual contexts. Specifically, we evaluate the effectiveness of parameterizing mod-ule textual inputs using IEP-Ref (Liu et al., 2019) as the baseline NMN implementation. IEP-Ref, a NMN solution based on IEP (Johnson et al., 2017b), is the current state-of-the-art model on CLEVR-Ref+ dataset. [Cite_Footnote_1] As shown Figure 2(a), the neural modules in IEP-Ref are represented using a stan-dard Residual Convolution Block (RCB). Formally, each RCB module (f n ) of arity n receives n feature maps (F i ) of shape 128 × 20 × 20 and outputs a same-sized tensor f o = f n (F 1 , F 2 , ..., F n ).",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.516_1_0,2021,Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions,Footnote
243,10245," https://github.com/ruotianluo/iep-ref"," ['A Appendix', 'A.2 Neural Modules in Parameterized IEP-Ref']","IEP-Ref (Liu et al., 2019), the current state-of-the-art neural module network (NMN) model for the CLEVR-Ref+ dataset, uses a generic design of neural module architecture adapted from IEP (John-son et al., 2017b), which was designed for VQA task. [Cite_Footnote_4]",4 We used the IEP-Ref implementation provided at the link https://github.com/ruotianluo/iep-ref,"IEP-Ref (Liu et al., 2019), the current state-of-the-art neural module network (NMN) model for the CLEVR-Ref+ dataset, uses a generic design of neural module architecture adapted from IEP (John-son et al., 2017b), which was designed for VQA task. [Cite_Footnote_4] The modules take either two visual inputs (bi-nary modules) or one visual input (unary modules). There are total 60 distinct modules in IEP-Ref. Af-ter parameterization (see Figure 8b), the distinct number of modules drop to 15 without any drop in the model performance (section 2 of main paper). That is, the number of a distinct set of modules (and the total number of parameters) used in the parameterized model reduces by 75%. Moreover, although the network parameters of each parameter-ized module slightly increase due to the additional LSTM unit, since each module in IEP-Ref can have multiple instantiations for the same textual input, we have fewer parameters than IEF-Ref in total. Table 11 presents the list of all the 15 modules in our parameterized NMN model. We compare the parameters per module of all baseline NMN mod-els and our proposed models (section 3 of main paper) in Table 10.",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.516_2_0,2021,Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions,Footnote
244,10246," http://data.statmt.org/news-crawl/"," ['5 Experiments', '5.1 Datasets']",We used the monolingual WMT news crawl datasets [Cite_Footnote_3] for each language.,3 http://data.statmt.org/news-crawl/,"Estonian (Et)–En translation tasks. The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [Cite_Footnote_3] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences. For the low-resource languages Ro and Et, we used all available monolingual news crawl training data. To make our experiments comparable with previous work (Lample and Conneau, 2019), we report the results on newstest2014 for Fr–En, newstest2016 for Ro–En, and newstest2018 for Et–En.",Material,Dataset,True,Use（引用目的）,True,2021.naacl-main.311_0_0,2021,Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios,Footnote
245,10247," https://github.com/diegma/neural-dep-srl"," ['1 Introduction']","When we stack GCNs on top of LSTM layers, we obtain a substantial improve-ment over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for En-glish and Chinese. [Cite_Footnote_1]",1 The code is available at https://github.com/diegma/neural-dep-srl.,"One layer GCN encodes only information about immediate neighbors and K layers are needed to encode K-order neighborhoods (i.e., informa-tion about nodes at most K hops aways). This contrasts with recurrent and recursive neural net-works (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence. However, as we will further discuss in Section 3.3, this is not a serious limitation when GCNs are used in combination with encoders based on recurrent networks (LSTMs). When we stack GCNs on top of LSTM layers, we obtain a substantial improve-ment over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for En-glish and Chinese. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D17-1159_0_0,2017,Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling,Footnote
246,10248," http://bookblog.net/gender/genie.php"," ['7 Gender Classification Results']","Also, as a second reference, performance is also cited for the popular “Gender Genie”, an online gender-detector [Cite_Footnote_7] , based on the manually weighted word-level sociolinguistic fea-tures discussed in Argamon et al. (2003).",7 http://bookblog.net/gender/genie.php,"Table 4 combines the results of the experiments re-ported in the previous sections, assessed on both the Fisher and Switchboard corpora for gender classification. The evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. Baseline performance (always guessing female) yields 57.47% and 51.6% on Fisher and Switchboard respectively. As noted before, the standard reference algorithm is Boulis and Osten-dorf (2005), and all cited relative error reductions are based on this established standard, as imple-mented in this paper. Also, as a second reference, performance is also cited for the popular “Gender Genie”, an online gender-detector [Cite_Footnote_7] , based on the manually weighted word-level sociolinguistic fea-tures discussed in Argamon et al. (2003). The ad-ditional table rows are described in Sections 4-6, and cumulatively yield substantial improvements over the Boulis and Ostendorf (2005) standard.",Method,Tool,True,Introduce（引用目的）,True,P09-1080_0_0,2009,Modeling Latent Biographic Attributes in Conversational Genres,Footnote
247,10249," http://homes.cs.washington.edu/~eunsol/project_page/acl16"," ['1 Introduction']",We evaluate the approach on a newly gathered corpus with dense document-level sentiment la-bels in news articles. [Cite_Footnote_1],"1 All data will be made publicly available. You can browse it at http://homes.cs.washington.edu/˜eunsol/project_page/acl16, and download it from the author’s webpage.","We evaluate the approach on a newly gathered corpus with dense document-level sentiment la-bels in news articles. [Cite_Footnote_1] This data includes compre-hensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demon-strate that the global model significantly improves performance over a pairwise classifier and other strong baselines. We also perform a detailed ab-lation and error analysis, showing cases where the global constraints contribute and pointing towards important areas for future work.",Material,Dataset,True,Extend（引用目的）,True,P16-1032_0_0,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context",Footnote
248,10250," https://www.upwork.com"," ['4 Data', '4.2 Sentiment Data Collection']","We hired freelancers from UpWork, [Cite_Footnote_3] after ex-amining performance on five documents.",3 https://www.upwork.com,"Evaluation Dataset We provide exhaustive an-notations covering all pairs for the evaluation set. We hired freelancers from UpWork, [Cite_Footnote_3] after ex-amining performance on five documents. They labeled entity pairs with one of the following classes.",補足資料,Website,True,Introduce（引用目的）,True,P16-1032_1_0,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context",Footnote
249,10251," http://www.crowdflower.com"," ['4 Data', '4.2 Sentiment Data Collection']","We used CrowdFlower, [Cite_Footnote_6] where annotators were randomly presented test questions for quality con-trol.",6 http://www.crowdflower.com,"We used CrowdFlower, [Cite_Footnote_6] where annotators were randomly presented test questions for quality con-trol. We collected labels from three annotators for each entity pair, and considered labels when at least two agreed. The resulting annotation con-tains total 2,995 labels on 914 documents, 682 positive, 836 negative and 474 without sentiment, which we discarded.",補足資料,Website,False,Use（引用目的）,True,P16-1032_2_0,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context",Footnote
250,10252," http://tinyurl.com/joccfqy"," ['5 Experimental Setup']",Implementation Details We use CPLEX4 [Cite_Footnote_11] to solve the ILP described in Sec. 2.,11 http://tinyurl.com/joccfqy,"We also report a proxy for doing similar ag-gregation over a state-of-the-art entity-entity sen-timent classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annota-tions, we can simply predict the union of the orig-inal gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 [Cite_Footnote_11] to solve the ILP described in Sec. 2. For compu-tational efficiency and to avoid erroneous propa-gation, soft constraints associated with reciprocity and balance theory are introduced only on pairs for which a high-precision classifier assigned po-larity. For the pairwise classifier, we use a class-weighted linear SVM. We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset. We made two versions of pair-wise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search. One is tuned for high precision to be used as a base classifier for ILP (ILP base), and the other is tuned for the best F1 (Pairwise).",Method,Tool,False,Use（引用目的）,True,P16-1032_3_0,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context",Footnote
251,10253," http://scikit-learn.org/"," ['5 Experimental Setup']","For the pairwise classifier, we use a class-weighted linear SVM. [Cite_Footnote_12]",12 http://scikit-learn.org/,"We also report a proxy for doing similar ag-gregation over a state-of-the-art entity-entity sen-timent classifier. Here, because we added our new labels to the original KBP and MPQA3.0 annota-tions, we can simply predict the union of the orig-inal gold annotations using mention string overlap to align the entities (KM Gold). This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 to solve the ILP described in Sec. 2. For compu-tational efficiency and to avoid erroneous propa-gation, soft constraints associated with reciprocity and balance theory are introduced only on pairs for which a high-precision classifier assigned po-larity. For the pairwise classifier, we use a class-weighted linear SVM. [Cite_Footnote_12] We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset. We made two versions of pair-wise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search. One is tuned for high precision to be used as a base classifier for ILP (ILP base), and the other is tuned for the best F1 (Pairwise).",Method,Code,True,Use（引用目的）,True,P16-1032_4_0,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context",Footnote
252,10254," http://www.nist.gov/tac/2014/KBP/Sentiment"," ['7 Related Work']","Sentiment Inference Our sentiment inference task is related to the recent KBP sentiment task, [Cite_Footnote_14] in that we aim to find opinion target and holder.",14 http://www.nist.gov/tac/2014/KBP/ Sentiment,"Sentiment Inference Our sentiment inference task is related to the recent KBP sentiment task, [Cite_Footnote_14] in that we aim to find opinion target and holder. While we study the complete document-level anal-ysis over all entity pairs, the KBP task is for-mulated as query-focused retrieval of entity sen-timent from a large pool of potentially relevant documents. Thus, their annotations focus only on query entities and relatively sparse compared to ours (see Sec. 6). Another recent dataset is MPQA 3.0 (Deng and Wiebe, 2015b), which cap-tures various aspects of sentiment. Their senti-ment pair annotations are only at the sentence-level and are therefore much sparser than we pro-vide (see Sec. 6) for entity-entity relation analysis.",補足資料,Website,True,Introduce（引用目的）,True,P16-1032_5_0,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context",Footnote
253,10255," http://www.csie.ntu.edu.tw/~cjlin/libsvm/"," ['3 Prosodic Model', '3.3 Prosodic Model Training']","It uses L i (i = 1, 2) to train two distinct classifiers: the acoustic classifier h [Cite_Footnote_1] , and the lexical classifier h 2 .","1 LIBSVM – A Library for Support Vector Machines, loca-tion: http://www.csie.ntu.edu.tw/˜cjlin/libsvm/","In our experiments, we investigate two kinds of training methods for prosodic modeling. The first one is a supervised method where models are trained using all the labeled data. The second is a semi-supervised method using co-training algo-rithm (Blum and Mitchell, 1998), described in Algo-rithm 1. Given a set L of labeled data and a set U of unlabeled data with two views, it then iterates in the following procedure. The algorithm first creates a smaller pool U ′ containing unlabeled data from U. It uses L i (i = 1, 2) to train two distinct classifiers: the acoustic classifier h [Cite_Footnote_1] , and the lexical classifier h 2 . We use function V i (i = 1, 2) to represent that only a single view is used for training h 1 or h 2 . These two classifiers are used to make predictions for the unla-beled set U ′ , and only when they agree on the predic-tion for a sample, their predicted class is used as the label for this sample. Then among these self-labeled samples, the most confident ones by one classifier are added to the data set L i for training the other classifier. This iteration continues until reaching the defined number of iterations. In our experiment, the size of the pool U´ is 5 times of the size of training data L i , and the size of the added self-labeled ex-ample set, D h i , is 5% of L i . For the newly selected D h i , the distribution of the positive and negative ex-amples is the same as that of the training data L i .",Method,Code,False,Use（引用目的）,False,P11-1074_0_0,2011,N-Best Rescoring Based on Pitch-accent Patterns,Footnote
254,10256," http://www.speech.cs.cmu.edu/sphinx/tutorial.html"," ['5 Data and Baseline Systems']",The recognizer used for this data set was based on Sphinx-3 [Cite_Footnote_2] .,"2 CMU Sphinx - Speech Recognition Toolkit, location: http://www.speech.cs.cmu.edu/sphinx/tutorial.html","The first data set is the Boston University Radio News Corpus (BU) (Ostendorf et al., 1995), which consists of broadcast news style read speech. The BU corpus has about 3 hours of read speech from 7 speakers (3 female, 4 male). Part of the data has been labeled with ToBI-style prosodic annotations. In fact, the reason that we use this corpus, instead of other corpora typically used for ASR experiments, is because of its prosodic labels. We divided the entire data corpus into a training set and a test set. There was no speaker overlap between training and test sets. The training set has 2 female speakers (f2 and f3) and 3 male ones (m2, m3, m4). The test set is from the other two speakers (f1 and m1). We use 200 utterances for the recognition experiments. Each ut-terance in BU corpus consists of more than one sen-tences, so we segmented each utterance based on pause, resulting in a total number of 713 segments for testing. We divided the test set roughly equally into two sets, and used one for parameter tuning and the other for rescoring test. The recognizer used for this data set was based on Sphinx-3 [Cite_Footnote_2] . The context-dependent triphone acoustic models with 32 Gaus-sian mixtures were trained using the training par-tition of the BU corpus described above, together with the broadcast new data. A standard back-off tri-gram language model with Kneser-Ney smoothing was trained using the combined text from the train-ing partition of the BU, Wall Street Journal data, and part of Gigaword corpus. The vocabulary size was about 10K words and the out-of-vocabulary (OOV) rate on the test set was 2.1%.",Method,Tool,True,Use（引用目的）,True,P11-1074_1_0,2011,N-Best Rescoring Based on Pitch-accent Patterns,Footnote
255,10257," http://www.nlm.nih.gov/mesh/meshhome.html"," ['3 The Lexical Hierarchy: MeSH']","These nodes have children, for example, Abdomen (A01.047) and Back (A01.176) are level [Cite_Footnote_1] children of Body Regions.",1 http://www.nlm.nih.gov/mesh/meshhome.html; the work reported in this paper uses MeSH 2001.,"These nodes have children, for example, Abdomen (A01.047) and Back (A01.176) are level [Cite_Footnote_1] children of Body Regions. The longer the ID of the MeSH term, the longer the path from the root and the more precise the description. For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous",補足資料,Document,True,Introduce（引用目的）,True,P02-1032_0_0,2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 247-254. The Descent of Hierarchy, and Selection in Relational Semantics",Footnote
256,10258," http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/"," ['2 Experimental Setup', '2.3 Standardized English Tests']",The TOEIC scores were converted to the TOEFL scale by fitting a third degree polynomial on an unofficial score conversion table [Cite_Footnote_2] between the tests.,"2 http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/ Although both TOEFL and TOEIC are administered by the same company (ETS), to the best of our knowledge there is no publicly available official conversion table between the two tests.","TOEFL Berzak et al. (2017) also collected self-reported scores on the most recently taken offi-cial English proficiency test, which we use here as a secondary evaluation benchmark. We focus on the most commonly reported test, the TOEFL-iBT whose scores range from 0 to 120. We take into ac-count only test results obtained less than four years prior to the experiment, yielding 33 participants. We sum the scores of the reading and listening sec-tions of test, with a total possible score range of 0 to 60. In cases where participants reported only the overall score, we divided that score by two. We further augment this data with 20 participants who took the TOEIC Listening and Reading test within the same four years range, resulting in a total of 53 external proficiency scores. The TOEIC scores were converted to the TOEFL scale by fitting a third degree polynomial on an unofficial score conversion table [Cite_Footnote_2] between the tests. The converted scores were then divided by two. Henceforth we refer to both TOEFL-iBT and TOEIC scores con-verted to TOEFL-iBT scale as TOEFL scores. The mean TOEFL score is 47.6 (std 9.55). The Pear-son’s r correlation between the TOEFL and MET scores in the dataset is 0.74.",Method,Code,True,Use（引用目的）,True,N18-1180_0_0,2018,Assessing Language Proficiency from Eye Movements in Reading,Footnote
257,10259," https://en.wikipedia.org/wiki/International_English_Language_Testing_System"," ['1 Introduction']","In “high stakes” scenarios, official language profi-ciency tests are the de-facto standards for language assessment; they are accepted by educational and professional institutions, and are taken by millions of language learners every year (for example, in 2016 over three million people took the IELTS test (IELTS, 2017) [Cite_Ref] ).","IELTS. 2017. International English language testing system — Wikipedia, the free encyclopedia. On-line; accessed November 2017. https://en.wikipedia.org/wiki/International_English_Language_Testing_System.","It is currently estimated that over 1.5 billion peo-ple are learning English as a Second Language (ESL) worldwide. Their learning progress is com-monly evaluated with classroom tests prepared by language instructors, quizzes in language learn-ing software such as Duolingo and Rosetta Stone, and by official standardized language proficiency tests such as TOEFL, IELTS, MET and others. In “high stakes” scenarios, official language profi-ciency tests are the de-facto standards for language assessment; they are accepted by educational and professional institutions, and are taken by millions of language learners every year (for example, in 2016 over three million people took the IELTS test (IELTS, 2017) [Cite_Ref] ). These tests probe language profi-ciency based on performance on various linguistic tasks, including grammar and vocabulary exams, reading and listening comprehension questions, as well as essay writing and speaking assignments.",補足資料,Document,True,Introduce（引用目的）,True,N18-1180_1_0,2018,Assessing Language Proficiency from Eye Movements in Reading,Reference
258,10260," https://www.efset.org/research/~/media/centralefcom/efset/pdf/"," ['4 English Proficiency Scoring Based on Eye Movements in Reading', '4.1 Correlation with MET and TOEFL']","We further note an external study conducted by the testing com-pany Education First (EF) which measured the correlation of their flagship standardized English proficiency test EFSET-PLUS with TOEFL-iBT (Luecht, 2015) [Cite_Ref] .",Richard M Luecht. 2015. EFSET PLUS - TOEFL iBT correlation study report. https://www.efset.org/research/˜/media/centralefcom/efset/pdf/ EFSET_TOEFL_correlational_report_ Sep_v1.pdf.,"In order to contextualize the correlations ob-tained with the EyeScore approach, we first com-pare our results to raw reading speed, an informa-tive baseline which does not rely on eyetracking. EyeScore substantially outperforms this baseline for nearly all the feature sets on both MET and TOEFL, clearly showing the benefit of eye move-ment information for our task. Next, we consider possible upper bounds for our correlations. While obtaining such upper bounds is challenging, we can use correlations between different traditional standardized proficiency tests as informative refer-ence points. First, as mentioned previously, in our dataset the MET and reported TOEFL scores have a Pearson’s r correlation of 0.74. We further note an external study conducted by the testing com-pany Education First (EF) which measured the correlation of their flagship standardized English proficiency test EFSET-PLUS with TOEFL-iBT (Luecht, 2015) [Cite_Ref] . Using 384 participants who took both tests, the study found a Pearson’s r of 0.63 for the reading comprehension and 0.69 for the listen-ing comprehension sections of these tests. Despite the radical difference of our testing methodology, our strongest feature sets obtain rather competi-tive results relative to these correlations, further strengthening the evidence for the ability of our approach to capture language proficiency.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1180_2_0,2018,Assessing Language Proficiency from Eye Movements in Reading,Reference
259,10261," https://github.com/kentonl/e2e-coref"," ['7 Experiments', '7.1 Hyperparameters']","All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [Cite_Footnote_3]",3 https://github.com/kentonl/e2e-coref,"All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [Cite_Footnote_3]",Method,Code,True,Produce（引用目的）,True,D17-1018_0_0,2017,End-to-end Neural Coreference Resolution,Footnote
260,10262," https://github.com/yogarshi/bisparse-dep/"," ['References']",Our embeddings and datasets are publicly available. [Cite_Footnote_1],1 https://github.com/yogarshi/bisparse-dep/,"Cross-lingual Hypernymy Detection involves determining if a word in one language (“fruit”) is a hypernym of a word in another language (“pomme” i.e. apple in French). The abil-ity to detect hypernymy cross-lingually can aid in solving cross-lingual versions of tasks such as textual entailment and event coreference. We propose B I S PARSE -D EP , a family of un-supervised approaches for cross-lingual hyper-nymy detection, which learns sparse, bilingual word embeddings based on dependency con-texts. We show that B I S PARSE -D EP can sig-nificantly improve performance on this task, compared to approaches based only on lexical context. Our approach is also robust, show-ing promise for low-resource settings: our dependency-based embeddings can be learned using a parser trained on related languages, with negligible loss in performance. We also crowd-source a challenging dataset for this task on four languages – Russian, French, Arabic, and Chinese. Our embeddings and datasets are publicly available. [Cite_Footnote_1]",Material,Dataset,True,Produce（引用目的）,True,N18-1056_0_0,2018,Robust Cross-lingual Hypernymy Detection using Dependency Context,Footnote
261,10263," http://crowdflower.com"," ['4 Crowd-Sourcing Annotations']","Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [Cite_Footnote_4] .",4 http://crowdflower.com,"There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [Cite_Footnote_4] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English.",Method,Tool,True,Use（引用目的）,True,N18-1056_1_0,2018,Robust Cross-lingual Hypernymy Detection using Dependency Context,Footnote
262,10264," http://www.aclweb.org/anthology/N16-1142"," ['1 Introduction']","This motivates approaches that directly detect hypernymy in the cross-lingual setting by extend-ing distributional methods for detecting monolin-gual hypernymy, as in our prior work (Vyas and Carpuat, 2016) [Cite_Ref] .","Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187– 1197. http://www.aclweb.org/anthology/N16-1142.","This motivates approaches that directly detect hypernymy in the cross-lingual setting by extend-ing distributional methods for detecting monolin-gual hypernymy, as in our prior work (Vyas and Carpuat, 2016) [Cite_Ref] . State-of-the-art distributional ap-proaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy re-quire syntactic analysis (eg. dependency parsing), which may not available for many languages. Ad-ditionally, limited training resources make unsu-pervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk, 2016). Furthermore, monolingual distributional approaches cannot be applied directly to the cross-lingual task, because the vector spaces of two lan-guages need to be aligned using a cross-lingual re-source (a bilingual dictionary, for instance).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1056_24_0,2018,Robust Cross-lingual Hypernymy Detection using Dependency Context,Reference
263,10265," http://www.aclweb.org/anthology/N16-1142"," ['2 Related Work']","More closely related is our prior work (Vyas and Carpuat, 2016) [Cite_Ref] where we used lexical context based embeddings to detect cross-lingual lexical entailment.","Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187– 1197. http://www.aclweb.org/anthology/N16-1142.","More closely related is our prior work (Vyas and Carpuat, 2016) [Cite_Ref] where we used lexical context based embeddings to detect cross-lingual lexical entailment. In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment. Also, we improve upon our previ-ous approach by using dependency based embed-dings (§6.1), and show that the improvements hold even when exposed to data scarce settings (§6.3).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1056_24_1,2018,Robust Cross-lingual Hypernymy Detection using Dependency Context,Reference
264,10266," http://www.aclweb.org/anthology/N16-1142"," ['3 Our Approach – B I S PARSE -D EP', '3.3 Bilingual Sparse Coding']","Given a dependency based co-occurrence matrix described in the previous section(s), we generate B I S PARSE -D EP embeddings using the framework from our prior work (Vyas and Carpuat, 2016) [Cite_Ref] , which we henceforth call B I S PARSE .","Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187– 1197. http://www.aclweb.org/anthology/N16-1142.","Given a dependency based co-occurrence matrix described in the previous section(s), we generate B I S PARSE -D EP embeddings using the framework from our prior work (Vyas and Carpuat, 2016) [Cite_Ref] , which we henceforth call B I S PARSE . B I S PARSE generates sparse, bilingual word embeddings us-ing a dictionary learning objective with a spar-sity inducing l 1 penalty. We give a brief overview of this approach, the full details of which can be found in our prior work.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1056_24_2,2018,Robust Cross-lingual Hypernymy Detection using Dependency Context,Reference
265,10267," http://www.aclweb.org/anthology/N16-1142"," ['5 Experimental Setup', '5.2 Contrastive Approaches']","Predeces-sor of the B I S PARSE -D EP model from our previ-ous work (Vyas and Carpuat, 2016) [Cite_Ref] .","Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187– 1197. http://www.aclweb.org/anthology/N16-1142.","We compare our B I S PARSE -D EP embeddings with the following approaches: M ONO -D EP (Translation baseline) For word pair (p f , q e ) in test data, we translate p f to English using the most common translation in the transla-tion matrix. Hypernymy is then determined using sparse, dependency based embeddings in English. B I S PARSE -L EX (Window context) Predeces-sor of the B I S PARSE -D EP model from our previ-ous work (Vyas and Carpuat, 2016) [Cite_Ref] . This model induces sparse, cross-lingual embeddings using window based context.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1056_24_3,2018,Robust Cross-lingual Hypernymy Detection using Dependency Context,Reference
266,10268," http://www.aclweb.org/anthology/I08-3008"," ['3 Our Approach – B I S PARSE -D EP', '3.2 Dependency Contexts without a Treebank']","The rationale behind this is that related languages show common syntactic structure that can be transferred to the original language, with delex-icalized parsing (Zeman and Resnik, 2008 [Cite_Ref] ; Mc-Donald et al., 2011, inter alia) being one popular approach.",Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation be-tween related languages. In IJCNLP. http://www.aclweb.org/anthology/I08-3008.,"We train a delexicalized parser using treebanks of related languages, where the word form based features are turned off, so that the parser is trained on purely non-lexical features (e.g. POS tags). The rationale behind this is that related languages show common syntactic structure that can be transferred to the original language, with delex-icalized parsing (Zeman and Resnik, 2008 [Cite_Ref] ; Mc-Donald et al., 2011, inter alia) being one popular approach. 3",補足資料,Paper,True,Introduce（引用目的）,True,N18-1056_26_0,2018,Robust Cross-lingual Hypernymy Detection using Dependency Context,Reference
267,10269," https://github.com/machelreid/afromt"," ['References']",All code and pre-trained models will be released as further steps towards larger reproducible benchmarks for African languages. [Cite_Footnote_1],"1 Source code, pretrained models, and data can be found at https://github.com/machelreid/afromt","Reproducible benchmarks are crucial in driv-ing progress of machine translation research. However, existing machine translation bench-marks have been mostly limited to high-resource or well-represented languages. De-spite an increasing interest in low-resource ma-chine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by mil-lions of speakers but have less digitized tex-tual data. To tackle these challenges, we pro-pose A FRO MT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretrain-ing multilingual sequence-to-sequence mod-els. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong base-lines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pre-trained models will be released as further steps towards larger reproducible benchmarks for African languages. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.99_0_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
268,10270," https://en.wikipedia.org/wiki/Niger%E2%80%93Congo_languages"," ['2 A FRO MT benchmark', '2.1 Language Selection Criteria']","One particular characteristic feature of these languages is their morphosyntax, especially their system of noun classification, with noun classes often exceeding 10, ranging from markers denoting male/female/animate/inanimate and more [Cite_Footnote_2] .",2 https://en.wikipedia.org/wiki/Niger% E2%80%93Congo_languages,"Linguistic Characteristics With the exception of English and Afrikaans, which belong to the Indo-European language family, all of the consid-ered languages belong to the Niger-Congo fam-ily which is Africa’s largest language family in terms of geographical area and speaking popula-tion (see Appendix). Similar to English, the Niger-Congo family generally follows the SVO word or-der. One particular characteristic feature of these languages is their morphosyntax, especially their system of noun classification, with noun classes often exceeding 10, ranging from markers denoting male/female/animate/inanimate and more [Cite_Footnote_2] . These noun classes can be likened in some sense to the male/female designation found in romance lan-guages. However, in contrast with these languages, noun markers in Niger-Congo languages are of-ten integrated within the word, usually as a prefix (Bendor-Samuel and Hartell, 1989). For exam-ple: in Zulu, isiZulu refers to the Zulu language, whereas amaZulu refers to the Zulu people. Ad-ditionally, these languages also use “verb exten-sions”, verb-suffixes used to modify the meaning of the verb. These qualities contribute to the mor-phological richness of these languages — a stark contrast with European languages.",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.99_1_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
269,10271," http://opus.nlpl.eu/"," ['2 A FRO MT benchmark', '2.2 Data Sources']","This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [Cite_Footnote_3] (Tiedemann, 2012) and (2) ParaCrawl (Esplà et al., 2019).",3 http://opus.nlpl.eu/,"For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [Cite_Footnote_3] (Tiedemann, 2012) and (2) ParaCrawl (Esplà et al., 2019). From OPUS, we use the JW300 corpus (Agić and Vulić, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014). Despite the existence of this paral-lel data, these text datasets were often collected from large, relatively unclean multilingual corpora, e.g. JW300 which was extracted from Jehovah’s Witnesses text, or QED which was extracted from transcribed educational videos. This leads to many sentences with high lexical overlap, inconsistent tokenization, and other undesirable properties for a clean, reproducible benchmark.",Material,DataSource,True,Extend（引用目的）,True,2021.emnlp-main.99_2_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
270,10272," https://github.com/moses-smt/mosesdecoder/"," ['2 A FRO MT benchmark', '2.3 Data Preparation']","Tokenization normalization We perform detok-enization on all corpora using the detokenization script provided in the Moses (Koehn et al., 2007) toolkit [Cite_Footnote_5] .",5 https://github.com/moses-smt/mosesdecoder/,"Tokenization normalization We perform detok-enization on all corpora using the detokenization script provided in the Moses (Koehn et al., 2007) toolkit [Cite_Footnote_5] . Given that we collect data from various sources, this step is important to allow for consis-tent tokenization across corpora.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.99_3_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
271,10273," https://github.com/maxbachmann/RapidFuzz"," ['2 A FRO MT benchmark', '2.3 Data Preparation']","To do this, we use Levenshtein-based fuzzy string matching [Cite_Footnote_6] and remove sentences that have a similarity score of over 60.",6 https://github.com/maxbachmann/ RapidFuzz pseudo monolingual data and dictionaries,"Removal of sentences with high text overlap To prevent data leakage, we remove sentences with high text overlap. To do this, we use Levenshtein-based fuzzy string matching [Cite_Footnote_6] and remove sentences that have a similarity score of over 60. Given that measuring this score against all sentences in a corpus grows quadratically with respect to cor-pus length, we use the following two heuristics to remove sentences with high overlap in an effi-cient manner: (1) scoring similarity between the 50 alphabetically-sorted previous sentences, (2): ex-tracting the top 100K four-grams and performing the similarity score within each group of sentences containing at least one instance of a certain four-gram.",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.99_4_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
272,10274," https://github.com/robertostling/eflomal/"," ['3 AfroBART', '3.2 Dictionary Augmentation']","Dictionary Extraction As our data augmenta-tion technique requires a dictionary, we propose to extract the dictionary from parallel corpora using a statistical word aligner, eflomal [Cite_Footnote_7] (Östling and Tiedemann, 2016).",7 https://github.com/robertostling/eflomal/,"Dictionary Extraction As our data augmenta-tion technique requires a dictionary, we propose to extract the dictionary from parallel corpora using a statistical word aligner, eflomal [Cite_Footnote_7] (Östling and Tiedemann, 2016). Once we produce word align-ments between tokens in our parallel corpora, we simply take word alignments that appear over 20 times to produce our bilingual dictionary.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.99_5_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
273,10275," http://data.statmt.org/cc-100/"," ['4 Experimental Setup', '4.1 Pretraining']","When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 × [Cite_Footnote_10] −5 and warm up over 5K iterations and train for 35K iterations.",10 http://data.statmt.org/cc-100/,"Hyperparameters We use the following setup to train our AfroBART models, utilizing the mBART implementation in the fairseq library (Ott et al., 2019). We tokenize data using Sentence-Piece (Kudo and Richardson, 2018), using a 80K subword vocabulary. We use the Transformer-base architecture of a hidden dimension of 512, feed-forward size of 2048, and 6 layers for both the encoder and decoder. We set the maximum se-quence length to be 512, using a batch size of 1024 for 100K iterations with 32 NVIDIA V100 GPUs for one day. When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 × [Cite_Footnote_10] −5 and warm up over 5K iterations and train for 35K iterations.",Material,Dataset,False,Use（引用目的）,True,2021.emnlp-main.99_6_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
274,10276," https://github.com/pytorch/fairseq"," ['4 Experimental Setup', '4.1 Pretraining']","Hyperparameters We use the following setup to train our AfroBART models, utilizing the mBART implementation in the fairseq [Cite_Footnote_12] library (Ott et al., 2019).",12 https://github.com/pytorch/fairseq,"Hyperparameters We use the following setup to train our AfroBART models, utilizing the mBART implementation in the fairseq [Cite_Footnote_12] library (Ott et al., 2019). We tokenize data using Sentence-Piece (Kudo and Richardson, 2018), using a 80K subword vocabulary. We use the Transformer-base architecture of a hidden dimension of 512, feed-forward size of 2048, and 6 layers for both the encoder and decoder. We set the maximum se-quence length to be 512, using a batch size of 1024 for 100K iterations with 32 NVIDIA V100 GPUs for one day. When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 × −5 and warm up over 5K iterations and train for 35K iterations.",Method,Code,True,Extend（引用目的）,False,2021.emnlp-main.99_7_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
275,10277," https://github.com/mjpost/sacrebleu"," ['4 Experimental Setup', '4.2 Finetuning']","Both metrics are measured using the SacreBLEU library [Cite_Footnote_13] (Post, 2018).",13 https://github.com/mjpost/sacrebleu,"Evaluation We evaluate our system outputs us-ing two automatic evaluation metrics: detokenized BLEU (Papineni et al., 2002; Post, 2018) and chrF (Popović, 2015). Although BLEU is a standard metric for machine translation, being cognizant of the morphological richness of the languages in the A FRO MT benchmark, we use chrF to measure per-formance at a character level. Both metrics are measured using the SacreBLEU library [Cite_Footnote_13] (Post, 2018).",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.99_8_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
276,10278," https://spacy.io/"," ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']","Specifically, we first leverage an exist-ing English POS tagger in the spaCy [Cite_Footnote_14] library to annotate the English source sentences.",14 https://spacy.io/,"We further provide a suite of fine-grained analysis tools to compare the baseline systems. In partic-ular, we are interested in evaluating the transla-tion accuracy of noun classes in the considered African languages in the Niger-Congo family, as these languages are morphologically rich and of-ten have more than 10 classes based on the prefix of the word. For example, kitabu and vitabu in Swahili refer to book and books in English, respec-tively. Based on this language characteristic, our fine-grained analysis tool calculates the translation accuracy of the nouns with the top 10 most fre-quent prefixes in the test data. To do so, one of the challenges is to identify nouns in a sentence written in the target African language. However, there is no available part-of-speech (POS) tagger for these languages. To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy [Cite_Footnote_14] library to annotate the English source sentences. We then use the fast_align tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language. We then measure the transla-tion accuracy of the nouns in the African language by checking whether the correct nouns are included in the translated sentences by systems in compar-ison. Notably, our analysis tool can also measure the translation accuracy of the words in the other POS tags, (e.g. verbs, adjectives) which are often adjusted with different noun classes.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.99_9_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
277,10279," https://github.com/clab/fast_align"," ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']","We then use the fast_align [Cite_Footnote_15] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data.",15 https://github.com/clab/fast_align,"We further provide a suite of fine-grained analysis tools to compare the baseline systems. In partic-ular, we are interested in evaluating the transla-tion accuracy of noun classes in the considered African languages in the Niger-Congo family, as these languages are morphologically rich and of-ten have more than 10 classes based on the prefix of the word. For example, kitabu and vitabu in Swahili refer to book and books in English, respec-tively. Based on this language characteristic, our fine-grained analysis tool calculates the translation accuracy of the nouns with the top 10 most fre-quent prefixes in the test data. To do so, one of the challenges is to identify nouns in a sentence written in the target African language. However, there is no available part-of-speech (POS) tagger for these languages. To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [Cite_Footnote_15] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language. We then measure the transla-tion accuracy of the nouns in the African language by checking whether the correct nouns are included in the translated sentences by systems in compar-ison. Notably, our analysis tool can also measure the translation accuracy of the words in the other POS tags, (e.g. verbs, adjectives) which are often adjusted with different noun classes.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.99_10_0,2021,A FRO MT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,Footnote
278,10280," http://wiki.dbpedia.org/Downloads2015-04"," ['4 Evaluation', '4.3 End-to-End Evaluation']",We trained IDFs (Inverse Document Frequencies) on a background corpus [Cite_Footnote_4] to im-prove the original algorithm.,4 We used DBpedia long abstract: http://wiki.dbpedia.org/Downloads2015-04.,• TF*IDF weighting: This simple heuristic was introduced by Luhn (1958). Each sen-tence receives a score from the TF*IDF of its terms. We trained IDFs (Inverse Document Frequencies) on a background corpus [Cite_Footnote_4] to im-prove the original algorithm.,Material,Knowledge,True,Use（引用目的）,True,P16-1172_0_0,2016,Optimizing an Approximation of ROUGE – a Problem-Reduction Approach to Extractive Multi-Document Summarization,Footnote
279,10281," https://github.com/miso-belica/sumy"," ['4 Evaluation', '4.3 End-to-End Evaluation']","Sentences are scored ac-cording to their PageRank score in G. For our experiments, we use the implementation available in the sumy package. [Cite_Footnote_5]",5 https://github.com/miso-belica/sumy,"the cosine similarity between them is above a given threshold. Sentences are scored ac-cording to their PageRank score in G. For our experiments, we use the implementation available in the sumy package. [Cite_Footnote_5]",Method,Code,True,Use（引用目的）,True,P16-1172_1_0,2016,Optimizing an Approximation of ROUGE – a Problem-Reduction Approach to Extractive Multi-Document Summarization,Footnote
280,10282," http://www.cs.cornell.edu/~rs/sfour/"," ['4 Evaluation', '4.3 End-to-End Evaluation']",We use the publicly available imple-mentation. [Cite_Footnote_6],6 http://www.cs.cornell.edu/˜rs/sfour/,"• SFOUR: SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE (Sipos et al., 2012). We use the publicly available imple-mentation. [Cite_Footnote_6]",Method,Code,True,Use（引用目的）,True,P16-1172_2_0,2016,Optimizing an Approximation of ROUGE – a Problem-Reduction Approach to Extractive Multi-Document Summarization,Footnote
281,10283," https://news.google.com/"," ['1 Introduction']","Online news platforms such as Google News [Cite_Footnote_1] and MSN News have gained huge popularity for on-line digital news reading (Das et al., 2007).",1 https://news.google.com/,"Online news platforms such as Google News [Cite_Footnote_1] and MSN News have gained huge popularity for on-line digital news reading (Das et al., 2007). Tens of thousands of news articles are streamed from various sources every day, making it very difficult for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is critical for online news platforms to target user interests and alleviate in-formation overload (IJntema et al., 2010).",補足資料,Website,True,Introduce（引用目的）,True,D19-1493_0_0,2019,Neural News Recommendation with Heterogeneous User Behavior,Footnote
282,10284," https://www.msn.com/en-us/news"," ['1 Introduction']","Online news platforms such as Google News and MSN [Cite_Footnote_2] News have gained huge popularity for on-line digital news reading (Das et al., 2007).",2 https://www.msn.com/en-us/news SearchQueries whether,"Online news platforms such as Google News and MSN [Cite_Footnote_2] News have gained huge popularity for on-line digital news reading (Das et al., 2007). Tens of thousands of news articles are streamed from various sources every day, making it very difficult for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is critical for online news platforms to target user interests and alleviate in-formation overload (IJntema et al., 2010).",補足資料,Website,True,Introduce（引用目的）,True,D19-1493_1_0,2019,Neural News Recommendation with Heterogeneous User Behavior,Footnote
283,10285," https://github.com/wuch15/NRHUB"," ['4 Experiments', '4.1 Datasets and Experimental Settings']","We conducted experiments on a real-world news recommendation dataset [Cite_Footnote_3] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019.",3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.,"We conducted experiments on a real-world news recommendation dataset [Cite_Footnote_3] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine. The detailed statistics of this dataset are summarized in Table 1. The news data in the last week is used for test, and the rest is used for model training. In addition, we randomly sampled 10% of the training data for validation.",Material,Dataset,True,Use（引用目的）,True,D19-1493_2_0,2019,Neural News Recommendation with Heterogeneous User Behavior,Footnote
284,10286," https://www.msn.com/en-us/news"," ['4 Experiments', '4.1 Datasets and Experimental Settings']","We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_4] logs during Dec. 13, 2018 and Jan. 12, 2019.",4 https://www.msn.com/en-us/news,"We conducted experiments on a real-world news recommendation dataset collected from MSN News [Cite_Footnote_4] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine. The detailed statistics of this dataset are summarized in Table 1. The news data in the last week is used for test, and the rest is used for model training. In addition, we randomly sampled 10% of the training data for validation.",Material,DataSource,True,Use（引用目的）,True,D19-1493_3_0,2019,Neural News Recommendation with Heterogeneous User Behavior,Footnote
285,10287," https://github.com/syxu828/CSRL_dataset"," ['1 Introduction']",Our code is publicly available at [Cite] https: //github.com/syxu828/CSRL_dataset.,,"In addition, we introduce a multi-task learning method with two new objectives. Experimental re-sults on benchmark datasets show that our model substantially outperforms existing baselines. Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at [Cite] https: //github.com/syxu828/CSRL_dataset.",Material,Dataset,True,Produce（引用目的）,True,2021.emnlp-main.177_0_0,2021,CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling,Body
286,10288," http://www.cs.mu.oz.au/∼edwardi/papers/datransitions.html"," ['4 Training on Speech Acts']",This intuition is reflected in the bigram transition probabilities obtained from our corpus. [Cite_Footnote_1],"1 Due to space constraints, the dialogue act transition ta-ble has been omitted from this paper and is made available at http://www.cs.mu.oz.au/∼edwardi/papers/datransitions.html","The use of P(d) in Equation 3 assumes that dia-logue acts are independent of one another. However, we intuitively know that if someone asks a Y ES -N O - Q UESTION then the response is more likely to be a Y ES -A NSWER rather than, say, C ONVENTIONAL - C LOSING . This intuition is reflected in the bigram transition probabilities obtained from our corpus. [Cite_Footnote_1]",Material,Knowledge,True,Produce（引用目的）,True,P05-2014_0_0,2005,Dialogue Act Tagging for Instant Messaging Chat Sessions,Footnote
287,10289," https://github.com/wangcunxiang/Sen-Making-and-Explanation"," ['1 Introduction']",Our dataset is released at [Cite] https://github.com/wangcunxiang/Sen-Making-and-Explanation.,,"Note that there has been dataset which fo-cus on non-linguistic world knowledge plausibil-ity (Wang et al., 2018) or only limited attributes or actions of physical knowledge like verbphysics (Forbes and Choi, 2017). They are related to our dataset but serve robotic research mainly. Our dataset is the first benchmark for direct linguis-tic sense making and explanation. We hope this benchmark can promote commonsense reason-ing by the NLP community, and further applied on other applications such as machine transla-tion and dialogue. Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at [Cite] https://github.com/wangcunxiang/Sen-Making-and-Explanation.",Material,Dataset,True,Produce（引用目的）,True,P19-1393_0_0,2019,Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation,Body
288,10290," https://github.com/kandorm/CLINE"," ['1 Introduction']","That is, with the training on the proposed objectives, CLINE si-multaneously gains the robustness of adversarial attacks and sensitivity of semantic changes [Cite_Footnote_1] .",1 The source code of CLINE will be publicly available at https://github.com/kandorm/CLINE,"To train a robust semantic-aware PLM, we pro-pose Contrastive Learning with semantIc Negative Examples (CLINE). CLINE is a simple and effec-tive method to generate adversarial and contrastive examples and contrastively learn from both of them. The contrastive manner has shown effectiveness in learning sentence representations (Luo et al., 2020; Wu et al., 2020; Gao et al., 2021), yet these studies neglect the generation of negative instances. In CLINE, we use external semantic knowledge, i.e., WordNet (Miller, 1995), to generate adversarial and contrastive examples by unsupervised replac-ing few specific representative tokens. Equipped by replaced token detection and contrastive objec-tives, our method gathers similar sentences with semblable semantics and disperse ones with differ-ent even opposite semantics, simultaneously im-proving the robustness and semantic sensitivity of PLMs. We conduct extensive experiments on sev-eral widely used text classification benchmarks to verify the effectiveness of CLINE. To be more spe-cific, our model achieves +1.6% absolute improve-ment on 4 contrastive test sets and +0.5% absolute improvement on 4 adversarial test sets compared to RoBERTa model (Liu et al., 2019). That is, with the training on the proposed objectives, CLINE si-multaneously gains the robustness of adversarial attacks and sensitivity of semantic changes [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2021.acl-long.181_0_0,2021,CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding,Footnote
289,10291," https://github.com/explosion/spaCy"," ['3 Method', '3.1 Generation of Examples']","In specific, we utilize spaCy [Cite_Footnote_2] to conduct segmentation and POS for the original sentences, extracting verbs, nouns, adjectives, and adverbs.",2 https://github.com/explosion/spaCy,"We generate two sentences from the original in-put sequence x ori , which express substantially dif-ferent semantics but have few different words. One of the sentences is semantically close to x ori (de-noted as x syn ), while the other is far from or even opposite to x ori (denoted as x ant ). In specific, we utilize spaCy [Cite_Footnote_2] to conduct segmentation and POS for the original sentences, extracting verbs, nouns, adjectives, and adverbs. x syn is generated by re-placing the extracted words with synonyms, hy-pernyms and morphological changes, and x ant is generated by replacing them with antonyms and random words. For x syn , about 40% tokens are replaced. For x ant , about 20% tokens are replaced.",Method,Code,True,Use（引用目的）,True,2021.acl-long.181_1_0,2021,CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding,Footnote
290,10292," https://github.com/allenai/contrast-sets"," ['4 Experiments', '4.3 Experiments on Contrastive Sets']","We evaluate our model on four contrastive sets: IMDB, PERSPECTRUM, BoolQ and SNLI, which were provided by Contrast Sets [Cite_Footnote_3] (Gardner et al., 2020).",3 https://github.com/allenai/ contrast-sets,"We evaluate our model on four contrastive sets: IMDB, PERSPECTRUM, BoolQ and SNLI, which were provided by Contrast Sets [Cite_Footnote_3] (Gardner et al., 2020). We compare our approach with BERT and",Material,DataSource,False,Use（引用目的）,True,2021.acl-long.181_2_0,2021,CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding,Footnote
291,10293," https://github.com/artetxem/phrase2vec"," ['3 Principled unsupervised SMT', '3.1 Initial phrase-table']","More concretely, we train our n-gram embed-dings using phrase2vec [Cite_Footnote_1] , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs.",1 https://github.com/artetxem/ phrase2vec,"More concretely, we train our n-gram embed-dings using phrase2vec [Cite_Footnote_1] , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature τ is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction. In addition to the phrase translation probabilities in both direc-tions, the forward and reverse lexical weightings are also estimated by aligning each word in the tar-get phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. The reader is referred to Artetxe et al. (2018b) for more details.",Method,Code,True,Use（引用目的）,True,P19-1019_0_0,2019,An Effective Approach to Unsupervised Machine Translation,Footnote
292,10294," https://github.com/artetxem/vecmap"," ['3 Principled unsupervised SMT', '3.1 Initial phrase-table']","Having done that, we map the embeddings to a cross-lingual space us-ing VecMap [Cite_Footnote_3] with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning.",3 https://github.com/artetxem/vecmap,"More concretely, we train our n-gram embed-dings using phrase2vec , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap [Cite_Footnote_3] with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature τ is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction. In addition to the phrase translation probabilities in both direc-tions, the forward and reverse lexical weightings are also estimated by aligning each word in the tar-get phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. The reader is referred to Artetxe et al. (2018b) for more details.",Method,Code,True,Use（引用目的）,True,P19-1019_1_0,2019,An Effective Approach to Unsupervised Machine Translation,Footnote
293,10295," http://www.statmt.org/moses/"," ['5 Experiments and results']","Our SMT implementation is based on Moses [Cite_Footnote_10] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing.",10 http://www.statmt.org/moses/,"Our SMT implementation is based on Moses [Cite_Footnote_10] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018).",Method,Code,True,Extend（引用目的）,True,P19-1019_2_0,2019,An Effective Approach to Unsupervised Machine Translation,Footnote
294,10296," https://github.com/pytorch/fairseq"," ['5 Experiments and results']","Finally, we use the big transformer implementation from fairseq [Cite_Footnote_11] for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018).",11 https://github.com/pytorch/fairseq,"Our SMT implementation is based on Moses , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq [Cite_Footnote_11] for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018).",Method,Code,True,Use（引用目的）,True,P19-1019_3_0,2019,An Effective Approach to Unsupervised Machine Translation,Footnote
295,10297," https://github.com/artetxem/monoses"," ['6 Conclusions and future work']",Our code is available as an open source project at [Cite] https: //github.com/artetxem/monoses.,,"In this paper, we identify several deficiencies in previous unsupervised SMT systems, and pro-pose a more principled approach that addresses them by incorporating subword information, us-ing a theoretically well founded unsupervised tun-ing method, and developing a joint refinement pro-cedure. In addition to that, we use our improved SMT approach to initialize a dual NMT model that is further improved through on-the-fly back-translation. Our experiments show the effective-ness of our approach, as we improve the previous state-of-the-art in unsupervised machine transla-tion by 5-7 BLEU points in French-English and German-English WMT 2014 and 2016. Our code is available as an open source project at [Cite] https: //github.com/artetxem/monoses.",Method,Code,True,Produce（引用目的）,True,P19-1019_4_0,2019,An Effective Approach to Unsupervised Machine Translation,Body
296,10298," http://www.slt.atr.jp/IWSLT2006/"," ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']","Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation [Cite_Footnote_1] , NIST Machine Translation Evaluation , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques.",1 http://www.slt.atr.jp/IWSLT2006/,"Two main types of data are used for evaluating MT quality. One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation [Cite_Footnote_1] , NIST Machine Translation Evaluation , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation. They are classified as the former type of data mentioned above. On the other hand, the data provided by NTT (Ikehara et al., 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type. Almost all the data mentioned above consist of only parallel translations in two languages. Data with information for evaluat-ing MT results, such as JEIDA’s are rarely found. In this paper, we call data that consist of parallel trans-lations collected for MT evaluation and that the in-formation for MT evaluation is assigned to, a test set.",補足資料,Website,True,Introduce（引用目的）,True,N07-1005_0_0,2007,Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals,Footnote
297,10299," http://www.nist.gov/speech/tests/mt/index.htm"," ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']","Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation [Cite_Footnote_2] , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques.",2 http://www.nist.gov/speech/tests/mt/index.htm,"Two main types of data are used for evaluating MT quality. One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation [Cite_Footnote_2] , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation. They are classified as the former type of data mentioned above. On the other hand, the data provided by NTT (Ikehara et al., 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type. Almost all the data mentioned above consist of only parallel translations in two languages. Data with information for evaluat-ing MT results, such as JEIDA’s are rarely found. In this paper, we call data that consist of parallel trans-lations collected for MT evaluation and that the in-formation for MT evaluation is assigned to, a test set.",補足資料,Website,True,Introduce（引用目的）,True,N07-1005_1_0,2007,Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals,Footnote
298,10300," http://www.863data.org.cn/"," ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']","Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation , and HTRDP Evaluation [Cite_Footnote_3] were organized to sup-port the improvement of MT techniques.",3 http://www.863data.org.cn/,"Two main types of data are used for evaluating MT quality. One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation , and HTRDP Evaluation [Cite_Footnote_3] were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation. They are classified as the former type of data mentioned above. On the other hand, the data provided by NTT (Ikehara et al., 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type. Almost all the data mentioned above consist of only parallel translations in two languages. Data with information for evaluat-ing MT results, such as JEIDA’s are rarely found. In this paper, we call data that consist of parallel trans-lations collected for MT evaluation and that the in-formation for MT evaluation is assigned to, a test set.",補足資料,Website,True,Introduce（引用目的）,True,N07-1005_2_0,2007,Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals,Footnote
299,10301," http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html"," ['2 Test Set for Evaluating Machine Translation Quality', 'Translation Quality']",Japanese word segmentation was performed by using JUMAN [Cite_Footnote_4] in our experiments.,4 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html,"The term S i indicates a similarity between a trans-lated sentence and its reference translation, and λ S i is a weight for the similarity. Many methods for cal-culating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gimeńez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (ex-act) (Banerjee and Lavie, 2005), WER (Niessen et al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S∗, SU∗, W-1.2), were used to cal-culate each similarity S i . Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN [Cite_Footnote_4] in our experiments.",Method,Tool,True,Use（引用目的）,True,N07-1005_3_0,2007,Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals,Footnote
300,10302," http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf"," ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']","The instruction for the subjective evaluation based on fluency and ad-equacy followed that given in the TIDES specifi-cation (TIDES, 2002) [Cite_Ref] .",TIDES. 2002. Linguistic Data Annotation Specifi-cation: Assessment of Fluency and Adequacy in Arabic-English and Chinese-English Translations. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf.,"The 769 sentences were translated by using five commercial MT systems to investigate the relation-ship between subjective evaluation based on yes/no questions and conventional subjective evaluation based on fluency and adequacy. The instruction for the subjective evaluation based on fluency and ad-equacy followed that given in the TIDES specifi-cation (TIDES, 2002) [Cite_Ref] . The subjective evaluation based on yes/no questions was done by manually answering each question for each translation. The subjective evaluation based on the yes/no questions was stable; namely, it was almost independent of the human subjects in our preliminary investigation. There were only two questions for which the an-swers generated inconsistency in the subjective eval-uation when 1,500 question-answer pairs were ran-domly sampled and evaluated by two human sub-jects.",補足資料,Document,True,Use（引用目的）,True,N07-1005_4_0,2007,Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals,Reference
301,10303," http://sourceforge.net/projects/mecab/files/"," ['5 Experiments', '5.1 Setting']","Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [Cite_Footnote_4] for Japanese.",4 http://sourceforge.net/projects/mecab/files/,"We evaluated the effectiveness of the proposed ap-proach for Chinese-to-English (CE), Japanese-to- English (JE) and French-to-English (FE) transla-tion tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [Cite_Footnote_4] for Japanese. For the FE language pair, we used stan-dard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively.",Method,Tool,True,Use（引用目的）,True,D15-1250_0_0,2015,"A Binarized Neural Network Joint Model for Machine Translation Jingyi Zhang 1,2 , Masao Utiyama 1 , Eiichro Sumita 1",Footnote
302,10304," http://hlt.fbk.eu/en/irstlm"," ['5 Experiments', '5.1 Setting']","We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [Cite_Footnote_5] with improved Kneser-Ney smoothing.",5 http://hlt.fbk.eu/en/irstlm,"For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [Cite_Footnote_5] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).",Method,Tool,True,Extend（引用目的）,False,D15-1250_1_0,2015,"A Binarized Neural Network Joint Model for Machine Translation Jingyi Zhang 1,2 , Masao Utiyama 1 , Eiichro Sumita 1",Footnote
303,10305," http://www.cs.toronto.edu/~mbweb/"," ['3 Experimental Setup', '3.1 Data']",We use the Toronto Book Corpus [Cite_Footnote_1] to train word embeddings.,"1 The corpus can be downloaded from http://www.cs.toronto.edu/˜mbweb/;cf. (Zhu et al., 2015).","We use the Toronto Book Corpus [Cite_Footnote_1] to train word embeddings. This corpus contains 74,004,228 already pre-processed sentences in total, which are made up of 1,057,070,918 tokens, originating from 7,087 unique books. In our experiments, we consider tokens appearing 5 times or more, which leads to a vocabulary of 315,643 words.",Material,Dataset,True,Use（引用目的）,True,P16-1089_0_0,2016,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,Footnote
304,10306," https://code.google.com/archive/p/word2vec/"," ['3 Experimental Setup', '3.2 Baselines']",We average word embeddings trained with word2vec. [Cite_Footnote_2],2 The code is available from https://code.google.com/archive/p/word2vec/.,"We employ two baselines for producing sentence embeddings in our experiments. We obtain simi-larity scores between sentence pairs from the base-lines in the same way as the ones produced by Siamese CBOW, i.e., we calculate the cosine sim-ilarity between the sentence embeddings they pro-duce. Word2vec We average word embeddings trained with word2vec. [Cite_Footnote_2] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples.",Method,Code,True,Use（引用目的）,True,P16-1089_1_0,2016,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,Footnote
305,10307," https://github.com/ryankiros/skip-thoughts/"," ['3 Experimental Setup', '3.2 Baselines']","Skip-thought As a second baseline we use the sentence representations produced by the skip-thought architecture (Kiros et al., 2015). [Cite_Footnote_3]",3 The code and the trained models can be down-loaded from https://github.com/ryankiros/skip-thoughts/.,"Skip-thought As a second baseline we use the sentence representations produced by the skip-thought architecture (Kiros et al., 2015). [Cite_Footnote_3] Skip-thought is a recently proposed method that learns sentence representations in a different way from ours, by using recurrent neural networks. This al-lows it to take word order into account. As it trains sentence embeddings from unlabeled data, like we do, it is a natural baseline to consider.",Method,Code,True,Use（引用目的）,True,P16-1089_2_0,2016,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,Footnote
306,10308," https://bitbucket.org/TomKenter/siamese-cbow"," ['3 Experimental Setup', '3.4 Network']","We use Theano (Theano Development Team, 2016) to implement our network. [Cite_Footnote_4]",4 The code for Siamese CBOW is available under an open-source license at https://bitbucket.org/TomKenter/siamese-cbow.,"We use Theano (Theano Development Team, 2016) to implement our network. [Cite_Footnote_4] We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al., 2016).",Method,Code,True,Produce（引用目的）,True,P16-1089_3_0,2016,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,Footnote
307,10309," http://propaganda.qcri.org/emnlp20-tutorial"," ['2 Outline of the Tutorial']",More informa-tion and materials are available online. [Cite_Footnote_1],1 http://propaganda.qcri.org/ emnlp20-tutorial,Here is an outline of the tutorial. More informa-tion and materials are available online. [Cite_Footnote_1],補足資料,Document,True,Produce（引用目的）,True,2020.emnlp-tutorials.2_0_0,2020,"Fact-Checking, Fake News, Propaganda, and Media Bias: Truth Seeking in the Post-Truth Era",Footnote
308,10310," http://tanbih.qcri.org"," ['6 Lecturers', '6.1 Preslav Nakov']","At QCRI, he leads the Tanbih project, [Cite_Footnote_2] devel-oped in collaboration with MIT, which aims to limit the effect of “fake news”, propaganda and media bias by making users aware of what they are reading.",2 Tanbih project: http://tanbih.qcri.org,"At QCRI, he leads the Tanbih project, [Cite_Footnote_2] devel-oped in collaboration with MIT, which aims to limit the effect of “fake news”, propaganda and media bias by making users aware of what they are reading. The project was featured by over 100 news outlets, including Forbes, Boston Globe, Al-jazeera, MIT Technology Review, Science Daily, Popular Science, Fast Company, The Register, WIRED, and Engadget, among others.",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-tutorials.2_1_0,2020,"Fact-Checking, Fake News, Propaganda, and Media Bias: Truth Seeking in the Post-Truth Era",Footnote
309,10311," https://dev.twitter.com/streaming/"," ['1 Introduction']",The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [Cite_Footnote_1],1 https://dev.twitter.com/streaming/overview. Last accessed on 10-01-2018.,"In this paper, we first develop a temporal-orientation classifier to classify tweets into past, present, and future and then group over the users to create user-level assessments. We use a Bidi-rectional Long Short Term Memory (Bi-LSTM) network for tweet temporal classification where tweet vectors are fed to generate the classifica-tion model. We propose a hash tag-based mini-mally supervised method with the two-pass filter-ing to create the past, present and future-oriented tweets for the training of the Bi-LSTM network. We manually examined trending hashtags in Twit-ter for a specific period of time and selected hash-tags which represent past, present/ongoing, or fu-ture events. The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [Cite_Footnote_1] The tweet temporal orienta-tion classifier is validated on a manually annotated test set. Finally, we use this classifier to automat-ically classify a large dataset consisting of ≈10 million tweets from 5,191 users mapped to their user-level features.",Method,Code,True,Use（引用目的）,True,N18-1061_0_0,2018,Fine-grained Temporal Orientation and its Relationship with Psycho-demographic Correlates,Footnote
310,10312," https://nlp.stanford.edu/projects/glove/"," ['3 Methodology', '3.1 Temporal Orientation Classification']","Tweet vectors are generated by existing Glove vectors (Pennington et al., 2014) for tweets [Cite_Footnote_2] of 200 dimensions which are trained on 27 billion tweets.",2 https://nlp.stanford.edu/projects/glove/,"Our experiment uses Bi-LSTM with 200 neu-rons at the input layer. The loss function we used is categorical cross-entropy and the opti-mizer used is Root Mean Square Propagation (rm-sprop). We repeat the training for 100 number of epochs with batch size set to 128. We also employ dropout (Srivastava et al., 2014) for reg-ularization with a dropout rate of 0.2 to prevent over-fitting. All of these attributes are finalized by parameter tuning with the performance obtained on 10-fold cross-validation using the grid search method. Tweet vectors are generated by existing Glove vectors (Pennington et al., 2014) for tweets [Cite_Footnote_2] of 200 dimensions which are trained on 27 billion tweets. We also validate our model on the valida-tion set which was 10% of the training set.",Method,Code,True,Use（引用目的）,True,N18-1061_1_0,2018,Fine-grained Temporal Orientation and its Relationship with Psycho-demographic Correlates,Footnote
311,10313," http://www.iitp.ac.in/~ai-nlp-ml/resources.html"," ['4 Data Sets']","Train-ing set consists of 27k tweets, whereas the test set is manually annotated with 741 tweets. [Cite_Footnote_4]",4 All the developed resources are available at http://www.iitp.ac.in/˜ai-nlp-ml/resources.html,"For experiments we categorize the datasets into three kinds: training, test and user-level. Train-ing set consists of 27k tweets, whereas the test set is manually annotated with 741 tweets. [Cite_Footnote_4] The user-level tweets consist of ≈10 million tweets from 5,191 users mapped to their user-level features.",Material,Dataset,True,Produce（引用目的）,True,N18-1061_2_0,2018,Fine-grained Temporal Orientation and its Relationship with Psycho-demographic Correlates,Footnote
312,10314," https://developer.twitter.com/en/docs"," ['4 Data Sets', '4.1 Training Set']",Training tweets are collected using the Twitter streaming API. [Cite_Footnote_5],5 https://developer.twitter.com/en/docs,"Training tweets are collected using the Twitter streaming API. [Cite_Footnote_5] The tweets are collected for the duration of September 2017 and October 2017. We consider day-wise trending topics during this period. We only consider those hashtags which signify a temporal event. Finally, we chose world-wide trending events and collected the tweets based on the hashtags.",Method,Code,True,Use（引用目的）,True,N18-1061_3_0,2018,Fine-grained Temporal Orientation and its Relationship with Psycho-demographic Correlates,Footnote
313,10315," http://labs.google.com/sets"," ['1 Introduction']","Looking at expansions from state of the art systems such as GoogleSets [Cite_Footnote_1] , we found systematic errors such as those resulting from ambiguous seed instances.","1 http://labs.google.com/sets the North American Chapter of the ACL, pages 290–298,","Even for state of the art methods, expansion er-rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art systems such as GoogleSets [Cite_Footnote_1] , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed instances for the target set Roman Gods:",Method,Tool,False,Introduce（引用目的）,True,N09-1033_0_0,2009,Semi-Automatic Entity Set Refinement,Footnote
314,10316," http://demo.patrickpantel.com/"," ['1 Introduction']",[Cite_Footnote_2] can identify comet as similar to asteroid and therefore potentially also as an error.,2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus.,"The inherent semantic similarity between the errors can be leveraged to quickly clean up the expan-sion. For example, given a manually tagged error “asteroid”, a distributional similarity thesaurus such as (Lin 1998) [Cite_Footnote_2] can identify comet as similar to asteroid and therefore potentially also as an error. This method has its limitations since a manually tagged error such as Earth would correctly remove Moon and Sun, but it would also incorrectly re-move Mars, Venus and Jupiter since they are also similar to Earth .",Method,Tool,False,Introduce（引用目的）,True,N09-1033_1_0,2009,Semi-Automatic Entity Set Refinement,Footnote
315,10317," http://demo.patrickpantel.com/"," ['3 Dynamic Similarity Modeling', '3.2 Feature Modification Method (FMM)']","Looking at the contexts of these words in a large corpus, we construct a cen-troid context vector for S by taking a weighted av-erage of the contexts of the seeds in S. In Wikipedia articles we see contexts (i.e., features) such as [Cite_Footnote_4] :",4 The full feature vector for these and all other terms in Wiki-pedia can be found at http://demo.patrickpantel.com/..,"Consider the set of seed terms S and an errone-ous expanded instance e. In the SIM method of Section 3.1 all set elements that have a feature vec-tor (i.e., context vector) similar to e are removed. The Feature Modification Method (FMM) instead tries to identify the subset of features of the error e which represent the unintended sense of the seed terms S. For example, let S = {Minerva, Neptune, Baccus, Juno, Apollo}. Looking at the contexts of these words in a large corpus, we construct a cen-troid context vector for S by taking a weighted av-erage of the contexts of the seeds in S. In Wikipedia articles we see contexts (i.e., features) such as [Cite_Footnote_4] :",補足資料,Document,False,Introduce（引用目的）,True,N09-1033_2_0,2009,Semi-Automatic Entity Set Refinement,Footnote
316,10318," http://voice.etri.re.kr/db/dbpop.asp?code=88"," ['2 A Semantically Annotated Korean Corpus']",We annotated predicate-argument structure of verbs in a corpus from the Electronics and Telecommunications Research Institute of Korea (ETRI). [Cite_Footnote_1],1 http://voice.etri.re.kr/db/dbpop.asp?code=88,"We annotated predicate-argument structure of verbs in a corpus from the Electronics and Telecommunications Research Institute of Korea (ETRI). [Cite_Footnote_1] Our corpus was developed over two years using a specialized annotation tool (Song et al., 2012), resulting in more than 8,000 semanti-cally annotated sentences. As much as possible, annotations followed the PropBank guidelines for English (Bonial et al., 2010).",補足資料,Website,True,Introduce（引用目的）,True,P14-2104_0_0,2014,Training a Korean SRL System with Rich Morphological Features,Footnote
317,10319," http://catalog.ldc.upenn.edu/LDC2006T03"," ['2 A Semantically Annotated Korean Corpus']",We view our work as building on the efforts of the Penn Korean PropBank (PKPB). [Cite_Footnote_2],2 http://catalog.ldc.upenn.edu/LDC2006T03,"We view our work as building on the efforts of the Penn Korean PropBank (PKPB). [Cite_Footnote_2] Our corpus is roughly similar in size to the PKPB, and taken together, the two Korean corpora now total about half the size of the Penn English PropBank. One advantage of our corpus is that it is built on top of the ETRI Korean corpus, which uses a richer Ko-rean morphological tagging scheme than the Penn Korean Treebank. Our experiments will show that these finer-grained tags are crucial for achieving high SRL accuracy.",Material,DataSource,True,Introduce（引用目的）,True,P14-2104_1_0,2014,Training a Korean SRL System with Rich Morphological Features,Footnote
318,10320," http://www.donga.com"," ['6 Experiments and Results']","For latent morpheme representations, we used the Donga news article corpus. [Cite_Footnote_3]",3 http://www.donga.com,"For latent morpheme representations, we used the Donga news article corpus. [Cite_Footnote_3] The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions.",Material,Dataset,True,Use（引用目的）,True,P14-2104_2_0,2014,Training a Korean SRL System with Rich Morphological Features,Footnote
319,10321," http://kkma.snu.ac.kr/"," ['6 Experiments and Results']",We ran Kokoma Korean morpheme analyzer [Cite_Footnote_4] on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions.,4 http://kkma.snu.ac.kr/,"For latent morpheme representations, we used the Donga news article corpus. The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer [Cite_Footnote_4] on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions.",Method,Tool,True,Use（引用目的）,True,P14-2104_3_0,2014,Training a Korean SRL System with Rich Morphological Features,Footnote
320,10322," https://github.com/cxsoto/article-regions"," ['4 Novel Labeled Dataset']","The col-lection will be available at [Cite] https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats.",,"Therefore, a novel dataset was created. We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015) to manually annotate regions. Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set. The col-lection will be available at [Cite] https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats. The default format is PASCAL VOC. Nine labeled region classes are included in the annotations:",Material,Dataset,True,Produce（引用目的）,True,D19-1348_0_0,2019,Visual Detection with Context for Document Layout Analysis,Body
321,10324," https://github.com/eriklindernoren/PyTorch-YOLOv3"," ['5 Implementation and Experiments']","Figure 4 shows per-class performance over 30 training epochs, as well as comparative per-formance against the baseline Faster R-CNN model and reference model implementations of YOLOv3 (Linder-Noren, 2018) [Cite_Ref] and RetinaNet (Henon, 2018).",Erik Linder-Noren. 2018. A mini-mal pytorch implementation of yolov3. https://github.com/eriklindernoren/PyTorch-YOLOv3.,"Figure 4 shows per-class performance over 30 training epochs, as well as comparative per-formance against the baseline Faster R-CNN model and reference model implementations of YOLOv3 (Linder-Noren, 2018) [Cite_Ref] and RetinaNet (Henon, 2018). Most models plateaued early on this small dataset, except YOLOv3 which peak-ing at 68.9% after 49 epochs (beyond the figure bounds, but still below our model’s results). Pro-cessing time for our model averaged 0.65 seconds per article. By contrast, CERMINE averaged 9.4 seconds per article on the same set of articles, on the same machine.",Method,Code,True,Compare（引用目的）,False,D19-1348_2_0,2019,Visual Detection with Context for Document Layout Analysis,Reference
322,10325," https://datathief.org"," ['6 Discussion and Ongoing Work']","Visual region detection can serve as a precursor to numerous existing information ex-traction techniques or adaptations of them, in-cluding parsing of reference (Lammey, 2015), ta-bles (Rastan et al., 2015), and equations (Smithies et al., 2001), as well as data extraction from fig-ures (Tummers, 2006) [Cite_Ref] .","B. Tummers. 2006. DataThief III: A program to ex-tract (reverse engineer) data points from a graph. https://datathief.org, accessed: 2019-05-01.","The value of this work extends beyond text ex-traction. Visual region detection can serve as a precursor to numerous existing information ex-traction techniques or adaptations of them, in-cluding parsing of reference (Lammey, 2015), ta-bles (Rastan et al., 2015), and equations (Smithies et al., 2001), as well as data extraction from fig-ures (Tummers, 2006) [Cite_Ref] . And so it is valuable to all these efforts to be able to accurately and quickly segment document pages into regions of interest.",補足資料,Document,True,Introduce（引用目的）,True,D19-1348_3_0,2019,Visual Detection with Context for Document Layout Analysis,Reference
323,10326," https://github.com/tzutalin/labelImg"," ['4 Novel Labeled Dataset']","We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015) [Cite_Ref] to manually annotate regions.",Tzutalin. 2015. Labelimg. https://github.com/tzutalin/labelImg.,"Therefore, a novel dataset was created. We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015) [Cite_Ref] to manually annotate regions. Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set. The col-lection will be available at https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats. The default format is PASCAL VOC. Nine labeled region classes are included in the annotations:",Method,Tool,False,Use（引用目的）,True,D19-1348_4_0,2019,Visual Detection with Context for Document Layout Analysis,Reference
324,10327," https://github.com/jwyang/faster-rcnn.pytorch"," ['5 Implementation and Experiments']","Using the novel labeled dataset described in Sec-tion 4, a baseline model was trained using a stan-dard Faster R-CNN implementation (Yang et al., 2017) [Cite_Ref] .","Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh. 2017. A faster pytorch implementation of faster r-cnn. https://github.com/jwyang/faster-rcnn.pytorch.","Using the novel labeled dataset described in Sec-tion 4, a baseline model was trained using a stan-dard Faster R-CNN implementation (Yang et al., 2017) [Cite_Ref] . The model was trained using a single NVIDIA P100 GPU for 30 epochs on 600 images, and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pre-trained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0.0001, with decay of 0.1 every 5 epochs. Stan-dard anchor scales of [8, 16, 32] and anchor ratios of [0.5, 1.0, 2.0] were used. At a intersection-over-union (IOU) threshold of 0.5, the model achieved a mean average precision (mAP) of 46.38% on all nine region labels, with peak class performance on ‘body’ regions (87.49%) and lowest performance on ‘authors’ (1.22%).",Method,Code,True,Use（引用目的）,True,D19-1348_5_0,2019,Visual Detection with Context for Document Layout Analysis,Reference
325,10328," https://github.com/Microsoft/NeuronBlocks"," ['References']","In this pa-per, we introduce NeuronBlocks [Cite_Footnote_1] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture.",1 Code: https://github.com/Microsoft/NeuronBlocks,"Deep Neural Networks (DNN) have been widely employed in industry to address vari-ous Natural Language Processing (NLP) tasks. However, many engineers find it a big over-head when they have to choose from multi-ple frameworks, compare different types of models, and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [Cite_Footnote_1] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effective-ness of NeuronBlocks.",Method,Tool,True,Produce（引用目的）,True,D19-3028_0_0,2019,NeuronBlocks: Building Your NLP DNN Models Like Playing Lego,Footnote
326,10329," https://youtu.be/x6cOpVSZcdo"," ['References']","In this pa-per, we introduce NeuronBlocks [Cite_Footnote_2] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture.",2 Demo: https://youtu.be/x6cOpVSZcdo,"Deep Neural Networks (DNN) have been widely employed in industry to address vari-ous Natural Language Processing (NLP) tasks. However, many engineers find it a big over-head when they have to choose from multi-ple frameworks, compare different types of models, and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [Cite_Footnote_2] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effective-ness of NeuronBlocks.",Method,Tool,True,Produce（引用目的）,True,D19-3028_1_0,2019,NeuronBlocks: Building Your NLP DNN Models Like Playing Lego,Footnote
327,10330," https://github.com/Microsoft/pai"," ['1 Introduction']","• Platform Compatibility: support both Linux and Windows machines, CPU/GPU chips, as well as GPU platforms such as PAI [Cite_Footnote_3] .",3 https://github.com/Microsoft/pai,"• Platform Compatibility: support both Linux and Windows machines, CPU/GPU chips, as well as GPU platforms such as PAI [Cite_Footnote_3] .",補足資料,Website,True,Introduce（引用目的）,True,D19-3028_2_0,2019,NeuronBlocks: Building Your NLP DNN Models Like Playing Lego,Footnote
328,10331," https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md"," ['3 Design', '3.3 User Interface']","NeuronBlocks provides convenient user interface [Cite_Footnote_4] for users to build, train, and test DNN models.",4 https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md,"NeuronBlocks provides convenient user interface [Cite_Footnote_4] for users to build, train, and test DNN models. The details are described in the following.",補足資料,Document,True,Produce（引用目的）,True,D19-3028_3_0,2019,NeuronBlocks: Building Your NLP DNN Models Like Playing Lego,Footnote
329,10332," https://github.com/microsoft/NeuronBlocks/blob/master/Contributing.md"," ['3 Design', '3.4 Workflow']","Advanced users can also contribute novel customized blocks [Cite_Footnote_5] into Block Zoo, as long as they follow the same interface guidelines with the existing blocks.",5 https://github.com/microsoft/NeuronBlocks/blob/master/Contributing.md,"For model hyper-parameter tuning or architec-ture modification, users just need to change the JSON configuration file. Advanced users can also contribute novel customized blocks [Cite_Footnote_5] into Block Zoo, as long as they follow the same interface guidelines with the existing blocks. These new blocks can be further shared across all users for model architecture design. Moreover, Neuron-Blocks has flexible platform support, such as GPU/CPU, GPU management platforms like PAI.",補足資料,Document,True,Introduce（引用目的）,True,D19-3028_4_0,2019,NeuronBlocks: Building Your NLP DNN Models Like Playing Lego,Footnote
330,10333," https://github.com/Yupei-Du/bias-in-wat"," ['References']","More importantly, compar-ing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words. [Cite_Footnote_1]",1 Our code is publicly available at https://github.com/Yupei-Du/bias-in-wat.,"Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really re-flect true bias levels? For a small set of words (e.g. occupations), we can rely on hu-man annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which con-tains rich types of word connections anno-tated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, compar-ing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D19-1635_0_0,2019,Exploring Human Gender Stereotypes with Word Association Test,Footnote
331,10334," https://en.wikipedia.org/wiki/Word_Association"," ['2 Word Association Test']",Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being pre-sented with a word (the cue) [Cite_Footnote_2] .,2 https://en.wikipedia.org/wiki/Word_ Association,"Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being pre-sented with a word (the cue) [Cite_Footnote_2] . Table 1 lists some examples of the test. It is considered to be one of the most straightforward approaches for gain-ing insight into our semantic knowledge (Steyvers and Tenenbaum, 2005), and is also a common ap-proach to measure words’ meanings in our mind (so called mental lexicons (Jackendoff and Jack-endoff, 2002)). For studies of gender stereotypes, word association test provides a way to inspect how a gender-specific word (e.g., “he”, “she”) connecting with other words, and the patterns of the connections may help us to probe gender stereotypes in our brains.",補足資料,Document,True,Introduce（引用目的）,True,D19-1635_1_0,2019,Exploring Human Gender Stereotypes with Word Association Test,Footnote
332,10335," https://github.com/SimonDeDeyne/SWOWEN-2018"," ['3 Word Association Graph']",Pre-Processing Our pre-processing steps are mostly based on De Deyne et al. (2018) [Cite_Footnote_3] .,3 Raw word association test results and pre-process scripts are available in https://github.com/SimonDeDeyne/SWOWEN-2018. We re-implement their R codes with Python.,"Pre-Processing Our pre-processing steps are mostly based on De Deyne et al. (2018) [Cite_Footnote_3] . These procedures consist of spell-check, unifying cue forms, removing untrustworthy participants, and Americanize all non-American spellings. Through these steps, we reduce word association records from 88722 participants, 12292 cues, and 4069086 responses to 83863 participants, 12217 cues, and 3665100 responses.",補足資料,Document,True,Introduce（引用目的）,True,D19-1635_2_0,2019,Exploring Human Gender Stereotypes with Word Association Test,Footnote
333,10336," https://github.com/uclanlp/gn_glove"," ['5 Empirical Evaluation and Application', '5.5 Case Study: Do De-bias Methods Really Remove Gender Bias?']","For Hard-Debias, we compare with em-beddings before these procedures and for Gn-Glove, we compare with the original Glove (Pen-nington et al., 2014) offered in its project home-page [Cite_Footnote_13] .",13 https://github.com/uclanlp/gn_glove,"We experiment with de-bias methods of both categories, Hard-Debias refers to the method in (Bolukbasi et al., 2016) and Gn-Glove refers to the method in (Zhao et al., 2018). For each method, we compare it with an unde-biased ver-sion. For Hard-Debias, we compare with em-beddings before these procedures and for Gn-Glove, we compare with the original Glove (Pen-nington et al., 2014) offered in its project home-page [Cite_Footnote_13] .",補足資料,Website,True,Compare（引用目的）,True,D19-1635_3_0,2019,Exploring Human Gender Stereotypes with Word Association Test,Footnote
334,10337," http://www.nist.gov/speech/tests/ace/index.htm"," ['2 Related Work']","One of the recent evaluations, ACE, [Cite_Footnote_2] uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes.",2 http://www.nist.gov/speech/tests/ace/index.htm,"Our system automatically generates a template that captures the generally most important infor-mation for a particular domain and is reusable across multiple instances of that domain. Decid-ing what slots to include in the template, and what restrictions to place on their potential fillers, is a knowledge representation problem (Hobbs and Israel, 1994). Templates were used in the main IE competitions, the Message Understanding Con-ferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE, [Cite_Footnote_2] uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems. The domain templates were created manually by experts to capture the structure of the facts sought.",補足資料,Website,True,Introduce（引用目的）,True,P06-2027_0_0,2006,Automatic Creation of Domain Templates,Footnote
335,10338," http://news.bbc.co.uk/shared/bsp/search2/advanced/news_ifs.stm"," ['4 Data Description', '4.2 Test Data']","In this way, we obtained TDT document clusters for 2 instances of airplane crashes, [Cite_Footnote_3] instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks.",3 http://news.bbc.co.uk/shared/bsp/search2/advanced/news_ifs.stm,"To test our system, we used document clusters from the Topic Detection and Tracking (TDT) cor-pus (Fiscus et al., 1999). Each TDT topic has a topic label, such as Accidents or Natural Disas-ters. 4 These categories are broader than our do-mains. Thus, we manually filtered the TDT topics relevant to our four training domains (e.g., Acci-dents matching Airplane Crashes). In this way, we obtained TDT document clusters for 2 instances of airplane crashes, [Cite_Footnote_3] instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrorist attacks). This variation in the number of documents per topic is typical for the TDT corpus. Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents.",Material,DataSource,True,Use（引用目的）,True,P06-2027_1_0,2006,Automatic Creation of Domain Templates,Footnote
336,10339," http://chasen.org/~taku/software/freqt/"," ['5 Creating Templates']",To do this we use FREQuent Tree miner. [Cite_Footnote_5],5 http://chasen.org/˜taku/software/freqt/,"Step 3: Identify most frequent subtrees containing the top 50 verbs. A domain template should con-tain not only the most important actions for the do-main, but also the entities that are linked to these actions or to each other through these actions. The lexemes referring to such entities can potentially be used within the domain template slots. Thus, we analyze those portions of the syntactic trees which contain the verbs themselves plus other lex-emes used in the same subtrees as the verbs. To do this we use FREQuent Tree miner. [Cite_Footnote_5] This software is an implementation of the algorithm presented by (Abe et al., 2002; Zaki, 2002), which extracts frequent ordered subtrees from a set of ordered trees. Following (Sudo et al., 2003) we are inter-ested only in the lexemes which are near neighbors of the most frequent verbs. Thus, we look only for those subtrees which contain the verbs themselves and from four to ten tree nodes, where a node is either a syntactic tag or a lexeme with its tag. We analyze not only NPs which correspond to the sub-ject or object of the verb, but other syntactic con-stituents as well. For example, PPs can potentially link the verb to locations or dates, and we want to include this information into the template. Table 1 contains a sample of subtrees for the terrorist at-tack domain mined from the sentences containing the verb killed. The first column of Table 1 shows how many nodes are in the subtree.",Method,Tool,True,Use（引用目的）,True,P06-2027_2_0,2006,Automatic Creation of Domain Templates,Footnote
337,10340," http://www.cs.rochester.edu/~gildea/Verbs/"," ['5 Creating Templates']","Slot structures are similar to verb frames, which are manually created for the PropBank annota-tion (Palmer et al., 2005). [Cite_Footnote_6]",6 http://www.cs.rochester.edu/˜gildea/Verbs/,"Slot structures are similar to verb frames, which are manually created for the PropBank annota-tion (Palmer et al., 2005). [Cite_Footnote_6] An example of the PropBank frame for the verb to kill is:",補足資料,Website,True,Introduce（引用目的）,True,P06-2027_3_0,2006,Automatic Creation of Domain Templates,Footnote
338,10341," https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020"," ['1 Introduction']",By framing the task as a style-transfer problem we make three contributions: [Cite_Footnote_4],4 Code & Data at https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020,"We focus on the task of generating a simile starting from a literal utterance that contains the TOPIC, EVENT and PROPERTY. We frame this task as a style-transfer problem (Shen et al., 2017; Fu et al., 2017; Li et al., 2018; Sudhakar et al., 2019), where the author’s intent is to make the description of the TOPIC more emphatic by in-troducing a comparison with the VEHICLE via a shared PROPERTY (See Figure 1 for examples of literal descriptive sentences and the generated sim-iles). We call our approach SCOPE (Style trans-fer through COmmonsense PropErty). There are two main challenges we need to address: 1) the lack of training data that consists of pairs of lit-eral utterances and their equivalent simile in or-der to train a supervised model; 2) ensuring that the generated simile makes a meaningful compar-ison between the TOPIC and the VEHICLE via the shared PROPERTY explicitly or implicitly ex-pressed (e.g., Figure 1 GenSimile1 and GenSim-ile2, respectively). To the best of our knowledge, this is the first work in attempting to generate simi-les. By framing the task as a style-transfer problem we make three contributions: [Cite_Footnote_4]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.524_0_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
339,10342," https://www.reddit.com/r/WritingPrompts/"," ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']","Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS [Cite_Footnote_5] and FUNNY from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed.",5 https://www.reddit.com/r/WritingPrompts/,"Simile Dataset Collection. One of the possible ways to collect similes would be to train a super-vised model using existing data and methods for simile detection but most data sets are very small in size (in the order of a few hundreds). The only large-scale dataset is that of (Niculae and Danescu-Niculescu-Mizil, 2014), however their data is from a rather restricted domain of product reviews on Amazon, which might lack variety, diversity and creativity needed for this task. For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS [Cite_Footnote_5] and FUNNY from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, “The boy was as strong as an ox” gives strong as the PROPERTY shared by the boy and ox. But most similes do not give an explicit PROP-ERTY such as the Open Simile (e.g., “The boy was like an ox”) leaving the reader to infer that the boy is strong/large/fast (Qadir et al., 2016). Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io to mine comments. Through this process we collect 87,843 from Reddit. For each example, we show the top five commonsense properties associated with the vehicle obtained from COMET, and the best literal sentence constructed from these properties. The blue italic texts in the literal sentences represent the property inferred from the vehicle in the simile (denoted in black italic). self-labeled human written similes, from which we use 82,697 samples for training and 5,146 for validation.",Material,DataSource,False,Extend（引用目的）,False,2020.emnlp-main.524_1_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
340,10343," https://www.reddit.com/r/funny/"," ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']","Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS and FUNNY [Cite_Footnote_6] from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed.",6 https://www.reddit.com/r/funny/,"Simile Dataset Collection. One of the possible ways to collect similes would be to train a super-vised model using existing data and methods for simile detection but most data sets are very small in size (in the order of a few hundreds). The only large-scale dataset is that of (Niculae and Danescu-Niculescu-Mizil, 2014), however their data is from a rather restricted domain of product reviews on Amazon, which might lack variety, diversity and creativity needed for this task. For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS and FUNNY [Cite_Footnote_6] from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, “The boy was as strong as an ox” gives strong as the PROPERTY shared by the boy and ox. But most similes do not give an explicit PROP-ERTY such as the Open Simile (e.g., “The boy was like an ox”) leaving the reader to infer that the boy is strong/large/fast (Qadir et al., 2016). Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io to mine comments. Through this process we collect 87,843 from Reddit. For each example, we show the top five commonsense properties associated with the vehicle obtained from COMET, and the best literal sentence constructed from these properties. The blue italic texts in the literal sentences represent the property inferred from the vehicle in the simile (denoted in black italic). self-labeled human written similes, from which we use 82,697 samples for training and 5,146 for validation.",Material,DataSource,False,Extend（引用目的）,False,2020.emnlp-main.524_2_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
341,10344," https://pushshift.io/"," ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']",We use the API provided by pushshift.io [Cite_Footnote_8] to mine comments.,8 https://pushshift.io/,"Simile Dataset Collection. One of the possible ways to collect similes would be to train a super-vised model using existing data and methods for simile detection but most data sets are very small in size (in the order of a few hundreds). The only large-scale dataset is that of (Niculae and Danescu-Niculescu-Mizil, 2014), however their data is from a rather restricted domain of product reviews on Amazon, which might lack variety, diversity and creativity needed for this task. For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS and FUNNY from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, “The boy was as strong as an ox” gives strong as the PROPERTY shared by the boy and ox. But most similes do not give an explicit PROP-ERTY such as the Open Simile (e.g., “The boy was like an ox”) leaving the reader to infer that the boy is strong/large/fast (Qadir et al., 2016). Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io [Cite_Footnote_8] to mine comments. Through this process we collect 87,843 from Reddit. For each example, we show the top five commonsense properties associated with the vehicle obtained from COMET, and the best literal sentence constructed from these properties. The blue italic texts in the literal sentences represent the property inferred from the vehicle in the simile (denoted in black italic). self-labeled human written similes, from which we use 82,697 samples for training and 5,146 for validation.",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.524_3_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
342,10345," https://mosaickg.apps.allenai.org/comet_conceptnet"," ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']",Our work only leverages the HasProp-erty relation from COMET [Cite_Footnote_9] .,9 https://mosaickg.apps.allenai.org/ comet_conceptnet,"To generate the common sense PROPERTY that is implied by the VEHICLE in the simile, we take advantage of the simple syntactic structure of a simile. We extract the VEHICLE by extract-ing the phrase after like a and feed it as input to COMET (Bosselut et al., 2019). COMET is an adaptation framework for constructing common-sense knowledge based on pre-trained language models. Our work only leverages the HasProp-erty relation from COMET [Cite_Footnote_9] .",Method,Tool,False,Use（引用目的）,True,2020.emnlp-main.524_4_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
343,10346," http://conceptnet.io/"," ['3 Experimental Setup', '3.1 Baseline Systems']",[Cite_Footnote_10] .,10 ConceptNet is a weighted graph with multiple relations as can be viewed here http://conceptnet.io/. We use ‘has property” for our work. There are multiple edges for objects with their properties. We choose the edge with the highest weight with the title Sunset and Car Accident. We replace the literal sentences with generated similes from SCOPE.,"2. Retrieval (RTRVL): We also experiment with a retrieval approach where we retrieve a VEHICLE from ConceptNet (Speer et al., 2017) having the highest HasProperty rela-tion w.r.t our input (i.e., an adjective or adverb at the end of literal sentence) [Cite_Footnote_10] . For the in-put The city was beautiful we query Concept-Net with beautiful, which returns sunset as the VEHICLE having the highest weight for HasProperty beautiful. We take this retrieved VEHICLE and append it to the prefix ending in like a. If the word is not in ConceptNet, we fall back to its synonyms obtained from WordNet (Miller, 1995).",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.524_5_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
344,10347," https://github.com/atcbosselut/comet-commonsense"," ['References']","For retrieving commonsense properties of the vehi-cle, we use the pre-trained COMET model [Cite_Footnote_11] and retrieve top 5 candidates for each input.",11 https://github.com/atcbosselut/ comet-commonsense,"For retrieving commonsense properties of the vehi-cle, we use the pre-trained COMET model [Cite_Footnote_11] and retrieve top 5 candidates for each input. Vehicle and Overall Quality. WORKERS denote num-ber of workers employed for each task and α denotes Krippendorff’s alpha (α ) , reliability coefficient used for our study scheme (Fan et al., 2018). At each timestep, the model generates the probability of each word in the vocabulary being the likely next word. We randomly sample from the k = 5 most likely candidates from this distribution. We also use a softmax temperature of 0.7.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.524_6_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
345,10348," https://github.com/pytorch/fairseq/tree/master/examples/bart"," ['References']","For BART we use the BART large checkpoint (400M parame-ters) and use the implementation by FAIRSEQ (Ott et al., 2019). [Cite_Footnote_12]",12 https://github.com/pytorch/fairseq/tree/master/examples/bart,"1. Number of Parameters: For BART we use the BART large checkpoint (400M parame-ters) and use the implementation by FAIRSEQ (Ott et al., 2019). [Cite_Footnote_12]",Material,Dataset,False,Use（引用目的）,True,2020.emnlp-main.524_7_0,2020,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,Footnote
346,10349," http://www.wjh.harvard.edu/inquirer"," ['1 Introduction']","Certain semantic orientation lexicons have been manually compiled for English—the most notable being the General Inquirer (GI) (Stone et al., 1966). [Cite_Footnote_1]",1 http://www.wjh.harvard.edu/ inquirer,"Certain semantic orientation lexicons have been manually compiled for English—the most notable being the General Inquirer (GI) (Stone et al., 1966). [Cite_Footnote_1] However, the GI lexicon has orientation labels for only about 3,600 entries. The Pitts-burgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words.",補足資料,Website,False,Introduce（引用目的）,True,D09-1063_0_0,2009,Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus,Footnote
347,10350," http://sentiwordnet.isti.cnr.it/"," ['2 Related Work']",The set of semantic orientation scores of all WordNet synsets is released by the name SentiWordNet. [Cite_Footnote_2],2 http://sentiwordnet.isti.cnr.it/,"Esuli and Sebastiani (2006) used a supervised algorithm to attach semantic orientation scores to WordNet glosses. They train a set of ternary clas-sifiers using different training data and learning methods. The set of semantic orientation scores of all WordNet synsets is released by the name SentiWordNet. [Cite_Footnote_2] An evaluation of SentiWordNet by comparing orientation scores of about 1,000 WordNet glosses to scores assigned by human an-notators is presented in Esuli (2008). Our ap-proach uses a Roget-like thesaurus, and it does not use any supervised classifiers.",Material,Dataset,True,Introduce（引用目的）,True,D09-1063_1_0,2009,Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus,Footnote
348,10351," http://www.cs.pitt.edu/mpqa"," ['4 Evaluation', '4.2 Extrinsic: Identifying phrase polarity']",The MPQA corpus contains news articles man-ually annotated for opinions and private states. [Cite_Footnote_4],4 http://www.cs.pitt.edu/mpqa,"The MPQA corpus contains news articles man-ually annotated for opinions and private states. [Cite_Footnote_4] Notably, it also has polarity annotations (posi-tive/negative) at the phrase-level. We conducted an extrinsic evaluation of the manually-generated and automatically-generated lexicons by using them to determine the polarity of phrases in the MPQA version 1.1 collection of positive and neg-ative phrases (1,726 positive and 4,485 negative).",Material,Dataset,True,Produce（引用目的）,False,D09-1063_2_0,2009,Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus,Footnote
349,10352," http://www.codeplex.com/NodeXL"," ['6 Visualizing the semantic orientation of thesaurus categories']","To better aid our understanding of the automatically deter-mined category relationships we visualized this network using the Fruchterman-Reingold force-directed graph layout algorithm (Fruchterman and Reingold, 1991) and the NodeXL network analy-sis tool (Smith et al., 2009) [Cite_Footnote_5] .",5 Available from http://www.codeplex.com/NodeXL,"As discussed in Section 3.1.1, the affix seeds set connects the thesaurus words with opposite se-mantic orientation. Usually these pairs of words occur in different thesaurus categories, but this is not necessary. We can think of these connections as relationships of contrast in meaning and seman-tic orientation, not just between the two words but also between the two categories. To better aid our understanding of the automatically deter-mined category relationships we visualized this network using the Fruchterman-Reingold force-directed graph layout algorithm (Fruchterman and Reingold, 1991) and the NodeXL network analy-sis tool (Smith et al., 2009) [Cite_Footnote_5] .",Method,Tool,True,Use（引用目的）,True,D09-1063_3_0,2009,Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus,Footnote
350,10353," http://answers.yahoo.com"," ['1 Introduction']","Answers [Cite_Footnote_1] , Quora and the StackExchange family of forums.",1 http://answers.yahoo.com,"In contrast to factoid question answering (QA), non-factoid QA is concerned with questions whose an-swer is not easily expressed as an entity or list of en-tities and can instead be quite complex – compare, for example, the factoid question Who is the secre-tary general of the UN? with the non-factoid manner question How is the secretary general of the UN cho-sen? A significant amount of research has been car-ried out on factoid QA, with non-factoid questions receiving less attention. This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers [Cite_Footnote_1] , Quora and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.",補足資料,Website,True,Introduce（引用目的）,True,N16-1154_0_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
351,10354," http://quora.com"," ['1 Introduction']","Answers , Quora [Cite_Footnote_2] and the StackExchange family of forums.",2 http://quora.com,"In contrast to factoid question answering (QA), non-factoid QA is concerned with questions whose an-swer is not easily expressed as an entity or list of en-tities and can instead be quite complex – compare, for example, the factoid question Who is the secre-tary general of the UN? with the non-factoid manner question How is the secretary general of the UN cho-sen? A significant amount of research has been car-ried out on factoid QA, with non-factoid questions receiving less attention. This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora [Cite_Footnote_2] and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.",補足資料,Website,True,Introduce（引用目的）,True,N16-1154_1_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
352,10355," http://stackexchange.com/"," ['1 Introduction']","Answers , Quora and the StackExchange [Cite_Footnote_3] family of forums.",3 http://stackexchange.com/,"In contrast to factoid question answering (QA), non-factoid QA is concerned with questions whose an-swer is not easily expressed as an entity or list of en-tities and can instead be quite complex – compare, for example, the factoid question Who is the secre-tary general of the UN? with the non-factoid manner question How is the secretary general of the UN cho-sen? A significant amount of research has been car-ried out on factoid QA, with non-factoid questions receiving less attention. This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora and the StackExchange [Cite_Footnote_3] family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.",補足資料,Website,True,Introduce（引用目的）,True,N16-1154_2_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
353,10356," http://webscope.sandbox.yahoo.com/"," ['3 Experiments', '3.1 Data']",An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [Cite_Footnote_4],4 http://webscope.sandbox.yahoo.com/,"Our approach requires unlabelled data for unsu-pervised pre-training of the word and paragraph vec-tors. For these purposes we use the L6 Yahoo! An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [Cite_Footnote_4] This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. The Yahoo! Answers man-ner question dataset prepared by Jansen et al. (2014) and described in the previous paragraph, was ini-tially sampled from this larger dataset. We want to emphasize that the L6 dataset is only used for unsu-pervised pretraining – no meta-information is used in our experiments.",Material,DataSource,True,Use（引用目的）,True,N16-1154_3_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
354,10357," https://sites.google.com/site/trecliveqa2015/"," ['3 Experiments', '3.1 Data']","Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. [Cite_Footnote_5]",5 https://sites.google.com/site/trecliveqa2015/,"Our approach requires unlabelled data for unsu-pervised pre-training of the word and paragraph vec-tors. For these purposes we use the L6 Yahoo! An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. [Cite_Footnote_5] The Yahoo! Answers man-ner question dataset prepared by Jansen et al. (2014) and described in the previous paragraph, was ini-tially sampled from this larger dataset. We want to emphasize that the L6 dataset is only used for unsu-pervised pretraining – no meta-information is used in our experiments.",補足資料,Website,True,Introduce（引用目的）,True,N16-1154_4_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
355,10358," https://catalog.ldc.upenn.edu/LDC2003T05"," ['3 Experiments', '3.1 Data']","We also experiment with the English Gigaword corpus, [Cite_Footnote_6] which contains data from several English newswire sources.",6 https://catalog.ldc.upenn.edu/ LDC2003T05,"We also experiment with the English Gigaword corpus, [Cite_Footnote_6] which contains data from several English newswire sources. Jansen et al. (2014) used this cor-pus to train word embeddings, which were then in-cluded as features in their answer reranker.",Material,Dataset,True,Use（引用目的）,True,N16-1154_5_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
356,10359," https://radimrehurek.com/gensim/models/doc2vec.html"," ['3 Experiments', '3.2 Experimental Setup']",We use the gensim [Cite_Footnote_7] implementation of the DBOW and DM paragraph vector models.,7 https://radimrehurek.com/gensim/models/doc2vec.html,We use the gensim [Cite_Footnote_7] implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. The data was tokenized with the Stan-ford tokenizer and then lowercased.,Method,Code,True,Use（引用目的）,True,N16-1154_6_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
357,10360," https://code.google.com/p/word2vec/"," ['3 Experiments', '3.2 Experimental Setup']",The word em-beddings for the SkipAvg model are obtained with word2vec. [Cite_Footnote_8],8 https://code.google.com/p/word2vec/,We use the gensim implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. [Cite_Footnote_8] The data was tokenized with the Stan-ford tokenizer and then lowercased.,Method,Tool,True,Use（引用目的）,True,N16-1154_7_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
358,10361," http://nlp.stanford.edu/software/tokenizer.shtml"," ['3 Experiments', '3.2 Experimental Setup']",The data was tokenized with the Stan-ford tokenizer [Cite_Footnote_9] and then lowercased.,9 http://nlp.stanford.edu/software/tokenizer.shtml,We use the gensim implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec. The data was tokenized with the Stan-ford tokenizer [Cite_Footnote_9] and then lowercased.,Method,Tool,True,Use（引用目的）,True,N16-1154_8_0,2016,This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,Footnote
359,10362," https://github.com/Justin1904/Low-rank-Multimodal-Fusion"," ['4 Experimental Methodology', '4.3 Model Architecture']","In order to compare our fusion method with previ-ous work, we adopt a simple and straightforward model architecture [Cite_Footnote_2] for extracting unimodal rep-resentations.",2 The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion,"In order to compare our fusion method with previ-ous work, we adopt a simple and straightforward model architecture [Cite_Footnote_2] for extracting unimodal rep-resentations. Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as f a , f v , f l , to extract unimodal representations z a , z v , z l from uni-modal input features x a , x v , x l . For acoustic and visual modality, the sub-embedding network is a simple 2-layer feed-forward neural network, and for language modality, we used an LSTM (Hochre-iter and Schmidhuber, 1997) to extract represen-tations. The model architecture is illustrated in Figure 1.",Method,Code,True,Produce（引用目的）,True,P18-1209_0_0,2018,Efficient Low-rank Multimodal Fusion with Modality-Specific Factors,Footnote
360,10363," https://textessence.github.io"," ['References']",TextEssence can be found at [Cite] https: //textessence.github.io.,,"Embeddings of words and concepts capture syntactic and semantic regularities of lan-guage; however, they have seen limited use as tools to study characteristics of different cor-pora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of cor-pora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for cor-pus analysis. A case study on COVID-19 sci-entific literature illustrates the utility of the sys-tem. TextEssence can be found at [Cite] https: //textessence.github.io.",Method,Tool,True,Produce（引用目的）,True,2021.naacl-demos.13_0_0,2021,TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora,Body
361,10364," https://github.com/drgriffis/"," ['7 Conclusion']","Our implementation and experimental code is avail-able at [Cite] https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958.",,"TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at [Cite] https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io.",Method,Code,True,Produce（引用目的）,True,2021.naacl-demos.13_1_0,2021,TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora,Body
362,10365," https://doi.org/10.5281/zenodo.4432958"," ['7 Conclusion']","Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at [Cite] https://doi.org/10.5281/zenodo.4432958.",,"TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at [Cite] https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io.",Material,Dataset,True,Produce（引用目的）,True,2021.naacl-demos.13_2_0,2021,TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora,Body
363,10366," https://youtu.be/1xEEfsMwL0k"," ['7 Conclusion']",A screencast of TextEssence in action is available at [Cite] https://youtu.be/1xEEfsMwL0k.,,"TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at [Cite] https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io.",補足資料,Media,True,Produce（引用目的）,True,2021.naacl-demos.13_3_0,2021,TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora,Body
364,10367," https://textessence.github.io"," ['7 Conclusion']",All associated resources for TextEssence may be found at [Cite] https://textessence.github.io.,,"TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at [Cite] https://textessence.github.io.",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-demos.13_4_0,2021,TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora,Body
365,10368," https://infotabs.github.io/"," ['1 Introduction']","The dataset, along with associated scripts, are avail-able at [Cite] https://infotabs.github.io/.",,"The dataset, along with associated scripts, are avail-able at [Cite] https://infotabs.github.io/.",Material,Dataset,True,Produce（引用目的）,True,2020.acl-main.210_0_0,2020,I NFO T AB S: Inference on Tables as Semi-structured Data,Body
366,10369," https://github.com/mponza/SalIE"," ['1 Introduction']",The source code and the processed datasets are publicly available [Cite_Footnote_1] to encourage further develop-ments of the fact salience task.,1 https://github.com/mponza/SalIE,The source code and the processed datasets are publicly available [Cite_Footnote_1] to encourage further develop-ments of the fact salience task.,Method,Code,True,Produce（引用目的）,True,D18-1129_0_0,2018,Facts That Matter,Footnote
367,10370," https://github.com/zdou0830/DAFE"," ['References']","In addition, we show that combining our method with back translation can further improve the performance of the model. [Cite_Footnote_1]",1 Our code is publicly available at: https://github.com/zdou0830/DAFE.,"The recent success of neural machine transla-tion models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised do-main adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these meth-ods use generic representations for text regard-less of domain shift, which makes it infeasible for translation models to control outputs con-ditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental set-tings. In addition, we show that combining our method with back translation can further improve the performance of the model. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D19-1147_0_0,2019,Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings,Footnote
368,10371," http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/"," ['3 Experiments', '3.1 Setup']","In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) [Cite_Ref] and law, medical OPUS datasets.",Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/˜kevinduh/a/multitarget-tedtalks/.,"Datasets. We validate our models in two differ-ent data settings. First, we train on the law, medi-cal and IT datasets of the German-English OPUS corpus (Tiedemann, 2012) and test our methods’ ability to adapt from one domain to another. The dataset contain 2K development and test sentences in each domain, and about 715K, 1M and 337K training sentences respectively. These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) [Cite_Ref] and law, medical OPUS datasets. For this setting, we con-sider two language pairs, namely Czech and Ger-man to English. The Czech-English and German-English datasets consist of 1M and 4.5M sentences and the development and test sets contain about 2K sentences.",Material,Dataset,True,Use（引用目的）,True,D19-1147_1_0,2019,Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings,Reference
369,10372," http://proofwiki.org/"," ['3 The Natural Language Premise Selection task']","In order to evaluate the premise selection, we used a corpus extracted from ProofWiki [Cite_Footnote_1] .",1 http://proofwiki.org/,"In order to evaluate the premise selection, we used a corpus extracted from ProofWiki [Cite_Footnote_1] . ProofWiki is an online compendium of mathemati-cal proofs, with a goal to collect and classify math-ematical proofs. ProofWiki contains links between theorems, definitions and axioms in the context of a mathematical proof, determining which dependen-cies are present. Definitions and axioms are state-ments accepted without formal proof, while theo-rems, lemmas and corollaries require one (Solow, 2002). All entries are composed by a statement written in a combination of natural language and mathematical latex notation. The extracted cor-pus, which is named PS-ProofWiki, contains more than 18, 000 entries. We also computed how many times each statement is used as a premise, and we observed that most of the statements are used as dependencies for only a small subset of premises. A total of 6, 866 statements has between one and three dependants. On average, statements contain a total length of 289 symbols (characters and math-ematical symbols). The specific number of tokens will depend on the type of tokenisation used for the mathematical symbols. A complete analysis of this corpus is made available in (Ferreira and Freitas, 2020).",Material,DataSource,True,Use（引用目的）,True,2020.acl-main.657_0_0,2020,Premise Selection in Natural Language Mathematical Texts,Footnote
370,10373," https://github.com/ai-systems/premise"," ['6 Evaluation']","All evaluation data, as well as the experimental pipeline, can be found online [Cite_Footnote_2] for reproducibility purposes.",2 https://github.com/ai-systems/premise selection graph,"For the experiments using BERT and the pro-posed approach, we split the dataset using a 50/20/30 (train/dev/test) split. We run all exper-iments ten times, evaluating on the test set, and report the average Precision, Recall and F1-score. All evaluation data, as well as the experimental pipeline, can be found online [Cite_Footnote_2] for reproducibility purposes.",Mixed,Mixed,False,Produce（引用目的）,False,2020.acl-main.657_1_0,2020,Premise Selection in Natural Language Mathematical Texts,Footnote
371,10374," https://github.com/rkadlec/ubuntu-ranking-dataset-creator"," ['5 Experiments', '5.1 Experimental Settings', '5.1.1 Datasets']","For the specific-requirement scenario, we use the Ubuntu dialogue corpus [Cite_Footnote_1] extracted from Ubuntu question-answering forum, named Ubuntu (Lowe et al., 2015).",1 https://github.com/rkadlec/ubuntu-ranking-dataset- creator,"We use two public datasets in our experiments. For the specific-requirement scenario, we use the Ubuntu dialogue corpus [Cite_Footnote_1] extracted from Ubuntu question-answering forum, named Ubuntu (Lowe et al., 2015). The original training data consists of 7 million conversational post-responses pairs from 2014 to April 27,2012. The validation data are conversational pairs from April 27,2014 to Au-gust 7,2012, and the test data are from August 7,2012 to December 1,2012. We set the number of positive examples as 4,000,000 in the Github to directly sample data from the whole corpus. Then we construct post and response pairs based on the period from both context and utterance. We also conduct some data pro-processing. For ex-ample, we use the official script to tokenize, stem and lemmatize, and the duplicates and sentences with length less than 5 or longer than 50 are re-moved. Finally, we obtain 3,200,000, 100,000 and 100,000 for training, validation and testing, re-spectively.",Material,Dataset,True,Use（引用目的）,True,P18-1137_0_0,2018,Tailored Sequence to Sequence Models to Different Conversation Scenarios,Footnote
372,10375," https://github.com/zhanghainan/TailoredSeq2Seq2DifferentConversationScenarios"," ['5 Experiments', '5.1 Experimental Settings', '5.1.1 Datasets']","We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. [Cite_Footnote_2]",2 https://github.com/zhanghainan/TailoredSeq2Seq2 DifferentConversationScenarios,"For the diverse-requirement scenario, we use the Chinese Weibo dataset, named STC (Shang et al., 2015). It consists of 3,788,571 post-response pairs extracted from the Chinese Weibo website and cleaned by the data publishers. We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. [Cite_Footnote_2]",Material,Dataset,True,Use（引用目的）,True,P18-1137_1_0,2018,Tailored Sequence to Sequence Models to Different Conversation Scenarios,Footnote
373,10376," https://github.com/yangkevin2/emnlp2020-stream-beam-mt"," ['1 Introduction']",Our approach also improves decoding efficiency in lightweight models for se-mantic and syntactic parsing. [Cite_Footnote_1],1 Code available at https://github.com/yangkevin2/emnlp2020-stream-beam-mt.,"We apply our method to variable-width beam search. For variable-output-length decoding even in batched settings, variable-width beam search often modestly decreases accuracy in exchange for substantial speedups over fixed-width beam search (Freitag and Al-Onaizan, 2017; Wu et al., 2016). When decoding with Fairseq’s state-of-the-art WMT’19 model (Ng et al., 2019), our method further improves over the speed of base-line variable-width beam search: up to 16.5% on a 32GB V100 GPU, without changing BLEU (Pa-pineni et al., 2002). Our approach also improves decoding efficiency in lightweight models for se-mantic and syntactic parsing. [Cite_Footnote_1] In principle, our method can be applied to any task which sequen-tially processes variable-length data.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.366_0_0,2020,A Streaming Approach For Efficient Batched Beam Search,Footnote
374,10377," http://www.statmt.org/wmt19/translation-task.html"," ['-', 'Hyperparameters']",A.5 Dataset Details A.5.1 Machine Translation Evaluation datasets (newstest2018 and new-stest2017) are available at [Cite] http://www.statmt.,,"In Tables 6 and 7, we present results from applying our method to the JOBS and GEO datasets. We use the same hyperparameters and heuristics as for ATIS, and operate under the same candidate-expansion constraint. V AR -S TREAM is substan-tially faster than Fixed and V AR -B ATCH under this setting. A.5 Dataset Details A.5.1 Machine Translation Evaluation datasets (newstest2018 and new-stest2017) are available at [Cite] http://www.statmt.org/wmt19/translation-task.html. new-stest2018 contains 2998 and 3000 examples for De-En and Ru-En respectively, while newstest2017 contains 3004 and 3001.",Material,Dataset,True,Produce（引用目的）,False,2020.emnlp-main.366_1_0,2020,A Streaming Approach For Efficient Batched Beam Search,Body
375,10378," https://github.com/Alex-Fabbri/"," ['-', 'A.5.2 Semantic Parsing']","Datasets can be obtained by running the data scripts at [Cite] https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch.",,"Datasets can be obtained by running the data scripts at [Cite] https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch. We use Dong and Lapata (2016)’s training, development (for ATIS), and test sets. ATIS, JOBS, and GEO contain 5410, 640, and 880 examples respectively.",Method,Code,False,Use（引用目的）,True,2020.emnlp-main.366_2_0,2020,A Streaming Approach For Efficient Batched Beam Search,Body
376,10379," https://github.com/jhcross/span-parser"," ['-', 'A.5.3 Syntactic Parsing']",The Penn Treebank dataset and splits are available at [Cite] https://github.com/jhcross/span-parser.,,"The Penn Treebank dataset and splits are available at [Cite] https://github.com/jhcross/span-parser. The training, data, and test splits are the standard Penn Treebank splits (sections 2-21 for training, 22 for development, and 23 for test, containing 39832, 1700, and 2416 examples respectively).",Mixed,Mixed,True,Extend（引用目的）,False,2020.emnlp-main.366_3_0,2020,A Streaming Approach For Efficient Batched Beam Search,Body
377,10380," https://github.com/neulab/NL2code"," ['1 Introduction']",Our model also gives competitive performance on a standard semantic parsing benchmark [Cite_Footnote_1] .,1 Implementation available at https://github.com/neulab/NL2code,"Experiments (§ 5) on two Python code gener-ation tasks show 11.7% and 9.3% absolute im-provements in accuracy against the state-of-the-art system (Ling et al., 2016). Our model also gives competitive performance on a standard semantic parsing benchmark [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,P17-1041_0_0,2017,A Syntactic Neural Model for General-Purpose Code Generation,Footnote
378,10381," http://www.github.com/neubig/lamtram"," ['5 Experimental Evaluation', '5.3 Results']","For completeness, we also compare with a strong neural machine translation ( NMT ) system (Neubig, 2015) [Cite_Ref] using a standard encoder-decoder architecture with atten-tion and unknown word replacement , and include numbers from other baselines used in Ling et al. (2016).",Graham Neubig. 2015. lamtram: A toolkit for lan-guage and translation modeling using neural net-works. http://www.github.com/neubig/lamtram.,"Evaluation results for Python code generation tasks are listed in Tab. 3. Numbers for our sys-tems are averaged over three runs. We compare primarily with two approaches: (1) Latent Pre-dictor Network ( LPN ), a state-of-the-art sequence-to-sequence code generation model (Ling et al., 2016), and (2) S EQ 2T REE , a neural semantic pars-ing model (Dong and Lapata, 2016). S EQ 2T REE generates trees one node at a time, and the tar-get grammar is not explicitly modeled a priori, but implicitly learned from data. We test both the original S EQ 2T REE model released by the au-thors and our revised one (S EQ 2T REE –UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015). For completeness, we also compare with a strong neural machine translation ( NMT ) system (Neubig, 2015) [Cite_Ref] using a standard encoder-decoder architecture with atten-tion and unknown word replacement , and include numbers from other baselines used in Ling et al. (2016). On the HS dataset, which has relatively large ASTs, we use unary closure for our model and S EQ 2T REE , and for D JANGO we do not.",Method,Tool,True,Compare（引用目的）,True,P17-1041_1_0,2017,A Syntactic Neural Model for General-Purpose Code Generation,Reference
379,10382," https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/"," ['6 Related Work']","A similar line is to use NL queries for code re-trieval (Wei et al., 2015 [Cite_Ref] ; Raza et al., 20150.","Yi Wei, Nirupama Chandrasekaran, Sumit Gul-wani, and Youssef Hamadi. 2015. Build-ing bing developer assistant. Techni-cal report. https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/.","Code Generation and Analysis Most works on code generation focus on generating code for do-main specific languages (DSLs) (Kushman and Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches re-cently explored (Liu et al., 2016; Parisotto et al., 2016; Balog et al., 2016). For general-purpose code generation, besides the general framework of Ling et al. (2016), existing methods often use language and task-specific rules and strate-gies (Lei et al., 2013; Raghothaman et al., 2016). A similar line is to use NL queries for code re-trieval (Wei et al., 2015 [Cite_Ref] ; Allamanis et al., 2015). The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016). Finally, our work falls into the broad field of probabilistic modeling of source code (Maddison and Tarlow, 2014; Nguyen et al., 2013). Our approach of factoring an AST using probabilistic models is closely related to Allama-nis et al. (2015), which uses a factorized model to measure the semantic relatedness between NL and ASTs for code retrieval, while our model tackles the more challenging generation task.",補足資料,Paper,True,Compare（引用目的）,True,P17-1041_3_0,2017,A Syntactic Neural Model for General-Purpose Code Generation,Reference
380,10383," http://www.nlm.nih.gov/mesh"," ['2 MeSH and Medline']","In this paper we use the MeSH (Medical Subject Head-ings) lexical hierarchy [Cite_Footnote_1] , but the approach should be equally applicable to other domains using other thesauri and ontologies.",1 http://www.nlm.nih.gov/mesh,"In this paper we use the MeSH (Medical Subject Head-ings) lexical hierarchy [Cite_Footnote_1] , but the approach should be equally applicable to other domains using other thesauri and ontologies. In MeSH, each concept is assigned one or more alphanumeric descriptor codes corresponding to particular positions in the hierarchy. For example, A (Anatomy), A01 (Body Regions), A01.456 (Head), A01.456.505 (Face), A01.456.505.420 (Eye). Eye is ambiguous according to MeSH and has a second code: A09.371 (A09 represents Sense Organs).",Material,Knowledge,False,Use（引用目的）,True,N03-2023_0_0,2003,Category-Based Pseudowords,Footnote
381,10384," https://github.com/Elbria/xformal-FoST-meta"," ['1 Introduction']",[Cite] https://github.com/Elbria/xformal-FoST-meta.,,• Our analysis code and meta-evaluation files with system outputs are made public to facil-itate further work in developing better auto-matic metrics for ST : [Cite] https://github.com/Elbria/xformal-FoST-meta.,Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.100_0_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Body
382,10385," https://github.com/fuzhenxin/Style-Transfer-in-Text"," ['2 Background', '2.2 Structured Review of ST Evaluation']",Tables 1 and 2 summarize evaluation details for all F o ST methods in papers from the ST survey by Jin et al. (2020). [Cite_Footnote_1],1 The complete list is hosted at: https://github.com/fuzhenxin/Style-Transfer-in-Text,"We systematically review automatic evaluation practices in ST with formality as a case study. We select F o ST for this work since it is one of the most frequently studied styles (Jin et al., 2020) and there is human annotated data including human ref-erences available for these evaluations (Rao and Tetreault, 2018; Briakou et al., 2021b). Tables 1 and 2 summarize evaluation details for all F o ST methods in papers from the ST survey by Jin et al. (2020). [Cite_Footnote_1] Most works employ automatic evaluation for style (87%) and meaning preservation (83%). Fluency is the least frequently evaluated dimension (43%), while 74% of papers employ automatic met-rics to assess the overall quality of system outputs that captures all desirable aspects.",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.100_1_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Footnote
383,10386," https://github.com/huggingface/transformers"," ['4 Experiment Settings']","All models are based on the Hugging-Face Transformers (Wolf et al., 2020) [Cite_Footnote_2] library.",2 https://github.com/huggingface/ transformers,"Supervised Metrics For all supervised model-based approaches, we experiment with fine-tuning two multilingual pre-trained language models: 1. multilingual BERT , dubbed m BERT (Devlin et al., 2019)—a transformer-based model pre-trained with a masked language model objective on the concatenation of monolingual Wikipedia corpora from the 104 languages with the largest Wikipedias. 2. XLM - R (Conneau et al., 2020)—a transformer-based masked language model trained on 100 languages using monolingual Common-Crawl data. All models are based on the Hugging-Face Transformers (Wolf et al., 2020) [Cite_Footnote_2] library. We fine-tune with the Adam optimizer (Kingma and Ba, 2015), a batch size of 32, and a learning rate of 5e−5 for 3 and 5 epochs for classification and regression tasks, respectively. We perform a grid search on held-out validation sets over learning rate with values: 2e−3, 2e−4, 2e−5, and 5e−5 and over number of epochs with values: 3, 5, and 8.",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.100_2_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Footnote
384,10387," https://github.com/mjpost/sacrebleu"," ['4 Experiment Settings']",Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the [Cite_Footnote_3] ST evaluation aspects.,3 https://github.com/mjpost/sacrebleu,"Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popović (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the [Cite_Footnote_3] ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -",Method,Code,False,Introduce（引用目的）,False,2021.emnlp-main.100_3_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Footnote
385,10388," https://github.com/awslabs/mlm-scoring"," ['4 Experiment Settings']","For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. [Cite_Footnote_8]",8 https://github.com/awslabs/ mlm-scoring,"Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popović (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. [Cite_Footnote_8] PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.100_8_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Footnote
386,10389," https://github.com/kpu/kenlm"," ['4 Experiment Settings']","PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). [Cite_Footnote_9]",9 https://github.com/kpu/kenlm,"Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popović (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). [Cite_Footnote_9] Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.100_9_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Footnote
387,10390," https://github.com/PhilipMay/stsb-multi-mt"," ['4 Experiment Settings']","For datasets that are only available for EN , we use the already available machine translated re-sources for STS [Cite_Footnote_10] and formality datasets (Briakou et al., 2021b).",10 https://github.com/PhilipMay/ stsb-multi-mt,"Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popović (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS [Cite_Footnote_10] and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -",Material,DataSource,False,Use（引用目的）,True,2021.emnlp-main.100_10_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Footnote
388,10391," https://aws.amazon.com/translate"," ['4 Experiment Settings']",The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service [Cite_Footnote_11] (with reported BLEU scores of 37.16 ( BR -,11 https://aws.amazon.com/translate,"Unsupervised Metrics For meaning preserva-tion metrics, we use the open-sourced implemen-tations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR ; Popović (2015) for chr F . 3,4,5 For BERT -score we use the implementation of Zhang et al. (2020a); 6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings. 7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudo-likelihood. PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service [Cite_Footnote_11] (with reported BLEU scores of 37.16 ( BR -",補足資料,Website,True,Introduce（引用目的）,True,2021.emnlp-main.100_11_0,2021,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,Footnote
389,10392," http://knowitall.github.io/openie"," ['1 Introduction']","This is sustained empiri-cally by showing that extractions from Open-IE4 [Cite_Footnote_1] significantly outperform ClearNLP’s SRL (Choi, 2012) in textual similarity, analogies, and reading comprehension tasks.",1 http://knowitall.github.io/openie,"Further, Stanovsky et al. (2015) compared the performance of several off-the-shelf parsers in dif-ferent semantic tasks. Most relevant to this work is the comparison between Open-IE and SRL. Specifically, they suggest that SRL’s longer argu-ments introduce noise which hurts performance for downstream tasks. This is sustained empiri-cally by showing that extractions from Open-IE4 [Cite_Footnote_1] significantly outperform ClearNLP’s SRL (Choi, 2012) in textual similarity, analogies, and reading comprehension tasks.",Material,DataSource,False,Use（引用目的）,True,P16-2077_0_0,2016,Specifying and Annotating Reduced Argument Span Via QA-SRL,Footnote
390,10393," https://www.mturk.com"," ['4 Annotation Experiment', '4.2 Crowdsourcing']",We created an Amazon Mechanical Turk [Cite_Footnote_7] project to investigate the possible scalability of our anno-tation using non-trained annotators.,7 https://www.mturk.com,We created an Amazon Mechanical Turk [Cite_Footnote_7] project to investigate the possible scalability of our anno-tation using non-trained annotators.,補足資料,Website,True,Produce（引用目的）,True,P16-2077_1_0,2016,Specifying and Annotating Reduced Argument Span Via QA-SRL,Footnote
391,10394," http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/"," ['1 Introduction']","The increasing popularity of social media has drastically changed how our daily news are pro-duced, disseminated and consumed. [Cite_Footnote_1]",1 The latest Pew Research statistics show that 68% American adults at least occasionally get news on social media. http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/,"The increasing popularity of social media has drastically changed how our daily news are pro-duced, disseminated and consumed. [Cite_Footnote_1] Without sys-tematic moderation, a large volume of informa-tion based on false or unverified claims (e.g., fake news, rumours, propagandas, etc.) can proliferate online. Such misinformation poses unprecedented challenges to information credibility, which tradi-tionally relies on fact-checkers to manually assess whether specific claims are true or not.",補足資料,Document,True,Introduce（引用目的）,True,P19-1244_0_0,2019,Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks,Footnote
392,10395," http://www.fakenewschallenge.org/"," ['1 Introduction']","In the Fake News Challenge [Cite_Footnote_2] , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline.",2 http://www.fakenewschallenge.org/,"A recent trend is that researchers are trying to establish more objective tasks and evidence-based verification solutions, which focus on the use of evidence obtained from more reliable sources, e.g., encyclopedia articles, verified news, etc., as an important distinguishing factor (Thorne and Vlachos, 2018). Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge [Cite_Footnote_2] , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al. (2018a) formulate the Fact Extraction and VER-ification (FEVER) task which requires extract-ing evidence from Wikipedia and synthesizing in-formation from multiple documents to verify the claim. Popat et al. (2018) propose DeClarE, an evidence-aware neural attention model to aggre-gate salient words from source news articles as the main evidence to obtain claim-specific representa-tion based on the attention score of each token.",補足資料,Website,True,Introduce（引用目的）,True,P19-1244_1_0,2019,Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks,Footnote
393,10396," http://deeplearning.net/software/theano/"," ['5 Experiments and Results', '5.2 Experiments on Veracity-based Datasets']","We implement our models and DeClarE with Theano [Cite_Footnote_3] , and use the original codes of other base-lines.",3 http://deeplearning.net/software/theano/,"We implement our models and DeClarE with Theano [Cite_Footnote_3] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation.",Method,Tool,False,Use（引用目的）,True,P19-1244_2_0,2019,Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks,Footnote
394,10397," https://github.com/gabrielStanovsky/mt_gender"," ['References']",Our data and code are pub-licly available at [Cite] https://github.com/gabrielStanovsky/mt_gender.,,"We present the first challenge set and eval-uation protocol for the analysis of gender bias in machine translation (MT). Our ap-proach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help her in the operation”). We devise an automatic gender bias evaluation method for eight tar-get languages with grammatical gender, based on morphological analysis (e.g., the use of fe-male inflection for the word “doctor”). Our analyses show that four popular industrial MT systems and two recent state-of-the-art aca-demic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are pub-licly available at [Cite] https://github.com/gabrielStanovsky/mt_gender.",Mixed,Mixed,True,Produce（引用目的）,True,P19-1164_0_0,2019,Evaluating Gender Bias in Machine Translation,Body
395,10398," https://translate.google.com"," ['3 Evaluation', '3.1 Experimental Setup']","MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [Cite_Footnote_1] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation.",1 https://translate.google.com,"MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [Cite_Footnote_1] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit.",Method,Tool,True,Use（引用目的）,True,P19-1164_1_0,2019,Evaluating Gender Bias in Machine Translation,Footnote
396,10399," https://www.bing.com/translator"," ['3 Evaluation', '3.1 Experimental Setup']","MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [Cite_Footnote_2] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation.",2 https://www.bing.com/translator,"MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [Cite_Footnote_2] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit.",Method,Tool,True,Use（引用目的）,True,P19-1164_2_0,2019,Evaluating Gender Bias in Machine Translation,Footnote
397,10400," https://aws.amazon.com/translate"," ['3 Evaluation', '3.1 Experimental Setup']","MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [Cite_Footnote_3] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation.",3 https://aws.amazon.com/translate,"MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [Cite_Footnote_3] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit.",Method,Tool,True,Use（引用目的）,True,P19-1164_3_0,2019,Evaluating Gender Bias in Machine Translation,Footnote
398,10401," http://www.systransoft.com"," ['3 Evaluation', '3.1 Experimental Setup']","MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, [Cite_Footnote_4] (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation.",4 http://www.systransoft.com,"MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, [Cite_Footnote_4] (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit.",Method,Tool,True,Use（引用目的）,True,P19-1164_4_0,2019,Evaluating Gender Bias in Machine Translation,Footnote
399,10402," https://github.com/pytorch/fairseq"," ['3 Evaluation', '3.1 Experimental Setup']","We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit. [Cite_Footnote_5]",5 https://github.com/pytorch/fairseq,"MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-to- German translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit. [Cite_Footnote_5]",Method,Tool,True,Use（引用目的）,True,P19-1164_5_0,2019,Evaluating Gender Bias in Machine Translation,Footnote
400,10403," https://www.bls.gov/cps/cpsaat11.htm"," ['3 Evaluation', '3.2 Results']","Perhaps most tellingly, ∆ S measures the differ-ence in performance (F 1 score) between stereo-typical and non-stereotypical gender role assign-ments, as defined by Zhao et al. (2018) who use statistics provided by the US Department of Labor. [Cite_Footnote_6]",6 https://www.bls.gov/cps/cpsaat11.htm,"Perhaps most tellingly, ∆ S measures the differ-ence in performance (F 1 score) between stereo-typical and non-stereotypical gender role assign-ments, as defined by Zhao et al. (2018) who use statistics provided by the US Department of Labor. [Cite_Footnote_6] This metric shows that all tested sys-tems have a significant and consistently better per-formance when presented with pro-stereotypical assignments (e.g., a female nurse), while their performance deteriorates when translating anti-stereotypical roles (e.g., a male receptionist). For instance, Figure 2 depicts Google Trans-late absolute accuracies on stereotypical and non-stereotypical gender roles across all tested lan-guages. Other tested systems show similar trends.",補足資料,Document,True,Introduce（引用目的）,True,P19-1164_6_0,2019,Evaluating Gender Bias in Machine Translation,Footnote
401,10404," https://github.com/gabrielStanovsky/mt_gender"," ['5 Conclusions']",Our data and code are pub-licly available at [Cite] https://github.com/gabrielStanovsky/mt_gender.,,"We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT mod-els are significantly prone to translate based on gender stereotypes rather than more mean-ingful context. Our data and code are pub-licly available at [Cite] https://github.com/gabrielStanovsky/mt_gender.",Mixed,Mixed,True,Produce（引用目的）,True,P19-1164_7_0,2019,Evaluating Gender Bias in Machine Translation,Body
402,10405," https://www.gutenberg.org/"," ['1 Introduction']","We also qualitatively demonstrate that high-quality micro-clusters emerge from a smaller, more ho-mogeneous data set of Gutenberg [Cite_Footnote_3] novels.",3 https://www.gutenberg.org/,"We show that rich multi-view representations produce better micro-clusters compared to com-petitive but simpler models, and that interpretabil-ity of the learnt representations is not compro-mised despite the more complex objective. We also qualitatively demonstrate that high-quality micro-clusters emerge from a smaller, more ho-mogeneous data set of Gutenberg [Cite_Footnote_3] novels.",補足資料,Website,True,Introduce（引用目的）,True,D17-1200_0_0,2017,Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels,Footnote
403,10406," https://github.com/miyyer/rmn"," ['6 Evaluation']","We train both RMN and MVPlot for 15 epochs using SGD and ADAM (Kingma and Ba, 2014). [Cite_Footnote_8]",8 Our implementation builds on the available RMN code https://github.com/miyyer/rmn.,"Parameter settings Across all experiments and corpus-specific models, we set β=0.99 for MV-Plot, and for both MVPlot and RMN we set α=0.5, λ=10 −5 , k=50. We train both RMN and MVPlot for 15 epochs using SGD and ADAM (Kingma and Ba, 2014). [Cite_Footnote_8]",Method,Code,True,Use（引用目的）,True,D17-1200_1_0,2017,Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels,Footnote
404,10407," https://allennlp.org/iirc"," ['References']","The dataset, code for the baseline system, and a leaderboard can be found at [Cite] https://allennlp.org/iirc.",,"Humans often have to read multiple doc-uments to address their information needs. However, most existing reading comprehen-sion (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system’s performance at identifying a poten-tial lack of sufficient information and locat-ing sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13K questions over paragraphs from En-glish Wikipedia that provide only partial in-formation to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the con-texts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, in-creasing the difficulty of the task. We fol-low recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at [Cite] https://allennlp.org/iirc.",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.86_0_0,2020,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,Body
405,10408," http://fjoch.com/mkcls.html"," ['2 Background']","This model has been popular for language mod-elling and bilingual word alignment, and an imple-mentation with improved inference called mkcls (Och, 1999) [Cite_Footnote_1] has become a standard part of statis-tical machine translation systems.",1 Available from http://fjoch.com/mkcls.html.,"Past research in unsupervised PoS induction has largely been driven by two different motivations: a task based perspective which has focussed on induc-ing word classes to improve various applications, and a linguistic perspective where the aim is to induce classes which correspond closely to anno-tated part-of-speech corpora. Early work was firmly situtated in the task-based setting of improving gen-eralisation in language models. Brown et al. (1992) presented a simple first-order HMM which restricted word types to always be generated from the same class. Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the sig-nificant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language mod-elling and bilingual word alignment, and an imple-mentation with improved inference called mkcls (Och, 1999) [Cite_Footnote_1] has become a standard part of statis-tical machine translation systems.",Method,Tool,True,Introduce（引用目的）,True,P11-1087_0_0,2011,A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction,Footnote
406,10409," http://www.pascal-network.org/Challenges/RTE"," ['2 Related Work']",Inferencing is a core requirement of systems that participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see [Cite] http://www.pascal-network.org/Challenges/RTE and .../RTE2).,,"Inferencing is a core requirement of systems that participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see [Cite] http://www.pascal-network.org/Challenges/RTE and .../RTE2). It is also used in at least two of the more visible end-to-end QA systems of the present day. The LCC system (Moldovan & Rus, 2001) uses a Logic Prover to establish the connection between a candidate answer passage and the question. Text terms are converted to logical forms, and the ques-tion is treated as a goal which is “proven”, with real-world knowledge being provided by Extended WordNet. The IBM system PIQUANT (Chu-Carroll et al., 2003) used Cyc (Lenat, 1995) in an-swer verification. Cyc can in some cases confirm or reject candidate answers based on its own store of instance information; in other cases, primarily of a numerical nature, Cyc can confirm whether candi-dates are within a reasonable range established for their subtype.",補足資料,Website,True,Introduce（引用目的）,True,P06-1135_0_0,2006,Improving QA Accuracy by Question Inversion,Body
407,10410," http://groups.csail.mit.edu/rbg/code/typetagging/"," ['References']","On several languages, we report performance exceeding that of more complex state-of-the art systems. [Cite_Footnote_1]",1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/.,"Part-of-speech (POS) tag distributions are known to exhibit sparsity — a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incor-porating this sparsity constraint improves tag-ging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity. This paper proposes a sim-ple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parame-ters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial per-formance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D10-1083_0_0,2010,Simple Type-Level Unsupervised POS Tagging,Footnote
408,10411," http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents"," ['4 Evaluation', '4.2 Setup']","To estimate the timeliness and semantic cred-ibility indicators, we use AQUAINT-2, a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007) [Cite_Ref] .",AQUAINT-2 (2007). URL: http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents.,"To estimate the timeliness and semantic cred-ibility indicators, we use AQUAINT-2, a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007) [Cite_Ref] . Articles are in English and come from a variety of sources.",Material,Dataset,True,Compare（引用目的）,True,P08-1105_0_0,2008,Credibility Improves Topical Blog Post Retrieval,Reference
409,10412," http://ebiquity.umbc.edu/resource/html/id/212/Splog-Blog-Dataset"," ['3 Modeling', '3.3.2 Blog level credibility indicators']","We train an SVM classifier on a labeled splog blog dataset (Kolari et al., 2006) [Cite_Ref] using the top 1500 words for both spam and non-spam blogs as features.","Kolari, P., Finin, T., Java, A., and Joshi, A. (2006). Splog blog dataset. URL: http: //ebiquity.umbc.edu/resource/html/ id/212/Splog-Blog-Dataset.","Spam filtering To estimate the spaminess of a blog, we take a simple approach. We train an SVM classifier on a labeled splog blog dataset (Kolari et al., 2006) [Cite_Ref] using the top 1500 words for both spam and non-spam blogs as features. For each classified blog d we have a confidence value s(d). If the clas-sifier cannot make a decision (s(d) = 0) we set p spam (d) to 0, otherwise we use the following to transform s(d) into a spam prior p spam (d): where n(r, d) is the number of comments on post d. Regularity To estimate the regularity prior we use where σ interval expresses the standard deviation of the temporal intervals between two successive posts. Topical consistency Here we use an approach similar to query clarity (Cronen-Townsend and Croft, 2002): based on the list of posts from the same blog we compare the topic distribution of blog B to the topic distribution in the collection C and assign a ‘clarity’ value to B; a score further away from zero indicates a higher topical consistency. We estimate the topical consistency prior as where clarity(d) is estimated by",Material,Knowledge,True,Use（引用目的）,True,P08-1105_1_0,2008,Credibility Improves Topical Blog Post Retrieval,Reference
410,10413," http://www.consumerwebwatch.org/news/report3_credibilityresearch/slicedbread.pdf"," ['2 Credibility Indicators', '2.1 Rubin and Liddy (2006)’s work']","Rubin and Liddy (2006) proposed a four factor an-alytical framework for blog-readers’ credibility as-sessment of blog sites, based in part on evidential-ity theory (Chafe, 1986), website credibility assess-ment surveys (Stanford et al., 2002) [Cite_Ref] , and Van House (2004)’s observations on blog credibility.","Stanford, J., Tauber, E., Fogg, B., and Marable, L. (2002). Experts vs online consumers: A comparative cred-ibility study of health and finance web sites. URL: http://www.consumerwebwatch.org/news/report3_credibilityresearch/slicedbread.pdf.","Rubin and Liddy (2006) proposed a four factor an-alytical framework for blog-readers’ credibility as-sessment of blog sites, based in part on evidential-ity theory (Chafe, 1986), website credibility assess-ment surveys (Stanford et al., 2002) [Cite_Ref] , and Van House (2004)’s observations on blog credibility. The four factors—plus indicators for each of them—are:",補足資料,Paper,True,Introduce（引用目的）,True,P08-1105_2_0,2008,Credibility Improves Topical Blog Post Retrieval,Reference
411,10414," http://www.lycos.com/retriever.html"," ['1 Introduction']",Lycos Retriever [Cite_Footnote_1] is something new on the Web: a patent-pending information fusion engine.,1 http://www.lycos.com/retriever.html. Work on Retriever was done while author was employed at Lycos.,"Lycos Retriever [Cite_Footnote_1] is something new on the Web: a patent-pending information fusion engine. That is, unlike a search engine, rather than returning ranked documents links in response to a query, Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia article. These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching.",Method,Tool,True,Produce（引用目的）,False,N06-2045_0_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
412,10415," http://www.wikipedia.org"," ['1 Introduction']","That is, unlike a search engine, rather than returning ranked documents links in response to a query, Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia [Cite_Footnote_2] article.",2 http://www.wikipedia.org,"Lycos Retriever is something new on the Web: a patent-pending information fusion engine. That is, unlike a search engine, rather than returning ranked documents links in response to a query, Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia [Cite_Footnote_2] article. These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching.",補足資料,Website,True,Compare（引用目的）,True,N06-2045_1_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
413,10416," http://www.lycos.com/info/king-kong-1933.html"," ['3 Lycos Retriever pages']",Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4],"4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html","Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux’s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs.",Material,Knowledge,False,Produce（引用目的）,False,N06-2045_2_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
414,10417," http://www.lycos.com/info/zoloft.html"," ['3 Lycos Retriever pages']",Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4],"4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html","Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux’s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs.",Material,Knowledge,False,Produce（引用目的）,False,N06-2045_3_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
415,10418," http://www.lycos.com/info/public-key-cryptography.html"," ['3 Lycos Retriever pages']",Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4],"4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html","Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux’s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs.",Material,Knowledge,False,Produce（引用目的）,False,N06-2045_4_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
416,10419," http://www.lycos.com/info/lyme-disease.html"," ['3 Lycos Retriever pages']",Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4],"4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html","Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux’s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs.",Material,Knowledge,False,Produce（引用目的）,False,N06-2045_5_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
417,10420," http://www.lycos.com/info/reggaeton.html"," ['3 Lycos Retriever pages']",Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4],"4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html","Figure 1 shows a sample Retriever page for the topic “Mario Lemieux”. [Cite_Footnote_4] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario. The main body of the page is a set of para-graphs beginning with a biographical paragraph complete with Lemieux’s birth date, height, weight and position extracted from Nationmaster.com, followed by paragraphs outlining his career from other sources. The source for each extract is indi-cated in shortened form in the left margin of the page; mousing over the shortened URL reveals the full title and URL. Associated images are thumb-nailed alongside the extracted paragraphs.",Material,Knowledge,False,Produce（引用目的）,False,N06-2045_6_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
418,10421," http://www.dmoz.com"," ['4 Topic Selection']","After a topic was input to the system, the Retriever system assigned it a category using a naïve Bayes classifier built on a spidered DMOZ [Cite_Footnote_5] hierarchy.",5 http://www.dmoz.com,"After a topic was input to the system, the Retriever system assigned it a category using a naïve Bayes classifier built on a spidered DMOZ [Cite_Footnote_5] hierarchy. Various heuristics were implemented to make the returned set of categories uniform in length and depth, up-to-date, and readable.",Material,Knowledge,False,Use（引用目的）,False,N06-2045_7_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
419,10422," http://www.link.cs.cmu.edu/link/"," ['7 Passage Extraction']","For our purposes, however, we for-mulated a set of patterns for identifying Discourse Topics on the basis of the output of the CMU Link Parser [Cite_Footnote_6] the system uses.",6 http://www.link.cs.cmu.edu/link/,"When a passage was identified as being potentially interesting, it was then fully parsed to see if an expression denoting the topic was the Discourse Topic of the passage. Discourse Topic is an under-theorized notion in linguistic theory: not all linguists agree that the notion of Discourse Topic is required in discourse analysis at all (cf. Asher, 2004). For our purposes, however, we for-mulated a set of patterns for identifying Discourse Topics on the basis of the output of the CMU Link Parser [Cite_Footnote_6] the system uses.",Method,Tool,True,Use（引用目的）,False,N06-2045_8_0,2006,Lycos Retriever: An Information Fusion Engine,Footnote
420,10423,https://github.com/LDNOOBW,"['6 Analysis of Safety and Gender Bias', '6.3 Safety']","To mitigate this problem, we first measure our models’ toxicity using an openly available blocklist [Cite_Footnote_3] and an offen-sive language classifier presented in Dinan et al. (2019b).","3 https://github.com/LDNOOBWdataset. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 5370–5381, Florence, Italy. Association for Computational Linguistics.","The MMB models may demonstrate offensiveness beyond gender bias for several reasons: (1) its gen-erative nature makes it rather difficult to define a limited set of utterances; (2) the model’s training data contains real-world conversations from the Internet; and (3) the Image-Chat dataset has neg-ative styles to better capture the range of human styles. All of these factors could lead to an unsafe response given a multi-modal context. To mitigate this problem, we first measure our models’ toxicity using an openly available blocklist [Cite_Footnote_3] and an offen-sive language classifier presented in Dinan et al. (2019b). We define the term “toxicity"" to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model. We evaluate our model on the Image-Chat validation set, with a fixed style trait to control the generation, presenting results for different choices of fixed trait. We first evaluate our model in the first round of the Image-Chat validation set. The results in Table 9 indicate that positive styles reduce the level of toxicity by a large margin for both metrics (classifier and blocklist). The results also align well with our previous experiments on degendering, as toxicity is reduced across all styles after applying the degendering process. After degendering, we can considerably improve our model’s safety by en-forcing that it uses positive styles. We also evaluate our model in the second round of the conversation and collect the statistics based on the first round style, as shown in Table 23. This result suggests that even if the model is controlled with a positive style, it is less safe when responding to negative conversations.",Material,Knowledge,False,Use（引用目的）,True,2021.emnlp-main.398_0_0,2021,Multi-Modal Open-Domain Dialogue,Footnote
421,10424," https://github.com/facebookresearch/detectron"," ['2 Related Work', '2.3 Comparison to Existing Models']","2AMMC: a retrieval model in which multiple Transformers are attended over in order to make use of a combination of ResNeXt-IG-3.5B and Faster R-CNN image features (Girshick et al., 2018) [Cite_Ref] .","Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron.","2AMMC: a retrieval model in which multiple Transformers are attended over in order to make use of a combination of ResNeXt-IG-3.5B and Faster R-CNN image features (Girshick et al., 2018) [Cite_Ref] . We specifically use the 2AMMC model from Ju et al. (2019) because that model has the best test-set per-formance on Image-Chat in that work.",Method,Tool,True,Compare（引用目的）,False,2021.emnlp-main.398_1_0,2021,Multi-Modal Open-Domain Dialogue,Reference
422,10425,https://github.com/facebookresearch/detectron," [\'References\']","Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018) [Cite_Ref] ; specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) .","Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron.","Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018) [Cite_Ref] ; specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) . The Faster R-CNN features are 2048×100-dimensional representations, and we re-fer to these features as “Faster R-CNN"".",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.398_1_1,2021,Multi-Modal Open-Domain Dialogue,Reference
423,10426,https://github.com/facebookresearch/mmf,[\'References\'],"Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [Cite_Ref] .","Amanpreet Singh, Vedanuj Goswami, Vivek Natara-jan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020a. Mmf: A multimodal framework for vision and language research. https://github.com/facebookresearch/mmf.","Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [Cite_Ref] . The Faster R-CNN features are 2048×100-dimensional representations, and we re-fer to these features as “Faster R-CNN"".",Material,Knowledge,False,Use（引用目的）,True,2021.emnlp-main.398_2_0,2021,Multi-Modal Open-Domain Dialogue,Reference
424,10427,https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/,[\'References\'],"ResNeXt WSL We first experiment with im-age representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion pub-lic images (Mahajan et al., 2018), with subse-quent fine-tuning on the ImageNet1K dataset (Rus-sakovsky et al., 2015) [Cite_Footnote_4] .",4 https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/,"ResNeXt WSL We first experiment with im-age representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion pub-lic images (Mahajan et al., 2018), with subse-quent fine-tuning on the ImageNet1K dataset (Rus-sakovsky et al., 2015) [Cite_Footnote_4] . The output of this model is a 2048-dimensional vector, and we refer to these representations as “ResNeXt WSL"" features.",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.398_3_0,2021,Multi-Modal Open-Domain Dialogue,Footnote
425,10428,https://github.com/facebookresearch/vilbert-multi-task,[\'References\'],"Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [Cite_Footnote_5] .",5 https://github.com/facebookresearch/ vilbert-multi-task,"Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [Cite_Footnote_5] . The Faster R-CNN features are 2048×100-dimensional representations, and we re-fer to these features as “Faster R-CNN"".",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.398_4_0,2021,Multi-Modal Open-Domain Dialogue,Footnote
426,10429," https://github.com/stanfordnlp/treelstm"," ['1 Introduction']",Implementations of our mod-els and experiments are available at [Cite] https://github.com/stanfordnlp/treelstm.,,"In our evaluations, we demonstrate the empiri-cal strength of Tree-LSTMs as models for repre-senting sentences. We evaluate the Tree-LSTM architecture on two tasks: semantic relatedness prediction on sentence pairs and sentiment clas-sification of sentences drawn from movie reviews. Our experiments show that Tree-LSTMs outper-form existing systems and sequential LSTM base-lines on both tasks. Implementations of our mod-els and experiments are available at [Cite] https://github.com/stanfordnlp/treelstm.",Mixed,Mixed,True,Produce（引用目的）,True,P15-1150_0_0,2015,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,Body
427,10430," http://nlp.stanford.edu/projects/glove/"," ['5 Experiments', '5.3 Hyperparameters and Training Details']","We initialized our word representations using publicly available 300-dimensional Glove vec-tors [Cite_Footnote_5] (Pennington et al., 2014).","5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.","We initialized our word representations using publicly available 300-dimensional Glove vec-tors [Cite_Footnote_5] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned.",Method,Code,False,Use（引用目的）,True,P15-1150_1_0,2015,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,Footnote
428,10431," https://github.com/ZhangShiyue/ChrEn"," ['References']","Our best re-sults are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/En-Chr translations, respectively, and we hope that our dataset and systems will encourage fu-ture work by the community for Cherokee lan-guage revitalization. [Cite_Footnote_1]","1 Our data, code, and demo will be publicly available at https://github.com/ZhangShiyue/ChrEn.","Cherokee is a highly endangered Native Amer-ican language spoken by the Cherokee peo-ple. The Cherokee culture is deeply embed-ded in its language. However, there are ap-proximately only 2,000 fluent first language Cherokee speakers remaining in the world, and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some pop-ular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our paral-lel data in ways that facilitate both in-domain and out-of-domain evaluation. We also col-lect 5k Cherokee monolingual data to en-able semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation sys-tems. We compare SMT (phrase-based) ver-sus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best re-sults are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/En-Chr translations, respectively, and we hope that our dataset and systems will encourage fu-ture work by the community for Cherokee lan-guage revitalization. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.43_0_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
429,10432," https://dailp.northeastern.edu/"," ['2 Related Works']","Cherokee language documents (Bourns, 2019; Cushman, 2019). [Cite_Footnote_4]",4 https://dailp.northeastern.edu/,"Cherokee Language Revitalization. In 2008, the Cherokee Nation launched the 10-year lan-guage preservation plan (Nation, 2001), which aims to have 80% or more of the Cherokee peo-ple be fluent in this language in 50 years. Af-ter that, a lot of revitalization works were pro-posed. Cherokee Nation and the EBCI have es-tablished language immersion programs and k-12 language curricula. Several universities, including the University of Oklahoma, Stanford University, etc., have begun offering Cherokee as a second lan-guage. However, given Cherokee has been rated at the highest level of learning difficulty (Peake Ray-mond, 2008), it is hard to be mastered without fre-quent language exposure. As mentioned by Crys-tal (2014), an endangered language will progress if its speakers can make use of electronic technol-ogy. Currently, the language is included among existing Unicode-compatible fonts, is supported by Gmail, and has a Wikipedia page. To revital-ize Cherokee, a few Cherokee pedagogical books have been published (Holmes and Smith, 1976; Joyner, 2014), as well as several online learning platforms. Feeling (2018) provided detailed En-glish translations and linguistic analysis of a num-ber of Cherokee stories. A Digital Archive for American Indian Languages Preservation and Per-severance (DAILP) has been developed for tran-scribing, translating, and contextualizing histori-cal Cherokee language documents (Bourns, 2019; Cushman, 2019). [Cite_Footnote_4] However, the translation be-tween Cherokee and English still can only be done by human translators. Given that only 2,000 flu-ent first-language speakers are left, and the major-ity of them are elders, it is important and urgent to have a machine translation system that could as-sist them with translation. Therefore, we introduce a clean Cherokee-English parallel dataset to facili-tate machine translation development and propose multiple translation systems as starting points of fu-ture works. We hope our work could attract more attention from the NLP community in helping to save and revitalize this endangered language. An initial version of our data and its implications was introduced in (Frey, 2020). Note that we are not the first to propose a Cherokee-English parallel dataset. There is Chr-En parallel data available on OPUS (Tiedemann, 2012). 5 The main difference is that our parallel data contains 99% of their data and has 6K more examples from diverse domains. Low-Resource Machine Translation. Even though machine translation has been studied for several decades, the majority of the initial research effort was on high-resource translation pairs, e.g., French-English, that have large-scale parallel datasets available. However, most of the language pairs in the world lack large-scale parallel data. In the last five years, there is an increasing research interest in these low-resource translation settings. The DARPA’s LORELEI language packs contain the monolingual and parallel texts of three dozen languages that are considered as low-resource (Strassel and Tracey, 2016). Riza et al. (2016) pro-posed several low-resource Asian language pairs. Lakew et al. (2020) and Duh et al. (2020) pro-posed benchmarks for five and two low-resource African languages, respectively. Guzmán et al. (2019) introduced two low-resource translation evaluation benchmarks: Nepali–English and Sinhala–English. Besides, most low-resource languages rely on the existing parallel translations of the Bible (Christodouloupoulos and Steed-man, 2015). Because not many low-resource parallel datasets were publicly available, some low-resource machine translation research was done by sub-sampling high-resource language pairs (Johnson et al., 2017; Lample et al., 2018), but it may downplay the fact that low-resource translation pairs are usually distant languages. Our ChrEn dataset can not only be another open resource of low-resource MT research but also challenge MT methods with an extremely morphology rich language and a distant language pair. Two methods have been largely explored by existing works to improve low-resource MT. One is semi-supervised learning to use monolingual data (Gulcehre et al., 2015; Sennrich et al., 2016b). The other is cross-lingual transfer learning or multilingual joint learning (Kocmi and Bojar, 2018; Johnson et al., 2017). We explore both of them to improve Chr-En/En-Chr translations.",補足資料,Document,True,Introduce（引用目的）,True,2020.emnlp-main.43_1_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
430,10433," http://opus.nlpl.eu/"," ['2 Related Works']","There is Chr-En parallel data available on OPUS (Tiedemann, 2012). [Cite_Footnote_5]",5 http://opus.nlpl.eu/,"Cherokee Language Revitalization. In 2008, the Cherokee Nation launched the 10-year lan-guage preservation plan (Nation, 2001), which aims to have 80% or more of the Cherokee peo-ple be fluent in this language in 50 years. Af-ter that, a lot of revitalization works were pro-posed. Cherokee Nation and the EBCI have es-tablished language immersion programs and k-12 language curricula. Several universities, including the University of Oklahoma, Stanford University, etc., have begun offering Cherokee as a second lan-guage. However, given Cherokee has been rated at the highest level of learning difficulty (Peake Ray-mond, 2008), it is hard to be mastered without fre-quent language exposure. As mentioned by Crys-tal (2014), an endangered language will progress if its speakers can make use of electronic technol-ogy. Currently, the language is included among existing Unicode-compatible fonts, is supported by Gmail, and has a Wikipedia page. To revital-ize Cherokee, a few Cherokee pedagogical books have been published (Holmes and Smith, 1976; Joyner, 2014), as well as several online learning platforms. 3 Feeling (2018) provided detailed En-glish translations and linguistic analysis of a num-ber of Cherokee stories. A Digital Archive for American Indian Languages Preservation and Per-severance (DAILP) has been developed for tran-scribing, translating, and contextualizing histori-cal Cherokee language documents (Bourns, 2019; Cushman, 2019). 4 However, the translation be-tween Cherokee and English still can only be done by human translators. Given that only 2,000 flu-ent first-language speakers are left, and the major-ity of them are elders, it is important and urgent to have a machine translation system that could as-sist them with translation. Therefore, we introduce a clean Cherokee-English parallel dataset to facili-tate machine translation development and propose multiple translation systems as starting points of fu-ture works. We hope our work could attract more attention from the NLP community in helping to save and revitalize this endangered language. An initial version of our data and its implications was introduced in (Frey, 2020). Note that we are not the first to propose a Cherokee-English parallel dataset. There is Chr-En parallel data available on OPUS (Tiedemann, 2012). [Cite_Footnote_5] The main difference is that our parallel data contains 99% of their data and has 6K more examples from diverse domains. Low-Resource Machine Translation. Even though machine translation has been studied for several decades, the majority of the initial research effort was on high-resource translation pairs, e.g., French-English, that have large-scale parallel datasets available. However, most of the language pairs in the world lack large-scale parallel data. In the last five years, there is an increasing research interest in these low-resource translation settings. The DARPA’s LORELEI language packs contain the monolingual and parallel texts of three dozen languages that are considered as low-resource (Strassel and Tracey, 2016). Riza et al. (2016) pro-posed several low-resource Asian language pairs. Lakew et al. (2020) and Duh et al. (2020) pro-posed benchmarks for five and two low-resource African languages, respectively. Guzmán et al. (2019) introduced two low-resource translation evaluation benchmarks: Nepali–English and Sinhala–English. Besides, most low-resource languages rely on the existing parallel translations of the Bible (Christodouloupoulos and Steed-man, 2015). Because not many low-resource parallel datasets were publicly available, some low-resource machine translation research was done by sub-sampling high-resource language pairs (Johnson et al., 2017; Lample et al., 2018), but it may downplay the fact that low-resource translation pairs are usually distant languages. Our ChrEn dataset can not only be another open resource of low-resource MT research but also challenge MT methods with an extremely morphology rich language and a distant language pair. Two methods have been largely explored by existing works to improve low-resource MT. One is semi-supervised learning to use monolingual data (Gulcehre et al., 2015; Sennrich et al., 2016b). The other is cross-lingual transfer learning or multilingual joint learning (Kocmi and Bojar, 2018; Johnson et al., 2017). We explore both of them to improve Chr-En/En-Chr translations.",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.43_2_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
431,10434," https://github.com/tesseract-ocr/"," ['3 Data Description', '3.1 Parallel Data']",We apply the Optical Character Recog-nition (OCR) via Tesseract OCR engine [Cite_Footnote_6] to ex-tract the Cherokee and English text.,6 https://github.com/tesseract-ocr/,"Fifty-six percent of our parallel data is derived from the Cherokee New Testament. Other texts are novels, children’s books, newspaper articles, etc. These texts vary widely in dates of publica-tion, the oldest being dated to 1860. Addition-ally, our data encompasses both existing dialects of Cherokee: the Overhill dialect, mostly spoken in Oklahoma (OK), and the Middle dialect, mostly used in North Carolina (NC). These two dialects are mainly phonologically different and only have a few lexical differences (Uchihara, 2016). In this work, we do not explicitly distinguish them during translation. The left pie chart of Figure 2 shows the parallel data distributions over text types and dialects, and the complete information is in Ta-ble 14 of Appendix A.1. Many of these texts were translations of English materials, which means that the Cherokee structures may not be 100% natural in terms of what a speaker might spontaneously produce. But each text was translated by people who speak Cherokee as the first language, which means there is a high probability of grammatical-ity. These data were originally available in PDF version. We apply the Optical Character Recog-nition (OCR) via Tesseract OCR engine [Cite_Footnote_6] to ex-tract the Cherokee and English text. Then our co-author, a proficient second-language speaker of Cherokee, manually aligned the sentences and fixed the errors introduced by OCR. This process is time-consuming and took several months.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.43_3_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
432,10435," http://data.statmt.org/news-crawl/en/"," ['5 Results', '5.1 Experimental Details']",We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [Cite_Footnote_10] as our English monolingual data.,10 http://data.statmt.org/news-crawl/en/,"We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [Cite_Footnote_10] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 . We apply tokenizer and truecaser from Moses (Koehn et al., 2007). We also apply the BPE tokonization (Sennrich et al., 2016c), but instead of using it as default, we treat it as hyper-parameter. For systems with BERT, we apply the WordPiece tokenizer (Devlin et al., 2019). We compute detokenized and case-sensitive BLEU score (Papineni et al., 2002) using SacreBLEU (Post, 2018). 13",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.43_4_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
433,10436," http://data.statmt.org/news-crawl/en/"," ['B Experimental Details', 'B.1 Data and Preprocessing']","For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. [Cite_Footnote_18]",18 http://data.statmt.org/news-crawl/en/,"For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. [Cite_Footnote_18] We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 19 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 20 , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.43_7_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
434,10437," http://www.statmt.org/wmt18/index.html"," ['B Experimental Details', 'B.1 Data and Preprocessing']","We sample these exam-ples either only from News Commentary v13 of WMT2018 [Cite_Footnote_19] or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS , because half of in-domain Chr-En data is the Bible.",19 http://www.statmt.org/wmt18/index.html,"For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. 18 We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 [Cite_Footnote_19] or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.43_8_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
435,10438," http://opus.nlpl.eu/bible-uedin.php"," ['B Experimental Details', 'B.1 Data and Preprocessing']","We sample these exam-ples either only from News Commentary v13 of WMT2018 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS [Cite_Footnote_20] , because half of in-domain Chr-En data is the Bible.",20 http://opus.nlpl.eu/bible-uedin.php,"For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. 18 We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS [Cite_Footnote_20] , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.43_9_0,2020,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,Footnote
436,10439," https://github.com/jiesutd/SubwordEncoding-CWS"," ['1 Introduction']","Our code [Cite_Footnote_1] is based on NCRF++ (Yang and Zhang, 2018).",1 Our code is released at https://github.com/jiesutd/SubwordEncoding-CWS.,"In this paper, we fill this gap by proposing a subword-based neural word segmentor, by inte-grating two strands of works: the byte pair en-coding (BPE) algorithm (Gage, 1994) and the lat-tice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In partic-ular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code [Cite_Footnote_1] is based on NCRF++ (Yang and Zhang, 2018).",Method,Code,True,Produce（引用目的）,True,N19-1278_0_0,2019,Subword Encoding in Lattice LSTM for Chinese Word Segmentation,Footnote
437,10440," https://catalog.ldc.upenn.edu/LDC2011T13"," ['4 Experiments', '4.1 Experimental Settings']","We take the same character un-igram and bigram embeddings as Zhang et al. (2016), who pretrain embeddings using word2vec (Mikolov et al., 2013) on Chinese Gigaword [Cite_Footnote_3] .",3 https://catalog.ldc.upenn.edu/LDC2011T13.,"Embeddings. We take the same character un-igram and bigram embeddings as Zhang et al. (2016), who pretrain embeddings using word2vec (Mikolov et al., 2013) on Chinese Gigaword [Cite_Footnote_3] . The vocabulary of subword is constructed with 200000 merge operations and the subword embeddings are also trained using word2vec (Heinzerling and Strube, 2018). Trie (Fredkin, 1960) is used to ac-celerate lattice building. All the embeddings are fine-tuned during training.",Method,Code,False,Use（引用目的）,True,N19-1278_1_0,2019,Subword Encoding in Lattice LSTM for Chinese Word Segmentation,Footnote
438,10441," http://people.csail.mit.edu/~jrennie/20Newsgroups/"," ['4 Experimental Studies', '4.1 Experimental Setup']","And 20NG is a collection of approximately 20,000 20-category documents [Cite_Footnote_1] .",1 http://people.csail.mit.edu/~jrennie/20Newsgroups/,"Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [Cite_Footnote_1] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories.",Material,Dataset,True,Use（引用目的）,True,P09-1078_0_0,2009,A Framework of Feature Selection Methods for Text Categorization,Footnote
439,10442," http://www.cs.cornell.edu/People/pabo/movie-review-data/"," ['4 Experimental Studies', '4.1 Experimental Setup']","In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset [Cite_Footnote_2] (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).",2 http://www.cs.cornell.edu/People/pabo/movie-review-data/,"Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset [Cite_Footnote_2] (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories.",Material,Dataset,True,Use（引用目的）,True,P09-1078_1_0,2009,A Framework of Feature Selection Methods for Text Categorization,Footnote
440,10443," http://www.seas.upenn.edu/~mdredze/datasets/sentiment/"," ['4 Experimental Studies', '4.1 Experimental Setup']","In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD [Cite_Footnote_3] (Blitzer et al., 2007).",3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/,"Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD [Cite_Footnote_3] (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories.",Material,DataSource,True,Use（引用目的）,True,P09-1078_2_0,2009,A Framework of Feature Selection Methods for Text Categorization,Footnote
441,10444," http://www.csie.ntu.edu.tw/~cjlin/libsvm/"," ['4 Experimental Studies', '4.1 Experimental Setup']",Hence we apply SVM algorithm with the help of the LIBSVM [Cite_Footnote_4] tool.,4 http://www.csie.ntu.edu.tw/~cjlin/libsvm/,"Classification Many Algorithm: classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 2002). Hence we apply SVM algorithm with the help of the LIBSVM [Cite_Footnote_4] tool. Almost all parameters are set to their default values except the kernel function which is changed from a polynomial kernel function to a linear one because the linear one usually performs better for text classification tasks.",補足資料,Website,True,Introduce（引用目的）,True,P09-1078_3_0,2009,A Framework of Feature Selection Methods for Text Categorization,Footnote
442,10445," http://trec.nist.gov/data/qa.html"," ['1 Introduction']","We are now studying open-domain question an-swering systems [Cite_Footnote_1] , and we expect QA systems to benefit from anaphora resolution.",1 http://trec.nist.gov/data/qa.html,"We are now studying open-domain question an-swering systems [Cite_Footnote_1] , and we expect QA systems to benefit from anaphora resolution. Typical QA sys-tems try to answer a user’s question by finding rel-evant phrases from large corpora. When a correct answer phrase is far from the keywords given in the question, the systems will not succeed in find-ing the answer. If the system can correctly resolve anaphors, it will find keywords or answers repre-sented by anaphors, and the chances of finding the answer will increase. From this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles.",補足資料,Website,False,Use（引用目的）,False,W03-1024_0_0,2003,Japanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning,Footnote
443,10446," http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html"," ['1 Introduction']","Therefore, we obtained Seki’s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus [Cite_Footnote_2] 2.0.",2 http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html,"It takes a long time to construct high-quality an-notated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki’s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus [Cite_Footnote_2] 2.0. These data are divided into two groups: gen-eral and editorial. General contains 30 general news articles, and editorial contains 30 editorial articles. According to his experiments, editorial is harder than general. Perhaps this is caused by the differ-ence in rhetorical styles and the lengths of articles. The average number of sentences in an editorial ar-ticle is 28.7, while that in a general article is 13.9.",Material,DataSource,True,Extend（引用目的）,True,W03-1024_1_0,2003,Japanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning,Footnote
444,10447," http://chasen.aist-nara.ac.jp/"," ['1 Introduction']","In addition, we decided to use the output of ChaSen 2.2.9 [Cite_Footnote_3] and CaboCha 0.34 instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose.",3 http://chasen.aist-nara.ac.jp/,"In addition, we decided to use the output of ChaSen 2.2.9 [Cite_Footnote_3] and CaboCha 0.34 instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha’s depen-dency output is very similar to that of the Corpus.",Method,Tool,True,Use（引用目的）,True,W03-1024_2_0,2003,Japanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning,Footnote
445,10448," http://cl.aist-nara.ac.jp/~taku-ku/software/cabocha/"," ['1 Introduction']","In addition, we decided to use the output of ChaSen 2.2.9 and CaboCha 0.34 [Cite_Footnote_4] instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose.",4 http://cl.aist-nara.ac.jp/˜taku-ku/software/cabocha/,"In addition, we decided to use the output of ChaSen 2.2.9 and CaboCha 0.34 [Cite_Footnote_4] instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha’s depen-dency output is very similar to that of the Corpus.",Method,Tool,True,Use（引用目的）,True,W03-1024_3_0,2003,Japanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning,Footnote
446,10449," http://www2.crl.go.jp/jt/a132/members/mutiyama/software.html"," ['3 Results']",[Cite_Footnote_6] C ? .),6 http://www2.crl.go.jp/jt/a132/members/mutiyama/software. html,"When we use SVM, we have to choose a good linear kernel ( = 6 kernel for better performance &? [Cite_Footnote_6] C ? .) forHereSVM, webecauseused theit was best according to our preliminary experiments. We set maxDi at 3 because it gave the best results.",補足資料,Paper,False,Introduce（引用目的）,False,W03-1024_4_0,2003,Japanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning,Footnote
447,10450," https://github.com/linjieli222/HERO"," ['References']","We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities. [Cite_Footnote_1]",1 Code and new datasets publicly available at: https: //github.com/linjieli222/HERO.,"We present H ERO , a novel framework for large-scale video+language omni-representation learning. H ERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addi-tion to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. H ERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep under-standing of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that H ERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Re-trieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.161_0_0,2020,H ERO : Hierarchical Encoder for Video+Language Omni-representation Pre-training,Footnote
448,10451," https://pytorch.org/"," ['-', 'A.5 Implementation Details']","Our models are implemented based on Py-Torch (Paszke et al., 2017). [Cite_Footnote_15]",15 https://pytorch.org/,"Our models are implemented based on Py-Torch (Paszke et al., 2017). [Cite_Footnote_15] To speed up training, we use Nvidia Apex 16 for mixed precision train-ing. Gradient accumulation (Ott et al., 2018) is applied to reduce multi-GPU communication over-heads. All pre-training experiments are run on Nvidia V100 GPUs (32GB VRAM; NVLink con-nection). We use AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 3e−5 and weight decay of 0.01 to pre-train our model. The best pre-trained model is trained on 16 V100 GPUs for about 3 weeks. Finetuning experiments are implemented on the same hardware or Titan RTX GPUs (24GB VRAM) with AdamW optimizer but different learning rates.",Method,Code,False,Extend（引用目的）,False,2020.emnlp-main.161_1_0,2020,H ERO : Hierarchical Encoder for Video+Language Omni-representation Pre-training,Footnote
449,10452," http://twentyone.tpd.tno.nl/olive/"," ['8 Recent Research Projects']",A Multilingual Indexing Tool for Broadcast Material Based on Speech Recognition ( [Cite] http://twentyone.tpd.tno.nl/olive/) addressed methods to automate the disclosure of the infor-mation content of broadcast data thus allowing content-based indexation.,,"The European project LE-4 O LIVE : A Multilingual Indexing Tool for Broadcast Material Based on Speech Recognition ( [Cite] http://twentyone.tpd.tno.nl/olive/) addressed methods to automate the disclosure of the infor-mation content of broadcast data thus allowing content-based indexation. Speech recognition was used to produce a time-linked transcript of the audio channel of a broadcast, which was then used to produce a concept index for retrieval. Broadcast news transcription systems for French and German were developed. The French data come from a variety of television news shows and radio stations. The German data consist of TV news and documentaries from ARTE. O LIVE also developed tools for users to query the database, as well as cross-lingual access based on off-line machine translation of the archived documents, and online query translation.",補足資料,Website,True,Introduce（引用目的）,True,P01-1002_0_0,2001,Processing Broadcast Audio for Information Access,Body
450,10453," http://www.fb9-ti.uni-duisburg.de/alert"," ['8 Recent Research Projects']",The European project IST A LERT : Alert sys-tem for selective dissemination ( [Cite] http://www.fb9-ti.uni-duisburg.de/alert) aims to associate state-of-the-art speech recognition with audio and video segmentation and automatic topic index-ing to develop an automatic media monitoring demonstrator and evaluate it in the context of real world applications.,,"The European project IST A LERT : Alert sys-tem for selective dissemination ( [Cite] http://www.fb9-ti.uni-duisburg.de/alert) aims to associate state-of-the-art speech recognition with audio and video segmentation and automatic topic index-ing to develop an automatic media monitoring demonstrator and evaluate it in the context of real world applications. The targeted languages are French, German and Portuguese. Major media-monitoring companies in Europe are participating in this project.",補足資料,Website,False,Introduce（引用目的）,True,P01-1002_1_0,2001,Processing Broadcast Audio for Information Access,Body
451,10454," http://coretex.itc.it/"," ['8 Recent Research Projects']","C ORETEX ( [Cite] http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology.",,"Two other related FP5 IST projects are: C ORE - TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line. C ORETEX ( [Cite] http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology. In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions. The E CHO project (http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives. The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation.",補足資料,Website,True,Introduce（引用目的）,True,P01-1002_2_0,2001,Processing Broadcast Audio for Information Access,Body
452,10455," http://pc-erato2.iei.pi.cnr.it/echo"," ['8 Recent Research Projects']",The E CHO project ( [Cite] http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives.,,"Two other related FP5 IST projects are: C ORE - TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line. C ORETEX (http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology. In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions. The E CHO project ( [Cite] http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives. The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation.",Method,Code,True,Introduce（引用目的）,True,P01-1002_3_0,2001,Processing Broadcast Audio for Information Access,Body
453,10456," http://wacky.sslmit.unibo.it/"," ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']","Our source corpus is the concatenation of the ukWaC corpus [Cite_Footnote_2] , a mid-2009 dump of the English Wikipedia and the British National Corpus .",2 http://wacky.sslmit.unibo.it/,"Our source corpus is the concatenation of the ukWaC corpus [Cite_Footnote_2] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.",Material,DataSource,True,Extend（引用目的）,True,D12-1112_0_0,2012,First-order vs. higher-order modification in distributional semantics,Footnote
454,10457," http://en.wikipedia.org"," ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']","Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [Cite_Footnote_3] and the British National Corpus .",3 http://en.wikipedia.org,"Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [Cite_Footnote_3] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.",Material,DataSource,True,Extend（引用目的）,True,D12-1112_1_0,2012,First-order vs. higher-order modification in distributional semantics,Footnote
455,10458," http://www.natcorp.ox.ac.uk/"," ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']","Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia and the British National Corpus [Cite_Footnote_4] .",4 http://www.natcorp.ox.ac.uk/,"Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia and the British National Corpus [Cite_Footnote_4] . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.",Material,DataSource,True,Extend（引用目的）,True,D12-1112_2_0,2012,First-order vs. higher-order modification in distributional semantics,Footnote
456,10459," http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip"," ['p = Bv (5)', '3.3 Datasets']","We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. [Cite_Footnote_6]",6 Available at http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip. See Bruni et al. (to appear) for an analysis of the color term dataset from a multi-modal perspective.,"We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. [Cite_Footnote_6] Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above. From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low), since the remaining (grey, orange, pink, and purple) are not in the 700 most frequent set of ad-jectives in the corpora used. From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety). The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole). They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN. In this paper, only the first label (most frequent interpretation, accord-ing to the judges) has been used. The κ coefficient of the annotation on the three categories (first interpre-tation only) was 0.87 (conf. int. 0.82-0.92, according to Fleiss et al. (1969)), observed agreement 0.96. 7 There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. 8 The dataset as used here consists of 239 intersective and 130 subsective ANs.",Material,Dataset,True,Produce（引用目的）,True,D12-1112_3_0,2012,First-order vs. higher-order modification in distributional semantics,Footnote
457,10460," http://www.collocations.de/temp/kappa_example.zip"," ['p = Bv (5)', '3.3 Datasets']","0.82-0.92, according to Fleiss et al. (1969)), observed agreement 0.96. [Cite_Footnote_7]","7 Code for the computation of inter-annotator agreement by Stefan Evert, available at http://www.collocations.de/temp/kappa_example.zip.","We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. 6 Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above. From the 11 colors in the ba-sic set proposed by Berlin and Kay (1969), we cover 7 (black, blue, brown, green, red, white, and yel-low), since the remaining (grey, orange, pink, and purple) are not in the 700 most frequent set of ad-jectives in the corpora used. From an original set of 412 ANs, 43 were manually removed because of suspected parsing errors (e.g. white photograph, for black and white photograph) or because the head noun was semantically transparent (white variety). The remaining 369 ANs were tagged independently by the second and fourth authors of this paper, both native English speaker linguists, as intersective (e.g. white towel), subsective (e.g. white wine), or id-iomatic, i.e. compositionally non-transparent (e.g. black hole). They were allowed the assignment of at most two labels in case of polysemy, for instance for black staff for the person vs. physical object senses of the noun or yellow skin for the race vs. literally painted interpretations of the AN. In this paper, only the first label (most frequent interpretation, accord-ing to the judges) has been used. The κ coefficient of the annotation on the three categories (first interpre-tation only) was 0.87 (conf. int. 0.82-0.92, according to Fleiss et al. (1969)), observed agreement 0.96. [Cite_Footnote_7] There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. The dataset as used here consists of 239 intersective and 130 subsective ANs.",Method,Code,True,Use（引用目的）,True,D12-1112_4_0,2012,First-order vs. higher-order modification in distributional semantics,Footnote
458,10461," http://lemurproject.org/clueweb09/FACC1/"," ['2 Related Work']","All of the above work resorted to using the Free-base annotation of ClueWeb (Gabrilovich et al., 2013) [Cite_Ref] to gain extra advantage of paraphrasing QA pairs or dealing with data sparsity problem.","Evgeniy Gabrilovich, Michael Ringgaard, , and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora. http://lemurproject.org/clueweb09/FACC1/.","All of the above work resorted to using the Free-base annotation of ClueWeb (Gabrilovich et al., 2013) [Cite_Ref] to gain extra advantage of paraphrasing QA pairs or dealing with data sparsity problem. How-ever, ClueWeb is proprietary data and costs hun-dreds of dollars to purchase. Moreover, even though the implementation systems from (Berant et al., 2013; Yao and Van Durme, 2014; Reddy et al., 2014) are open-source, they all take considerable disk space (in tens of gigabytes) and training time (in days). In this paper we present a system that can be easily implemented in 300 lines of Python code with no compromise in accuracy and speed.",Material,Dataset,True,Use（引用目的）,True,N15-3014_0_0,2015,Lean Question Answering over Freebase from Scratch,Reference
459,10462," https://x-factr.github.io"," ['References']",Bench-mark data and code have be released at [Cite] https: //x-factr.github.io.,,"Language models (LMs) have proven surpris-ingly successful at capturing factual knowl-edge by completing cloze-style fill-in-the-blank questions such as “Punta Cana is lo-cated in _.” However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on En-glish. To assess factual knowledge retrieval in LMs in different languages, we create a mul-tilingual benchmark of cloze-style probes for 23 typologically diverse languages. To prop-erly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algo-rithms to generate multi-token predictions. Ex-tensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We fur-ther propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effective-ness on several benchmark languages. Bench-mark data and code have be released at [Cite] https: //x-factr.github.io.",Mixed,Mixed,True,Compare（引用目的）,False,2020.emnlp-main.479_0_0,2020,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,Body
460,10463," https://www.wikidata.org/"," ['2 Retrieving Facts from LMs']","In this paper we follow the protocol of Petroni et al. (2019)’s English-language LAMA bench-mark, which targets factual knowledge expressed in the form of subject-relation-object triples from Wikidata [Cite_Footnote_1] curated in the T-REx dataset (ElSahar et al., 2018).",1 https://www.wikidata.org/,"In this paper we follow the protocol of Petroni et al. (2019)’s English-language LAMA bench-mark, which targets factual knowledge expressed in the form of subject-relation-object triples from Wikidata [Cite_Footnote_1] curated in the T-REx dataset (ElSahar et al., 2018). The cloze-style prompts used therein are manually created and consist of a sequence of tokens, where [X] and [Y] are placeholders for sub-jects and objects (e.g. “[X] is a [Y] by profession.”). To assess the existence of a certain fact, [X] is re-placed with the actual subject (e.g. “Obama is a hmaski by profession.”) and the model predicts the object in the blank ŷ i = argmax y i p(y i |s i:i ), where s i:i is the sentence with the i-th token masked out. Finally, the predicted fact is compared to the ground truth. In the next section, we extend this setting to more languages and predict multiple tokens instead of a single one.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.479_1_0,2020,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,Footnote
461,10464," https://github.com/antonisa/unimorph_inflect"," ['3 Multilingual Multi-token Factual Retrieval Benchmark', '3.3 Prompts']","Once all the morphological features have been specified as detailed above, we use the unimorph_inflect package (Anastasopoulos and Neubig, 2019) to generate the appropriately inflected surface form of the bracketed words. [Cite_Footnote_3]",3 https://github.com/antonisa/unimorph_inflect,"Once all the morphological features have been specified as detailed above, we use the unimorph_inflect package (Anastasopoulos and Neubig, 2019) to generate the appropriately inflected surface form of the bracketed words. [Cite_Footnote_3] We note that the target entity ([Y]) might also need to be inflected, as in the above Russian example, in which case we require the model’s predictions to match the inflected target forms.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.479_2_0,2020,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,Footnote
462,10465," https://x-factr.github.io"," ['5 X-FACTR Benchmark Performance', '5.1 Experimental Results']","We run both the independent and confidence-based decoding methods with 3 M-LMs, and when avail-able 8 monolingual LMs, across 23 languages, [Cite_Footnote_6] with results shown in Fig.",6 Check https://x-factr.github.io for latest results.,"We run both the independent and confidence-based decoding methods with 3 M-LMs, and when avail-able 8 monolingual LMs, across 23 languages, [Cite_Footnote_6] with results shown in Fig. 3. Overall, even in the most favorable settings, the performance of state-of-that-art M-LMs at retrieving factual knowl-edge in the X-FACTR benchmark is relatively low, achieving less than 15% on high-resource lan-guages (e.g., English and Spanish) and less than 5% for some low-resource languages (e.g., Marathi and Yoruba). This may initially come as a sur-prise, given the favorable performance reported in previous papers (Petroni et al., 2019; Jiang et al., 2020), which achieved accuracies over 30% on En-glish. We justify this discrepancy in our following analysis. We note that, although we provide base-line results in almost all languages, we perform our extensive analysis on a representative subset, consisting of 13 languages.",補足資料,Website,False,Introduce（引用目的）,True,2020.emnlp-main.479_3_0,2020,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,Footnote
463,10466," https://huggingface.co/transformers/pretrained_models.html"," ['D Details of Pre-trained LMs']",We provide the shortcut name of each LM in the Hugging-Face’s Transformer library ( [Cite] https://huggingface.,,"LMs examined in this paper share similar archi-tecture and pre-training setting as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), but are trained on different corpora. We provide the shortcut name of each LM in the Hugging-Face’s Transformer library ( [Cite] https://huggingface.co/transformers/pretrained_models.html) and their training corpora in Tab. 8, from which you can find more information.",補足資料,Website,False,Produce（引用目的）,False,2020.emnlp-main.479_4_0,2020,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,Body
464,10467," https://github.com/dadangewp/ACL19-SRW"," ['1 Introduction']",All code and resources are available at [Cite] https://github.com/dadangewp/ACL19-SRW.,,"In this study, we conduct an extensive exper-iment to explore cross-domain and cross-lingual abusive language classification in social media data, by proposing a hybrid approach with deep learning and a multilingual lexicon. We exploit several available Twitter datasets in different do-mains and languages. We present three main con-tributions in this work. First, we characterize the available datasets as capturing various phe-nomena related to abusive language, and inves-tigate this characterization in cross-domain clas-sification. Second, we explored the use of a domain-independent, multilingual lexicon of abu-sive words called HurtLex (Bassignana et al., 2018) in both cross-domain and cross-lingual set-tings. Last, we take advantage of the availability of multilingual word embeddings to build a joint-learning approach in the cross-lingual setting. All code and resources are available at [Cite] https://github.com/dadangewp/ACL19-SRW.",Mixed,Mixed,True,Produce（引用目的）,True,P19-2051_0_0,2019,Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon,Body
465,10468," http://hatespeech.di.unito.it/resources.html"," ['4 Cross-domain Classification']","This lexi-con is semi-automatically extended and translated into 53 languages by using BabelNet (Navigli and Ponzetto, 2012), and the lexical items are divided into 17 categories such as homophobic slurs, eth-nic slurs, genitalia, cognitive and physical disabil-ities, animals and more [Cite_Footnote_1] .",1 http://hatespeech.di.unito.it/resources.html,"In this experiment, we investigate the performance of machine learning classifiers which are trained on a particular dataset and tested on different datasets ones. We focus on investigating the in-fluence of captured phenomena coverage between datasets. We hypothesize that a classifier which is trained on a broader coverage dataset and tested on narrower coverage dataset will give better perfor-mance than the opposite. Furthermore, we analyse the impact of using the HurtLex lexicon (Bassig-nana et al., 2018) to transfer knowledge between domains. HurtLex is a multilingual lexicon of hate words, originally built from 1,082 Italian hate words compiled in a manual fashion by the linguist Tullio De Mauro (De Mauro, 2016). This lexi-con is semi-automatically extended and translated into 53 languages by using BabelNet (Navigli and Ponzetto, 2012), and the lexical items are divided into 17 categories such as homophobic slurs, eth-nic slurs, genitalia, cognitive and physical disabil-ities, animals and more [Cite_Footnote_1] .",Material,Dataset,True,Introduce（引用目的）,True,P19-2051_1_0,2019,Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon,Footnote
466,10469," http://translate.google.com/"," ['5 Cross-lingual Classification']",We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate [Cite_Footnote_5] ) to translate training data from source to target language.,5 http://translate.google.com/,"monolingual word embedding. We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate [Cite_Footnote_5] ) to translate training data from source to target language. In this model, we use pre-trained word embedding from FastText .",Method,Tool,True,Use（引用目的）,True,P19-2051_2_0,2019,Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon,Footnote
467,10470," https://fasttext.cc/"," ['5 Cross-lingual Classification']","In this model, we use pre-trained word embedding from FastText [Cite_Footnote_6] .",6 https://fasttext.cc/,"monolingual word embedding. We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate ) to translate training data from source to target language. In this model, we use pre-trained word embedding from FastText [Cite_Footnote_6] .",Method,Tool,True,Use（引用目的）,True,P19-2051_3_0,2019,Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon,Footnote
468,10471," https://github.com/facebookresearch/MUSE"," ['5 Cross-lingual Classification']",We take advantage of the availability of mul-tilingual word embeddings [Cite_Footnote_7] to build a joint-learning model.,7 https://github.com/facebookresearch/ MUSE,"(b). JL + ME. We also propose a joint-learning model with multilingual word embedding. We take advantage of the availability of mul-tilingual word embeddings [Cite_Footnote_7] to build a joint-learning model. Figure 1 summarize how the data is transformed and learned in this model. We create bilingual training data automatically by using Google Translate to translate the data in both directions (training from source to target language and testing from target to source language), then using it as training data for the two LSTM-based ar-chitectures (similar architecture of the model in cross-domain experiment). We concate-nate these two architectures before the output layer, which produces the final prediction. In the, we expect to reduce some of the noise from the translation while keeping the origi-nal structure of the training set.",Method,Code,False,Use（引用目的）,True,P19-2051_4_0,2019,Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon,Footnote
469,10472," https://www.urbandictionary.com/define.php?term=Hoe"," ['6 Discussion']",[Cite_Footnote_8] .,8 https://www.urbandictionary.com/define.php?term=Hoe,"Translating swearing is indeed challenging. In the first example, Google Translate is unable to pro-vide an Italian translation for the English word “skank” (a proper translation could be “sciac-quetta” or “sciattona”, which means “slut”). We found 134 occurrences of the word “skank” in EN-AMI Evalita and 185 in the EN-HatEval dataset. The second example shows, instead, a problem related to context and disambiguation issues. In-deed, the word “hoe” here is used informally in its derogatory sense, meaning “A woman who engages in sexual intercourse for money” (syn-onyms: slut, whore, floozy) [Cite_Footnote_8] . But, disregarding the context, it is translated in Spanish by relying on a different conventional meaning (hoe as agri-cultural and horticultural hand tool). The term “hoe” is also very frequent in the EN-AMI Evalita (292 occurrences) and EN-HatEval dataset (348 occurrences).",補足資料,Document,True,Introduce（引用目的）,True,P19-2051_5_0,2019,Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon,Footnote
470,10473," https://github.com/airsplay/vokenization"," ['References']","Trained with these contextually gener-ated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. [Cite_Footnote_1]",1 Code and pre-trained models publicly available at: https://github.com/airsplay/vokenization.,"Humans learn language by listening, speak-ing, writing, reading, and also, via interaction with the multimodal real world. Existing lan-guage pre-training frameworks show the ef-fectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distribu-tions between the visually-grounded language datasets and pure-language corpora. There-fore, we develop a technique named “vok-enization” that extrapolates multimodal align-ments to language-only data by contextually mapping language tokens to their related im-ages (which we call “vokens”). The “vo-kenizer” is trained on relatively small im-age captioning datasets and we then apply it to generate vokens for large language cor-pora. Trained with these contextually gener-ated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.162_0_0,2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",Footnote
471,10474," http://cocodataset.org/"," ['A Appendices', 'A.1 Full Implementation Details']",We train our contextual token-image matching model (in Sec. 3.1) on MS COCO image cap-tioning dataset [Cite_Footnote_8] for 20 epochs.,8 http://cocodataset.org/,We train our contextual token-image matching model (in Sec. 3.1) on MS COCO image cap-tioning dataset [Cite_Footnote_8] for 20 epochs. The concatena-tion of the last 4 layers of BERT outputs (fol-lowing Devlin et al. (2019)) and mean pooling of,Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.162_1_0,2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",Footnote
472,10475," https://github.com/withattardi/wikiextractor"," ['A Appendices', 'A.1 Full Implementation Details']",We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia [Cite_Footnote_9] for 200K steps from scratch.,9 Downloaded https://github.com/withattardi/wikiextractor,"When pre-training the model on pure language corpus, we unify the training process to avoid pos-sible side effects from different training protocols. We follow previous work to conduct two simplifi-cations: 1. Removing the next-sentence-prediction task (Liu et al., 2019) 2. Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia [Cite_Footnote_9] for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison. It results in around 10% more training steps in pure MLM training. All models take batch sizes of 256 and a learning rate of 2e-4.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.162_2_0,2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",Footnote
473,10476," https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/"," ['A Appendices', 'A.1 Full Implementation Details']",We also take a reduced 6-layer model and train it on Wiki103 [Cite_Footnote_10] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset.,10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/,"When pre-training the model on pure language corpus, we unify the training process to avoid pos-sible side effects from different training protocols. We follow previous work to conduct two simplifi-cations: 1. Removing the next-sentence-prediction task (Liu et al., 2019) 2. Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [Cite_Footnote_10] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison. It results in around 10% more training steps in pure MLM training. All models take batch sizes of 256 and a learning rate of 2e-4.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.162_3_0,2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",Footnote
474,10477," https://github.com/huggingface/transformers"," ['A Appendices', 'A.1 Full Implementation Details']","The implementations of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are borrowed from PyTorch Trans-formers (Wolf et al., 2019) [Cite_Footnote_11] .",11 https://github.com/huggingface/ transformers,"The whole framework is built on Py-Torch (Paszke et al., 2019). The implementations of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are borrowed from PyTorch Trans-formers (Wolf et al., 2019) [Cite_Footnote_11] . All evaluation code is from the PyTorch Transformers as well.",Method,Code,False,Extend（引用目的）,False,2020.emnlp-main.162_4_0,2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",Footnote
475,10478," https://github.com/marcotcr/checklist"," ['1 Introduction']","In this work, we propose CheckList, a new eval-uation methodology and accompanying tool [Cite_Footnote_1] for comprehensive behavioral testing of NLP models.",1 https://github.com/marcotcr/checklist,"In this work, we propose CheckList, a new eval-uation methodology and accompanying tool [Cite_Footnote_1] for comprehensive behavioral testing of NLP models. CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks. To break down potential ca-pability failures into specific behaviors, CheckList introduces different test types, such as prediction invariance in the presence of certain perturbations, or performance on a set of “sanity checks.” Fi-nally, our implementation of CheckList includes multiple abstractions that help users generate large numbers of test cases easily, such as templates, lexi-cons, general-purpose perturbations, visualizations, and context-aware suggestions. (examples of each type in A, B and C).",Method,Tool,True,Produce（引用目的）,True,2020.acl-main.442_0_0,2020,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,Footnote
476,10479," https://github.com/marcotcr/"," ['2 CheckList', '2.3 Generating Test Cases at Scale']",Open source We release an implementation of CheckList at [Cite] https://github.com/marcotcr/checklist.,,"Open source We release an implementation of CheckList at [Cite] https://github.com/marcotcr/checklist. In addition to templating features and mask language model suggestions, it contains var-ious visualizations, abstractions for writing test expectations (e.g. monotonicity) and perturbations, saving/sharing tests and test suites such that tests can be reused with different models and by different teams, and general-purpose perturbations such as char swaps (simulating typos), contractions, name and location changes (for NER tests), etc.",Method,Code,True,Produce（引用目的）,True,2020.acl-main.442_3_0,2020,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,Body
477,10480," https://github.com/marcotcr/checklist"," ['6 Conclusion']","CheckList is open source, and available at [Cite] https://github.com/marcotcr/checklist.",,"Our user studies indicate that CheckList is easy to learn and use, and helpful both for expert users who have tested their models at length as well as for practitioners with little experience in a task. The tests presented in this paper are part of Check-List’s open source release, and can easily be in-corporated into existing benchmarks. More impor-tantly, the abstractions and tools in CheckList can be used to collectively create more exhaustive test suites for a variety of tasks. Since many tests can be applied across tasks as is (e.g. typos) or with minor variations (e.g. changing names), we ex-pect that collaborative test creation will result in evaluation of NLP models that is much more ro-bust and detailed, beyond just accuracy on held-out data. CheckList is open source, and available at [Cite] https://github.com/marcotcr/checklist.",Method,Code,True,Produce（引用目的）,True,2020.acl-main.442_4_0,2020,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,Body
478,10481," https://github.com/snakeztc/NeuralDialog-LAED"," ['References']",Our methods have been validated on real-world dialog datasets to discover semantic representa-tions and enhance encoder-decoder mod-els with interpretable generation. [Cite_Footnote_1],1 Data and code are available at https://github.com/snakeztc/NeuralDialog-LAED.,"The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex do-mains. Yet it is limited because it can-not output interpretable actions as in tra-ditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sen-tence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable re-sponse generation. Building upon vari-ational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover inter-pretable semantics via either auto encod-ing or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representa-tions and enhance encoder-decoder mod-els with interpretable generation. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P18-1101_0_0,2018,Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation,Footnote
479,10482," http://joshua-decoder.com/language-packs/paraphrase/"," ['2 Language pack description']",Several different size language packs are available for download. [Cite_Footnote_1],1 http://joshua-decoder.com/language-packs/paraphrase/,"Several different size language packs are available for download. [Cite_Footnote_1] The components of the language packs are described below. Grammar Our approach to sentential paraphras-ing is analogous to machine translation. As a trans-lation grammar, we use PPDB 2.0, which contains 170-million lexical, phrasal, and syntactic para-phrases (Pavlick et al., 2015). Each language pack contains a PPDB grammar that has been packed into a binary form for faster computation (Ganitkevitch et al., 2012), and users can select which size gram-mar to use. The rules present in each grammar are determined by the PPDB 2.0 score, which indicates the paraphrase quality (as given by a supervised re-gression model) and correlates strongly with human judgments of paraphrase appropriateness (Pavlick et al., 2015). Grammars of different sizes are cre-ated by changing the paraphrase score thresholds; larger grammars therefore contain a wider diversity of paraphrases, but with lower confidences.",Mixed,Mixed,True,Produce（引用目的）,True,N16-3013_0_0,2016,Sentential Paraphrasing as Black-Box Machine Translation,Footnote
480,10483," http://joshua-decoder.com/language-packs/paraphrase/"," ['5 Interactive tool']",[Cite] http://joshua-decoder.com/language-packs/paraphrase/,,"This tool can be used to demonstrate and test a model or to hand-tune the model in order to de-termine the parameters for a configuration file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the config-uration file, are available at the language pack home page: [Cite] http://joshua-decoder.com/language-packs/paraphrase/",補足資料,Document,False,Introduce（引用目的）,False,N16-3013_1_0,2016,Sentential Paraphrasing as Black-Box Machine Translation,Body
481,10484," http://www.askoxford.com"," ['4 Evaluating the GMM Approach', '4.1 Data']","Since we are interested in improving the performance of NLP ap-plications such as MT, we take a pragmatic approach and classify usages as ’figurative’ if they are not lex-icalized, i.e., if the specific sense is not listed in a dictionary. [Cite_Footnote_2]",2 We used http://www.askoxford.com.,"To determine how well our model deals with dif-ferent types of figurative usage, we distinguish four phenomena: Phrase-level figurative means that the whole phrase is used figuratively. We further divide this class into expressions which are potentially am-biguous between literal and figurative usage (nsa), e.g., spill the beans, and those that are unambigu-ously figurative irrespective of the context (nsu), e.g., trip the light fantastic. The latter can, theoreti-cally, be detected by dictionary look-up, the former cannot. The label token-level figurative (nw) is used when part of the phrase is used figuratively (e.g., sparrow in (2)). Often it is difficult to determine whether a word is still used in a ’literal’ sense or whether it is already used figuratively. Since we are interested in improving the performance of NLP ap-plications such as MT, we take a pragmatic approach and classify usages as ’figurative’ if they are not lex-icalized, i.e., if the specific sense is not listed in a dictionary. [Cite_Footnote_2] For example, we would classify summit in the ’meeting’ sense as ’literal’ (l). In our data set, 7.3% of the instances were annotated as ’nsa’, 1.9% as ’nsu’, 9.2% as ’nw’ and 81.5% as ’l’. A randomly selected sample (100 instances) was annotated inde-pendently by a second annotator. The kappa score (Cohen, 1960) is 0.84, which suggest that the anno-tations are reliable.",Material,Knowledge,True,Use（引用目的）,True,N10-1039_0_0,2010,Using Gaussian Mixture Models to Detect Figurative Language in Context,Footnote
482,10485," http://www.cnts.ua.ac.be/conll2000/chunking/"," ['4 Experiments']","To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task on the CoNLL-2000 dataset [Cite_Footnote_4] .",4 http://www.cnts.ua.ac.be/conll2000/chunking/,"To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task on the CoNLL-2000 dataset [Cite_Footnote_4] . We split the origi-nal training set into a dev set (top 1,000 sent.) and used the rest as train set (7,936 sent.); the test set was kept intact (2,012 sent.). For an input sentence x, each CRF node x i carries an observable word and its part-of-speech tag, and has to be assigned a chunk tag c i out of 3 labels: Beginning, Inside, or Outside (of a noun phrase). Chunk labels are not nested. As in Sha and Pereira (2003), we use second order Markov dependencies (bigram chunk tags), such that for sentence position i, the state is y i = c i−1 c i , increasing the label set size from 3 to 9. Out of the full list of Sha and Pereira (2003)’s features we implemented all except two feature templates, y i = y and c(y i ) = c, to simplify im-plementation. Impossible bigrams (OI) and label transitions of the pattern ?O → I? were prohib-ited by setting the respective potentials to −∞. As the active feature count in the train set was just un-der 2M, we hashed all features and weights into a sparse array of 2M entries. Despite the reduced train size and feature set, and hashing, our full in-formation baseline trained with log-likelihood at-tained the test F1-score of 0.935, which is compa-rable to the original result of 0.9438.",Material,Dataset,True,Use（引用目的）,True,P16-1152_0_0,2016,Learning Structured Predictors from Bandit Feedback for Interactive NLP,Footnote
483,10486," http://www.wikipedia.org"," ['5 Experiments', '5.1 Data']",We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia [Cite_Footnote_1] and labeled a to-tal of 4701 relation instances.,1 http://www.wikipedia.org,"We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia [Cite_Footnote_1] and labeled a to-tal of 4701 relation instances. In addition to a large set of person-to-person relations, we also included links between people and organizations, as well as biographical facts such as birthday and jobTitle . In all, there are 53 labels in the training data (Table 1).",Material,DataSource,True,Extend（引用目的）,True,N06-1038_0_0,2006,Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text,Footnote
484,10487," http://mallet.cs.umass.edu"," ['5 Experiments', '5.1 Data']","We use the MALLET CRF implementation (Mc-Callum, 2002) [Cite_Ref] with the default regularization pa-rameters.",Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.,"We use the MALLET CRF implementation (Mc-Callum, 2002) [Cite_Ref] with the default regularization pa-rameters.",Method,Code,False,Use（引用目的）,True,N06-1038_1_0,2006,Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text,Reference
485,10488," https://github.com/Anna146/CHARM"," ['1 Introduction']",The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user’s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [Cite_Footnote_1],1 https://github.com/Anna146/CHARM,The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user’s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [Cite_Footnote_1],Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.434_0_0,2020,CHARM: Inferring Personal Attributes from Conversations,Footnote
486,10489," https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb"," ['1 Introduction']",The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user’s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [Cite_Footnote_2],2 https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb,The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user’s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [Cite_Footnote_2],Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.434_1_0,2020,CHARM: Inferring Personal Attributes from Conversations,Footnote
487,10490," https://files.pushshift.io/reddit/"," ['4 Dataset', '4.1 Users’ utterances']",We consider publicly-available Reddit submissions and comments [Cite_Footnote_4] from 2006 to 2018 as users’ ut-terances.,4 https://files.pushshift.io/reddit/,"We consider publicly-available Reddit submissions and comments [Cite_Footnote_4] from 2006 to 2018 as users’ ut-terances. Given a Reddit user having a set of ut-terances U = u 0 ..u N , we aim to label the user with a set of profession and hobby values, based on explicit personal assertions (e.g., “I work as a doctor”) found in the user’s posts. To label the candidate users with attribute values we utilized the Snorkel framework (Ratner et al., 2017). We provide details on our data labeling using Snorkel in Appendix A.1.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.434_2_0,2020,CHARM: Inferring Personal Attributes from Conversations,Footnote
488,10491," https://github.com/Anna146/CHARM"," ['4 Dataset', '4.2 Document collection']",To provide more diversity and comprehensiveness we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms. [Cite_Footnote_5],5 Available at https://github.com/Anna146/ CHARM,"The scope of possible attribute values may be open-ended in nature, and thus, calls for an automatic method for collecting Web documents. In this work, we consider three different Web document collec-tions; summary statistics on the number of docu-ments per attribute value are provided in Table 1. Each document may be associated with multiple attribute values. To provide more diversity and comprehensiveness we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms. [Cite_Footnote_5]",Material,Knowledge,False,Produce（引用目的）,True,2020.emnlp-main.434_3_0,2020,CHARM: Inferring Personal Attributes from Conversations,Footnote
489,10492," https://github.com/Anna146/CHARM"," ['A Data']",All datasets used in the experiments are available at [Cite] https://github.com/Anna146/CHARM.,,"All datasets used in the experiments are available at [Cite] https://github.com/Anna146/CHARM. We pro-vide IDs and texts of the posts used as training and test data for CHARM. All users are anonymized by replacing usernames with IDs. Additionally, we provide the posts containing explicit personal assertions, which have been used for ground truth labeling with the Snorkel framework.",Material,Dataset,True,Produce（引用目的）,True,2020.emnlp-main.434_4_0,2020,CHARM: Inferring Personal Attributes from Conversations,Body
490,10493," http://www-nlp.stanford.edu/software/sempre/"," ['4 Experiments', '4.1 Data & evaluation metric']",The average F 1 score is reported as the main evaluation metric [Cite_Footnote_6] .,6 We used the official evaluation script from http://www-nlp.stanford.edu/software/sempre/.,"We use the W EB Q UESTIONS dataset (Berant et al., 2013), which consists of 5,810 ques-tion/answer pairs. These questions were collected using Google Suggest API and the answers were obtained from Freebase with the help of Amazon MTurk. The questions are split into training and testing sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively. This dataset has several unique properties that make it appealing and was used in several recent papers on semantic parsing and question answering. For instance, although the questions are not directly sampled from search query logs, the selection pro-cess was still biased to commonly asked questions on a search engine. The distribution of this ques-tion set is thus closer to the “real” information need of search users than that of a small number of human editors. The system performance is ba-sically measured by the ratio of questions that are answered correctly. Because there can be more than one answer to a question, precision, recall and F 1 are computed based on the system output for each individual question. The average F 1 score is reported as the main evaluation metric [Cite_Footnote_6] .",Method,Code,True,Use（引用目的）,True,P15-1128_0_0,2015,Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base,Footnote
491,10494," http://alt.qcri.org/tools/"," ['3 Our Discourse-Based Measures', '3.1 Generating Discourse Trees']",These high scores allowed us to de-velop successful discourse similarity metrics. [Cite_Footnote_2],2 The discourse parser is freely available from http://alt.qcri.org/tools/,"The discourse parser uses a dynamic Condi-tional Random Field (Sutton et al., 2007) as a pars-ing model in order to infer the probability of all possible discourse tree constituents. The inferred (posterior) probabilities are then used in a proba-bilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F 1 -score of 79.8%, which is very close to the human agree-ment of 83%. These high scores allowed us to de-velop successful discourse similarity metrics. [Cite_Footnote_2]",Method,Tool,True,Produce（引用目的）,False,P14-1065_0_0,2014,Using Discourse Structure Improves Machine Translation Evaluation,Footnote
492,10495," http://www.statmt.org/wmt{11,12}/results.html"," ['4 Experimental Setup']","In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [Cite_Footnote_3]","3 http://www.statmt.org/wmt{11,12}/results.html","In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [Cite_Footnote_3] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.",Material,Dataset,True,Use（引用目的）,True,P14-1065_1_0,2014,Using Discourse Structure Improves Machine Translation Evaluation,Footnote
493,10496," http://nlp.lsi.upc.edu/asiya/"," ['4 Experimental Setup', '4.1 MT Evaluation Metrics']",We used the freely avail-able version of the A SIYA toolkit [Cite_Footnote_4] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task.,4 http://nlp.lsi.upc.edu/asiya/,"Metrics from A SIYA . We used the freely avail-able version of the A SIYA toolkit [Cite_Footnote_4] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. A SIYA (Giménez and Màrquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation. For reproducibility, below we explain the individual metrics with the exact names required by the toolkit to calculate them.",Method,Tool,True,Use（引用目的）,True,P14-1065_2_0,2014,Using Discourse Structure Improves Machine Translation Evaluation,Footnote
494,10497," https://github.com/pytorch/fairseq/pull/1095"," ['References']",Our implementation has been open-sourced [Cite_Footnote_1] .,1 Code can be found at https://github.com/pytorch/fairseq/pull/1095,"The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, tradi-tional statistical word alignment models of-ten remain the go-to solution. In this pa-per, we present an approach to train a Trans-former model to produce both accurate trans-lations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine trans-lation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces com-petitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accu-racies compared to GIZA++ on three publicly available data sets. Our implementation has been open-sourced [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,D19-1453_0_0,2019,Jointly Learning to Align and Translate with Transformer Models,Footnote
495,10498," https://github.com/lilt/alignment-scripts"," ['4 Proposed Method', '4.3 Providing Full Target Context']","This is imple-mented by masking the self attention probabilities, i.e. while computing the representation for the i th target token, the decoder can only self-attend to the representations of {1, [Cite_Footnote_2] . . .",2 https://github.com/lilt/ alignment-scripts,"The Transformer decoder computes the probabil-ity of the next target token conditioned on the past target tokens and all source tokens. This is imple-mented by masking the self attention probabilities, i.e. while computing the representation for the i th target token, the decoder can only self-attend to the representations of {1, [Cite_Footnote_2] . . . i − 1} tokens from the previous layer. This auto-regressive behavior of the decoder is crucial for the model to repre-sent a valid probability distribution over the target sentence. However, conditioning on just the past target tokens is limiting for the alignment task. As described in Section 4.2, the alignment head is trained to model the alignment distribution for the i th target token given only the past target to-kens and all source tokens. Since the alignment head does not know the identity of the next tar-get token, it becomes difficult for it to learn this token’s alignment to the source tokens. Previous work has also identified this problem and alleviate it by feeding the target token to be aligned as an input to the module computing the alignment (Pe-ter et al., 2017), or forcing the module to predict the target token (Zenkel et al., 2019) or its prop-erties, e.g. POS tags (Li et al., 2018). Feeding the next target token assumes that we know it in ad-vance and thus calls for separate translation and alignment models. Forcing the alignment module to predict target token’s properties helps but still passes the information of the target token in an in-direct manner. We overcome these limitations by conditioning the two components of our loss func-tion on different amounts of context. The NLL loss L t is conditioned on the past target tokens to preserve the auto-regressive property:",Method,Code,False,Introduce（引用目的）,False,D19-1453_1_0,2019,Jointly Learning to Align and Translate with Transformer Models,Footnote
496,10499," http://web.eecs.umich.edu/~mihalcea/wpt/index.html#resources"," ['5 Experiments', '5.1 Setup', '5.1.1 Alignment Task']","Train-ing data and test data for Romanian→English and English→French are provided by the NAACL’03 Building and Using Parallel Texts word align-ment shared task [Cite_Footnote_3] (Mihalcea and Pedersen, 2003).",3 http://web.eecs.umich.edu/˜mihalcea/wpt/index.html#resources,"The purpose of the this task is to fairly com-pare with state-of-the-art results in terms of alignment quality and perform a hyperparame-ter search. We use the same experimental setup as described in (Zenkel et al., 2019). The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian→English, English→French and German→English. Train-ing data and test data for Romanian→English and English→French are provided by the NAACL’03 Building and Using Parallel Texts word align-ment shared task [Cite_Footnote_3] (Mihalcea and Pedersen, 2003). The Romanian→English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German→English we use the Europarl v7 cor-pus as training data and the gold alignments pro-vided by Vilar et al. (2006). The reference align-ments were created by randomly selecting a subset of the Europarl v7 corpus and manually annotating them following the guidelines suggested in (Och and Ney, 2003). Data statistics are shown in Ta-ble 1.",Material,Dataset,True,Use（引用目的）,True,D19-1453_2_0,2019,Jointly Learning to Align and Translate with Transformer Models,Footnote
497,10500," https://www-i6.informatik.rwth-aachen.de/goldAlignment/"," ['5 Experiments', '5.1 Setup', '5.1.1 Alignment Task']",The Romanian→English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German→English we use the Europarl v7 cor-pus as training data and the gold alignments [Cite_Footnote_4] pro-vided by Vilar et al. (2006).,4 https://www-i6.informatik.rwth-aachen.de/goldAlignment/,"The purpose of the this task is to fairly com-pare with state-of-the-art results in terms of alignment quality and perform a hyperparame-ter search. We use the same experimental setup as described in (Zenkel et al., 2019). The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian→English, English→French and German→English. Train-ing data and test data for Romanian→English and English→French are provided by the NAACL’03 Building and Using Parallel Texts word align-ment shared task (Mihalcea and Pedersen, 2003). The Romanian→English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German→English we use the Europarl v7 cor-pus as training data and the gold alignments [Cite_Footnote_4] pro-vided by Vilar et al. (2006). The reference align-ments were created by randomly selecting a subset of the Europarl v7 corpus and manually annotating them following the guidelines suggested in (Och and Ney, 2003). Data statistics are shown in Ta-ble 1.",Material,Dataset,True,Use（引用目的）,True,D19-1453_3_0,2019,Jointly Learning to Align and Translate with Transformer Models,Footnote
498,10501," https://github.com/moses-smt/mgiza/"," ['5 Experiments', '5.2 Statistical Baseline']","In particular, we perform [Cite_Footnote_5] iterations of IBM1, HMM, IBM3 and IBM4.",5 https://github.com/moses-smt/mgiza/,"For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit 5 implemented by Gao and Vo-gel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vo-gel et al., 1996), respectively. In particular, we perform [Cite_Footnote_5] iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and sym-metrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use the resulting word alignments to supervise the alignment loss for the method described in Section 4.4.",Material,Knowledge,False,Use（引用目的）,True,D19-1453_4_0,2019,Jointly Learning to Align and Translate with Transformer Models,Footnote
499,10502," https://github.com/WeimingWen/CCRV"," ['1 Introduction']",We published our code and dataset on GitHub [Cite_Footnote_1] .,1 https://github.com/WeimingWen/CCRV,We published our code and dataset on GitHub [Cite_Footnote_1] .,Mixed,Mixed,True,Produce（引用目的）,True,D18-1385_0_0,2018,Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content,Footnote
500,10503," http://www.fakenewschallenge.org/"," ['5 Cross-lingual Cross-platform Features', '5.2 Agreement Features']",We first pre-train an agreement classifier on a stance detection dataset provided by the Fake News Challenge [Cite_Footnote_2] .,2 http://www.fakenewschallenge.org/,"We first pre-train an agreement classifier on a stance detection dataset provided by the Fake News Challenge [Cite_Footnote_2] . This dataset provides pairs of English sentences with their agreement annota-tions in “agree”, “disagree”, “discuss” and “unre-lated”. Figure 3 shows four example body texts of a headline corresponding to each type of annota-tion in the dataset. During the training process, we embed the sentences in the Fake News Challenge dataset using our pre-trained multilingual sentence embedding. We then concatenate the embeddings of the sentence pair as the input. We use a multi-layer perceptron to pre-train our agreement classi-fier. We randomly select a balanced development set containing 250 pairs for each label (1000 in to-tal) and train our agreement classifier on the rest of 74,385 pairs. The agreement classifier achieves 0.652 in the macro-averaged F1-score on the de-velopment set. Our published code also includes the details.",補足資料,Website,True,Introduce（引用目的）,True,D18-1385_1_0,2018,Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content,Footnote
501,10504," http://new.qq.com/cmsn/20140605/20140605002796"," ['8 Low-resource Rumor Verification via Transfer Learning', '8.2 Analysis']","During the manual annota-tion process, we found out that it is a real event confirmed by official accounts according to one news article from Chinese social media [Cite_Footnote_3] , while CCMR Twitter labeled such tweets as fake.",3 http://new.qq.com/cmsn/20140605/20140605002796,"Transfer obtains pretty low F1-scores in event 07 (Passport hoax). The annotation conflict caused its weak performance. This event is about a Child drew all over his dads passport and made his dad stuck in South Korea. During the manual annota-tion process, we found out that it is a real event confirmed by official accounts according to one news article from Chinese social media [Cite_Footnote_3] , while CCMR Twitter labeled such tweets as fake. Since Transfer is pre-trained using Twitter dataset, it is not surprising that Transfer achieves 0 in F1-score on this event. The annotation conflict also brings out that rumor verification will benefit from utiliz-ing cross-lingual and cross-platform information.",補足資料,Document,True,Use（引用目的）,True,D18-1385_2_0,2018,Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content,Footnote
502,10505," http://duc.nist.gov"," ['2 References to people', '2.1 Data']","The Document Understanding Conference ( [Cite] http://duc.nist.gov) has been run annu-ally since 2001 and is the biggest summarization evaluation effort, with participants from all over the world.",,"We used data from the DUC 2004 Multilingual summarization task. The Document Understanding Conference ( [Cite] http://duc.nist.gov) has been run annu-ally since 2001 and is the biggest summarization evaluation effort, with participants from all over the world. In 2004, for the first time, there was a multi-lingual multi-document summarization task. There were 25 sets to be summarized. For each set con-sisting of 10 Arabic news reports, the participants were provided with 2 different machine translations into English (using translation software from ISI and IBM). The data provided under DUC includes 4 human summaries for each set for evaluation pur-poses; the human summarizers were provided a hu-man translation into English of each of the Arabic New reports, and did not have to read the MT output that the machine summarizers took as input.",補足資料,Website,True,Introduce（引用目的）,True,H05-1005_0_0,2005,Improving Multilingual Summarization: Using Redundancy in the Input to Correct MT errors,Body
503,10506," http://www.cia.gov/cia/publications/factbook"," ['2 References to people', '2.3 Automatic semantic tagging']",We marked up countries and (American) states using a list obtained from the CIA factsheet [Cite_Footnote_1] .,"1 http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian.","As the task definition above suggests, our approach is to identify particular semantic attributes for a per-son, and generate a reference formally from this se-mantic input. Our analysis of human summaries tells us that the semantic attributes we need to identify are role, organization, country, state, location and temporal modifier. In addi-tion, we also need to identify the person name. We used BBN’s I DENTI F INDER (Bikel et al., 1999) to mark up person names, organizations and lo-cations. We marked up countries and (American) states using a list obtained from the CIA factsheet [Cite_Footnote_1] . To mark up roles, we used a list derived from Word-Net (Miller et al., 1993) hyponyms of the person synset. Our list has 2371 entries including multi-word expressions such as chancellor of the exche-quer, brother in law, senior vice president etc. The list is quite comprehensive and includes roles from the fields of sports, politics, religion, military, busi-ness and many others. We also used WordNet to ob-tain a list of 58 temporal adjectives. WordNet classi-fies these as pre- (eg. occasional, former, incoming etc.) or post-nominal (eg. elect, designate, emeritus etc.). This information is used during generation. Further, we identified elementary noun phrases us-ing the LT TTT noun chunker (Grover et al., 2000), and combined NP of NP sequences into one com-plex noun phrase. An example of the output of our semantic tagging module on a portion of machine translated text follows:",Material,DataSource,True,Use（引用目的）,True,H05-1005_1_0,2005,Improving Multilingual Summarization: Using Redundancy in the Input to Correct MT errors,Footnote
504,10507," http://www.nist.gov/speech/tests/mt/resources/scoring.htm"," ['2 References to people', '2.6 Evaluation']","The table also shows the popular BLEU (Papineni et al., 2002) and NIST [Cite_Footnote_2] MT metrics.",2 http://www.nist.gov/speech/tests/mt/resources/scoring.htm,"We used 6 document sets from DUC’04 for devel-opment purposes and present the average P, R and F for the remaining 18 sets in Table 1. There were 210 generated references in the 18 testing sets. The table also shows the popular BLEU (Papineni et al., 2002) and NIST [Cite_Footnote_2] MT metrics. We also provide two base-lines - most frequent initial reference to the person in the input (Base1) and a randomly selected initial reference to the person (Base2). As Table 1 shows, Base1 performs better than random selection. This is intuitive as it also uses redundancy to correct er-rors, at the level of phrases rather than words. The generation module outperforms both baselines, par-ticularly on precision - which for unigrams gives an indication of the correctness of lexical choice, and for higher ngrams gives an indication of grammati-cality. The unigram recall of x‡}‰XŒC indicates that we are not losing too much information at the noise fil-tering stage. Note that we expect a low ¥§¦5¨ for our approach, as we only generate particular attributes that are important for a summary. The important measure is , on which we do well. This is also reflected in the high scores on BLEU and NIST.",Method,Code,True,Use（引用目的）,True,H05-1005_2_0,2005,Improving Multilingual Summarization: Using Redundancy in the Input to Correct MT errors,Footnote
505,10508," http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf"," ['3 Arbitrary noun phrases', '3.5 Manual Evaluation']",We selected 50 of these 94 sentences at ran-dom and asked 2 human judges to rate each sen-tence and its rewritten form on a scale of 1–5 for accuracy and fluency [Cite_Footnote_3] .,"3 We followed the DARPA/LDC guidelines from http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf. For fluency, the scale was 5:Flawless, 4:Good, 3:Non-native, 2:Disfluent, 1:Incomprehensible. The accuracy scale for information covered (comparing with human translation) was 5:All, 4:Most, 3:Much, 2:Little, 1:None.","To evaluate how much impact the rewrites have on summaries, we ran our summarizer on the 18 test sets, and manually evaluated the selected sentences and their rewritten versions for accuracy and flu-ency. There were 118 sentences, out of which 94 had at least one modification after the rewrite pro-cess. We selected 50 of these 94 sentences at ran-dom and asked 2 human judges to rate each sen-tence and its rewritten form on a scale of 1–5 for accuracy and fluency [Cite_Footnote_3] . We used 4 human judges, each judging 25 sentence pairs. The original and rewritten sentences were presented in random order, so judges did not know which sentences were rewrit-ten. Fluency judgments were made before seeing the human translated sentence, and accuracy judgments were made by comparing with the human transla-tion. The average scores before and after rewrite were ^‚ VŒ and ^‚ ‡N‚C respectively for fluency and ^› C and ^› respectively for accuracy. Thus the rewrite operations increases both scores by around 0.2.",補足資料,Document,True,Use（引用目的）,True,H05-1005_3_0,2005,Improving Multilingual Summarization: Using Redundancy in the Input to Correct MT errors,Footnote
506,10509," https://www.zippia.com/contract-attorney-jobs/salary/"," ['1 Introduction']","Moreover, Subject Matter Experts (SMEs) qualified to label ground truth are expensive [Cite_Footnote_1] .","1 According to https://www.zippia.com/contract-attorney-jobs/salary/, the average annual salary for a contract attorney is $86,000 ( $41.35/hour).","C2: Lack of Representative Data. Contracts, while often proprietary, also vary significantly across domains and businesses. Thus, one cannot assume the presence of representative contracts towards building models. Moreover, Subject Matter Experts (SMEs) qualified to label ground truth are expensive [Cite_Footnote_1] . As such, NLP models may need to be developed with limited non-representative labeled data but still be able to generalize well over previously unseen data. C3: Need for Model Stability. CU models are integrated into existing business processes to drive decisions. As the models evolve over time (e.g. due to availability of new data, updates to existing labeled data, etc.), users expect the models to be-have in a stable manner and produce no surprising results (Kearns and Ron, 1997).",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-industry.28_0_0,2021,Development of an Enterprise-Grade Contract Understanding System,Footnote
507,10510," https://blog.lawgeex.com/contractcosts/"," ['1 Introduction']","For instance, a procurement contract requires 5 hours of legal review on average, con-tributing to thousands of dollars in total cost (Cum-mins, 2017) [Cite_Ref] .",Tim Cummins. 2017. Cost of processing a ba-sic contract soars to $6900 (accessed: 2021- 04-09). https://blog.lawgeex.com/contractcosts/.,"A contract is an agreement between businesses and/or individuals to create mutual obligations en-forceable by law (Cornell Law School). Written contracts are also used by companies to safeguard their resources and as such, legal advice is sought prior to participating in a binding contract. Cur-rently, legal review remains an arduous and expen-sive process. For instance, a procurement contract requires 5 hours of legal review on average, con-tributing to thousands of dollars in total cost (Cum-mins, 2017) [Cite_Ref] .",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-industry.28_9_0,2021,Development of an Enterprise-Grade Contract Understanding System,Reference
508,10511," https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/"," ['5 Experiments', '5.2 Experimental setting']","We resolve spelling errors with a public spell checker [Cite_Footnote_4] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do.",4 https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/,"We resolve spelling errors with a public spell checker [Cite_Footnote_4] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do.",Method,Tool,True,Use（引用目的）,True,P18-1097_0_0,2018,Fluency Boost Learning and Inference for Neural Grammatical Error Correction,Footnote
509,10512," http://www.itl.nist.gov/iad/mig/tests/mt/"," ['4 Experimentation', '4.1 Experimental Settings']","We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. [Cite_Footnote_5]",5 http://www.itl.nist.gov/iad/mig/tests/mt/,"Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC cor-pora, with 27.9M Chinese words and 34.5M En-glish words respectively. We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. [Cite_Footnote_5] To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.",Material,Dataset,True,Use（引用目的）,True,P17-1064_0_0,2017,Modeling Source Syntax for Neural Machine Translation,Footnote
510,10513," https://github.com/slavpetrov/berkeleyparser"," ['4 Experimentation', '4.1 Experimental Settings']","To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser [Cite_Footnote_6] (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005).",6 https://github.com/slavpetrov/ berkeleyparser,"Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC cor-pora, with 27.9M Chinese words and 34.5M En-glish words respectively. We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser [Cite_Footnote_6] (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.",Method,Tool,True,Use（引用目的）,True,P17-1064_1_0,2017,Modeling Source Syntax for Neural Machine Translation,Footnote
511,10514," https://github.com/redpony/cdec"," ['4 Experimentation', '4.1 Experimental Settings']","• cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [Cite_Footnote_7]",7 https://github.com/redpony/cdec,"• cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [Cite_Footnote_7]",Material,Dataset,True,Use（引用目的）,True,P17-1064_2_0,2017,Modeling Source Syntax for Neural Machine Translation,Footnote
512,10515," https://github.com/nyu-dl/dl4mt-tutorial"," ['4 Experimentation', '4.1 Experimental Settings']","(Bahdanauof theet alat-., 2015) with slight changes taken from dl4mt tutorial. [Cite_Footnote_8]",8 https://github.com/nyu-dl/ dl4mt-tutorial,"• RNNSearchtentional NMT: a re-implementationsystem (Bahdanauof theet alat-., 2015) with slight changes taken from dl4mt tutorial. [Cite_Footnote_8] For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by (Cho et al., 2014b). It incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the lastly gen-erated word. We use AdaDelta (Zeiler, 2012) to optimize model parameters in training with the mini-batch size of 80. For translation, a beam search with size 10 is employed.",補足資料,Document,True,Extend（引用目的）,True,P17-1064_3_0,2017,Modeling Source Syntax for Neural Machine Translation,Footnote
513,10516," http://nlp.cs.nyu.edu/evalb/"," ['2 Overview and Example', '2.1 Creation of Dependency Representation']","The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008) [Cite_Ref] , which ignores function tags and empty cate-gories.",Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.,"The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008) [Cite_Ref] , which ignores function tags and empty cate-gories. We return to this issue in the conclusion.",Method,Tool,True,Compare（引用目的）,True,N12-1031_0_0,2012,Using Supertags and Encoded Annotation Principles for Improved Dependency to Phrase Structure Conversion,Reference
514,10517," http://nlp.cs.nyu.edu/evalb/"," ['4 Results of Dependency to Phrase Structure Conversion']","We then compare the original PTB trees with the newly-created phrase structure trees, using the standard evalb scoring code (Sekine and Collins, 2008) [Cite_Ref] .",Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.,"To evaluate the correctness of conversion from de-pendency to phrase structure, we follow the same strategy as Xia and Palmer (2001) and Xia et al. (2009). We convert the phrase structure trees in the PTB to dependency structure and convert the depen-dency back to phrase structure. We then compare the original PTB trees with the newly-created phrase structure trees, using the standard evalb scoring code (Sekine and Collins, 2008) [Cite_Ref] . Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projec-tion chains, and where to attach dependent subtrees. They reported results for their system for Section 00 of the PTB, and we include in Table 2 only their highest scoring algorithm. The system of Xia et al. (2009) uses conversion rules learned from Section 19, and then tested on Sections 00 and Section 22.",Method,Code,True,Compare（引用目的）,True,N12-1031_0_1,2012,Using Supertags and Encoded Annotation Principles for Improved Dependency to Phrase Structure Conversion,Reference
515,10518," https://github.com/google-research/tensorflow_constrained_optimization"," ['3 Proposed Approaches', '3.2 Bias-constrained Model']","Specifically we use the implementations available in TensorFlow constrained optimization (Cotter et al., 2019a,b). [Cite_Footnote_3]",3 https://github.com/google-research/ tensorflow_constrained_optimization,"Each group-wise constraint is denoted as ψ g . The constraints involve a linear combination of indicator variables, which is not differentiable wrt θ. A common approach to handle this constrained optimization problems is using the Lagrangian, which is minimized over θ and maximized over λ. Similar formulations have been used for learn-ing fair models with structured data (Cotter et al., 2019b; Yang et al., 2020; Zafar et al., 2019). In this work, we apply this method to NLP tasks and use the two-player zero-sum game approach for optimization, where the first player chooses θ to minimize L(θ, λ), and the second player enforces fairness constraints by maximizing λ (Kearns et al., 2018; Cotter et al., 2019b; Yang et al., 2020). Specifically we use the implementations available in TensorFlow constrained optimization (Cotter et al., 2019a,b). [Cite_Footnote_3]",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.193_0_0,2021,Evaluating Debiasing Techniques for Intersectional Biases,Footnote
516,10519," https://github.com/masashi-y/depccg"," ['References']",Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [Cite_Footnote_1],1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.,"We propose a new A* CCG parsing model in which the probability of a tree is decom-posed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabil-ities and runs very efficiently, while mod-eling sentence structures explicitly via de-pendencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P17-1026_0_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Footnote
517,10520," https://github.com/uwnlp/taggerflow"," ['5 Tri-training']","Since they make this data publicly available [Cite_Footnote_5] , we obtain our silver data by assigning dependency",5 https://github.com/uwnlp/taggerflow,"We simply combine the two previous ap-proaches. Lewis et al. (2016) obtain their sil-ver data annotated with the high quality supertags. Since they make this data publicly available [Cite_Footnote_5] , we obtain our silver data by assigning dependency",Material,Dataset,True,Extend（引用目的）,True,P17-1026_1_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Footnote
518,10521," http://nlp.stanford.edu/projects/glove/"," ['6 Experiments', '6.1 English Experimental Settings']","We use as word representation the concatena-tion of word vectors initialized to GloVe [Cite_Footnote_8] (Pen-nington et al., 2014), and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016).",8 http://nlp.stanford.edu/projects/glove/,"We use as word representation the concatena-tion of word vectors initialized to GloVe [Cite_Footnote_8] (Pen-nington et al., 2014), and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to “UNK”.",Method,Code,True,Use（引用目的）,True,P17-1026_2_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Footnote
519,10522," https://github.com/mynlp/jigg"," ['6 Experiments', '6.2 Japanese Experimental Settings']","For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [Cite_Footnote_9] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.",9 https://github.com/mynlp/jigg,"We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [Cite_Footnote_9] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.",Method,Tool,True,Introduce（引用目的）,False,P17-1026_3_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Footnote
520,10523," http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/"," ['6 Experiments', '6.2 Japanese Experimental Settings']","For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector [Cite_Footnote_10] , and 100-dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014).",10 http://www.cl.ecei.tohoku.ac.jp/˜m-suzuki/jawiki_vector/,"For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector [Cite_Footnote_10] , and 100-dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014). We do not use affix vectors as affixes are less informative in Japanese. All characters ap-pearing less than two times are mapped to “UNK”. We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization.",Material,Knowledge,True,Use（引用目的）,True,P17-1026_4_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Footnote
521,10524," http://taku910.github.io/cabocha/"," ['6 Experiments', '6.2 Japanese Experimental Settings']","Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha [Cite_Footnote_11] and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b.",11 http://taku910.github.io/cabocha/,"One issue in Japanese experiments is evalua-tion. The Japanese CCGbank is encoded in a dif-ferent format than the English bank, and no stan-dalone script for extracting semantic dependen-cies is available yet. For this reason, we evaluate the parser outputs by converting them to bunsetsu dependencies, the syntactic representation ordi-nary used in Japanese NLP (Kudo and Matsumoto, 2002). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha [Cite_Footnote_11] and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b. For ex-ample, the sentence in Figure 4e is segmented as “Boku wa | eigo wo | hanashi tai”, from which we extract two dependencies (Boku wa) ← (hanashi tai) and (eigo wo) ← (hanashi tai). We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy. Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser.",Method,Tool,True,Use（引用目的）,True,P17-1026_5_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Footnote
522,10525," http://tensorflow.org/"," ['6 Experiments', '6.3 English Parsing Results']","There are also many implementation differences in our parser (C++ A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ Tensor-Flow (Abadi et al., 2015) [Cite_Ref] supertagger and recur-sive neural model in C++ DyNet (Neubig et al., 2017)).","Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Viégas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Sys-tems. Software available from tensorflow.org. http://tensorflow.org/.","Efficiency Comparison We compare the ef-ficiency of our parser with neuralccg and EasySRL reimpl. The results are shown in Table 4. For the overall speed (the third row), our parser is faster than neuralccg al-though lags behind EasySRL reimpl. Inspect-ing the details, our supertagger runs slower than those of neuralccg and EasySRL reimpl, while in A* search our parser processes over 7 times more sentences than neuralccg. The delay in supertagging can be attributed to sev-eral factors, in particular the differences in net-work architectures including the number of bi- LSTM layers (4 vs. 2) and the use of bilin-ear transformation instead of linear one. There are also many implementation differences in our parser (C++ A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ Tensor-Flow (Abadi et al., 2015) [Cite_Ref] supertagger and recur-sive neural model in C++ DyNet (Neubig et al., 2017)).",Method,Tool,True,Use（引用目的）,True,P17-1026_6_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Reference
523,10526," http://aclweb.org/anthology/J07-4004"," ['7 Related Work']","In the CCG parsing literature, some work op-timizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007 [Cite_Ref] ; Xu et al., 2014).","Stephen Clark and James R. Curran. 2007. Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Lin-guistics, Volume 33, Number 4, December 2007 http://aclweb.org/anthology/J07-4004.","In the CCG parsing literature, some work op-timizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007 [Cite_Ref] ; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric. Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score. Lewis et al. (2015) present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to our approach. They map each CCG semantic dependency to an SRL relation, for which they give the A* upper bound by the score from a predicate to the most proba-ble argument. Our approach is similar; the largest difference is that we instead model syntactic de-pendencies from each token to its head, and this is the key to our success. Since dependency parsing can be formulated as independent head selections similar to tagging, we can build the entire model on LSTMs to exploit features from the whole sen-tence. This formulation is not straightforward in the case of multi-headed semantic dependencies in their model.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1026_7_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Reference
524,10527," http://aclweb.org/anthology/P96-1011"," ['6 Experiments', '6.1 English Experimental Settings']","Unless otherwise stated, we only al-low normal form parses (Eisner, 1996 [Cite_Ref] ; Hocken-maier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).",Jason Eisner. 1996. Efficient Normal-Form Parsing for Combinatory Categorial Grammar. In 34th Annual Meeting of the Association for Computational Lin-guistics. http://aclweb.org/anthology/P96-1011.,"For our models, we adopt the pruning strate-gies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with β = 0.00001, and utilize a tag dictio-nary, which maps frequent words to the possible supertags 7 . Unless otherwise stated, we only al-low normal form parses (Eisner, 1996 [Cite_Ref] ; Hocken-maier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).",補足資料,Paper,True,Introduce（引用目的）,False,P17-1026_11_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Reference
525,10528," http://www.aclweb.org/anthology/D14-1162"," ['6 Experiments', '6.1 English Experimental Settings']","We use as word representation the concatena-tion of word vectors initialized to GloVe (Pen-nington et al., 2014) [Cite_Ref] , and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016).","Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532– 1543. http://www.aclweb.org/anthology/D14-1162.","We use as word representation the concatena-tion of word vectors initialized to GloVe (Pen-nington et al., 2014) [Cite_Ref] , and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to “UNK”.",Method,Code,True,Use（引用目的）,True,P17-1026_23_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Reference
526,10529," http://aclweb.org/anthology/E99-1026"," ['4 CCG to Dependency Conversion']","Japanese dependency parsing (Uchimoto et al., 1999 [Cite_Ref] ; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-to-right dependency arcs.","Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese Dependency Structure Analysis Based on Maximum Entropy Models. In Ninth Conference of the European Chapter of the Association for Computational Linguistics. http://aclweb.org/anthology/E99-1026.","H EAD F INAL Among SOV languages, Japanese is known as a strictly head final language, mean-ing that the head of every word always follows it. Japanese dependency parsing (Uchimoto et al., 1999 [Cite_Ref] ; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-to-right dependency arcs. Inspired by this tradition, we try a simple H EAD F INAL rule in Japanese CCG parsing, in which we always select the right argument as the head. For example we obtain the head final dependency tree in Figure 4e from the Japanese CCG tree in Figure 4b.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1026_28_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Reference
527,10530," http://aclweb.org/anthology/N10-1090"," ['7 Related Work']","Zhang et al. (2010) [Cite_Ref] use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.","Yao-zhong Zhang, Takuya Matsuzaki, and Jun’ichi Tsujii. 2010. A Simple Approach for HPSG Su-pertagging Using Dependency Information. In Hu-man Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Associ-ation for Computational Linguistics, pages 645–648. http://aclweb.org/anthology/N10-1090.","For Head-driven Phrase Structure Gram-mar (HPSG; Pollard and Sag (1994)), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy. Sagae et al. (2007) use dependencies to constrain the form of the output tree. As in our method, for every rule (schema) application they define which child becomes the head and impose a soft constraint that these dependencies agree with the output of the dependency parser. Our method is different in that we do not use the one-best dependency structure alone, but rather we search for a CCG tree that is optimal in terms of dependencies and CCG supertags. Zhang et al. (2010) [Cite_Ref] use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1026_33_0,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,Reference
528,10531," https://github.com/ChunchuanLv/Iterative_Inference"," ['1 Introduction']","Furthermore, we provide analysis on con-straint violations and errors on the English test set. [Cite_Footnote_2]",2 The code and experiment settings can be ac-cessed at https://github.com/ChunchuanLv/ Iterative_Inference,"We consider the CoNLL-2009 dataset (Hajič et al., 2009). We start with a strong factorized baseline model, which already achieves state-of-the-art results on a subset of the languages. Then, using our structure refinement network, we im-prove on this baseline on all 7 CoNLL-2009 lan-guages. The model achieves best-reported results in 5 languages, including English. We also ob-serve improvements on out-of-domain test sets, confirming the robustness of our approach. We perform experiments demonstrating the impor-tance of adding noise, and ablation studies show-ing the necessity of incorporating output interac-tions. Furthermore, we provide analysis on con-straint violations and errors on the English test set. [Cite_Footnote_2]",Mixed,Mixed,True,Use（引用目的）,True,D19-1099_0_0,2019,Semantic Role Labeling with Iterative Structure Refinement,Footnote
529,10532," https://github.com/fxsjy/jieba"," ['5 Experiments', '5.2 Neural Hidden Markov Model']","Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [Cite_Footnote_1] (Pascanu et al., 2014).",1 https://github.com/fxsjy/jieba,"Apart from the basic projection layer, we also applied LSTM layers for the source and target words embedding. The embedding layers have 350 nodes and the size of the projection layer is 800 (400 + 200 + 200, Figure 1). We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [Cite_Footnote_1] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.",Method,Code,False,Use（引用目的）,True,P18-2060_0_0,2018,Neural Hidden Markov Model for Machine Translation,Footnote
530,10533," http://rnnlm.org"," ['3 Experimental Setup', '3.1 Unlabeled tweets']","We modified the RNNLM toolkit (Mikolov, 2012a) [Cite_Ref] to record the activations of the hidden layer and ran it with the default learning rate schedule.",Tomáš Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.,"In order to train our SRN language model we col-lected a set of tweets using the Twitter sampling API. We use the raw sample directly without fil-tering it in any way, relying on the SRN to learn the structure of the data. The sample consists of 414 million bytes of UTF-8 encoded in a variety of languages and scripts text. We trained a 400-hidden-unit SRN, to predict the next byte in the sequence using backpropagation through time. In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a) [Cite_Ref] to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer. We did try to filter tweets by language and create specific em-beddings for English but this had negligible effect on tweet normalization performance.",Method,Tool,True,Extend（引用目的）,True,P14-2111_0_0,2014,Normalizing tweets with edit scripts and recurrent neural embeddings,Reference
531,10534," https://github.com/uhh-lt/wsd"," ['1 Introduction']",Implementation of the system is open source. [Cite_Footnote_1],1 https://github.com/uhh-lt/wsd,"We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambigua-tion, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledge-free framework. Implementation of the system is open source. [Cite_Footnote_1] A live demo featuring several dis-ambiguation models is available online.",Method,Tool,True,Produce（引用目的）,True,D17-2016_0_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
532,10535," http://jobimtext.org/wsd"," ['1 Introduction']",A live demo featuring several dis-ambiguation models is available online. [Cite_Footnote_2],2 http://jobimtext.org/wsd,"We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambigua-tion, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledge-free framework. Implementation of the system is open source. A live demo featuring several dis-ambiguation models is available online. [Cite_Footnote_2]",補足資料,Media,True,Produce（引用目的）,True,D17-2016_1_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
533,10536," https://github.com/alvations/pywsd"," ['2 Related Work']","PyWSD [Cite_Footnote_3] project also provides imple-mentations of popular WSD methods, but these are implemented in the Python language.",3 https://github.com/alvations/pywsd,"DKPro WSD (Miller et al., 2013) is a modu-lar, extensible Java framework for word sense dis-ambiguation. It implements multiple WSD meth-ods and also provides an interface to evaluation datasets. PyWSD [Cite_Footnote_3] project also provides imple-mentations of popular WSD methods, but these are implemented in the Python language.",補足資料,Website,True,Introduce（引用目的）,True,D17-2016_2_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
534,10537," http://spark.apache.org"," ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.1 Induction of the WSD Models']","Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework [Cite_Footnote_4] , enabling seamless processing of large text collections.",4 http://spark.apache.org,"Figure 1 presents architecture of the WSD sys-tem. As one may observe, no human labor is used to learn interpretable sense representa-tions and the corresponding disambiguation mod-els. Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework [Cite_Footnote_4] , enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are la-beled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) pat-terns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction con-fidence. Top sentences are used as sense usage ex-amples. For more details about the model induc-tion process refer to (Panchenko et al., 2017). Cur-rently, the following WSD models induced from a text corpus are available:",Method,Code,True,Use（引用目的）,True,D17-2016_3_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
535,10538," https://www.postgresql.org"," ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']","To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. [Cite_Footnote_5]",5 https://www.postgresql.org,"To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. [Cite_Footnote_5] In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework.",Material,Dataset,True,Use（引用目的）,True,D17-2016_4_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
536,10539," https://github.com/scalanlp/breeze"," ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']","Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. [Cite_Footnote_6]",6 https://github.com/scalanlp/breeze,"To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. [Cite_Footnote_6] Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework.",Material,Knowledge,True,Use（引用目的）,True,D17-2016_5_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
537,10540," https://www.playframework.com"," ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']",This back-end is implemented as a RESTful API using the Play framework. [Cite_Footnote_7],7 https://www.playframework.com,"To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework. [Cite_Footnote_7]",Method,Code,True,Use（引用目的）,True,D17-2016_6_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
538,10541," https://facebook.github.io/react"," ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']",The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [Cite_Footnote_8],8 https://facebook.github.io/react,"The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [Cite_Footnote_8] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:",Method,Code,True,Use（引用目的）,True,D17-2016_7_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
539,10542," https://azure.microsoft.com/en-us/services/cognitive-services/search"," ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']","We assign an image to each word in the cluster by querying an image search API [Cite_Footnote_9] us-ing a query composed of the ambiguous word and its hypernym, e.g. “jaguar animal”.",9 https://azure.microsoft.com/en-us/services/cognitive-services/search,"Single word disambiguation mode is illus-trated in Figure 2. In this mode, a user specifies an ambiguous word and its context. The output of the system is a ranked list of all word senses of the ambiguous word ordered by relevance to the input context. By default, only the best matching sense is displayed. The user can quickly understand the meaning of each induced sense by looking at the hypernym and the image representing the sense. Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API [Cite_Footnote_9] us-ing a query composed of the ambiguous word and its hypernym, e.g. “jaguar animal”. The first hit of this query is selected to represent the induced word sense. Interpretability of each sense is fur-ther ensured by providing to the user the list of related senses, the list of the most salient context clues, and the sense usage examples (cf. Figure 2). Note that all these elements are obtained without manual intervention.",Method,Code,True,Use（引用目的）,True,D17-2016_8_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
540,10543," http://www.scalanlp.org"," ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']","First, the text is processed with a part-of-speech and a named entity taggers. [Cite_Footnote_10]",10 http://www.scalanlp.org,"All words disambiguation mode is illustrated in Figure 3. In this mode, the system performs dis-ambiguation of all nouns and entities in the input text. First, the text is processed with a part-of-speech and a named entity taggers. [Cite_Footnote_10] Next, each detected noun or entity is disambiguated in the same way as in the single word disambiguation mode described above, yet the disambiguation re-sults are represented as annotations of a running text. The best matching sense is represented by a hypernym and an image as depicted in Figure 3. This mode performs “semantification” of a text, which can, for instance, assist language learners with the understanding of a text in a foreign lan-guage: Meaning of unknown to the learner words can be deduced from hypernyms and images.",Method,Tool,True,Use（引用目的）,True,D17-2016_9_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
541,10544," https://www.docker.com"," ['5 Conclusion']","Besides, in-house deployments of the system are made easy due to the use of the Docker containers. [Cite_Footnote_12]",12 https://www.docker.com,"We present the first openly available word sense disambiguation system that is unsupervised, knowledge-free, and interpretable at the same time. The system performs extraction of word and super sense inventories from a text corpus. The disambiguation models are learned in an unsuper-vised way for all words in the corpus on the ba-sis on the induced inventories. The user inter-face of the system provides efficient access to the produced WSD models via a RESTful API or via an interactive Web-based graphical user interface. The system is available online and can be directly used from external applications. The code and the WSD models are open source. Besides, in-house deployments of the system are made easy due to the use of the Docker containers. [Cite_Footnote_12] A prominent direction for future work is supporting more lan-guages and establishing cross-lingual sense links.",Method,Tool,False,Use（引用目的）,True,D17-2016_10_0,2017,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",Footnote
542,10545," http://ai.stanford.edu/~amaas/data/sentiment/"," ['5 Experimental Settings', '5.2 Datasets']",We use the IMDB Movie Review Corpus (IMDB) prepared by Maas et al. (2011). [Cite_Footnote_1],1 http://ai.stanford.edu/˜amaas/data/sentiment/,We use the IMDB Movie Review Corpus (IMDB) prepared by Maas et al. (2011). [Cite_Footnote_1] This cor-pus has 75k training reviews and 25k test reviews.,Material,Dataset,True,Use（引用目的）,True,P16-1125_0_0,2016,Larger-Context Language Modelling with Recurrent Neural Network ∗,Footnote
543,10546," http://mlg.ucd.ie/datasets/bbc.html"," ['5 Experimental Settings', '5.2 Datasets']",We use the BBC corpus prepared by Greene and Cunningham (2006). [Cite_Footnote_2],2 http://mlg.ucd.ie/datasets/bbc.html,"BBC Similarly to movie reviews, each new ar-ticle tends to convey a single theme. We use the BBC corpus prepared by Greene and Cunningham (2006). [Cite_Footnote_2] Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro-posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages. We use the 10k most frequent words in the training corpus for recurrent language models.",Material,Dataset,True,Use（引用目的）,True,P16-1125_1_0,2016,Larger-Context Language Modelling with Recurrent Neural Network ∗,Footnote
544,10547," https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl"," ['5 Experimental Settings', '5.2 Datasets']","Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization. [Cite_Footnote_3]",3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl,"Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization. [Cite_Footnote_3] Penn Treebank We evaluate a normal recurrent language model, count-based n-gram language model as well as the proposed RLM-BoW-EF-n and RLM-BoW-LF-n with varying n = 1, 2, 4, 8 on the Penn Treebank Corpus. We preprocess the corpus according to (Mikolov et al., 2011) and use a vocabulary of 10k words from the training cor-pus.",Method,Tool,True,Use（引用目的）,True,P16-1125_2_0,2016,Larger-Context Language Modelling with Recurrent Neural Network ∗,Footnote
545,10548," http://mattmahoney.net/dc/textdata"," ['5 Experimental Settings', '5.2 Datasets']","Fil9 Fil9 is a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens, and is pro-vided on Matthew Mahoney’s website. [Cite_Footnote_4]",4 http://mattmahoney.net/dc/textdata,"Fil9 Fil9 is a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens, and is pro-vided on Matthew Mahoney’s website. [Cite_Footnote_4] We tok-enized the corpus and used the 44k most frequent words in the training corpus for recurrent language models.",Material,Dataset,True,Use（引用目的）,True,P16-1125_3_0,2016,Larger-Context Language Modelling with Recurrent Neural Network ∗,Footnote
546,10549," http://nlp.stanford.edu/software/tagger.shtml"," ['6 Results and Analysis']","We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [Cite_Footnote_5]",5 http://nlp.stanford.edu/software/tagger.shtml,"We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [Cite_Footnote_5] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.",Method,Tool,True,Use（引用目的）,True,P16-1125_4_0,2016,Larger-Context Language Modelling with Recurrent Neural Network ∗,Footnote
547,10550," https://github.com/abhijith-athreya/ASDUS"," ['References']","These results exhibit a robust level of accuracy suitable for enhancing question answering, in-formation extraction, and summarization. [Cite_Footnote_1]",1 The source code and corpora generated by this research are available at https://github.com/abhijith-athreya/ASDUS.,"The text in many web documents is organized into a hierarchy of section titles and corre-sponding prose content, a structure which pro-vides potentially exploitable information on discourse structure and topicality. However, this organization is generally discarded dur-ing text collection, and collecting it is not straightforward: the same visual organization can be implemented in a myriad of different ways in the underlying HTML. To remedy this, we present a flexible system for automat-ically extracting the hierarchical section titles and prose organization of web documents ir-respective of differences in HTML representa-tion. This system uses features from syntax, semantics, discourse and markup to build two models which classify HTML text into section titles and prose text. When tested on three different domains of web text, our domain-independent system achieves an overall preci-sion of 0.82 and a recall of 0.98. The domain-dependent variation produces very high pre-cision (0.99) at the expense of recall (0.75). These results exhibit a robust level of accuracy suitable for enhancing question answering, in-formation extraction, and summarization. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D18-1099_0_0,2018,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,Footnote
548,10551," https://rule.alibaba.com/rule/detail/2034.htm"," ['1 Introduction']",Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2],"2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy","However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach.",Material,DataSource,True,Use（引用目的）,True,D18-1099_1_0,2018,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,Footnote
549,10552," https://www.apple.com/legal/privacy/en-ww/"," ['1 Introduction']",Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2],"2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy","However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach.",Material,DataSource,True,Use（引用目的）,True,D18-1099_2_0,2018,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,Footnote
550,10553," https://www.cbsinteractive.com/legal/cbsi/privacy-policy"," ['1 Introduction']",Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2],"2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy","However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach.",Material,DataSource,True,Use（引用目的）,True,D18-1099_3_0,2018,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,Footnote
551,10554," https://help.bet365.com/en/privacy-policy"," ['1 Introduction']",Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2],"2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy","However, detecting the titles and prose seg-ments in an HTML document is difficult for two reasons. One of them is the flexibility of HTML, which allows the same typographic layout to be represented in code in multiple ways. Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [Cite_Footnote_2] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers. Sieving only useful informa-tion from these pages requires a flexible approach.",Material,DataSource,True,Use（引用目的）,True,D18-1099_4_0,2018,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,Footnote
552,10555," https://www.academia.edu/6888756/"," ['3 Approach', '3.1 Domain-Independent Approach (DI)']",We cal-culated the number of discourse cues as the sum of explicit discourse markers provided by Denver (2018) [Cite_Ref] and the number of coreference chains.,Tanya Denver. 2018. Discourse Markers – Connec-tors a list of discourse markers with examples. https://www.academia.edu/6888756/ discourse_markers_connectors_a_ list_of_discourse_markers_with_ examples.,"Feature Extraction and Clustering: For each tuple, we extract a set of features which aids in differentiating titles from prose text. The features are own text length, next text length, number of punctuation symbols, number of sentences, num-ber of stop words (stop words were derived from Weka (Hall et al., 2009)), number of discourse cues, number of named entity slots and number of words with capitalized initial letter. We cal-culated the number of discourse cues as the sum of explicit discourse markers provided by Denver (2018) [Cite_Ref] and the number of coreference chains. We used Stanford CoreNLP (Manning et al., 2014) for identifying coreference chains and named entities.",補足資料,Paper,True,Introduce（引用目的）,True,D18-1099_5_0,2018,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,Reference
553,10556," https://jsoup.org/"," ['3 Approach', '3.1 Domain-Independent Approach (DI)']","Using jsoup (Hedley, 2017) [Cite_Ref] , we parse the HTML file and for each non-empty tag encountered we extract a tuple consisting of the text and its XPath.",Jonathan Hedley. 2017. jsoup (1.11.3). https://jsoup.org/.,"Text Collection: Using jsoup (Hedley, 2017) [Cite_Ref] , we parse the HTML file and for each non-empty tag encountered we extract a tuple consisting of the text and its XPath.",Method,Tool,True,Use（引用目的）,True,D18-1099_6_0,2018,Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents,Reference
554,10557," http://gun-violence.org/"," ['3 The Gun Violence Database']","In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [Cite_Footnote_2] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence.",2 http://gun-violence.org/,"In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [Cite_Footnote_2] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence. The GVDB is the result of a large crowdsourced annotation effort. This an-notation is ongoing, and the GVDB will be regularly updated with new data and new layers of annotation, making it an interesting and challenging data set on which to evaluate state-of-the-art NLP tools.",Material,Dataset,True,Use（引用目的）,True,D16-1106_0_0,2016,The Gun Violence Database: A new task and data set for NLP,Footnote
555,10558," http://gun-violence.org/"," ['3 The Gun Violence Database']",The latest version of the database will be main-tained and available for download at [Cite] http://gun-violence.org/.,,"At the time of writing, the GVDB contains 7,366 fully annotated articles (Table 1) coming from 1,512 US cities, and the database is continuing to grow. The latest version of the database will be main-tained and available for download at [Cite] http://gun-violence.org/.",Material,Dataset,True,Introduce（引用目的）,False,D16-1106_1_0,2016,The Gun Violence Database: A new task and data set for NLP,Body
556,10559," http://gun-violence.org/"," ['References']","Finally, we thank the devel-opers at 10clouds for the excellent engineering and design of [Cite] http://gun-violence.org/. Amit Singhal. 2012.",,"We would like to thank Douglas Wiebe for his ad-vice and insight on building a useful resource for public health researchers. We also thank the students of the University of Pennsylvania’s crowdsourcing class (NETS 213) for their involvement in building and testing a useful crowdsourcing pipeline for in-formation extraction. Finally, we thank the devel-opers at 10clouds for the excellent engineering and design of [Cite] http://gun-violence.org/. Amit Singhal. 2012. Introducing the knowledge graph:",補足資料,Website,True,Introduce（引用目的）,False,D16-1106_2_0,2016,The Gun Violence Database: A new task and data set for NLP,Body
557,10560," http://www.gunviolencearchive.org"," ['4 Related Efforts']",Perhaps the largest such ef-fort is the Gun Violence Archive [Cite_Footnote_5] .,5 http://www.gunviolencearchive.org,"Several projects collect data about gun violence via newspaper teams (Boyle, 2013; Swaine et al., 2015) or volunteer crowds (Burghart, 2014; Wagner, 2014; Kirk and Kois, 2013). Perhaps the largest such ef-fort is the Gun Violence Archive [Cite_Footnote_5] . However, none are aimed at the eventual automation of the process. We believe that automating this data collection is key to keeping it scalable, consistent, and unbiased. Our focus is therefore on collecting data that is well-suited for training and evaluating NLP systems.",補足資料,Website,True,Introduce（引用目的）,True,D16-1106_3_0,2016,The Gun Violence Database: A new task and data set for NLP,Footnote
558,10561," https://github.com/zhaozj89/TensorEmbeddingNLP"," ['1 Introduction']","Our implementation is open-sourced, and can be found at [Cite] https://github.com/zhaozj89/TensorEmbeddingNLP.",,"The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at [Cite] https://github.com/zhaozj89/TensorEmbeddingNLP.",Method,Tool,True,Produce（引用目的）,True,D19-1669_0_0,2019,Embedding Lexical Features via Tensor Decomposition for Small Sample Humor Recognition,Body
559,10562," https://www.bbc.com/news/technology-35890188"," ['1 Introductions']","For instance, in 2016, Microsoft released an AI chatbot named Tay [Cite_Footnote_1] , which is claimed to be able to improve itself through communicating with social media users.",1 https://www.bbc.com/news/technology-35890188,"For instance, in 2016, Microsoft released an AI chatbot named Tay [Cite_Footnote_1] , which is claimed to be able to improve itself through communicating with social media users. Nevertheless, within just 24 hours after Microsoft released the chatbot, it started to generate misogynistic and racist words. Microsoft had to suspend the chatbot account and conceded that the chatbot suffered from a “coordinated attack by a subset of people”. Such an incident demon-strates the vulnerability of existing Seq2seq meth-ods facing users’ abuse.",補足資料,Media,False,Introduce（引用目的）,True,2021.emnlp-main.418_0_0,2021,Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness,Footnote
560,10563," https://www.yelp.com/dataset"," ['4 Experiments', '4.1 Datasets']","Particularly, we conduct experiments on a subset of the widely used Yelp dataset [Cite_Footnote_2] .",2 https://www.yelp.com/dataset,"We use one of the classic NLP tasks that commonly suffer profanity issues - text style transfer, to eval-uate the effectiveness of the proposed framework. Particularly, we conduct experiments on a subset of the widely used Yelp dataset [Cite_Footnote_2] . The dataset consists of product reviews aligned with sentiment ratings from 1 to 5. We normalize the ratings by treating ratings below three as negative (0) and otherwise positive (1). After data cleaning, we use the method presented in (Li et al., 2018b) to construct pseudo sentence pairs, which is commonly used in the style transfer field. Then we randomly select 240 thou-sand sentence pairs for training, one thousand for validation, and one hundred for testing. Our task is to transfer the sentence from positive opinion to negative. Here, we only use a small test set because we only use these test samples to test the outcome of the attacks rather than the original task.",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.418_1_0,2021,Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness,Footnote
561,10564," https://github.com/rivercold/BERT-unsupervised-OOD"," ['References']",Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [Cite_Footnote_1],1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.,Deployed real-world machine learning appli-cations are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety is-sues. Prior studies on out-of-domain detec-tion require in-domain task labels and are lim-ited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent represen-tations of pre-trained transformers and pro-pose a simple yet effective method to trans-form features across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further pro-posed to boost detection accuracy. Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [Cite_Footnote_1],Method,Code,True,Produce（引用目的）,True,2021.acl-long.85_0_0,2021,Unsupervised Out-of-Domain Detection via Pre-trained Transformers,Footnote
562,10565," http://purduenlp.cs.purdue.edu/projects/twittermorals"," ['1 Introduction']","(2) We provide a descrip-tion of our annotation guidelines and an annotated dataset of 2,050 tweets. [Cite_Footnote_2]",2 The data will be available at http://purduenlp.cs.purdue.edu/projects/twittermorals.,"In summary, this paper makes the following contributions: (1) This work is among the first to explore jointly modeling language and polit-ical framing techniques for the classification of moral foundations used in the tweets of U.S. politicians on Twitter. (2) We provide a descrip-tion of our annotation guidelines and an annotated dataset of 2,050 tweets. [Cite_Footnote_2] (3) We suggest compu-tational models which easily adapt to new policy issues, for the classification of the moral founda-tions present in tweets.",Mixed,Mixed,True,Produce（引用目的）,True,P18-1067_0_0,2018,Classification of Moral Foundations in Microblog Political Discourse,Footnote
563,10566," https://www.texar.io.1"," ['References']","Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at [Cite] https: //www.texar.io.",,"We introduce Texar, an open-source toolkit aiming to support the broad set of text genera-tion tasks that transform any inputs into natural language, such as machine translation, sum-marization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar ex-tracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model archi-tecture, inference, and learning processes are properly decomposed. Modules at a high con-cept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suit-able for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at [Cite] https: //www.texar.io. 1",補足資料,Website,False,Introduce（引用目的）,True,P19-3027_0_0,2019,"Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",Body
564,10567," https://arxiv.org/abs/1809.00794"," ['References']","Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io. [Cite_Footnote_1]",1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794,"We introduce Texar, an open-source toolkit aiming to support the broad set of text genera-tion tasks that transform any inputs into natural language, such as machine translation, sum-marization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar ex-tracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model archi-tecture, inference, and learning processes are properly decomposed. Modules at a high con-cept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suit-able for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io. [Cite_Footnote_1]",補足資料,Document,True,Introduce（引用目的）,True,P19-3027_1_0,2019,"Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",Footnote
565,10568," https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow"," ['4 Case Study: Transformer on Different Tasks']","As a (rough) reference, a popular public TensorFlow code (Li, 2017) [Cite_Ref] of the same model has used around 400 lines of code for the same part (without line length limit).",Zhong-Yi Li. 2017. https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow.,"The first task we explored is the variational autoencoder (VAE) language modeling (Bowman et al., 2015). We test two models, one with an LSTM RNN decoder which is traditionally used in the task, and the other with a Transformer de-coder. All other model configurations including parameter size are the same across the two mod-els. Table 1, top panel, shows the Transformer VAE consistently improves over the LSTM VAE. With Texar, changing the decoder from an LSTM to a Transformer is easily achieved by modifying only 3 lines of code. It is also worth noting that, building the VAE language model (including data reading, model construction, and optimization) on Texar uses only 70 lines of code (with the length of each line < 80 chars). As a (rough) reference, a popular public TensorFlow code (Li, 2017) [Cite_Ref] of the same model has used around 400 lines of code for the same part (without line length limit).",Method,Code,True,Introduce（引用目的）,True,P19-3027_2_0,2019,"Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",Reference
566,10569," https://sf.net/projects/jobimtext/"," ['6 Conclusion']",[Cite_Footnote_3] under the ASL 2.0 licence.,3 https://sf.net/projects/jobimtext/,"We have introduced a highly scalable approach to DT computation and showed its adequacy for very large corpora. Evaluating against thesauri and WordNet, we demonstrated that our similarity mea-sure yields better-quality DTs and scales to corpora of billions of sentences, even on comparably small compute clusters. We achieve this by a number of pruning operations, and distributed processing. The framework and the DTs for Google Books, News-paper and Wikipedia are available online [Cite_Footnote_3] under the ASL 2.0 licence.",Method,Code,True,Introduce（引用目的）,True,D13-1089_0_0,2013,Scaling to Large 3 Data: An efficient and effective method to compute Distributional Thesauri,Footnote
567,10570," http://www.cis.hut.fi/projects/morpho/"," ['1 Introduction']","The approach presented in this paper relies on a data-driven algorithm called Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005 [Cite_Ref] ) which is a language independent unsupervised machine learn-ing method to find morpheme-like units (called sta-tistical morphs) from a large text corpus.","Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.","The approach presented in this paper relies on a data-driven algorithm called Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005 [Cite_Ref] ) which is a language independent unsupervised machine learn-ing method to find morpheme-like units (called sta-tistical morphs) from a large text corpus. This method has several advantages over the rule-based grammatical morphemes, e.g. that no hand-crafted rules are needed and all words can be processed, even the foreign ones. Even if good grammatical morphemes are available, the language modeling re-sults by the statistical morphs seem to be at least as good, if not better (Hirsimäki et al., 2005). In this paper we evaluate the statistical morphs for three agglutinative languages and describe three different speech recognition systems that successfully utilize the n-gram language models trained for these units in the corresponding LVCSR tasks.",補足資料,Paper,True,Introduce（引用目的）,True,N06-1062_1_0,2006,Unlimited vocabulary speech recognition for agglutinative languages,Reference
568,10571," http://www.cis.hut.fi/projects/morpho/"," ['4 Speech recognition experiments', '4.4 Turkish']","The same Morfessor tool (Creutz and Lagus, 2005) [Cite_Ref] as in Finnish and Estonian was applied to Turkish texts as well.","Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.","Turkish is another a highly-inflected and agglutina-tive language with relatively free word order. The same Morfessor tool (Creutz and Lagus, 2005) [Cite_Ref] as in Finnish and Estonian was applied to Turkish texts as well. Using the 360k most common words from the training corpus, 34k morph units were obtained. The training corpus consists of approximately 27M words taken from literature, law, politics, social sciences, popular science, information technology, medicine, newspapers, magazines and sports news. N-gram language models for different orders with interpolated Kneser-Ney smoothing as well as en-tropy based pruning were built for this morph lexi-con using the SRILM toolkit (Stolcke, 2002). The number of n-grams for the highest order we tried (6-grams without entropy-based pruning) are reported in Table 4. In average, there are 2.37 morphs per word including the word break symbol.",Method,Tool,True,Use（引用目的）,True,N06-1062_1_1,2006,Unlimited vocabulary speech recognition for agglutinative languages,Reference
569,10572," http://www.csc.fi/kielipankki/"," ['4 Speech recognition experiments', '4.2 Finnish']","The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001) [Cite_Ref] .","CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other. http://www.csc.fi/kielipankki/.","Finnish is a highly inflected language, in which words are formed mainly by agglutination and com-pounding. Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001) [Cite_Ref] . We obtained a lexicon of 25k morphs by feeding the learning algorithm with the word list containing the 160k most common words. For language model training we used the same text corpus and the re-cently developed growing n-gram training algorithm (Siivola and Pellom, 2005). The amount of resulted n-grams are listed in Table 4. The average length of a morph is such that a word corresponds to 2.52 morphs including a word break symbol.",Material,Dataset,True,Use（引用目的）,True,N06-1062_2_0,2006,Unlimited vocabulary speech recognition for agglutinative languages,Reference
570,10573," http://sweaglesw.org/linguistics/ace/"," ['3 System Description']","This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3]","3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.","The new system described here is what we call the MRS Crawler. This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below). We used the grammar together with one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures. Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an ‘off the shelf’ setting. We combine our system with the outputs from the best-performing 2012 submission, the system of Read et al. (2012), firstly by relying on the latter for system negation cue detection, 4 and secondly as a fall-back in sys-tem combination as described in § 3.4 below.",Method,Tool,True,Use（引用目的）,True,P14-1007_0_0,2014,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,Footnote
571,10574," http://www.delph-in.net"," ['3 System Description']","This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3]","3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.","The new system described here is what we call the MRS Crawler. This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [Cite_Footnote_3] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below). We used the grammar together with one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures. Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an ‘off the shelf’ setting. We combine our system with the outputs from the best-performing 2012 submission, the system of Read et al. (2012), firstly by relying on the latter for system negation cue detection, 4 and secondly as a fall-back in sys-tem combination as described in § 3.4 below.",Method,Tool,True,Use（引用目的）,True,P14-1007_1_0,2014,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,Footnote
572,10575," http://www.delph-in.net/crawler/"," ['4 Experiments', '4.1 Data Sets']",All numbers reported here reflect this frozen system. [Cite_Footnote_10],"10 The code and data are available from http://www.delph-in.net/crawler/ , for replicability (Fokkens et al., 2013).","Being rule-based, our system does not require any training data per se. However, the majority of our rule development and error analysis were per-formed against the designated training data. We used the designated development data for a single final round of error analysis and corrections. The system was declared frozen before running with the formal evaluation data. All numbers reported here reflect this frozen system. [Cite_Footnote_10]",Mixed,Mixed,True,Produce（引用目的）,True,P14-1007_2_0,2014,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,Footnote
573,10576," http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip"," ['4 Experiment', '4.1 Experiment Setting']","The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset [Cite_Footnote_1] .",1 http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip,"The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset [Cite_Footnote_1] . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC. 94,651 unlabeled Chinese products re-views from corresponding categories are used as the development set, denoted as Dev_CHN. In the testing set, there are 12,000 Chinese product reviews (shown in Table.1). This dataset is de-signed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN : where c is either DVD, BOOK or MUSIC.",Material,Dataset,True,Use（引用目的）,True,P14-2139_0_0,2014,Cross-lingual Opinion Analysis via Negative Transfer Detection,Footnote
574,10577," http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf"," ['4 Experiment', '4.1 Experiment Setting']","The performance is evaluated by the correct classification accuracy for each category in Test_CHN [Cite_Footnote_2] : where c is either DVD, BOOK or MUSIC.",2 http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf,"The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC. 94,651 unlabeled Chinese products re-views from corresponding categories are used as the development set, denoted as Dev_CHN. In the testing set, there are 12,000 Chinese product reviews (shown in Table.1). This dataset is de-signed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN [Cite_Footnote_2] : where c is either DVD, BOOK or MUSIC.",Material,Knowledge,False,Use（引用目的）,True,P14-2139_1_0,2014,Cross-lingual Opinion Analysis via Negative Transfer Detection,Footnote
575,10578," https://translate.google.com"," ['4 Experiment', '4.1 Experiment Setting']","The Chinese word seg-mentation tool is ICTCLAS (Zhang et al, 2003) and Google Translator [Cite_Footnote_3] is the MT for the source language.",3 https://translate.google.com,"In the experiment, the basic transfer learning algorithm is co-training. The Chinese word seg-mentation tool is ICTCLAS (Zhang et al, 2003) and Google Translator [Cite_Footnote_3] is the MT for the source language. The monolingual opinion classifier is SVM light4 , word unigram/bigram features are em-ployed.",Method,Tool,True,Use（引用目的）,True,P14-2139_2_0,2014,Cross-lingual Opinion Analysis via Negative Transfer Detection,Footnote
576,10579," https://github.com/tud-fop/"," ['5 Evaluation and Conclusion']","The implementation is available as a part of Rus-tomata, [Cite_Footnote_5] a framework for weighted automata with storage written in the programming language Rust.",5 available on https://github.com/tud-fop/rustomata. We used commit 867a451 for evaluation.,"We implemented the parser with the modifica-tions sketched in sec. 4 for ε-free and simple wMCFGs, but no problems should arise gener-alising this implementation to arbitrary wMCFGs. The implementation is available as a part of Rus-tomata, [Cite_Footnote_5] a framework for weighted automata with storage written in the programming language Rust. We used the NeGra corpus (German newspaper articles, 20,602 sentences, 355,096 tokens; Skut et al., 1998) to compare our parser to Grammat-ical Framework (Angelov and Ljunglöf, 2014), rparse (Kallmeyer and Maier, 2013), and disco-dop (van Cranenburgh et al., 2016) with respect to parse time and accuracy. Our experiments were conducted on defoliated trees, i.e. we removed the leaves from each tree in the corpus. Parsing was performed on gold part-of-speech tags.",Method,Tool,True,Extend（引用目的）,False,N19-1016_0_0,2019,Implementation of a Chomsky-Schützenberger n-Best Parser for Weighted Multiple Context-Free Grammars,Footnote
577,10580," https://github.com/truprecht/rustomata-eval"," ['5 Evaluation and Conclusion']","We used the NeGra corpus (German newspaper articles, 20,602 sentences, 355,096 tokens; Skut et al., 1998) to compare our parser to Grammat-ical Framework (Angelov and Ljunglöf, 2014), rparse (Kallmeyer and Maier, 2013), and disco-dop (van Cranenburgh et al., 2016) with respect to parse time and accuracy. [Cite_Footnote_6]",6 The evaluation scripts are available on https://github.com/truprecht/rustomata-eval.,"We implemented the parser with the modifica-tions sketched in sec. 4 for ε-free and simple wMCFGs, but no problems should arise gener-alising this implementation to arbitrary wMCFGs. The implementation is available as a part of Rus-tomata, a framework for weighted automata with storage written in the programming language Rust. We used the NeGra corpus (German newspaper articles, 20,602 sentences, 355,096 tokens; Skut et al., 1998) to compare our parser to Grammat-ical Framework (Angelov and Ljunglöf, 2014), rparse (Kallmeyer and Maier, 2013), and disco-dop (van Cranenburgh et al., 2016) with respect to parse time and accuracy. [Cite_Footnote_6] Our experiments were conducted on defoliated trees, i.e. we removed the leaves from each tree in the corpus. Parsing was performed on gold part-of-speech tags.",補足資料,Document,True,Introduce（引用目的）,True,N19-1016_1_0,2019,Implementation of a Chomsky-Schützenberger n-Best Parser for Weighted Multiple Context-Free Grammars,Footnote
578,10581," https://github.com/TaoMiner/inferwiki"," ['References']",Our datasets can be found in [Cite] https://github.com/TaoMiner/inferwiki.,,"We present InferWiki, a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, as-sumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. To ensure it, we propose to utilize rule-guided train/test generation, in-stead of conventional random split. Second, InferWiki initiates the evaluation following the open-world assumption and improves the in-ferential difficulty of the closed-world assump-tion, by providing manually annotated nega-tive and unknown triples. Third, we include various inference patterns (e.g., reasoning path length and types) for comprehensive evalua-tion. In experiments, we curate two settings of InferWiki varying in sizes and structures, and apply the construction process on CoDEx as comparative datasets. The results and em-pirical analyses demonstrate the necessity and high-quality of InferWiki. Nevertheless, the performance gap among various inferential as-sumptions and patterns presents the difficulty and inspires future research direction. Our datasets can be found in [Cite] https://github.com/TaoMiner/inferwiki.",Material,Dataset,True,Produce（引用目的）,True,2021.acl-long.534_0_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Body
579,10582," https://www.wikidata.org/"," ['3 Dataset Design', '3.1 Data Preprocessing']",More and more studies utilize Wikidata [Cite_Footnote_1] as a knowledge resource due to its high quality and large quantity.,1 https://www.wikidata.org/,"More and more studies utilize Wikidata [Cite_Footnote_1] as a knowledge resource due to its high quality and large quantity. We utilize the September 2019 En-glish dump in experiments. Data preprocessing aims to define relation vocabulary and extract two sets of triples from Wikidata: a large one for rule mining T r and a relatively small one for dataset generation T d . The reason for using two sets is to avoid the leakage of rules. In other words, some frequent rules on the large set may be very few on the small set. The different distributions shall avoid that rule mining methods will easily achieve high performance. Besides, more triples can improve the quality of mined rules. In contrast, the relatively small set is enough for efficient KGC training and evaluation.",Material,Dataset,True,Introduce（引用目的）,True,2021.acl-long.534_1_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Footnote
580,10583," http://web.informatik.uni-mannheim.de/AnyBURL/"," ['3 Dataset Design', '3.1 Data Preprocessing']","We utilize AnyBURL [Cite_Footnote_2] in experiments due to its ticularly, we follow the suggested configuration of AnyBURL.",2 http://web.informatik.uni-mannheim.de/AnyBURL/,"3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Galárraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL [Cite_Footnote_2] in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets λ p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation — most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop 8 , and AnyBURL 9 . Because we utilize various",Method,Tool,True,Use（引用目的）,True,2021.acl-long.534_2_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Footnote
581,10584," https://www.procyclingstats.com/team/bahrain-merida-2019"," ['A Literature Review']","• Professional websites: To annotate the triple (Bahrain-Merida 2019, hasPart, Carlos Betancur), you may search the person in professional websites, such as [Cite] https://www.procyclingstats.com/team/bahrain-merida-2019.",,"• Professional websites: To annotate the triple (Bahrain-Merida 2019, hasPart, Carlos Betancur), you may search the person in professional websites, such as [Cite] https://www.procyclingstats.com/team/bahrain-merida-2019. Since there is no Carlos Betancur listed in that website, please choose false.",補足資料,Website,True,Introduce（引用目的）,True,2021.acl-long.534_3_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Body
582,10585," https://github.com/thunlp/OpenKE"," ['F Experiment Setup']","We use OpenKE [Cite_Footnote_5] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters.",5 https://github.com/thunlp/OpenKE,"Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [Cite_Footnote_5] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data. We then search the embedding size in the range of {256, 512}, number of negative samples in the range of {15, 25} and margin in the range of {4, 8}. The optimal parameters of each model on all of three datasets are listed in Table 10. The thresholds in triples classification are listed in Ta-ble 11",Method,Code,False,Use（引用目的）,True,2021.acl-long.534_4_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Footnote
583,10586," https://github.com/TimDettmers/ConvE"," ['F Experiment Setup']","We use OpenKE for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE [Cite_Footnote_6] , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters.",6 https://github.com/TimDettmers/ConvE,"Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE [Cite_Footnote_6] , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data. We then search the embedding size in the range of {256, 512}, number of negative samples in the range of {15, 25} and margin in the range of {4, 8}. The optimal parameters of each model on all of three datasets are listed in Table 10. The thresholds in triples classification are listed in Ta-ble 11",Method,Code,False,Use（引用目的）,True,2021.acl-long.534_5_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Footnote
584,10587," https://github.com/salesforce/MultiHopKG"," ['F Experiment Setup']","Note that the patterns 7 https://github.com/ibalazevic/TuckER hop [Cite_Footnote_8] , and AnyBURL .",8 https://github.com/salesforce/ MultiHopKG,"3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Galárraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL 2 in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets λ p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation — most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop [Cite_Footnote_8] , and AnyBURL . Because we utilize various",Method,Code,False,Use（引用目的）,True,2021.acl-long.534_7_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Footnote
585,10588," http://web.informatik.uni-mannheim.de/AnyBURL/"," ['F Experiment Setup']","Note that the patterns 7 https://github.com/ibalazevic/TuckER hop , and AnyBURL [Cite_Footnote_9] .",9 http://web.informatik.uni-mannheim.de/AnyBURL/,"3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Galárraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL 2 in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets λ p > 0.1 have been selected as the rule set to guide dataset construction. 3.3 Rule-guided Dataset Construction Different from existing benchmarks, InferWiki pro-vides inferential testing triples with supportive data in the training set. Moreover, it aims to include as many inference patterns as possible and these pat-terns are better evenly distributed to avoid biased evaluation. Thus, this step has four objectives: rule-guided split, path extension, negative supplement, and inference pattern balance. Rule-guided Split grounds the mined rules F on triples T d to obtain premise triples and correspond-ing conclusion triples. All premise triples form a training set, and all conclusion triples form a test set. Thus, they are naturally guaranteed to be in-ferential. For correctness, all of premise triples to 7,050. This agree with the original paper that reports 20.56% triples are symmetry or compo-sitional through AMIE+ analysis. We find more paths due to more extensive rules extracted from a large set of triples. This also demonstrates the ne-cessity of rule-guided train/test generation — most test triples are not guaranteed inferential when us-ing random split. Relation Pattern Following convention, we count reasoning paths for various patterns: symmetry, in-version, hierarchy, composition, and others, whose detailed explanations and examples can be found in Appendix C. If a triple has multiple paths, we count all of them. As Figure 1 shows, we can see that (1) there are no inversion and only a few sym-metry and hierarchy patterns in CoDEx-m, as most current datasets remove them to avoid train/test leakage. But, we argue that learning and remem-bering such patterns are also an essential capacity of inference. It just needs to control their numbers for a fair comparison. (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop , and AnyBURL [Cite_Footnote_9] . Because we utilize various",Method,Code,False,Use（引用目的）,True,2021.acl-long.534_8_0,2021,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,Footnote
586,10589," https://github.com/kakaoenterprise/KorAdvMRSTestData"," ['2 Adversarial Test Dataset']",We release this data set at [Cite] https: //github.com/kakaoenterprise/ KorAdvMRSTestData.,,"Five annotators generate a total of 200 dialogue sessions. For each session i, annotators create two correct responses and an arbitrary number(M i ) of incorrect responses based on the instruction described above. All sessions and responses are reviewed and filtered by experts. We set up one test case to consist of context, one correct response, and one incorrect response. Therefore, 2 ∗ M i test cases were extracted for each session, and a total of 2,220 test cases are constructed. It evaluates whether the model gives the correct answer a higher score than the incorrect one for a given context. Statistics and examples are described in Table 1. We release this data set at [Cite] https: //github.com/kakaoenterprise/ KorAdvMRSTestData.",Material,Dataset,True,Produce（引用目的）,True,2021.emnlp-main.180_0_0,2021,An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model,Body
587,10590," https://corpus.korean.go.kr"," ['4 Experiments and Results', '4.1 Experiment Setup']",We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [Cite_Footnote_1] .,"1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285–294.","We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [Cite_Footnote_1] . We split these corpora into three, and each is for training, validation, and test. Statistics of each dataset are described in Table 2. #pairs denote the number of context-response pairs, #cands denotes the number of candidates per context, pos:neg denotes the ratio of positive and negative responses in candidates, and #turns de-note the average turns per context. Details on the construction are as follows.",Material,Dataset,True,Extend（引用目的）,True,2021.emnlp-main.180_1_0,2021,An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model,Footnote
588,10591," http://groups.csail.mit.edu/nlp/dpo3/"," ['3. A free distribution of our implementation. 2']",A free distribution of our implementation. [Cite_Footnote_2],2 http://groups.csail.mit.edu/nlp/dpo3/,3. A free distribution of our implementation. [Cite_Footnote_2],Method,Tool,True,Produce（引用目的）,True,P10-1001_0_0,2010,Efficient Third-order Dependency Parsers,Footnote
589,10592," https://github.com/adhigunasurya/distillation_parser.git"," ['1 Introduction']",The code to reproduce our results is pub-licly available. [Cite_Footnote_1],1 https://github.com/adhigunasurya/distillation_parser.git,"The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the con-ventional Hamming cost function, (ii) recently pub-lished strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graph-based dependency parsing for English, Chinese, and German. The code to reproduce our results is pub-licly available. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D16-1180_0_0,2016,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,Footnote
590,10593," https://github.com/clab/lstm-parser"," ['3 Consensus and Minimum Bayes Risk']","As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [Cite_Footnote_3]","3 We use the standard data split (02–21 for training, 22 for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.","Next, note that if we let s(h, m, x) = votes(h, m)/N, this has no effect on the parser (we have only scaled by a constant factor). We can there-fore view s as a posterior marginal, and the ensemble parser as an MBR parser (Eq. 2). Experiment. We consider this approach on the Stanford dependencies version 3.3.0 (De Marneffe and Manning, 2008) Penn Treebank task. As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [Cite_Footnote_3]",Method,Tool,True,Introduce（引用目的）,False,D16-1180_1_0,2016,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,Footnote
591,10594," https://github.com/clab/cnn.git"," ['6 Experiments']",For the Adam optimizer we use the default settings in the CNN neural network library. [Cite_Footnote_8],8 https://github.com/clab/cnn.git,"Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library. [Cite_Footnote_8] Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncer-tainty under the ensemble. To obtain the ensemble uncertainty on each language, we use 21 base mod-els for English (see footnote 4), 17 for Chinese, and 11 for German.",Method,Code,True,Use（引用目的）,True,D16-1180_2_0,2016,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,Footnote
592,10595," http://lex4all.github.io/lex4all/"," ['1 Introduction']","This is the motivation behind lex4all, [Cite_Footnote_1] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL.",1 http://lex4all.github.io/lex4all/,"This is the motivation behind lex4all, [Cite_Footnote_1] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.",Material,Dataset,True,Produce（引用目的）,True,P14-5019_0_0,2014,lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition,Footnote
593,10596," http://msdn.microsoft.com/en-us/library/hh361572"," ['2 Background and related work']","If the target language is supported by the system – the Microsoft Speech Platform, [Cite_Footnote_2] for example, supports over 20 languages – this makes it very easy to create speech-driven applications.",2 http://msdn.microsoft.com/en-us/library/hh361572,"Several commercial speech recognition systems offer high-level Application Programming Inter-faces (APIs) that make it extremely simple to add voice interfaces to an application, requiring very little general technical expertise and virtually no knowledge of the inner workings of the recogni-tion engine. If the target language is supported by the system – the Microsoft Speech Platform, [Cite_Footnote_2] for example, supports over 20 languages – this makes it very easy to create speech-driven applications.",補足資料,Website,True,Introduce（引用目的）,True,P14-5019_1_0,2014,lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition,Footnote
594,10597," http://www.cmusphinx.org"," ['2 Background and related work']","Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx [Cite_Footnote_3] ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs.",3 http://www.cmusphinx.org,"If, however, the target language is one of the many thousands of LRLs for which high-quality recognition engines have not yet been devel-oped, alternative strategies for developing speech-recognition interfaces must be employed. Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx [Cite_Footnote_3] ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit 4 (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low-or zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users.",Method,Tool,True,Introduce（引用目的）,True,P14-5019_2_0,2014,lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition,Footnote
595,10598," http://i19pc5.ira.uka.de/rlat-dev"," ['2 Background and related work']","In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit [Cite_Footnote_4] (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010).",4 http://i19pc5.ira.uka.de/rlat-dev,"If, however, the target language is one of the many thousands of LRLs for which high-quality recognition engines have not yet been devel-oped, alternative strategies for developing speech-recognition interfaces must be employed. Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx 3 ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit [Cite_Footnote_4] (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low-or zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users.",Method,Tool,True,Introduce（引用目的）,True,P14-5019_3_0,2014,lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition,Footnote
596,10599," http://github.com/lex4all/lex4all"," ['3 System overview']",The application and its source code are freely available via GitHub. [Cite_Footnote_6],6 http://github.com/lex4all/lex4all,"We have developed lex4all as a desktop applica-tion for Microsoft Windows, since it relies on the Microsoft Speech Platform (MSP) as explained in Section 4.1. The application and its source code are freely available via GitHub. [Cite_Footnote_6]",Mixed,Mixed,True,Produce（引用目的）,True,P14-5019_4_0,2014,lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition,Footnote
597,10600," http://www.w3.org/TR/pronunciation-lexicon/"," ['3 System overview']","Once pronunciations for all terms in the vocab-ulary have been generated, the application outputs a pronunciation lexicon for the given terms as an XML file conforming to the Pronunciation Lexi-con Specification. [Cite_Footnote_7]",7 http://www.w3.org/TR/pronunciation-lexicon/,"Once pronunciations for all terms in the vocab-ulary have been generated, the application outputs a pronunciation lexicon for the given terms as an XML file conforming to the Pronunciation Lexi-con Specification. [Cite_Footnote_7] This lexicon can then be di-rectly included in a speech recognition application built using the MSP API or a similar toolkit.",補足資料,Document,True,Use（引用目的）,True,P14-5019_5_0,2014,lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition,Footnote
598,10601," http://naudio.codeplex.com/"," ['5 User interface', '5.1 Audio input and recording']","The recorder, built using the open-source library NAudio, [Cite_Footnote_8] takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1).",8 http://naudio.codeplex.com/,"The recorder, built using the open-source library NAudio, [Cite_Footnote_8] takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1).",Method,Code,True,Extend（引用目的）,True,P14-5019_6_0,2014,lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition,Footnote
599,10602," https://ai.baidu.com/broad/subordinate?dataset=duconv"," ['4 Experiments', '4.1 Datasets']",The DuConv dataset [Cite_Footnote_2] : a knowledge graph en-hanced conversation dataset in Chinese proposed by Wu et al. (2019).,2 https://ai.baidu.com/broad/subordinate?dataset=duconv,"The DuConv dataset [Cite_Footnote_2] : a knowledge graph en-hanced conversation dataset in Chinese proposed by Wu et al. (2019). It has 29,858 dialogues and 270,399 utterances in the domain of Movies. DuConv constructs the knowledge graph with the information crawled from a movie website as the external knowledge, which contains 3,598,246 fact triples over 143,627 entities and 45 relations. How-ever, only the training data is released with the knowledge information, which contains 19,858 dia-logues. After filtering the noisy data, we randomly split the corpus into the train (80%), validation (10%), and test sets (10%). The test set consists of the seen test set (5%) and the unseen test set (5%), where the former contains the knowledge graphs that appeared during the training process, and the latter contains the knowledge graphs, of which the subject entities and most of the object entities are unseen in the training process. The statistics is shown in Table 2.",補足資料,Media,True,Introduce（引用目的）,True,2021.emnlp-main.184_0_0,2021,EARL: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning,Footnote
600,10603," https://github.com/facebookresearch/opendialkg"," ['4 Experiments', '4.1 Datasets']",The OpenDialKG dataset [Cite_Footnote_3] : a knowledge graph enhanced conversation dataset in English proposed by Moon et al. (2019).,3 https://github.com/facebookresearch/opendialkg,"The OpenDialKG dataset [Cite_Footnote_3] : a knowledge graph enhanced conversation dataset in English proposed by Moon et al. (2019). It has 15,673 dia-logues and 91,209 utterances in four domains, in-cluding Movies, Books, Sports, and Music. Open-DialKG uses the Freebase (Bast et al., 2014) knowl-edge graph as the external knowledge, which con-tains 1,190,658 fact triples over top 100,813 enti-ties and 1,358 relations. However, the released data only consists of 13,776 dialogues, which contains some noisy data, e.g. empty utterances in the dia-logue. After filtering the noisy data, we randomly split the corpus in the same way as DuConv. The statistics is presented in Table 2.",補足資料,Media,True,Introduce（引用目的）,True,2021.emnlp-main.184_1_0,2021,EARL: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning,Footnote
601,10604," https://github.com/thu-coai/earl"," ['4 Experiments', '4.3 Implementation Details']",[Cite] https://github.com/thu-coai/earl.,,"We used the stochastic gradient descent (SGD) algorithm with mini-batch. The batch size and learning rate are set to 100 and 0.5, respectively. The model was run at most 20 epochs, and the training stage of each model took about one day on a GPU machine. We selected the model performing best in the validation set to evaluate in test sets. Our code is available at: [Cite] https://github.com/thu-coai/earl.",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.184_2_0,2021,EARL: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning,Body
602,10605," https://github.com/nyu-mll/msgs"," ['1 Introduction']","To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: [Cite] https://github.com/nyu-mll/msgs.",,"We conclude that helpful inductive biases can be learned through pretraining, but current mod-els require abundant data to do so. The implica-tions of this conclusion point in two directions: First, we can probably continue to pretrain on increasingly massive training sets to improve on the generalization and few-shot learning abilities of models like T5 (Raffel et al., 2019) and GPT-3 (Brown et al., 2020). Second, since models learn useful features early, there is hope that fu-ture advances could accelerate by reducing the amount of data needed to learn which features mat-ter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: [Cite] https://github.com/nyu-mll/msgs.",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.16_0_0,2020,Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),Body
603,10606," https://dumps.wikimedia.org/mirrors.html"," ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']","The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [Cite_Footnote_5]","5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.","Pretraining Data We pretrain RoBERTa using scaled-down recreations of the dataset used by Devlin et al. (2019) to train BERT, i.e English Wikipedia (2.5 billion tokens) and BookCorpus (800 million tokens). Both are included in the RoBERTa pretraining data. We download the lat-est Wikipedia dump as of Feb 1, 2020. The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [Cite_Footnote_5]",Mixed,Mixed,True,Use（引用目的）,True,2020.emnlp-main.16_1_0,2020,Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),Footnote
604,10607," https://github.com/attardi/wikiextractor"," ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']","The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [Cite_Footnote_5]","5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.","Pretraining Data We pretrain RoBERTa using scaled-down recreations of the dataset used by Devlin et al. (2019) to train BERT, i.e English Wikipedia (2.5 billion tokens) and BookCorpus (800 million tokens). Both are included in the RoBERTa pretraining data. We download the lat-est Wikipedia dump as of Feb 1, 2020. The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [Cite_Footnote_5]",Mixed,Mixed,True,Use（引用目的）,True,2020.emnlp-main.16_2_0,2020,Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),Footnote
605,10608," https://github.com/soskek/bookcorpus"," ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']","The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [Cite_Footnote_5]","5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.","Pretraining Data We pretrain RoBERTa using scaled-down recreations of the dataset used by Devlin et al. (2019) to train BERT, i.e English Wikipedia (2.5 billion tokens) and BookCorpus (800 million tokens). Both are included in the RoBERTa pretraining data. We download the lat-est Wikipedia dump as of Feb 1, 2020. The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [Cite_Footnote_5]",Mixed,Mixed,True,Use（引用目的）,True,2020.emnlp-main.16_3_0,2020,Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),Footnote
606,10609," https://github.com/jemmryx/EQG-RACE"," ['4 Experiment', '4.1 Experiment Setting']",We carry out the training and inference on EQG-RACE dataset [Cite_Footnote_1] proposed by Jia et al. (2020).,1 https://github.com/jemmryx/EQG-RACE,"We carry out the training and inference on EQG-RACE dataset [Cite_Footnote_1] proposed by Jia et al. (2020). The passage numbers of training set, validation set and test set are respectively 11457, 642, 609.",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.202_0_0,2021,Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data,Footnote
607,10610," http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html"," ['2 Japanese Morphology']","In order to understand the task of lexicon acquisi-tion, we briefly describe the Japanese morpholog-ical analyzer JUMAN. [Cite_Footnote_1]",1 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html,"In order to understand the task of lexicon acquisi-tion, we briefly describe the Japanese morpholog-ical analyzer JUMAN. [Cite_Footnote_1] We explain Japanese mor-phemes in Section 2.1, morphological constraints in Section 2.2, and unknown morpheme processing in Section 2.3.",Method,Tool,True,Introduce（引用目的）,True,D08-1045_0_0,2008,Online Acquisition of Japanese Unknown Morphemes using Morphological Constraints,Footnote
608,10611," http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html"," ['3 Lexicon Acquisition', '3.2 System Architecture']",Each sen-tence in texts is processed by the morphological an-alyzer JUMAN and the dependency parser KNP. [Cite_Footnote_2],2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html,Figure 1 shows the system architecture. Each sen-tence in texts is processed by the morphological an-alyzer JUMAN and the dependency parser KNP. [Cite_Footnote_2] JUMAN consults a hand-crafted dictionary and an automatically constructed dictionary. KNP is used to form a phrasal unit called bunsetsu by chunking morphemes.,Method,Tool,True,Use（引用目的）,True,D08-1045_1_0,2008,Online Acquisition of Japanese Unknown Morphemes using Morphological Constraints,Footnote
609,10612," https://github.com/mmihaltz/word2vec-GoogleNews-vectors"," ['6 Experiments', '6.3 Training Details']",We use the open-domain word embeddings [Cite_Footnote_1] for the initialization of word vectors.,1 https://github.com/mmihaltz/word2vec-GoogleNews-vectors,"We use the open-domain word embeddings [Cite_Footnote_1] for the initialization of word vectors. We initialize other model parameters from a uniform distribu-tion U(-0.05, 0.05). The dimension of the word embedding and the size of the hidden layers are 300. The learning rate is set to 0.01 and the dropout rate is set to 0.1. Stochastic gradient de-scent is used as our optimizer. The position encod-ing is also used (Tang et al., 2016). We also com-pare the memory networks in their multiple com-putational layers version (i.e., multiple hops) and the number of hops is set to 3 as used in the men-tioned previous studies. We implemented all mod-els in the TensorFlow environment using same in-put, embedding size, dropout rate, optimizer, etc. so as to test our hypotheses, i.e., to make sure the achieved improvements do not come from else-where. Meanwhile, we can also report all evalua-tion measures discussed above 2 . 10% of the train-ing data is used as the development set. We report the best results for all models based on their F-1 Macro scores.",Method,Code,True,Use（引用目的）,True,P18-1088_0_0,2018,Target-Sensitive Memory Networks for Aspect Sentiment Classification,Footnote
610,10613," http://www.metaversum.com/"," ['1 Introduction']","The KomParse system, described in this paper, provides NPCs for a virtual online world named Twinity, a product of the Berlin startup company Metaversum [Cite_Footnote_1] .",1 http://www.metaversum.com/,"The KomParse system, described in this paper, provides NPCs for a virtual online world named Twinity, a product of the Berlin startup company Metaversum [Cite_Footnote_1] . The KomParse NPCs offer vari-ous services through conversation with game users using question-answering and dialog functional-ity. The utilization of Semantic Web technology with RDF-encoded generic and domain-specific ontologies furthermore enables semantic search and inference.",補足資料,Website,True,Introduce（引用目的）,True,P10-4007_0_0,2010,Talking NPCs in a Virtual Game World,Footnote
611,10614," http://protege.stanford.edu/"," ['3 Knowledge Representation and Semantic Inference']",Our on-tologies are defined by the freely available ontol-ogy editor Protégé 4.0 [Cite_Footnote_2] .,"2 http://protege.stanford.edu/, as accessed 27 Oct 2009","The concepts and individuals of the particular domain are structured and organized in domain-specific ontologies. These ontologies are mod-elled in the Web Ontology Language (OWL). OWL allows us to define concept hierarchies, re-lations between concepts, domains and ranges of these relations, as well as specific relation in-stances between instances of a concept. Our on-tologies are defined by the freely available ontol-ogy editor Protégé 4.0 [Cite_Footnote_2] . The advantage of using an ontology for structuring the domain knowledge is the modular non-redundant encoding. When com-bined with a reasoner, only a few statements about an individual have to be asserted explicitely, while the rest can be inferred from the ontology. We em-ploy several ontologies, among which the follow-ing are relevant for modelling the specific domains of our NPCs:",Method,Tool,True,Use（引用目的）,True,P10-4007_1_0,2010,Talking NPCs in a Virtual Game World,Footnote
612,10615," http://www.ontotext.com/owlim/"," ['3 Knowledge Representation and Semantic Inference']",We use SwiftOwlim [Cite_Footnote_3] for storing and querying the data.,3 http://www.ontotext.com/owlim/,"We use SwiftOwlim [Cite_Footnote_3] for storing and querying the data. SwiftOwlim is a “triple store”, a kind of database which is specifically built for storing and querying RDF data. It provides a forward-chaining inference engine which evaluates the domain definitions when loading the knowledge repository, and makes implicit knowledge explicit by asserting triples that must also hold true accord-ing to the ontology. Once the reasoner is finished, the triple store can be queried directly using the RDF query language SPARQL.",Method,Tool,True,Use（引用目的）,True,P10-4007_2_0,2010,Talking NPCs in a Virtual Game World,Footnote
613,10616," https://github.com/tingc9/LinearTemplatesSRW"," ['9 Results']","On a reasonably modern machine, our implementation generated the above sentences in about 150 seconds while using 2.2 GB of memory. [Cite_Footnote_2]",2 The code used for this research will be made available on https://github.com/tingc9/LinearTemplatesSRW,"Computationally, the search time increases with increasing sentence lengths. On a reasonably modern machine, our implementation generated the above sentences in about 150 seconds while using 2.2 GB of memory. [Cite_Footnote_2]",Method,Code,True,Produce（引用目的）,True,P18-3017_0_0,2018,Exploring Chunk Based Templates for Generating a subset of English Text,Footnote
614,10617," https://github.com/najoungkim/pdtb3"," ['3 Proposed Evaluation Protocol']","We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [Cite_Footnote_4]",4 https://github.com/najoungkim/pdtb3,"While Xue et al. (2015) lay out one possible pro-tocol, it does not fully address the issues we have raised in Section 2. Another limitation is the un-availability of the preprocessing code as of the date of this submission. We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [Cite_Footnote_4] In addition to accounting for the variation previously discussed, we take Shi and Demberg (2017)’s concerns into consideration.",Method,Code,True,Use（引用目的）,False,2020.acl-main.480_0_0,2020,Implicit Discourse Relation Classification: We Need to Talk about Evaluation,Footnote
615,10618," https://github.com/huggingface/pytorch-transformers"," ['3 Proposed Evaluation Protocol', '3.1 Baseline results']","Following our proposed protocol, we report base-line results from two strong sentence encoder mod-els: BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), using a publicly available codebase. [Cite_Footnote_5]",5 https://github.com/huggingface/ pytorch-transformers,"Following our proposed protocol, we report base-line results from two strong sentence encoder mod-els: BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), using a publicly available codebase. [Cite_Footnote_5] See Appendix C for training details. We present L2 results on PDTB 2.0 in Table 1 and results on PDTB 3.0 in Table 2 (see Appendix D for L1 re-sults). To maintain backwards compatibility to the literature, we also report PDTB 2.0 results on Ji, Lin and P&K splits (see Section 2.1). Ji & Lin are the most common splits, and P&K is the split used by Nie et al. (2019) who claim the current state-of-the-art for L2. For PDTB 2.0 (Table 1), our baselines showed strong performance on all splits. XLNet-large was the single best model, signifi-cantly outperforming every best reported result.",Method,Code,True,Use（引用目的）,True,2020.acl-main.480_1_0,2020,Implicit Discourse Relation Classification: We Need to Talk about Evaluation,Footnote
616,10619," http://www.statmt.org/wmt15/"," ['4 Experimental Study', '4.1 Experimental Setting']","We use WMT-15 corpora [Cite_Footnote_1] to train the models, newstest-2013 for tuning and newstest-2015 as the test sets.",1 http://www.statmt.org/wmt15/,"In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochas-tic gradient descent with Adam (Kingma and Ba, 2015). Chung et al. (2016) and Sennrich et al. (2016) demonstrated that bpe boosts NMT, so sim-ilar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora [Cite_Footnote_1] to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English–Turkish (En–Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016). The training side of the English–German (En–De), English–Russian (En– Ru), and En–Tr corpora include 4.5, 2.1, and 4 million parallel sentences, respectively. We ran-domly select 3K sentences for each of the develop-ment and test sets for En–Tr. For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character.",Material,Dataset,True,Use（引用目的）,True,N18-1006_0_0,2018,Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation,Footnote
617,10620," http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93T3A"," ['4 Experiments', '4.1 Data']",We take a subset of the TIPSTER [Cite_Footnote_3] corpus – all Wall Street Journal articles from the period of 1987-92 (approx.,3 Description at http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93T3A.,"We take a subset of the TIPSTER [Cite_Footnote_3] corpus – all Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words) – and automatically anno-tate them with sentence boundaries, part of speech tags and dependency relations using the Stanford parser (Klein & Manning, 2003). We reserve a small subset of about 600 articles (340,000 words) for testing and use the rest to build a trigram LM with the CMU toolkit (Clarkson & Rosenfeld, 1997, with Good-Turing smoothing and vocabulary size of 30,000). To train the maximum entropy classifiers we use about 41,000 sentences.",Material,Dataset,True,Use（引用目的）,True,N09-2057_0_0,2009,Tree Linearization in English: Improving Language Model Based Approaches,Footnote
618,10621," https://aip-nlu.gitlab.io/resources/sas-japanese"," ['5 Experiments', '5.1 Dataset']",We use the Japanese short answer scoring dataset [Cite_Footnote_1] introduced by Mizumoto et al. (2019).,1 https://aip-nlu.gitlab.io/resources/ sas-japanese,"We use the Japanese short answer scoring dataset [Cite_Footnote_1] introduced by Mizumoto et al. (2019). The dataset consists of six prompts. Each prompt has its rubric, student responses, and scores. The prompts, rubrics, and student responses in the dataset were collected from the examinations conducted by a Japanese education company, Takamiya Gakuen Yoyogi Seminar. Each response was manually scored using the multiple analytic criteria for the prompt, and the subscore for each criterion was rated individually on the basis of the correspond-ing rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response.",Material,Dataset,True,Use（引用目的）,True,2020.acl-srw.32_0_0,2020,Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation,Footnote
619,10622," https://github.com/cl-tohoku/bert-japanese"," ['5 Experiments', '5.2 Settings']","We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [Cite_Footnote_3]","3 We adopted pretrained character-based BERT which is known to be suitable for processing Japanese texts. This is available at https://github.com/cl-tohoku/bert-japanese. for Low-Resource NLP, pages 175–182.","We split the dataset into training data (1, 600), val-idation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [Cite_Footnote_3] We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set. We trained five models with different random seeds and reported the aver-age of the results.",Material,Dataset,False,Use（引用目的）,False,2020.acl-srw.32_1_0,2020,Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation,Footnote
620,10623," https://sites.google.com/site/xianchaowu2012"," ['1 Introduction']","The Akamon system [Cite_Footnote_2] , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding (Liu et al., 2006; Mi et al., 2008).",2 Code available at https://sites.google.com/site/xianchaowu2012,"However, few tree/forest-to-string systems have been made open source and this makes it diffi-cult and time-consuming to testify and follow exist-ing proposals involved in recently published papers. The Akamon system [Cite_Footnote_2] , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding (Liu et al., 2006; Mi et al., 2008). We hope this system will help re-lated researchers to catch up with the achievements of tree/forest-based translations in the past several years without re-implementing the systems or gen-eral algorithms from scratch.",Method,Code,True,Produce（引用目的）,True,P12-3022_0_0,2012,Akamon: An Open Source Toolkit for Tree/Forest-Based Statistical Machine Translation ∗,Footnote
621,10624," http://www.kecl.ntt.co.jp/icl/lirg/ribes"," ['2 Akamon Toolkit Features']","Besides BLEU and NIST score, we further list RIBES score [Cite_Footnote_3] , , i.e., the software imple-mentation of Normalized Kendall’s τ as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes.",3 Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes,"Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score [Cite_Footnote_3] , , i.e., the software imple-mentation of Normalized Kendall’s τ as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes.",Method,Code,True,Introduce（引用目的）,True,P12-3022_1_0,2012,Akamon: An Open Source Toolkit for Tree/Forest-Based Statistical Machine Translation ∗,Footnote
622,10625," http://translate.google.com/"," ['2 Akamon Toolkit Features']","For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system [Cite_Footnote_4] are also listed in this table.",4 http://translate.google.com/,"Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT sys-tem. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system [Cite_Footnote_4] are also listed in this table.",Method,Tool,True,Use（引用目的）,False,P12-3022_2_0,2012,Akamon: An Open Source Toolkit for Tree/Forest-Based Statistical Machine Translation ∗,Footnote
623,10626," http://www.speech.sri.com/projects/srilm/"," ['2 Akamon Toolkit Features']","• language models: Akamon can make use of one or many n-gram language models trained by using SRILM [Cite_Footnote_5] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011).",5 http://www.speech.sri.com/projects/srilm/,"• language models: Akamon can make use of one or many n-gram language models trained by using SRILM [Cite_Footnote_5] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).",Method,Tool,True,Use（引用目的）,True,P12-3022_3_0,2012,Akamon: An Open Source Toolkit for Tree/Forest-Based Statistical Machine Translation ∗,Footnote
624,10627," http://code.google.com/p/berkeleylm/"," ['2 Akamon Toolkit Features']","• language models: Akamon can make use of one or many n-gram language models trained by using SRILM (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 [Cite_Footnote_6] (Pauls and Klein, 2011).",6 http://code.google.com/p/berkeleylm/,"• language models: Akamon can make use of one or many n-gram language models trained by using SRILM (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 [Cite_Footnote_6] (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).",Method,Tool,True,Use（引用目的）,True,P12-3022_4_0,2012,Akamon: An Open Source Toolkit for Tree/Forest-Based Statistical Machine Translation ∗,Footnote
625,10628," http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html"," ['4 Using Deep Syntactic Structures']","Similarly, Enju [Cite_Footnote_8] , a state-of-the-art and freely available HPSG parser for English, can be used to generate packed parse forests for source sentences .",8 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html,"In Akamon, we support the usage of deep syn-tactic structures for obtaining fine-grained transla-tion rules as described in our former work (Wu et al., 2010) . Similarly, Enju [Cite_Footnote_8] , a state-of-the-art and freely available HPSG parser for English, can be used to generate packed parse forests for source sentences . Deep syntactic structures are included in the HPSG trees/forests, which includes a fine-grained description of the syntactic property and a semantic representation of the sentence. We extract fine-grained rules from aligned HPSG forest-string pairs and use them in the forest-to-string decoder. The detailed algorithms can be found in (Wu et al., 2010; Wu et al., 2011a). Note that, in Akamon, we also provide the codes for generating HPSG forests from Enju.",Method,Tool,True,Introduce（引用目的）,True,P12-3022_5_0,2012,Akamon: An Open Source Toolkit for Tree/Forest-Based Statistical Machine Translation ∗,Footnote
626,10629," https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology"," ['6 Conclusion']",We make our code and predictions publicly available. [Cite_Footnote_4],4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology,"We show that training to imitate a simple ex-pert policy results in an effective neural transition-based model for morphological string transduc-tion. The fully end-to-end approach addresses var-ious shortcomings of previous training regimes (the need for an external character aligner, warm-start initialization, and MLE training biases), and leads to strong empirical results. We make our code and predictions publicly available. [Cite_Footnote_4]",Mixed,Mixed,True,Produce（引用目的）,True,D18-1314_0_0,2018,Imitation Learning for Neural Morphological String Transduction,Footnote
627,10630," https://github.com/Babylonpartners/corrsim"," ['5 Experiments']","The code for our experiments builds on the SentEval toolkit (Conneau and Kiela, 2018) and is available on GitHub [Cite_Footnote_1] .",1 https://github.com/Babylonpartners/ corrsim,"We now empirically demonstrate the power of the methods and statistical analysis presented in Sec-tion 4, through a set of evaluations on the Se-mantic Textual Similarity (STS) tasks series 2012- 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017). For methods involving pre-trained word embeddings, we use fastText (Bo-janowski et al., 2017) trained on Common Crawl (600B tokens), as previous evaluations have in-dicated that fastText vectors have uniformly the best performance on these tasks out of commonly-used pretrained unsupervised word vectors (Con-neau et al., 2017; Perone et al., 2018; Zhelezniak et al., 2019a,b). We provide experiments and sig-nificance analysis for additional word vector in the Appendix. The success metric for the STS tasks is the Pearson correlation between the sentence similarity scores provided by human annotators and the scores generated by a candidate algorithm. Note that the dataset for the STS13 SMT subtask is no longer publicly available, so the mean Pear-son correlation for STS13 reported in our exper-iments has been re-calculated accordingly. The code for our experiments builds on the SentEval toolkit (Conneau and Kiela, 2018) and is available on GitHub [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,D19-1008_0_0,2019,Correlations between Word Vector Sets,Footnote
628,10631," https://bert-as-service.github.com/hanxiao/bert-as-service"," ['5 Experiments']","Note that for BERT we evaluated all pooling strategies available in bert-as-service (Xiao, 2018) [Cite_Ref] applied to either the last or second-to-last layers and report results for the best-performing combi-nation, which was mean-pooling on the last layer for both model sizes.",Han Xiao. 2018. https://bert-as-service.github.com/hanxiao/bert-as-service.,"Note that for BERT we evaluated all pooling strategies available in bert-as-service (Xiao, 2018) [Cite_Ref] applied to either the last or second-to-last layers and report results for the best-performing combi-nation, which was mean-pooling on the last layer for both model sizes. Our results are presented in Table 1. We can clearly see that deep learning-based methods do not shine on STS tasks, while simple compositions of word vectors can perform extremely well, especially when an appropriate correlation coefficient is used as the similarity measure. Indeed, the performance of max-pooled vectors with Spearman correlation approaches or exceeds that of more expensive or offline methods like that of Arora et al. (2017), which performs PCA computations on the entire test set. Addi-tionally, while the multivariate correlation meth-ods such as CKA are more computationally ex-pensive than pooling-based approaches (see Ta-ble 2), they can provide performance boost on some tasks, making the cost worth it depending on the application. Finally, we conducted an ex-ploratory error analysis and found that many errors are due to the well-known inherent weaknesses of word embeddings. For example, the proposed ap-proaches heavily overestimate similarity when two sentences contain antonyms or when one sentence is the negation of the other. We illustrate these and other cases in the Appendix.",Method,Tool,False,Use（引用目的）,True,D19-1008_1_0,2019,Correlations between Word Vector Sets,Reference
629,10632," http://blog.twitter.com/2012/03/twitter-turns-six.html"," ['1 Introduction']",Twitter is perhaps the most popular such service with over 140 million active users as of 2012. [Cite_Footnote_1],1 For details see http://blog.twitter.com/2012/03/twitter-turns-six.html,"Online micro-blogging services have revolutionized the way people discover, share, and distribute infor-mation. Twitter is perhaps the most popular such service with over 140 million active users as of 2012. [Cite_Footnote_1] Twitter enables users to send and read text-based posts of up to 140 characters, known as tweets. Twitter users follow others or are followed. Being a follower on Twitter means that the user receives all the tweets from those she follows. Common prac-tice of responding to a tweet has evolved into a well-defined markup culture (e.g., RT stands for retweet, ‘@’ followed by an identifier indicates the user). The strict limit of 140 characters allows for quick and immediate communication in real time, whilst enforcing brevity. Moreover, the retweet mecha-nism empowers users to spread information of their choice beyond the reach of their original followers.",補足資料,Document,True,Introduce（引用目的）,True,P12-1054_0_0,2012,Tweet Recommendation with Graph Co-Ranking,Footnote
630,10633," http://blog.twitter.com/2011/03/numbers.html"," ['1 Introduction']",[Cite_Footnote_2] Twitter’s own search en-gine handles more than 1.6 billion search queries per day.,"2 In fact, the peak record is 6,939 tweets per second, reported by http://blog.twitter.com/2011/03/numbers.html.","Twitter has become a prominent broadcast-ing medium, taking priority over traditional news sources (Teevan et al., 2011). Shared information through this channel spreads faster than would have been possible with conventional news sites or RSS feeds and can reach a far wider population base. However, the proliferation of user-generated con-tent comes at a price. Over 340 millions of tweets are being generated daily amounting to thousands of tweets per second! [Cite_Footnote_2] Twitter’s own search en-gine handles more than 1.6 billion search queries per day. This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in. A hypothetical tweet recommendation system could alleviate this acute information overload, e.g., by limiting the stream of tweets to those of interest to the user, or by discovering intriguing content outside the user’s following network.",補足資料,Document,True,Introduce（引用目的）,True,P12-1054_1_0,2012,Tweet Recommendation with Graph Co-Ranking,Footnote
631,10634," http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html"," ['1 Introduction']",Twitter’s own search en-gine handles more than 1.6 billion search queries per day. [Cite_Footnote_3],3 See http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html,"Twitter has become a prominent broadcast-ing medium, taking priority over traditional news sources (Teevan et al., 2011). Shared information through this channel spreads faster than would have been possible with conventional news sites or RSS feeds and can reach a far wider population base. However, the proliferation of user-generated con-tent comes at a price. Over 340 millions of tweets are being generated daily amounting to thousands of tweets per second! Twitter’s own search en-gine handles more than 1.6 billion search queries per day. [Cite_Footnote_3] This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in. A hypothetical tweet recommendation system could alleviate this acute information overload, e.g., by limiting the stream of tweets to those of interest to the user, or by discovering intriguing content outside the user’s following network.",補足資料,Document,True,Introduce（引用目的）,True,P12-1054_2_0,2012,Tweet Recommendation with Graph Co-Ranking,Footnote
632,10635," http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html"," ['1 Introduction']",The resulting embeddings led to cross-lingual multi-label classifiers that achieved the highest re-ported scores on 10 out of these 11 languages. [Cite_Footnote_1],1 We provide the trained embeddings at http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html.,"On the TED corpus, we obtained general pur-pose multilingual embeddings for 11 target lan-guages, by considering the (auxiliary) task of reconstructing pre-trained English word vectors. The resulting embeddings led to cross-lingual multi-label classifiers that achieved the highest re-ported scores on 10 out of these 11 languages. [Cite_Footnote_1]",Material,Dataset,True,Produce（引用目的）,True,P16-1190_0_0,2016,Jointly Learning to Embed and Predict with Multiple Languages,Footnote
633,10636," https://github.com/dcferreira/multilingual-joint-embeddings"," ['5 Experiments']","We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus. [Cite_Footnote_5]",5 Our code is available at https: //github.com/dcferreira/ multilingual-joint-embeddings.,"We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus. [Cite_Footnote_5]",Method,Code,True,Produce（引用目的）,True,P16-1190_1_0,2016,Jointly Learning to Embed and Predict with Multiple Languages,Footnote
634,10637," http://www.clg.ox.ac.uk/tedcorpus"," ['5 Experiments', '5.2 TED Corpus']","To assess the ability of our framework to han-dle multiple target languages, we ran a second set of experiments on the TED corpus (Cettolo et al., 2012), using the training and test parti-tions created by Hermann and Blunsom (2014), downloaded from [Cite] http://www.clg.ox.ac.",,"To assess the ability of our framework to han-dle multiple target languages, we ran a second set of experiments on the TED corpus (Cettolo et al., 2012), using the training and test parti-tions created by Hermann and Blunsom (2014), downloaded from [Cite] http://www.clg.ox.ac.uk/tedcorpus. The corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference in 12 different languages, with 12,078 parallel docu-ments in the training partition (totalling 1,641,985 parallel sentences). Following their prior work, we used this corpus both as parallel data (D u ) and as the task dataset (D l ). There are L = 15 labels and documents can have multiple labels.",Material,Knowledge,False,Extend（引用目的）,False,P16-1190_2_0,2016,Jointly Learning to Embed and Predict with Multiple Languages,Body
635,10638," http://nlp.stanford.edu/projects/glove/"," ['5 Experiments', '5.2 TED Corpus']","For the Joint w/ Aux strategy, we used the 300-dimensional GloVe-840B vectors (Penning-ton et al., 2014), downloaded from [Cite] http://nlp.stanford.edu/projects/glove/.",,"For the Joint w/ Aux strategy, we used the 300-dimensional GloVe-840B vectors (Penning-ton et al., 2014), downloaded from [Cite] http://nlp.stanford.edu/projects/glove/.",Method,Code,True,Use（引用目的）,True,P16-1190_3_0,2016,Jointly Learning to Embed and Predict with Multiple Languages,Body
636,10639," http://godel.iis.sinica.edu.tw/ROCLING"," ['5 Related Work']","Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus [Cite_Footnote_2] .",2 See http://godel.iis.sinica.edu.tw/ROCLING.,"Bikel and Chiang (2000) and Xu et al. (2002) con-struct word-based statistical parsers on the first re-lease of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic context-free grammar (PCFG) similar to (Collins, 1997); the other is basedF-measureon statisticalis reportedTAGin((ChiangBikel and, 2000Chi-).About ang, 2000). Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus [Cite_Footnote_2] . Xu et al. (2002) reports an overall F-measure l when the same training and test set as (Bikel and Chiang, 2000) are used. Since our parser operates at character level, and more training data is used, the best results are not directly compa-rable. The middle point of the learning curve in Fig-ure 1, which is trained with roughly 100K words, is at the same ballpark of (Xu et al., 2002). The con-tribution of this work is that the proposed character-based parser does word-segmentation, POS tagging and parsing in a unified framework. It is the first at-tempt to our knowledge that syntactic information is used in word-segmentation.",Material,DataSource,True,Use（引用目的）,True,W03-1025_0_0,2003,A Maximum Entropy Chinese Character-Based Parser,Footnote
637,10640," https://spacy.io"," ['4 Model Training', '4.1 Corpora']","Preprocessing Each dataset is tokenized using the spaCy [Cite_Footnote_1] tokenizer, converted to lowercase, and non-",1 https://spacy.io,"Preprocessing Each dataset is tokenized using the spaCy [Cite_Footnote_1] tokenizer, converted to lowercase, and non-",Method,Tool,True,Use（引用目的）,True,N19-1374_0_0,2019,Affect-Driven Dialog Generation,Footnote
638,10641," https://docs.python.org/3/library/difflib.html"," ['4 Model Training', '4.1 Corpora']","Each word in the dataset is then compared with the vocabulary using the difflib library [Cite_Footnote_2] in Python (algorithm based on the Levenshtein distance), and mapped to the most similar word in the vocabu-lary.",2 https://docs.python.org/3/library/difflib.html,"ASCII symbols are removed. To restrain the vocab-ulary size and correct the typos, we use a default vocabulary of fixed size 42K words from spaCy. Each word in the dataset is then compared with the vocabulary using the difflib library [Cite_Footnote_2] in Python (algorithm based on the Levenshtein distance), and mapped to the most similar word in the vocabu-lary. If no word with more than 90% of similarity is found, the word is considered a rare word or a typo, and is mapped to the out-of-vocabulary (OOV) word. For Cornell, less than 1% of the uni-grams are OOV.",Method,Code,True,Compare（引用目的）,True,N19-1374_1_0,2019,Affect-Driven Dialog Generation,Footnote
639,10642," https://code.google.com/p/word2vec/"," ['3 Evaluation', '3.1 Evaluation Setup']","The implementation of Word2Vec [Cite_Footnote_1] from (Mikolov et al., 2013) is employed.",1 https://code.google.com/p/word2vec/,"Two versions of queries are included in our ex-periments: a short keyword query (title query), and a longer description query that restates the cor-responding keyword query’s information need in terms of natural language (description query). We evaluate each type of query separately using the metrics Mean Average Precision at 1,000 (MAP), Precision at 20 (P@20) (Manning et al., 2008), and NDCG@20 (Järvelin and Kekäläinen, 2002). Preprocessing. Stopword removal and Porter’s stemmer are applied (Manning et al., 2008). The word embeddings are pre-trained based on a pool of the top 2,000 documents returned by BM25 for individual queries as suggested by (Diaz et al., 2016). The implementation of Word2Vec [Cite_Footnote_1] from (Mikolov et al., 2013) is employed. In par-ticular, we employ CBOW with the dimension set to 300, window size to 10, minimum count to 5, and a subsampling threshold of 10 −3 . The CBOW model is trained for 10 iterations on the target cor-pus.",Method,Tool,True,Use（引用目的）,True,D18-1478_0_0,2018,NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval,Footnote
640,10643," https://github.com/ucasir/NPRF"," ['3 Evaluation', '3.1 Evaluation Setup']",Our implementation of the NPRF framework is available to enable fu-ture comparisons [Cite_Footnote_2] .,2 https://github.com/ucasir/NPRF,"Unsupervised ranking models serve as baselines for comparisons. We use the open source Terrier platform’s (Macdonald et al., 2012) implementa-tion of these ranking models: the DRMM and K-NRM models, and report re-sults for all six variants. Our implementation of the NPRF framework is available to enable fu-ture comparisons [Cite_Footnote_2] .",Method,Code,True,Produce（引用目的）,True,D18-1478_1_0,2018,NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval,Footnote
641,10644," http://data.gdeltproject.org/events/index.html"," ['4 Experiments', '4.1 Dataset']",We crawled and parsed the GDELT Even-t Database [Cite_Footnote_1] containing news articles published in May 2014.,1 http://data.gdeltproject.org/events/index.html,"We crawled and parsed the GDELT Even-t Database [Cite_Footnote_1] containing news articles published in May 2014. We manually annotated one-week da-ta containing 101,654 documents and identified 77 storylines for evaluation. We also report the result-s of our model on the one-month data containing 526,587 documents. But we only report the preci-sion and not recall of the storylines extracted since it is time consuming to identify all the true story-lines in such a large dataset. In our experiments, we used the Stanford Named Entity Recognizer for identifying the named entities. In addition, we removed common stopwords and only kept tokens which are verbs, nouns, or adjectives in these news articles.",Material,DataSource,True,Extend（引用目的）,False,D15-1225_0_0,2015,An Unsupervised Bayesian Modelling Approach to Storyline Detection from News Articles,Footnote
642,10645," http://aclweb.org/aclwiki/index.php?title=Textual_Entailment_Resource_Pool"," ['2 A new corpus for NLI']","To date, the primary sources of annotated NLI cor-pora have been the Recognizing Textual Entail-ment (RTE) challenge tasks. [Cite_Footnote_1]",1 http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool,"To date, the primary sources of annotated NLI cor-pora have been the Recognizing Textual Entail-ment (RTE) challenge tasks. [Cite_Footnote_1] These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical mod-els of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed rep-resentations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowl-edge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of en-tailments between sentences and artificially con-structed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup-plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) intro-duce a large corpus of semi-automatically anno-tated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) in-cludes automatically generated entailment anno-tations over a large corpus of pairs of words and short phrases.",補足資料,Website,True,Introduce（引用目的）,True,D15-1075_0_0,2015,A large annotated corpus for learning natural language inference,Footnote
643,10646," http://www.csie.ntu.edu.tw/~cjlin/liblinear"," ['5 Experiments']",1 using an off-the-shelf solver [Cite_Footnote_5] .,"5 We used liblinear (Fan et al., 2008) at http://www.csie.ntu.edu.tw/˜cjlin/liblinear with the solver type of 3.","We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO without our op-timization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with var-ied objectives, hinge loss or softmax loss . MERT learns parameters from forests (Kumar et al., 2009) with 4 restarts and 8 random directions in each it-eration. We experimented on a variant of PRO , in which the objective in Eq. 4 with the hinge loss of Eq. 5 was solved in each iteration in line 4 of Alg. 1 using an off-the-shelf solver [Cite_Footnote_5] . Our MIRA solves the problem in Equation 13 in line 7 of Alg. 2. For a sys-tematic comparison, we used our exhaustive oracle translation selection method in Section 3 for PRO, MIRA and ORO. For each learning algorithm, we ran 30 iterations and generated duplicate removed 1,000-best translations in each iteration. The hyper-parameter λ for PRO and ORO was set to 10 −5 , se-lected from among {10 −3 , 10 −4 , 10 −5 }, and 10 2 for MIRA, chosen from {10, 10 2 , 10 3 } by preliminary testing on MT06. Both decoding and learning are parallelized and run on 8 cores. Each online learn-ing took roughly 12 hours, and PRO took one day. It took roughly 3 days for MERT with 20 iterations. Translation results are measured by case sensitive BLEU.",Method,Code,True,Use（引用目的）,True,N12-1026_0_0,2012,Optimized Online Rank Learning for Machine Translation,Footnote
644,10647," http://www.inevent-project.eu"," ['References']",The work described in this article was sup-ported by the European Union through the inEvent project FP7-ICT n. 287872 (see [Cite] http://www.inevent-project.eu).,,The work described in this article was sup-ported by the European Union through the inEvent project FP7-ICT n. 287872 (see [Cite] http://www.inevent-project.eu).,補足資料,Website,True,Introduce（引用目的）,True,D14-1052_0_0,2014,Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis,Body
645,10648," http://lair.cse.msu.edu/lair/projects/actioneffect.html"," ['1 Introduction']","Third, we have cre-ated a dataset for this task, which is available to the community [Cite_Footnote_1] .",1 This dataset is available at http://lair.cse.msu.edu/lair/projects/actioneffect.html,"The contributions of this paper are three folds. First, it introduces a new task on physical action-effect prediction, a first step towards an under-standing of causal relations between physical ac-tions and the state of the physical world. Such ability is central to robots which not only perceive from the environment, but also act to the environ-ment through planning. To our knowledge, there is no prior work that attempts to connect actions (in language) and effects (in images) in this na-ture. Second, our approach harnesses the large amount of image data available on the web with minimum supervision. It has shown that physi-cal action-effect models can be learned through a combination of a few annotated examples and a large amount of un-annotated web data. This opens up the possibility for humans to teach robots new tasks through language communication with a small number of examples. Third, we have cre-ated a dataset for this task, which is available to the community [Cite_Footnote_1] . Our bootstrapping approach can serve as a baseline for future work on this topic.",Material,Dataset,True,Produce（引用目的）,True,P18-1086_0_0,2018,What Action Causes This? Towards Naive Physical Action-Effect Prediction,Footnote
646,10649," http://www.nist.gov/speech/tests/mt/mt2001/resource/"," ['2 Related Work']","Yasuda et al. (2003) tested DP matching (Su et al., 1992), BLEU (Papineni et al., 2002), and NIST [Cite_Footnote_2] , for the calculation of W H .",2 http://www.nist.gov/speech/tests/mt/mt2001/resource/,"As well as Yasuda’s method does, using W H is another way to calculate similarities between a summary to be evaluated and pooled summaries indirectly. Yasuda et al. (2003) tested DP matching (Su et al., 1992), BLEU (Papineni et al., 2002), and NIST [Cite_Footnote_2] , for the calculation of W H . However there are many other measures for summary evaluation.",Method,Code,False,Introduce（引用目的）,False,P06-2078_0_0,2006,An Automatic Method for Summary Evaluation Using Multiple Evaluation Results by a Manual Method,Footnote
647,10650," http://www.nist.gov/speech/tests/mt/mt2001/resource/"," ['4 Experiments', '4.3 Evaluation Methods']","Yasuda et al. (2003) tested DP matching (Su et al., 1992), BLEU (Papineni et al., 2002), and NIST [Cite_Footnote_3] , as automatic methods used in their evaluation.",3 http://www.nist.gov/speech/tests/mt/mt2001/resource/,"An arbitrary system was selected from the 10 systems, and Yasuda’s method estimated its manual score from the other nine systems. Yasuda’s method was evaluated by Gap, which is defined by Equation 7. where x k is the k th system, s(x k ) is a score of x k by Yasuda’s method, and y k is the manual score for the k th system. Yasuda et al. (2003) tested DP matching (Su et al., 1992), BLEU (Papineni et al., 2002), and NIST [Cite_Footnote_3] , as automatic methods used in their evaluation. Instead of those methods, we tested ROUGE and cosine distance, both of which have been used for summary evaluation.",Method,Code,False,Introduce（引用目的）,False,P06-2078_1_0,2006,An Automatic Method for Summary Evaluation Using Multiple Evaluation Results by a Manual Method,Footnote
648,10651," https://github.com/abhyudaynj/LSTM-CRF-models"," ['References']",We use these methods [Cite_Footnote_1] for structured prediction in or-der to improve the exact phrase detection of clinical entities.,1 Code is available at https://github.com/abhyudaynj/LSTM-CRF-models,"Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In the clinical domain one major ap-plication of sequence labeling involves ex-traction of relevant entities such as medica-tion, indication, and side-effects from Elec-tronic Health Record Narratives. Sequence la-beling in this domain presents its own set of challenges and objectives. In this work we experiment with Conditional Random Field based structured learning models with Recur-rent Neural Networks. We extend the pre-viously studied CRF-LSTM model with ex-plicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methods [Cite_Footnote_1] for structured prediction in or-der to improve the exact phrase detection of clinical entities.",Method,Code,True,Use（引用目的）,True,D16-1082_0_0,2016,Structured prediction models for RNN based sequence labeling in clinical text,Footnote
649,10652," https://github.com/abhyudaynj/LSTM-CRF-models/blob/master/annotation.md"," ['4 Dataset']",Each note was annotated [Cite_Footnote_2] by two annotators who label clinical entities into several categories.,2 The annotation guidelines can be found at https://github.com/abhyudaynj/LSTM-CRF-models/blob/master/annotation.md,"We use an annotated corpus of 1154 English Elec-tronic Health Records from cancer patients. Each note was annotated [Cite_Footnote_2] by two annotators who label clinical entities into several categories. These cate-gories can be broadly divided into two groups, Clin-ical Events and Attributes. Clinical events include any specific event that causes or might contribute to a change in a patient’s medical status. Attributes are phrases that describe certain important proper-ties about the events.",補足資料,Document,True,Produce（引用目的）,True,D16-1082_1_0,2016,Structured prediction models for RNN based sequence labeling in clinical text,Footnote
650,10653," http://www.freedict.com"," ['5 Experiment: English-French', '5.1 Translation Lexicon']","• An English-French dictionary (a total of 34,808 entries, 4,021 of which are not one-to-one). [Cite_Footnote_9]",9 This dictionary was generated using a dictionary de-rived from one available at http://www.freedict.com.,"• An English-French dictionary (a total of 34,808 entries, 4,021 of which are not one-to-one). [Cite_Footnote_9] It contains morphological variants but does not include character accents. Each n-to-m entry was processed by stoplisting and then extracting all word-pairs in the remaining cross-product as in section 4.1. Result: 39,348 word pairs, 9,045 of which contain two words present in the corpora.",Material,DataSource,True,Produce（引用目的）,True,W02-1013_0_0,2002,"Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 95-102.",Footnote
651,10654," https://github.com/mit-nlp/MITIE"," ['6 Experiments', '6.2 Language Modeling', '6.2.2 Vocabulary Indices']","We tested a 5–gram language model over CoNLL part-of-speech tags from MITIE (King, 2014) [Cite_Ref] .","Davis E. King. 2014. MITIE: MIT information extraction, January. https://github.com/mit-nlp/MITIE.","Class-based language models are often used alongside lexical language models to form gener-alizations. We tested a 5–gram language model over CoNLL part-of-speech tags from MITIE (King, 2014) [Cite_Ref] . There are fewer than 256 unique tags, fitting into a byte per word. We also cre-ated special KenLM and DALM query programs that read byte-encoded input. Figure 3 and Ta-ble 4 show total time while model sizes are shown in Figure 4. Performance plateaus for very small models, which is more clearly shown by plotting speed in Figure 5.",補足資料,Paper,True,Introduce（引用目的）,True,P15-2063_0_0,2015,Language Identification and Modeling in Specialized Hardware,Reference
652,10655," https://github.com/saffsd/langid.c"," ['6 Experiments', '6.1 Language Identification']","We tested the origi-nal Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013), a C implementation (Lui, 2014) [Cite_Ref] , and our replica in hardware.",Marco Lui. 2014. Pure C natural language iden-tifier with support for 97 languages. https://github.com/saffsd/langid.c.,"The langid.py model is 88.6–99.2% accurate (Lui and Baldwin, 2012). We tested the origi-nal Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013), a C implementation (Lui, 2014) [Cite_Ref] , and our replica in hardware. We also tested CLD2 (Sites, 2013) writ-ten in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs of feature extraction and modeling.",Method,Code,False,Compare（引用目的）,False,P15-2063_1_0,2015,Language Identification and Modeling in Specialized Hardware,Reference
653,10657," https://github.com/carrotsearch/langid-java"," ['6 Experiments', '6.1 Language Identification']","We tested the origi-nal Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013) [Cite_Ref] , a C implementation (Lui, 2014), and our replica in hardware.",Dawid Weiss. 2013. Java port of langid.py (language identifier). https://github.com/carrotsearch/langid-java.,"The langid.py model is 88.6–99.2% accurate (Lui and Baldwin, 2012). We tested the origi-nal Python, a Java implementation that “should be faster than anything else out there” (Weiss, 2013) [Cite_Ref] , a C implementation (Lui, 2014), and our replica in hardware. We also tested CLD2 (Sites, 2013) writ-ten in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005). Time includes the costs of feature extraction and modeling.",Method,Tool,False,Compare（引用目的）,False,P15-2063_3_0,2015,Language Identification and Modeling in Specialized Hardware,Reference
654,10658," http://www.speech.cs.cmu.edu/tools/lextool.html"," ['4 Experiment 2: Speaker ID from ASR']",• Lexicontions using: WethegeneratedLOGIOSthelexicalwordtoolpronuncia- [Cite_Footnote_1] .,1 http://www.speech.cs.cmu.edu/tools/lextool.html,• Lexicontions using: WethegeneratedLOGIOSthelexicalwordtoolpronuncia- [Cite_Footnote_1] . We decoded the audio in three ways:,Method,Tool,True,Introduce（引用目的）,False,N18-2117_0_0,2018,Role-specific Language Models for Processing Recorded Neuropsychological Exams,Footnote
655,10659," http://nlp.stanford.edu/projects/glove/"," ['3 Neural Sparse Topical Coding', '3.3 Neural Sparse Topical Coding']","Here, we adopt the pre-trained embeddings by GloVe based on a large Wikipedia dataset [Cite_Footnote_1] .",1 http://nlp.stanford.edu/projects/glove/,"Word embedding layer (W E ∈ R N×300 ): Sup-posing the word number of the vocabulary is N, this layer devotes to transform each word to a distributed embedding representation. Here, we adopt the pre-trained embeddings by GloVe based on a large Wikipedia dataset [Cite_Footnote_1] .",Method,Code,True,Extend（引用目的）,False,P18-1217_0_0,2018,Neural Sparse Topical Coding,Footnote
656,10660," http://www.qwone.com/jason/20Newsgroups/"," ['4 Experiments', '4.1 Data and Setting']","• 20Newsgroups: is comprised of 18775 newsgroup articles with 20 categories, and contains 60698 unique words [Cite_Footnote_2] .",2 http://www.qwone.com/jason/20Newsgroups/,"• 20Newsgroups: is comprised of 18775 newsgroup articles with 20 categories, and contains 60698 unique words [Cite_Footnote_2] .",Material,Dataset,True,Introduce（引用目的）,False,P18-1217_1_0,2018,Neural Sparse Topical Coding,Footnote
657,10661," http://jwebpro.sourceforge.net/data-web-snippets.tar.gz"," ['4 Experiments', '4.1 Data and Setting']","• Web Snippet: includes 12340 Web search snippets with 8 categories, we remove the words with fewer than 3 characters and with document frequency less than 3 in the dataset [Cite_Footnote_3] .",3 http://jwebpro.sourceforge.net/data-web-snippets.tar.gz,"• Web Snippet: includes 12340 Web search snippets with 8 categories, we remove the words with fewer than 3 characters and with document frequency less than 3 in the dataset [Cite_Footnote_3] .",Material,DataSource,False,Extend（引用目的）,False,P18-1217_2_0,2018,Neural Sparse Topical Coding,Footnote
658,10662," https://pypi.python.org/pypi/lda"," ['4 Experiments', '4.1 Data and Setting']",We use the LDA pack-age [Cite_Footnote_4] for its implementation.,4 https://pypi.python.org/pypi/lda,"• LDA (Blei et al., 2001). A classical proba-bilistic topic model. We use the LDA pack-age [Cite_Footnote_4] for its implementation. We use the set-tings with iteration number n = 2000, the Dirichlet parameter for distribution over top-ics α = 0.1 and the Dirichlet parameter for distribution over words η = 0.01.",Method,Tool,True,Use（引用目的）,True,P18-1217_3_0,2018,Neural Sparse Topical Coding,Footnote
659,10663," http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/"," ['4 Experiments', '4.1 Data and Setting']",We use the code released by the authors [Cite_Footnote_5] .,5 http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/,"• STC (Zhu and Xing, 2011). It is a sparsity-enhanced non-probabilistic topic model. We use the code released by the authors [Cite_Footnote_5] . We set the regularization constants as λ = 0.3, ρ = 0.0001 and the maximum number of itera-tions of hierarchical sparse coding, dictionary learning as 100.",Method,Code,True,Use（引用目的）,True,P18-1217_4_0,2018,Neural Sparse Topical Coding,Footnote
660,10664," https://github.com/huashiyiqike/TMBP/tree/master/DocNADE"," ['4 Experiments', '4.1 Data and Setting']",An unsupervised neural network topic model of documents and has shown that it is a com-petitive model both as a generative model and as a document representation learning algo-rithm [Cite_Footnote_6] .,6 https://github.com/huashiyiqike/TMBP/tree/master/DocN ADE,"• DocNADE (Larochelle and Lauly, 2012b). An unsupervised neural network topic model of documents and has shown that it is a com-petitive model both as a generative model and as a document representation learning algo-rithm [Cite_Footnote_6] . In DocNADE, the hidden size is 50, the learning rate is 0.0004 , the bath size is 64 and the max training number is 50000.",Method,Code,False,Introduce（引用目的）,False,P18-1217_5_0,2018,Neural Sparse Topical Coding,Footnote
661,10665," https://github.com/rajarshd/GaussianLDA"," ['4 Experiments', '4.1 Data and Setting']",A new technique for topic modeling by treating the document as a collection of word embed-dings and topics itself as multivariate Gaus-sian distributions in the embedding space [Cite_Footnote_7] .,7 https://github.com/rajarshd/Gaussian LDA,"• GaussianLDA (Das et al., 2015). A new technique for topic modeling by treating the document as a collection of word embed-dings and topics itself as multivariate Gaus-sian distributions in the embedding space [Cite_Footnote_7] . We use default values for the parameters.",Method,Code,False,Introduce（引用目的）,True,P18-1217_6_0,2018,Neural Sparse Topical Coding,Footnote
662,10666," https://github.com/askerlee/topicvec"," ['4 Experiments', '4.1 Data and Setting']",A model incorpo-rates generative word embedding model with LDA [Cite_Footnote_8] .,8 https://github.com/askerlee/topicvec,"• TopicVec (Li et al., 2016). A model incorpo-rates generative word embedding model with LDA [Cite_Footnote_8] . We also use default values for the pa-rameters.",Method,Code,True,Introduce（引用目的）,True,P18-1217_7_0,2018,Neural Sparse Topical Coding,Footnote
663,10667," https://www.tensorflow.org/"," ['4 Experiments', '4.1 Data and Setting']",Our three models are implemented in Python using TensorFlow [Cite_Footnote_9] .,9 https://www.tensorflow.org/,"Our three models are implemented in Python using TensorFlow [Cite_Footnote_9] . For both datasets, we use the pre-trained 300-dimensional word embeddings from Wikipedia by GloVe, and it is fixed during train-ing. For each out-of-vocab word, we sample a random vector from a normal distribution. In practice, we use a regular learning rate 0.00001 for both dataset. We set the regularization factor λ = 1,α = 1,λ 1 = 0.6,λ 2 = 0.4. In initial-ization, all weight matrices are randomly initial-ized with the uniformed distribution in the inter-val [0, 0.001] for web snippet, and [0, 0.0001] for 20Newsgroups.",Method,Code,False,Use（引用目的）,True,P18-1217_8_0,2018,Neural Sparse Topical Coding,Footnote
664,10668," http://nie.mn/QuD17Z"," ['3 Dataset Collection']","The idea behind the Mood Me-ter is actually “getting people to crowdsource the mood for the day” [Cite_Footnote_1] , and returning the percentage of votes for each emotion label for a given story.",1 http://nie.mn/QuD17Z,"To build our emotion lexicon we harvested all the news articles from rappler.com, as of June 3rd 2013: the final dataset consists of 13.5 M words over 25.3 K documents, with an average of 530 words per document. For each document, along with the text we also harvested the informa-tion displayed by Rappler’s Mood Meter, a small interface offering the readers the opportunity to click on the emotion that a given Rappler story made them feel. The idea behind the Mood Me-ter is actually “getting people to crowdsource the mood for the day” [Cite_Footnote_1] , and returning the percentage of votes for each emotion label for a given story. This way, hundreds of thousands votes have been collected since the launch of the service. In our novel approach to ‘crowdsourcing’, as compared to other NLP tasks that rely on tools like Ama-zon’s Mechanical Turk (Snow et al., 2008), the subjects are aware of the ‘implicit annotation task’ but they are not paid. From this data, we built a document-by-emotion matrix M DE , providing the voting percentages for each document in the eight affective dimensions available in Rappler. An ex-cerpt is provided in Table 1.",補足資料,Document,True,Introduce（引用目的）,True,P14-2070_0_0,2014,DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News,Footnote
665,10669," http://git.io/MqyoIg"," ['4 Emotion Lexicon Creation']","This matrix, that we call DepecheMood 2 , represents our emotion lex-icon, it contains 37k entries and is freely available for research purposes at [Cite] http://git.io/MqyoIg.",,"Finally, we transformed M WE by first apply-ing normalization column-wise (so to eliminate the over representation for happiness as discussed in Section 3) and then scaling the data row-wise so to sum up to one. An excerpt of the final Matrix M WE is presented in Table 3, and it can be in-terpreted as a list of words with scores that repre-sent how much weight a given word has in the af-fective dimensions we consider. So, for example, awe#n has a predominant weight in INSPIRED (0.38), comical#a has a predominant weight in AMUSED (0.51), while kill#v has a predomi-nant weight in AFRAID , ANGRY and SAD (0.23, 0.21 and 0.27 respectively). This matrix, that we call DepecheMood 2 , represents our emotion lex-icon, it contains 37k entries and is freely available for research purposes at [Cite] http://git.io/MqyoIg.",Material,Dataset,True,Produce（引用目的）,True,P14-2070_1_0,2014,DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News,Body
666,10670," https://github.com/shuningjin/discrete-text-rep"," ['References']","Interestingly, we find that an amor-tized variant of Hard EM performs particularly well in the lowest-resource regimes. [Cite_Footnote_1]",1 Code available on GitHub: https://github.com/shuningjin/discrete-text-rep,"While much work on deep latent variable models of text uses continuous latent vari-ables, discrete latent variables are interesting because they are more interpretable and typi-cally more space efficient. We consider sev-eral approaches to learning discrete latent vari-able models for text in the case where ex-act marginalization over these variables is in-tractable. We compare the performance of the learned representations as features for low-resource document and sentence classification. Our best models outperform the previous best reported results with continuous representa-tions in these low-resource settings, while learning significantly more compressed repre-sentations. Interestingly, we find that an amor-tized variant of Hard EM performs particularly well in the lowest-resource regimes. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.acl-main.437_0_0,2020,Discrete Latent Variable Representations for Low-Resource Text Classification,Footnote
667,10671," https://github.com/martin-arvidsson/InterpretableWordEmbeddings"," ['3 Experiments']","We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling [Cite_Footnote_1] .",1 Code is available at: https://github.com/martin-arvidsson/InterpretableWordEmbeddings,"We consider two semantic dimensions, gen-der, which is explored in SOTA, and sentiment, a dimension proven difficult to capture in stan-dard word embedding models (Tang et al., 2014). We follow SOTA when choosing prior anchors for gender, while using the AFINN dictionary (Nielsen, 2011) to find prior anchors for sentiment. To evaluate the effect of “few” vs. “many” prior anchors, we run experiments using between 2 and 276 words depending on the dimension of interest and the dataset at hand. All prior word types used in the experiments can be found in Sec. B in the supplementary material. We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling [Cite_Footnote_1] . We set the size of the embeddings K = 100, use a context window size of 8 and σ = 1 throughout all experiments.",Method,Code,True,Produce（引用目的）,False,D19-1661_0_0,2019,Interpretable Word Embeddings via Informative Priors,Footnote
668,10672," http://data.stanford.edu/congress_text"," ['3 Experiments']","We examine the proposed priors using three commonly sized English corpora for textual anal-ysis within CSSDH: the top 100 list of books in Project Gutenberg (2019), a sample from Twitter (Go et al., 2009) and the U.S. Congress Speeches 1981-2016 (Gentzkow et al., 2018) [Cite_Ref] .","M. Gentzkow, J. M. Shapiro, and M. Taddy. 2018. Congressional record for the 43rd-114th congresses: Parsed speeches and phrase counts. http://data.stanford.edu/congress_text. Ac-cessed: 2019-04-17.","We examine the proposed priors using three commonly sized English corpora for textual anal-ysis within CSSDH: the top 100 list of books in Project Gutenberg (2019), a sample from Twitter (Go et al., 2009) and the U.S. Congress Speeches 1981-2016 (Gentzkow et al., 2018) [Cite_Ref] . The num-ber of tokens ranges from 1.8M to 40M after pre-processing (see Sec. A in the supplementary ma-terial for details). The various origins, sizes and contents of these datasets work as a check of the effect of the priors in different types of corpora.",Material,Dataset,True,Compare（引用目的）,True,D19-1661_1_0,2019,Interpretable Word Embeddings via Informative Priors,Reference
669,10673," http://AFINN.localhost/pubdb/p.php?6010"," ['3 Experiments']","We follow SOTA when choosing prior anchors for gender, while using the AFINN dictionary (Nielsen, 2011) [Cite_Ref] to find prior anchors for sentiment.",F. Å. Nielsen. 2011. http://AFINN.localhost/pubdb/p.php?6010.,"We consider two semantic dimensions, gen-der, which is explored in SOTA, and sentiment, a dimension proven difficult to capture in stan-dard word embedding models (Tang et al., 2014). We follow SOTA when choosing prior anchors for gender, while using the AFINN dictionary (Nielsen, 2011) [Cite_Ref] to find prior anchors for sentiment. To evaluate the effect of “few” vs. “many” prior anchors, we run experiments using between 2 and 276 words depending on the dimension of interest and the dataset at hand. All prior word types used in the experiments can be found in Sec. B in the supplementary material. We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling . We set the size of the embeddings K = 100, use a context window size of 8 and σ = 1 throughout all experiments.",Material,Dataset,True,Use（引用目的）,True,D19-1661_2_0,2019,Interpretable Word Embeddings via Informative Priors,Reference
670,10674," http://www.gutenberg.org"," ['3 Experiments']","We examine the proposed priors using three commonly sized English corpora for textual anal-ysis within CSSDH: the top 100 list of books in Project Gutenberg (2019) [Cite_Ref] , a sample from Twitter (Go et al., 2009) and the U.S. Congress Speeches 1981-2016 (Gentzkow et al., 2018).",Project Gutenberg. 2019. http://www.gutenberg.org. Accessed: 2019-04-24.,"We examine the proposed priors using three commonly sized English corpora for textual anal-ysis within CSSDH: the top 100 list of books in Project Gutenberg (2019) [Cite_Ref] , a sample from Twitter (Go et al., 2009) and the U.S. Congress Speeches 1981-2016 (Gentzkow et al., 2018). The num-ber of tokens ranges from 1.8M to 40M after pre-processing (see Sec. A in the supplementary ma-terial for details). The various origins, sizes and contents of these datasets work as a check of the effect of the priors in different types of corpora.",補足資料,Website,True,Introduce（引用目的）,True,D19-1661_3_0,2019,Interpretable Word Embeddings via Informative Priors,Reference
671,10675," https://github.com/usc-sail/mica-riskybehavior-identification"," ['1 Introduction']","MovieBERT [Cite_Footnote_1] : A domain-specific fine-tuned BERT model (Devlin et al., 2019) pre-trained over a large collection of film and TV scripts.",1 https://github.com/usc-sail/ mica-riskybehavior-identification,"2. MovieBERT [Cite_Footnote_1] : A domain-specific fine-tuned BERT model (Devlin et al., 2019) pre-trained over a large collection of film and TV scripts. We use this model to obtain better represen-tations for the semantics of a character’s lan-guage",Material,Knowledge,False,Use（引用目的）,True,2020.emnlp-main.387_0_0,2020,Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,Footnote
672,10676," https://www.imdb.com/"," ['3 Method', '3.3 Role of Movie Genre']",Genres for each movie were obtained from IMDb [Cite_Footnote_3] and transformed into a multi-hot encoding.,3 https://www.imdb.com/,"Movie genres relate the elements of a story, plot, setting and characters to a specific category. Cate-gorizing a movie indirectly assists in shaping the characters and the story of the movie, and deter-mines the plot and best setting to use. Thus, movie genre contains information on the type of content one could expect in a movie (especially for the case of violent content (Martinez et al., 2019)). Thus, our models include movie genre as an additional feature. Genres for each movie were obtained from IMDb [Cite_Footnote_3] and transformed into a multi-hot encoding.",Material,DataSource,False,Extend（引用目的）,False,2020.emnlp-main.387_1_0,2020,Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,Footnote
673,10677," http://www.commonsensemedia.org"," ['4 Data']","These ratings come from Common Sense Media (CSM), a non-profit organization that promotes safe technology and media for children [Cite_Footnote_6] .",6 http://www.commonsensemedia.org,"To evaluate the performance of our model, and directly compare it to previous work, we manu-ally align a subset of 989 movie scripts from our dataset to the content ratings found in (Martinez et al., 2019). These ratings come from Common Sense Media (CSM), a non-profit organization that promotes safe technology and media for children [Cite_Footnote_6] . CSM experts rate movies from 0 (lowest) to 5 (high-est) with each rating manually checked by the ex-ecutive editor to ensure consistency across raters. A manual inspection of the dataset revealed that the movies with the least scores across all risk be-haviors correspond to the romantic genre, whereas the movies with the most risky content were in the horror genre. Additionally, we investigate if CSM expert raters capture the co-occurrence of risk be-havior portrayals. Figure 2 shows that, on average, when one risk-behavior rating increases so does the others. This was corroborated by significant posi-tive Spearman’s correlations between violence and sexual content ( r s = 0.161, p < 0.001 ); violence and substance-abuse ( r s = 0.129, p < 0.001 ), and sexual content and substance-abuse ( r s = 0.467, p < 0.001 ).",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.387_2_0,2020,Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,Footnote
674,10678," https://keras.io"," ['5 Experimental Setup', '5.1 Model Implementation']",Our model was implemented in Keras [Cite_Footnote_7] .,7 https://keras.io,"Our model was implemented in Keras [Cite_Footnote_7] . Although not common in most deep-learning approaches, we performed 10-fold cross-validation (CV) to obtain a more reliable estimation for our model’s perfor-mance. In each fold, the model was trained until convergence (i.e. loss in consecutive epochs was less than 10 −8 difference). To prevent over-fitting, we used Adam optimizer with a small learning rate ( 0.001 ), batch size of 16 , and high dropout probabil-ity ( p = 0.5 ). For the RNN layer, we used Gated Re-current Units (GRU; Cho et al. 2014). For the senti-ment models, Bi-LSTM parameters were informed by the work of Tai et al. (2015): 50-dimensional hidden representation, dropout ( p = 0.1 ), trained with Adam optimizer on a batch size of 25 and a L 2 penalty of 10 −4 . To allow for a fair comparison, all the BERT pre-trained models and movieBERT had the same set of parameters as the BERT-base model: 12 layers, 768 dimensions, learning rate of 2 × 10 −5 , sequence length of 128 and batch size of 32. For the initial experiments, we set the model parameters to hidden dimension size of d = 16, to help prevent overfitting, and the sequence length m = 500, which is approximately the duration of one movie act (i.e., one third). This selection was informed by previous works (Martinez et al., 2019; Shafaei et al., 2019).",Method,Tool,False,Introduce（引用目的）,True,2020.emnlp-main.387_3_0,2020,Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,Footnote
675,10679," https://bit.ly/35hKwhY"," ['6 Results', '6.1 Classification Results']","In fact, filmmakers are advised to avoid the repeated usage of sexually-derived words—either as an expletive or in a sexual context—as to avoid a non family-friendly rating (Myers, 2018) [Cite_Ref] .",Scott Myers. 2018. Reader question: Is there a rule as to how many “cuss words” can be used in a script? https://bit.ly/35hKwhY. Accessed: 04/09/2020.,"While the proposed model also improves sex-ual content rating prediction, this improvement is non-significant ( p > 0.05 ). As previously men-tioned, this could be attributed to the fact that MPAA’s ratings are particularly sensitive to sex-ual content (Thompson and Yokota, 2004). In fact, filmmakers are advised to avoid the repeated usage of sexually-derived words—either as an expletive or in a sexual context—as to avoid a non family-friendly rating (Myers, 2018) [Cite_Ref] . Thus, they might refer to sexual acts through the use of euphemisms or innuendos, which the model seems unable to pick up on. Our experiments in using BERT for sentiment representations (last row in Table 2) did not significantly improve performance any further ( p > 0.05 ). Future work will explore further fine-tuning to better capture affective language.",補足資料,Paper,True,Introduce（引用目的）,True,2020.emnlp-main.387_4_0,2020,Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,Reference
676,10680," https://github.com/ananthnyu/faithful-causal-rep/"," ['5 Results', '5.3 Re-alignment towards causation']",The distances are nor-malized between 0 and [Cite_Footnote_1] based on the maximum and minimum values of distances (cosine or d M ) in the sampled word-pairs.,1 https://github.com/ananthnyu/faithful-causal-rep/ and Antal van den Bosch. 2007. ILK: Machine learning of semantic relations with shallow features and almost no data. In Proceedings of the Fourth,"To understand the reason behind the improved per-formance, we performed a qualitative inspection of 100 randomly sampled word pairs from the Giga-word causal graph 1 that are at varying distances in the original pre-trained embedding and trace how they have re-aligned after fine-tuning with the faithfulness objective. We annotate each of these word-pairs as being either causal or not as shown in the confusion matrix with examples in Table 4. In Figure 3, we see re-alignment of these word pairs from association based RoBERTa embeddings to the causally aligned Faithful-RoBERTa embedding space, that is, causal word pairs (blue and orange) move closer, and non-causal word pairs (green and red) move further based on the quasi-pseudo met-ric d M . Specifically, the associative but non-causal word pairs (green) have moved further in Faithful-RoBERTa, while the non-associative but causal word pairs (orange) have moved closer. We see that in the cosine-similarity based RoBERTa, the causal word pairs had a mean distance of 0.48, while in the quasi-pseudo metric based Faithful-RoBERTa, the mean distance between the causal word pairs reduced to 0.28. The distances are nor-malized between 0 and [Cite_Footnote_1] based on the maximum and minimum values of distances (cosine or d M ) in the sampled word-pairs.",補足資料,Paper,False,Introduce（引用目的）,False,2021.acl-long.69_0_0,2021,Learning Faithful Representations of Causal Graphs,Footnote
677,10681," http://www.rulequest.com/Personal/"," ['5 Experiments']","[Cite_Footnote_3] , which learns first-order horn clauses.",3 http://www.rulequest.com/Personal/,"Our results are compared against the FOIL algorithm [Cite_Footnote_3] , which learns first-order horn clauses. In order to evaluate FOIL using MAP, its candidate beliefs are first ranked by the number of FOIL rules they match. We further report results using Random Walks with Restart (RWR), also known as personalized PageRank (Haveliwala, 2002), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., (Agirre and Soroa, 2009; Moro et al., 2014)). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P (s → t; π).",Method,Code,False,Compare（引用目的）,True,P15-1065_0_0,2015,Learning Relational Features with Backward Random Walks,Footnote
678,10682," http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/"," ['2 Related Work']","The Aleph algorithm (Srinivasan, 2001) [Cite_Ref] combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol.",Ashwin Srinivasan. 2001. The Aleph Manual. In http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.,"Bi-directional search is a popular strategy in AI, and in the ILP literature. The Aleph algorithm (Srinivasan, 2001) [Cite_Ref] combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol. FORTE (Richards and Mooney, 1991) was another early ILP system which enumerated paths via a bi-directional seach. Computing backward random walks for PRA can be seen as a particular way of bi-directional search, which is also assigned a random walk probability semantics. Unlike in prior work, we will use this probability semantics directly for feature selection.",Method,Code,True,Introduce（引用目的）,False,P15-1065_1_0,2015,Learning Relational Features with Backward Random Walks,Reference
679,10683," http://openie.cs.washington.edu/"," ['3 Textual Schema Matching', '3.4 Comparing database relations with extracted relations']","M ATCHER uses an API for the ReVerb Open IE system [Cite_Footnote_1] (Fader et al., 2011) to collect I(r T ), for each r T .",1 http://openie.cs.washington.edu/,"M ATCHER uses an API for the ReVerb Open IE system [Cite_Footnote_1] (Fader et al., 2011) to collect I(r T ), for each r T . The API for ReVerb allows for rela-tional queries in which some subset of the entity strings, entity categories, and relation string are specified. The API returns all matching triples; types must match exactly, but relation or argument strings in the query will match any relation or ar-gument that contains the query string as a sub-string. M ATCHER queries ReVerb with three dif-ferent types of queries for each r T , specifying the types for both arguments, or just the type of the first argument, or just the second argument. Types for arguments are taken from the types of argu-ments for a potentially matching r D in Freebase. To avoid overwhelming the ReVerb servers, for our experiments we limited M ATCHER to queries for the top 80 r T ∈ C(r D ), when they are ranked according to frequency during the candidate iden-tification process.",Method,Tool,False,Introduce（引用目的）,False,P13-1042_0_0,2013,Large-scale Semantic Parsing via Schema Matching and Lexicon Extension,Footnote
680,10684," https://github.com/pytorch/fairseq/tree/master/examples/wav2vec"," ['3 Experimental Setup', '3.2 Training']","We initialize the encoder using the open-sourced [Cite_Footnote_1] wav2vec 2.0 large architecture pretrained on unlabelled English-only (XMEF-En) audio from LibriVox (Baevski et al., 2020).",1 https://github.com/pytorch/fairseq/tree/master/examples/wav2vec.,"Encoder. We initialize the encoder using the open-sourced [Cite_Footnote_1] wav2vec 2.0 large architecture pretrained on unlabelled English-only (XMEF-En) audio from LibriVox (Baevski et al., 2020). For many-to-one experiments, we also experiment with a multi-lingual wav2vec 2.0 (XMEF-X), which was pre-trained on raw audio from 53 languages (Conneau et al., 2020). Encoder output is followed by 3 1- D convolution layers with stride 2 to achieve 8x down-sampling of audio encoder outputs.",Method,Code,False,Use（引用目的）,True,2021.acl-long.68_0_0,2021,Multilingual Speech Translation from Efficient Finetuning of Pretrained Models,Footnote
681,10685," https://github.com/pytorch/fairseq/tree/master/examples/multilingual"," ['3 Experimental Setup', '3.2 Training']","We initialize the decoder with open-sourced [Cite_Footnote_2] mBART50 models and the same vocab-ulary (Tang et al., 2020).",2 https://github.com/pytorch/fairseq/tree/master/examples/multilingual preprint arXiv:2005.00052.,"Decoder. We initialize the decoder with open-sourced [Cite_Footnote_2] mBART50 models and the same vocab-ulary (Tang et al., 2020). We use mBART50N1 (49 languages to English) for X-En ST directions and mBART501N (English to 49 languages) for translating En-X ST directions.",Method,Code,False,Use（引用目的）,True,2021.acl-long.68_1_0,2021,Multilingual Speech Translation from Efficient Finetuning of Pretrained Models,Footnote
682,10686," https://github.com/facebookresearch/covost"," ['-', 'A.2 Data']",The data could be downloaded from [Cite] https: //github.com/facebookresearch/covost .,,"The CoVoST 2 dataset (Wang et al., 2020b) is a large-scale multilingual ST corpus which cov-ers translations from English into 15 languages— Arabic, Catalan, Welsh, German, Estonian, Per-sian, Indonesian, Japanese, Latvian, Mongolian, Slovenian, Swedish, Tamil, Turkish, Chinese, and translations from 21 languages into English, includ-ing Spanish, French, Italian, Dutch, Portuguese, Russian in addition to the 15 target languages. It has total 2,880 hours of speech from 78K speak-ers. The data could be downloaded from [Cite] https: //github.com/facebookresearch/covost .",Material,Dataset,True,Produce（引用目的）,True,2021.acl-long.68_2_0,2021,Multilingual Speech Translation from Efficient Finetuning of Pretrained Models,Body
683,10687," https://github.com/pytorch/fairseq/tree/master/examples/wav2vec"," ['-', 'A.3 Implementation Details']",These models can be down-loaded from [Cite] https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and https://github.com/pytorch/fairseq/tree/master/examples/multilingual.,,"Pretrained models. We use the open-sourced models from wav2vec 2.0 and mBART50 pretrained with multilingual par-allel text data. These models can be down-loaded from [Cite] https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and https://github.com/pytorch/fairseq/tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model. For XMEF-X, we use the 56K-hour XLSR-53 Large model. For decoder, we use the pretrained “mMBART 50 finetuned many-to-one” model for many-to-one experiments and “mMBART 50 finetuned one-to-many” for one-to-many experiments.",Method,Code,True,Use（引用目的）,True,2021.acl-long.68_3_0,2021,Multilingual Speech Translation from Efficient Finetuning of Pretrained Models,Body
684,10688," https://github.com/pytorch/fairseq/tree/master/examples/multilingual"," ['-', 'A.3 Implementation Details']",These models can be down-loaded from https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and [Cite] https://github.com/pytorch/fairseq/tree/master/examples/multilingual.,,"Pretrained models. We use the open-sourced models from wav2vec 2.0 and mBART50 pretrained with multilingual par-allel text data. These models can be down-loaded from https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and [Cite] https://github.com/pytorch/fairseq/tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model. For XMEF-X, we use the 56K-hour XLSR-53 Large model. For decoder, we use the pretrained “mMBART 50 finetuned many-to-one” model for many-to-one experiments and “mMBART 50 finetuned one-to-many” for one-to-many experiments.",Method,Code,True,Use（引用目的）,True,2021.acl-long.68_4_0,2021,Multilingual Speech Translation from Efficient Finetuning of Pretrained Models,Body
685,10689," https://github.com/HKUST-KnowComp/Visual_PCR"," ['1 Introduction']",[Cite] https://github.com/HKUST-KnowComp/Visual_PCR.,,"The contribution of this paper is three-folded: (1) we formally define the task of visual PCR; (2) we present VisPro, the first dataset that focuses on PCR in visual-supported di-alogues; (3) we propose VisCoref, a visual-aware PCR model. Comprehensive experi-ments and case studies are conducted to demon-strate the quality of VisPro and the effective-ness of VisCoref. The dataset, code, and mod-els are available at: [Cite] https://github.com/HKUST-KnowComp/Visual_PCR.",Mixed,Mixed,True,Produce（引用目的）,True,D19-1516_0_0,2019,What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues,Body
686,10690," https://github.com/tensorflow/models/tree/master/research/object_detection"," ['5 The Experiment', '5.2 Implementation Details']",We adopt the “ssd resnet 50 fpn coco” model from Tensorflow detection model zoo [Cite_Footnote_5] as the object detection mod-ule.,5 https://github.com/tensorflow/models/tree/master/research/object_detection,"Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embed-ding (Pennington et al., 2014) and the ELMo (Pe-ters et al., 2018) embedding as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. We adopt the “ssd resnet 50 fpn coco” model from Tensorflow detection model zoo [Cite_Footnote_5] as the object detection mod-ule. The size of hidden states in the LSTM module is set to 200, and the size of the projected embed-ding for computing similarity between text spans and object labels is 512. The feed-forward net-works for contextual scoring and visual scoring have two 150-dimension hidden layers and one 100-dimension hidden layer, respectively.",Method,Code,True,Extend（引用目的）,False,D19-1516_1_0,2019,What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues,Footnote
687,10691," https://stanfordnlp.github.io/CoreNLP/coref.html"," ['5 The Experiment', '5.3 Baseline Methods']","As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit [Cite_Footnote_6] , we use their released model as baselines.",6 https://stanfordnlp.github.io/CoreNLP/coref.html,"As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit [Cite_Footnote_6] , we use their released model as baselines. For the End-to-end model, we also use their re-leased code .",Method,Tool,True,Use（引用目的）,True,D19-1516_2_0,2019,What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues,Footnote
688,10692," https://github.com/kentonl/e2e-coref"," ['5 The Experiment', '5.3 Baseline Methods']","For the End-to-end model, we also use their re-leased code [Cite_Footnote_7] .",7 https://github.com/kentonl/e2e-coref,"As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit , we use their released model as baselines. For the End-to-end model, we also use their re-leased code [Cite_Footnote_7] .",Method,Code,True,Use（引用目的）,True,D19-1516_3_0,2019,What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues,Footnote
689,10693," https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs"," ['References']",We publicly release our code. [Cite_Footnote_1],1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs,"Neural models for NLP typically use large numbers of parameters to reach state-of-the-art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learn-ing sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of mod-els that is closely connected to weighted finite-state automata (WFSAs). We take advan-tage of rational RNNs’ natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment anal-ysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parame-ters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of ra-tional RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90% of the weights. We publicly release our code. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D19-1110_0_0,2019,RNN Architecture Learning with Sparse Regularization,Footnote
690,10694," http://riejohnson.com/cnn_data.html"," ['3 Experiments']","We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015). [Cite_Footnote_7]",7 http://riejohnson.com/cnn_data.html,"Data We experiment with the Amazon reviews binary sentiment classification dataset (Blitzer et al., 2007), composed of 22 product categories. We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015). [Cite_Footnote_7] We also examine three of the largest individual categories as separate datasets (kitchen, dvd, and books), following Johnson and Zhang (2015). The three category datasets do not overlap with each other (though they do with original mix), and are sig-nificantly different in size (see Appendix A), so we can see how our approach behaves with differ-ent amounts of training data.",Material,Dataset,True,Use（引用目的）,True,D19-1110_1_0,2019,RNN Architecture Learning with Sparse Regularization,Footnote
691,10695," https://github.com/huggingface/pytorch-pretrained-BERT"," ['3 Experiments']","We ex-periment with both type-level word embeddings (GloVe.6B.300d; Pennington et al., 2014) and contextual embeddings (BERT large; Devlin et al., 2019). [Cite_Footnote_8]",8 https://github.com/huggingface/ pytorch-pretrained-BERT,"Implementation details To classify text, we concatenate the scores computed by each WFSA, then feed this d-dimensional vector of scores into a linear binary classifier. We use log loss. We ex-periment with both type-level word embeddings (GloVe.6B.300d; Pennington et al., 2014) and contextual embeddings (BERT large; Devlin et al., 2019). [Cite_Footnote_8] In both cases, we keep the embeddings fixed, so the vast majority of the learnable pa-rameters are in the WFSAs. We train mod-els using GloVe embeddings on all datasets. Due to memory constraints we evaluate BERT embed-dings (frozen, not fine-tuned) only on the smallest dataset (kitchen).",Method,Code,False,Compare（引用目的）,True,D19-1110_2_0,2019,RNN Architecture Learning with Sparse Regularization,Footnote
692,10696," https://github.com/umich-vl/think_visually"," ['References']",Experi-mental results validate the effectiveness of our proposed DSMN for visual thinking tasks [Cite_Footnote_1] .,1 Code and datasets: https://github.com/umich-vl/think_visually,"In this paper, we study the problem of geometric reasoning in the context of question-answering. We introduce Dy-namic Spatial Memory Network (DSMN), a new deep network architecture designed for answering questions that admit latent visual representations. DSMN learns to generate and reason over such representa-tions. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIn-tersection, to evaluate the geometric rea-soning capability of QA systems. Experi-mental results validate the effectiveness of our proposed DSMN for visual thinking tasks [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,P18-1242_0_0,2018,Think Visually: Question Answering through Virtual Imagery,Footnote
693,10697," https://github.com/domluna/memn2n"," ['5 Experiments']","For MemN2N, we use the publicly available im-plementation [Cite_Footnote_2] and train it exactly as all other mod-els (same optimizer, total epochs, and early stop-ping criteria) for fairness.",2 https://github.com/domluna/memn2n,"For MemN2N, we use the publicly available im-plementation [Cite_Footnote_2] and train it exactly as all other mod-els (same optimizer, total epochs, and early stop-ping criteria) for fairness. While the reported best result for MemN2N is on the version with posi-tion encoding, linear start training, and random-injection of time index noise (Sukhbaatar et al., 2015), the version we use has only position encod-ing. Note that the comparison is still meaningful because linear start training and time index noise are not used in DMN+ (and as a result, neither in our proposed DSMN).",Method,Code,True,Use（引用目的）,True,P18-1242_1_0,2018,Think Visually: Question Answering through Virtual Imagery,Footnote
694,10698," https://github.com/allenai/label_rationale_association"," ['References']","Via two tests, robustness equivalence and feature importance agreement, we find that state-of-the-art T5-based joint models exhibit desir-able properties for explaining commonsense question-answering and natural language infer-ence, indicating their potential for producing faithful free-text rationales. [Cite_Footnote_1]",1 Our code is available at https://github.com/allenai/label_rationale_association.,"In interpretable NLP, we require faithful ratio-nales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text nat-ural language rationales. We demonstrate that pipelines, models for faithful rationaliza-tion on information-extraction style tasks, do not work as well on “reasoning” tasks requir-ing free-text rationales. We turn to models that jointly predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the ex-tent to which the labels and rationales pre-dicted by these models are associated, a nec-essary property of faithful explanation. Via two tests, robustness equivalence and feature importance agreement, we find that state-of-the-art T5-based joint models exhibit desir-able properties for explaining commonsense question-answering and natural language infer-ence, indicating their potential for producing faithful free-text rationales. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.804_0_0,2021,Measuring Association Between Labels and Free-Text Rationales,Footnote
695,10699," https://christophm.github.io/interpretable-ml-book/"," ['1 Introduction']","Interpretable NLP aims to better understand pre-dictive models’ internals for purposes such as de-bugging, validating safety before deployment, or revealing unintended biases and behavior (Molnar, 2019) [Cite_Ref] .",Christoph Molnar. 2019. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.,"Interpretable NLP aims to better understand pre-dictive models’ internals for purposes such as de-bugging, validating safety before deployment, or revealing unintended biases and behavior (Molnar, 2019) [Cite_Ref] . These objectives require faithful rationales— explanations of the model’s behavior that are accu-rate representations of its decision process (Melis and Jaakkola, 2018).",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.804_1_0,2021,Measuring Association Between Labels and Free-Text Rationales,Reference
696,10700," https://github.com/salesforce/cos-e/issues/2"," ['A Additional Information', 'A.2 Details of Datasets']","The two versions (v1.0, v1.11) of CoS-E correspond to the first and second versions of the Common-senseQA dataset. CoS-E v1.11 has some noise in its annotations (Narang et al., 2020). [Cite_Footnote_9]",9 https://github.com/salesforce/cos-e/issues/2,"We summarize dataset statistics in Table 7. The two versions (v1.0, v1.11) of CoS-E correspond to the first and second versions of the Common-senseQA dataset. CoS-E v1.11 has some noise in its annotations (Narang et al., 2020). [Cite_Footnote_9] This is our primary motivation for reporting on v1.0 as well, which we observe does not have these issues. A.3 Details of T5 The T5 model (Raffel et al., 2020) is pretrained on a multi-task mixture of unsupervised and supervised tasks, including machine translation, question an-swering, abstractive summarization, and text classi-fication. Its inputs and outputs to every task are text sequences; we provide the input-output formatting for training and decoding of our T5 models in Ta-ble 8. T5 can provide any word in the vocabulary as an answer.",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.804_2_0,2021,Measuring Association Between Labels and Free-Text Rationales,Footnote
697,10701," https://huggingface.co/docs/datasets/master/#"," ['A Additional Information', 'A.4 Implementation Details']",Training ends if the validation set loss has not decreased for [Cite_Footnote_10] epochs.,10 https://huggingface.co/docs/datasets/master/#,"We use Huggingface Datasets 10 to access all datasets, and Huggingface Transformers (Wolf et al., 2020) to access pretrained T5 weights and to-kenizer. To optimize, we use Adam with = 1e-8, β 1 = 0.9, and β 2 = 0.99. We use gradient clipping to a maximum norm of 1.0 and a dropout rate of 0.1. We train each model on a NVIDIA RTX 8000 GPU (48GB memory) for maximum 200 epochs with a batch size of 64 and a learning rate linearly decaying from 5e-5. Training ends if the validation set loss has not decreased for [Cite_Footnote_10] epochs. Early stopping occurs within 15 epochs for most mod-els. Most CoS-E models train in less than 1 hour and most E-SNLI models in around 30. At infer-ence time, we greedy-decode until an EOS token is generated (or for 200 tokens). Approximating the 64-batch model with a batch-size of 16 and 4 gradient accumulation steps on 8GB memory cloud GPUs, we sweep starting learning rates of 1e-2, 1e- 3, 1e-4, 5e-5, and 1e-5. The two largest learning rates never result in good performance. Among the smallest three rates, performance across all model variants (I→R, I→OR, R→O, I→O, and IR→O) on E-SNLI and CoS-E v1.0 never varies by more than 1.58% accuracy or 0.34 BLEU.",Material,DataSource,False,Compare（引用目的）,False,2021.emnlp-main.804_3_0,2021,Measuring Association Between Labels and Free-Text Rationales,Footnote
698,10702," https://github.com/viczong/measuring_forecasting_skill_from_text"," ['References']",This could potentially be useful for identifying ac-curate predictions or potentially skilled fore-casters earlier. [Cite_Footnote_1],1 We provide our code and dataset descriptions at: https://github.com/viczong/measuring_forecasting_skill_from_text.,"People vary in their ability to make accurate predictions about the future. Prior studies have shown that some individuals can pre-dict the outcome of future events with con-sistently better accuracy. This leads to a nat-ural question: what makes some forecasters better than others? In this paper we explore connections between the language people use to describe their predictions and their forecast-ing skill. Datasets from two different fore-casting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an on-line prediction forum and (2) a corpus of com-pany earnings forecasts made by financial an-alysts. We present a number of linguistic met-rics which are computed over text associated with people’s predictions about the future in-cluding: uncertainty, readability, and emotion. By studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters. Fur-thermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language. This could potentially be useful for identifying ac-curate predictions or potentially skilled fore-casters earlier. [Cite_Footnote_1]",補足資料,Document,True,Produce（引用目的）,True,2020.acl-main.473_0_0,2020,Measuring Forecasting Skill from Text,Footnote
699,10703," https://www.gjopen.com/"," ['2 Linguistic Cues of Accurate Forecasting', '2.1 Geopolitical Forecasting Data']","To explore the connections between language and forecasting skill, we make use of data from Good Judgment Open, [Cite_Footnote_2] an online prediction forum.",2 https://www.gjopen.com/,"To explore the connections between language and forecasting skill, we make use of data from Good Judgment Open, [Cite_Footnote_2] an online prediction forum. Users of this website share predictions in response to a number of pre-specified questions about future events with uncertain outcomes, such as: “Will North Korea fire another intercontinental ballistic missile before August 2019?” Users’ predictions consist of an estimated chance the event will oc-cur (for example, 5%) in addition to an optional text justification that explains why the forecast was made. A sample is presented in Figure 1.",Material,DataSource,True,Use（引用目的）,True,2020.acl-main.473_1_0,2020,Measuring Forecasting Skill from Text,Footnote
700,10704," https://www.cfraresearch.com/"," ['3 Companies’ Earnings Forecasts']",We analyze reports from the Center for Fi-nancial Research and Analysis (CFRA). [Cite_Footnote_8],8 https://www.cfraresearch.com/,"Data. We analyze reports from the Center for Fi-nancial Research and Analysis (CFRA). [Cite_Footnote_8] These reports provide frequent updates for analysts’ esti-mates and are also organized in a structured way, enabling us to accurately extract numerical fore-casts and corresponding text justifications.",Material,DataSource,True,Use（引用目的）,True,2020.acl-main.473_2_0,2020,Measuring Forecasting Skill from Text,Footnote
701,10705," https://www.thomsonone.com/"," ['3 Companies’ Earnings Forecasts']",We collected CFRA’s analyst reports from the Thomson ONE database [Cite_Footnote_9] from 2014 to 2018.,9 https://www.thomsonone.com/,"We collected CFRA’s analyst reports from the Thomson ONE database [Cite_Footnote_9] from 2014 to 2018. All notes making forecasts are extracted under the “An-alyst Research Notes and other Company News” section. The dataset contains a total of 32,807 notes from analysts, covering 1,320 companies.",Material,DataSource,True,Use（引用目的）,True,2020.acl-main.473_3_0,2020,Measuring Forecasting Skill from Text,Footnote
702,10706," http://ckip.iis.sinica.edu.tw/DOMCAT/"," ['References']","In this paper, we propose a web-based bilingual concordancer, DOMCAT [Cite_Footnote_1] , for domain-specific computer assisted translation.",1 http://ckip.iis.sinica.edu.tw/DOMCAT/,"In this paper, we propose a web-based bilingual concordancer, DOMCAT [Cite_Footnote_1] , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively.",Method,Tool,True,Produce（引用目的）,True,P12-3010_0_0,2012,DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation,Footnote
703,10707," http://www.npm.gov.tw"," ['3 Experimental Results', '3.1 Experimental Setting']",We use the Chinese/English web pages of the National Palace Museum [Cite_Footnote_2] as our underlying parallel corpus.,2 http://www.npm.gov.tw,"We use the Chinese/English web pages of the National Palace Museum [Cite_Footnote_2] as our underlying parallel corpus. It contains about 30,000 sentences in each language. We exploited the Champollion Toolkit (Ma et al., 2006) to align the sentence pairs. The English sentences are tokenized and lemmatized by using the NLTK (Bird and Loper, 2004) and the Chinese sentences are segmented by the CKIP Chinese segmenter (Ma and Chen, 2003).",Material,DataSource,True,Use（引用目的）,True,P12-3010_1_0,2012,DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation,Footnote
704,10708," https://github.com/rtmdrr/testSignificanceNLP.git"," ['References']",We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [Cite_Footnote_1] .,1 The code for all statistical tests detailed in this pa-per is found on: https://github.com/rtmdrr/testSignificanceNLP.git,"Statistical significance testing is a standard sta-tistical tool designed to ensure that experimen-tal results are not coincidental. In this opin-ion/theoretical paper we discuss the role of statis-tical significance testing in Natural Language Pro-cessing (NLP) research. We establish the funda-mental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental se-tups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical pro-tocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental re-sults, statistical significance testing is often ig-nored or misused. We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,P18-1128_0_0,2018,The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing,Footnote
705,10709," https://github.com/ridiculouz/CKBQA"," ['References']","Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset [Cite_Footnote_1] with such strat-egy is also published to promote further re-search.",1 https://github.com/ridiculouz/CKBQA,"We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding men-tions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outper-forms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset [Cite_Footnote_1] with such strat-egy is also published to promote further re-search. An online demo of NAMER is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion of NAMER is also available.",Material,Dataset,True,Produce（引用目的）,True,2021.naacl-demos.3_0_0,2021,NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering,Footnote
706,10710," http://kbqademo.gstore.cn"," ['References']","An online demo of NAMER [Cite_Footnote_2] is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion of NAMER is also available.",2 http://kbqademo.gstore.cn,"We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding men-tions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outper-forms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset with such strat-egy is also published to promote further re-search. An online demo of NAMER [Cite_Footnote_2] is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion of NAMER is also available.",Method,Tool,True,Produce（引用目的）,True,2021.naacl-demos.3_1_0,2021,NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering,Footnote
707,10711," https://youtu.be/yetnVye_hg4"," ['References']","An online demo of NAMER is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion [Cite_Footnote_3] of NAMER is also available.",3 https://youtu.be/yetnVye_hg4,"We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding men-tions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outper-forms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset with such strat-egy is also published to promote further re-search. An online demo of NAMER is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion [Cite_Footnote_3] of NAMER is also available.",補足資料,Media,True,Introduce（引用目的）,True,2021.naacl-demos.3_2_0,2021,NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering,Footnote
708,10712," https://www.biendata.xyz/competition/ccks_2019_6/data/"," ['4 Experiments', '4.1 Experimental Setup']",Dataset We utilize the dataset published in CCKS Chinese KBQA Contest [Cite_Footnote_4] for evaluation.,4 https://www.biendata.xyz/competition/ccks_2019_6/data/,"Dataset We utilize the dataset published in CCKS Chinese KBQA Contest [Cite_Footnote_4] for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training.",Material,Dataset,True,Use（引用目的）,True,2021.naacl-demos.3_3_0,2021,NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering,Footnote
709,10713," http://pkubase.gstore.cn/"," ['4 Experiments', '4.1 Experimental Setup']","The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE [Cite_Footnote_5] , a Chinese KB based on Baidu Baike.",5 A KB endpoint: http://pkubase.gstore.cn/,"Dataset We utilize the dataset published in CCKS Chinese KBQA Contest for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE [Cite_Footnote_5] , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training.",Material,Knowledge,False,Use（引用目的）,True,2021.naacl-demos.3_4_0,2021,NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering,Footnote
710,10714,https://github.com/pkumod/gAnswer,"[\'4 Experiments\', \'4.1 Experimental Setup\']","Baselines We compare our results with the top ranking team ""jchl"" (Luo et al., 2019) in the contest and a competitive KBQA system gAnswer [Cite_Footnote_7] (Hu et al., 2018) that reached first place in QALD-9 (Ngomo, 2018).",7 https://github.com/pkumod/gAnswer,"Baselines We compare our results with the top ranking team ""jchl"" (Luo et al., 2019) in the contest and a competitive KBQA system gAnswer [Cite_Footnote_7] (Hu et al., 2018) that reached first place in QALD-9 (Ngomo, 2018). Since the NE and RE module in gAnswer does not officially support Chinese, we replace them with those in our system. Hence, the gAnswer evaluated can be partly viewed as our system with a rule-based QG module and its comparison with us indicates the effectiveness of our generative QG module.",Method,Code,True,Compare（引用目的）,True,2021.naacl-demos.3_5_0,2021,NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering,Footnote
711,10715," https://github.com/ymcui/Chinese-BERT-wwm"," ['4 Experiments', '4.1 Experimental Setup']","Setup We adopt Chinese RoBERTa-large (Cui et al., 2020) in transformers library (Wolf et al., 2020) released by HFL [Cite_Footnote_8] as encoder and a 6-layer 8-head transformer as decoder.",8 Pretrained weights: https://github.com/ymcui/Chinese-BERT-wwm,"Setup We adopt Chinese RoBERTa-large (Cui et al., 2020) in transformers library (Wolf et al., 2020) released by HFL [Cite_Footnote_8] as encoder and a 6-layer 8-head transformer as decoder. For our best results, we co-train the NE and QG models, remaining RE as a separate model. For NEQG, we train the encoder and decoder with learning rate 1e-6 and 4e-6 respectively with an Adam (Kingma and Ba, 2015) optimizer, setting hyperparameters to γ = 1, α = β = 2.5, θ = 0 and batch size to 40. For RE model, we set the learning rate and batch size to 1e-5 and 96 respectively with γ = α = β = 0, θ = 1. Both models are trained until no progress on validation accuracy for at most 10k steps.",Material,Knowledge,False,Produce（引用目的）,False,2021.naacl-demos.3_6_0,2021,NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering,Footnote
712,10716," https://web.stanford.edu/~jurafsky/ws97/manual.august1.html"," ['4 Experiment', '4.1 Switch Board Dialogue Act Corpus (SwDA)']",The SwDA cor-pus conforms to the damsl tag schema. [Cite_Footnote_1],1 https://web.stanford.edu/˜jurafsky/ws97/manual.august1.html,"We evaluate the accuracy of our model to pre-dict the DA of the next response using the SwDA corpus, which transcribes telephone conversation and annotates DAs of utterances. The SwDA cor-pus conforms to the damsl tag schema. [Cite_Footnote_1] We as-sembled the tag sets referring to easy damsl (Iso-mura et al., 2009) into 9 tags (Table 2) in order to consolidate tags with a significantly low fre-quency. The SwDA corpus provides transcriptions of 1, 155 conversations with 219, 297 utterances. One conversation contains 189 utterances on aver-age. Because the average length of utterance se-quences is large, we use a sliding window with a size of 5 to cut a sequence into several conversa-tions.",補足資料,Document,True,Use（引用目的）,True,P19-2027_0_0,2019,Dialogue-Act Prediction of Future Responses based on Conversation History,Footnote
713,10717," http://translate.google.com"," ['3 Problem Definition and Baseline Ap-proaches']",We adopt Google Translate [Cite_Footnote_1] for review transla-tion.,1 http://translate.google.com,"The task is a regression problem and it is chal-lenging due to the language gap between the la-beled training dataset and the test dataset. Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation. We adopt Google Translate [Cite_Footnote_1] for review transla-tion. After review translation, the training re-views and the test reviews are now in the same language, and any regression algorithm (e.g. lo-gistic regression, least squares regression, KNN regressor) can be applied for learning and predic-tion. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the SVMLight toolkit 2 as the basic regressor. For comparative analysis, we simply use the default parameter values in SVMLight with linear kernel. The features include all unigrams and bigrams in the review texts, and the value of each feature is simply set to its frequency (TF) in a review.",Method,Tool,True,Use（引用目的）,True,P13-2094_0_0,2013,Co-Regression for Cross-Language Review Rating Prediction,Footnote
714,10718," http://svmlight.joachims.org"," ['3 Problem Definition and Baseline Ap-proaches']","In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the SVMLight toolkit [Cite_Footnote_2] as the basic regressor.",2 http://svmlight.joachims.org,"The task is a regression problem and it is chal-lenging due to the language gap between the la-beled training dataset and the test dataset. Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation. We adopt Google Translate 1 for review transla-tion. After review translation, the training re-views and the test reviews are now in the same language, and any regression algorithm (e.g. lo-gistic regression, least squares regression, KNN regressor) can be applied for learning and predic-tion. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the SVMLight toolkit [Cite_Footnote_2] as the basic regressor. For comparative analysis, we simply use the default parameter values in SVMLight with linear kernel. The features include all unigrams and bigrams in the review texts, and the value of each feature is simply set to its frequency (TF) in a review.",Method,Tool,True,Use（引用目的）,True,P13-2094_1_0,2013,Co-Regression for Cross-Language Review Rating Prediction,Footnote
715,10719," http://www.uni-weimar.de/medien/webis/research/corpora/corpus-webis-cls-10.html"," ['5 Empirical Evaluation']","We used the WEBIS-CLS-10 corpus [Cite_Footnote_3] provided by (Prettenhofer and Stein, 2010) for evaluation.",3 http://www.uni-weimar.de/medien/webis/research/corpora/corpus-webis-cls-10.html,"We used the WEBIS-CLS-10 corpus [Cite_Footnote_3] provided by (Prettenhofer and Stein, 2010) for evaluation. It consists of Amazon product reviews for three product categories (i.e. books, dvds and music) written in different languages including English, German, etc. For each language-category pair there exist three sets of training documents, test documents, and unlabeled documents. The train-ing and test sets comprise 2000 documents each, whereas the number of unlabeled documents var-ies from 9000 – 170000. The dataset is provided with the rating score between 1 to 5 assigned by users, which can be used for the review rating prediction task. We extracted texts from both the summary field and the text field to represent a review text. We then extracted the rating score as a review’s corresponding real-valued label. In the cross-language scenario, we regarded English as the source language, and regarded German as the target language. The experiments were con-ducted on each product category separately. Without loss of generality, we sampled and used only 8000 unlabeled documents for each product category. We use Mean Square Error (MSE) as the evaluation metric, which penalizes more se-vere errors more heavily.",Material,Dataset,True,Compare（引用目的）,True,P13-2094_2_0,2013,Co-Regression for Cross-Language Review Rating Prediction,Footnote
716,10720," https://github.com/shacharosn/CRE"," ['1 Introduction']",We release the challenge set to encourage future research on better models. [Cite_Footnote_1],1 GitHub repository with data and code: https://github.com/shacharosn/CRE,"We show that state of the art models trained on TACRED are often “right for the wrong reasons” (McCoy et al., 2019): instead of learning to per-form the intended task, they rely on shallow heur-istics which are effective for solving many data-set instances, but which may fail on more challen-ging examples. In particular, we show two con-crete heuristics: classifying based on entity types, and classifying based on the existence of an event without linking the event to its arguments. We show that while they are not well attested in the dev and test sets, these challenging examples do occur in practice. We introduce CRE (Challenging RE), a challenge set for quantifying and demonstrating the problem, and show that four SOTA RC models significantly fail on the challenge set. We release the challenge set to encourage future research on better models. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.302_0_0,2020,Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data,Footnote
717,10721," https://github.com/thunlp/OpenQA"," ['References']",The source code and data of this paper can be obtained from [Cite] https: //github.com/thunlp/OpenQA,,"Distantly supervised open-domain ques-tion answering (DS-QA) aims to find an-swers in collections of unlabeled text. Ex-isting DS-QA models usually retrieve re-lated paragraphs from a large-scale corpus and apply reading comprehension tech-nique to extract answers from the most rel-evant paragraph. They ignore the rich in-formation contained in other paragraphs. Moreover, distant supervision data in-evitably accompanies with the wrong la-beling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we pro-pose a novel DS-QA model which em-ploys a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimen-tal results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from [Cite] https: //github.com/thunlp/OpenQA",Mixed,Mixed,True,Produce（引用目的）,True,P18-1161_0_0,2018,Denoising Distantly Supervised Open-Domain Question Answering,Body
718,10722," https://github.com/bdhingra/quasar"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']","Quasar-T [Cite_Footnote_1] (Dhingra et al., 2017b) consists of 43, 000 open-domain trivia question, and their an-swers are extracted from ClueWeb09 data source, and the paragraphs are obtained by retrieving 50 sentences for each question from the ClueWeb09 data source using LUCENE.",1 https://github.com/bdhingra/quasar,"Quasar-T [Cite_Footnote_1] (Dhingra et al., 2017b) consists of 43, 000 open-domain trivia question, and their an-swers are extracted from ClueWeb09 data source, and the paragraphs are obtained by retrieving 50 sentences for each question from the ClueWeb09 data source using LUCENE.",Material,Dataset,True,Use（引用目的）,False,P18-1161_1_0,2018,Denoising Distantly Supervised Open-Domain Question Answering,Footnote
719,10723," https://github.com/nyu-dl/SearchQA"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']","SearchQA [Cite_Footnote_2] (Dunn et al., 2017) is a large-scale open domain question answering dataset, which consists of question-answer pairs crawled from J!",2 https://github.com/nyu-dl/SearchQA,"SearchQA [Cite_Footnote_2] (Dunn et al., 2017) is a large-scale open domain question answering dataset, which consists of question-answer pairs crawled from J! Archive, and the paragraphs are obtained by retrieving 50 webpages for each question from Google Search API.",Material,Dataset,True,Introduce（引用目的）,True,P18-1161_2_0,2018,Denoising Distantly Supervised Open-Domain Question Answering,Footnote
720,10724," http://nlp.cs.washington.edu/triviaqa/"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']","TriviaQA [Cite_Footnote_3] (Joshi et al., 2017) includes 95, 000 question-answer pairs authored by trivia enthusi-asts and independently gathered evidence docu-ments, six per question on average, and utilizes Bing Web search API to collect 50 webpages re-lated to the questions.",3 http://nlp.cs.washington.edu/triviaqa/,"TriviaQA [Cite_Footnote_3] (Joshi et al., 2017) includes 95, 000 question-answer pairs authored by trivia enthusi-asts and independently gathered evidence docu-ments, six per question on average, and utilizes Bing Web search API to collect 50 webpages re-lated to the questions.",Material,Dataset,True,Introduce（引用目的）,True,P18-1161_3_0,2018,Denoising Distantly Supervised Open-Domain Question Answering,Footnote
721,10725," https://github.com/brmson/dataset-factoid-curated/tree/master/trec"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']","CuratedTREC [Cite_Footnote_4] (Voorhees et al., 1999) is based on the benchmark from the TREC QA tasks, which contains 2, 180 questions extracted from the datasets from TREC1999, 2000, 2001 and 2002.",4 https://github.com/brmson/dataset-factoid-curated/tree/master/trec,"CuratedTREC [Cite_Footnote_4] (Voorhees et al., 1999) is based on the benchmark from the TREC QA tasks, which contains 2, 180 questions extracted from the datasets from TREC1999, 2000, 2001 and 2002.",Material,Dataset,True,Introduce（引用目的）,False,P18-1161_4_0,2018,Denoising Distantly Supervised Open-Domain Question Answering,Footnote
722,10726," https://github.com/brmson/dataset-factoid-webquestions"," ['4 Experiments', '4.1 Datasets and Evaluation Metrics']","WebQuestions [Cite_Footnote_5] (Berant et al., 2013b) is de-signed for answering questions from the Free-base knowledge base, which is built by crawl-ing questions through the Google Suggest API and the paragraphs are retrieved from the English Wikipedia using .",5 https://github.com/brmson/ dataset-factoid-webquestions,"WebQuestions [Cite_Footnote_5] (Berant et al., 2013b) is de-signed for answering questions from the Free-base knowledge base, which is built by crawl-ing questions through the Google Suggest API and the paragraphs are retrieved from the English Wikipedia using .",Material,Dataset,True,Introduce（引用目的）,False,P18-1161_5_0,2018,Denoising Distantly Supervised Open-Domain Question Answering,Footnote
723,10727," http://nlp.stanford.edu/data/glove.840B.300d.zip"," ['4 Experiments', '4.3 Experimental Settings']","For pre-trained word embeddings, we use the 300-dimensional GloVe [Cite_Footnote_6] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data.",6 http://nlp.stanford.edu/data/glove.840B.300d.zip,"For training, our Our+FULL model is first ini-tialized by pre-training using Our+AVG model, and we set the iteration number over all the train-ing data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe [Cite_Footnote_6] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data.",Material,Dataset,True,Use（引用目的）,True,P18-1161_6_0,2018,Denoising Distantly Supervised Open-Domain Question Answering,Footnote
724,10728," http://tallinzen.net/media/papers/linzen_gallagher_2015.pdf"," ['1 Introduction']","Recent studies have investigated how humans acquire generalizations over phonological classes in an artificial language paradigm (Linzen and Gallagher, 2014; Linzen and Gallagher, 2015 [Cite_Ref] ).",Tal Linzen and Gillian Gallagher. 2015. Rapid generalization in phonotactic learning. http://tallinzen.net/media/papers/linzen_gallagher_2015.pdf.,"Recent studies have investigated how humans acquire generalizations over phonological classes in an artificial language paradigm (Linzen and Gallagher, 2014; Linzen and Gallagher, 2015 [Cite_Ref] ). The central finding of these studies was that participants rapidly learned abstract phonotactic constraints and exhibited evidence of generaliza-tions over classes of sounds before evidence of phoneme-specific knowledge.",補足資料,Paper,True,Introduce（引用目的）,True,D15-1134_0_0,2015,A model of rapid phonotactic generalization,Reference
725,10729," http://tallinzen.net/media/papers/linzen_gallagher_2015.pdf"," ['2 Summary of behavioral data']",The experiments are described in detail in Linzen and Gallagher (2014) and Linzen and Gallagher (2015) [Cite_Ref] ; we summarize the main details here.,Tal Linzen and Gillian Gallagher. 2015. Rapid generalization in phonotactic learning. http://tallinzen.net/media/papers/linzen_gallagher_2015.pdf.,The experiments are described in detail in Linzen and Gallagher (2014) and Linzen and Gallagher (2015) [Cite_Ref] ; we summarize the main details here.,補足資料,Paper,True,Introduce（引用目的）,True,D15-1134_0_1,2015,A model of rapid phonotactic generalization,Reference
726,10730," https://answers.yahoo.com/"," ['References']","Answers [Cite_Footnote_1] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.",1 https://answers.yahoo.com/,"Community Question Answering (cQA) services like Yahoo! Answers [Cite_Footnote_1] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the “lexico-syntactic” gap between the current and the previous questions. In this paper, we pro-pose a novel approach called “Siamese Convolutional Neural Network for cQA (SCQA)” to find the semantic similarity between the current and the archived ques-tions. SCQA consist of twin convolu-tional neural networks with shared param-eters and a contrastive loss function join-ing them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA fo-rum archives. The model projects semanti-cally similar question pairs nearer to each other and dissimilar question pairs far-ther away from each other in the seman-tic space. Experiments on large scale real-life “Yahoo! Answers” dataset reveals that SCQA outperforms current state-of-the-art approaches based on translation mod-els, topic models and deep neural network",補足資料,Website,True,Introduce（引用目的）,True,P16-1036_0_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
727,10731," http://zhidao.baidu.com/"," ['References']","Answers , Baidu Zhidao [Cite_Footnote_2] , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.",2 http://zhidao.baidu.com/,"Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao [Cite_Footnote_2] , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the “lexico-syntactic” gap between the current and the previous questions. In this paper, we pro-pose a novel approach called “Siamese Convolutional Neural Network for cQA (SCQA)” to find the semantic similarity between the current and the archived ques-tions. SCQA consist of twin convolu-tional neural networks with shared param-eters and a contrastive loss function join-ing them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA fo-rum archives. The model projects semanti-cally similar question pairs nearer to each other and dissimilar question pairs far-ther away from each other in the seman-tic space. Experiments on large scale real-life “Yahoo! Answers” dataset reveals that SCQA outperforms current state-of-the-art approaches based on translation mod-els, topic models and deep neural network",補足資料,Website,True,Introduce（引用目的）,True,P16-1036_1_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
728,10732," http://www.quora.com/"," ['References']","Answers , Baidu Zhidao , Quora [Cite_Footnote_3] , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.",3 http://www.quora.com/,"Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora [Cite_Footnote_3] , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the “lexico-syntactic” gap between the current and the previous questions. In this paper, we pro-pose a novel approach called “Siamese Convolutional Neural Network for cQA (SCQA)” to find the semantic similarity between the current and the archived ques-tions. SCQA consist of twin convolu-tional neural networks with shared param-eters and a contrastive loss function join-ing them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA fo-rum archives. The model projects semanti-cally similar question pairs nearer to each other and dissimilar question pairs far-ther away from each other in the seman-tic space. Experiments on large scale real-life “Yahoo! Answers” dataset reveals that SCQA outperforms current state-of-the-art approaches based on translation mod-els, topic models and deep neural network",補足資料,Website,True,Introduce（引用目的）,True,P16-1036_2_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
729,10733," http://stackoverflow.com/"," ['References']","Answers , Baidu Zhidao , Quora , StackOverflow [Cite_Footnote_4] etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.",4 http://stackoverflow.com/,"Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora , StackOverflow [Cite_Footnote_4] etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the “lexico-syntactic” gap between the current and the previous questions. In this paper, we pro-pose a novel approach called “Siamese Convolutional Neural Network for cQA (SCQA)” to find the semantic similarity between the current and the archived ques-tions. SCQA consist of twin convolu-tional neural networks with shared param-eters and a contrastive loss function join-ing them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA fo-rum archives. The model projects semanti-cally similar question pairs nearer to each other and dissimilar question pairs far-ther away from each other in the seman-tic space. Experiments on large scale real-life “Yahoo! Answers” dataset reveals that SCQA outperforms current state-of-the-art approaches based on translation mod-els, topic models and deep neural network",補足資料,Website,True,Introduce（引用目的）,True,P16-1036_3_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
730,10734," http://www.openchannelfoundation.org/projects/Qanda/"," ['2 Related Work']","On a different line of research, several Textual-based Question Answering (QA) systems (Qanda [Cite_Footnote_5] , QANUS , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources.",5 http://www.openchannelfoundation.org/projects/Qanda/,"On a different line of research, several Textual-based Question Answering (QA) systems (Qanda [Cite_Footnote_5] , QANUS , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog 8 , NLBean 9 etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA.",Method,Tool,True,Introduce（引用目的）,True,P16-1036_4_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
731,10735," http://www.qanus.com/"," ['2 Related Work']","On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS [Cite_Footnote_6] , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources.",6 http://www.qanus.com/,"On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS [Cite_Footnote_6] , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog 8 , NLBean 9 etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA.",Method,Tool,True,Introduce（引用目的）,True,P16-1036_5_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
732,10736," http://www.dzonesoftware.com/products/open-source-question-answer-software/"," ['2 Related Work']","On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS , QSQA [Cite_Footnote_7] etc.) are developed that retrieve answers from the Web and other tex-tual sources.",7 http://www.dzonesoftware.com/products/open-source-question-answer-software/,"On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS , QSQA [Cite_Footnote_7] etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog 8 , NLBean 9 etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA.",Method,Tool,True,Introduce（引用目的）,True,P16-1036_6_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
733,10737," http://technologies.kmi.open.ac.uk/aqualog/"," ['2 Related Work']","Similarly, structured QA systems (Aqualog [Cite_Footnote_8] , NLBean etc.) obtain answers from the sub-networks.",8 http://technologies.kmi.open.ac.uk/aqualog/,"On a different line of research, several Textual-based Question Answering (QA) systems (Qanda 5 , QANUS 6 , QSQA 7 etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog [Cite_Footnote_8] , NLBean etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA.",Method,Tool,True,Introduce（引用目的）,True,P16-1036_7_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
734,10738," http://www.markwatson.com/opensource/"," ['2 Related Work']","Similarly, structured QA systems (Aqualog , NLBean [Cite_Footnote_9] etc.) obtain answers from the sub-networks.",9 http://www.markwatson.com/opensource/,"On a different line of research, several Textual-based Question Answering (QA) systems (Qanda 5 , QANUS 6 , QSQA 7 etc.) are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog , NLBean [Cite_Footnote_9] etc.) obtain answers from the sub-networks. structured information sources with predefined on-tologies. QALL-ME Framework (Ferrandez et al., 2011) is a reusable multilingual QA architecture built using structured data modeled by an ontol-ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques-tion retrieval in SCQA.",Method,Tool,True,Introduce（引用目的）,True,P16-1036_8_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
735,10739," https://lucene.apache.org/"," ['6 Siamese Neural Network with Textual Similarity']",Lucene [Cite_Footnote_10] is used to cal-culate the BM25 scores for question pairs.,10 https://lucene.apache.org/,"Though SCQA can strongly model semantic relations between documents, it needs boosting in the area of textual similarity. The sense of word based similarity is infused to SCQA by using BM25 ranking algorithm. Lucene [Cite_Footnote_10] is used to cal-culate the BM25 scores for question pairs. The score from similarity metric of SCQA is com-bined with the BM25 score. A new similarity score is calculated by the weighted combination of the SCQA and BM25 score as: where α control the weights given to SCQA and BM25 models. It range from 0 to 1. SCQA with this improved similarity metric is called Siamese Convolutional Neural Network for cQA with Textual Similartity (T-SCQA). Figure 4 de-picts the testing phase of T-SCQA. This model will give better performance in datasets with good mix of questions that are lexically and semantically similar. The value of α can be tuned according to the nature of dataset.",Method,Tool,True,Use（引用目的）,True,P16-1036_9_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
736,10740," http://webscope.sandbox.yahoo.com/catalog.php?datatype=l"," ['7 Experiments']",Labs Webscope [Cite_Footnote_11] .,11 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,"We collected Yahoo! Answers dataset from Yahoo! Labs Webscope [Cite_Footnote_11] . Each question in the dataset contains title, description, best an-swer, most voted answers and meta-data like categories, sub categories etc. For training dataset, we randomly selected 2 million data and extracted question-relevant answer pairs and question-irrelevant answer pairs from them to train SCQA. Similarly, our validation dataset contains 400,000 question answer pairs. The hyperparam-eters of the network are tuned on the validation dataset. The values of the hyperparameters for which we obtained the best results is shown in Ta-ble 1.",Material,DataSource,True,Use（引用目的）,True,P16-1036_10_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
737,10741," http://www.statmt.org/moses/giza/GIZA++.html"," ['8 Results']",The models are trained using GIZA++ [Cite_Footnote_12] tool with the question and best answer pair as the parallel corpus.,12 http://www.statmt.org/moses/giza/GIZA++.html,"We did a comparative study of the results of the previous methods with respect to SCQA and T-SCQA. The baseline performance is shown by query likelihood language model (LM). For the translation based methods translation(word), translation+LM and translation(phrase) we implemented the papers by Jeon et al. (2005), Xue et al. (2008), Zhou et al. (2011) respec-tively. The first paper deals with word based trans-lation, the second enhanced the first by adding lan-guage model to it and the last paper implements phrase based translation method to bridge lexi-cal gap. As seen from Table 2, the translation based methods outperforms the baseline signifi-cantly. The models are trained using GIZA++ [Cite_Footnote_12] tool with the question and best answer pair as the parallel corpus. For the topic based Q-A topic model and Q-A topic model(s), we implemented the models QATM -PR (Question-Answer Topic Model) Ji et al.(2012) and T BLM SQATM−V (Su-pervised Question-Answer Topic Model with user votes as supervision) Zhang et al. (2014) respec-tively. Again it is visible from the Table 2 that topic based approaches show slight improvement over translation based methods but they show sig-nificant improvement over baseline. The mod-els DSQA and T-DSQA were built using convo-lutional neural sub-networks joined by a distance measure at the top. There is no sharing of parame-ters involved between the sub-networks of these models. It is clear from the comparison of re-sults between T-DSQA and T-SCQA that param-eter sharing definitely helps in the task of similar question retrieval in cQA forums. T-SCQA outper-forms all the previous approaches significantly.",Method,Tool,True,Use（引用目的）,True,P16-1036_11_0,2016,"Together We Stand: Siamese Networks for Similar Question Retrieval Arpita Das 1 Harish Yenala 1 Manoj Chinnakotla 2,1 Manish Shrivastava 1",Footnote
738,10742," http://ictclas.org/"," ['5 Experimentation', '5.1 Experimental Settings and Baseline']","Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) [Cite_Footnote_1] with all entities annotated in the corpus kept.",1 http://ictclas.org/,"For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) [Cite_Footnote_1] with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.",Method,Tool,True,Use（引用目的）,True,P13-1145_0_0,2013,Argument Inference from Relevant Event Mentions in Chinese Argument Extraction,Footnote
739,10743," http://code.google.com/p/berkeleyparser/"," ['5 Experimentation', '5.1 Experimental Settings and Baseline']",We use Berkeley Parser [Cite_Footnote_2] and Stanford Parser to create the constituent and dependency parse trees.,2 http://code.google.com/p/berkeleyparser/,"For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser [Cite_Footnote_2] and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.",Method,Tool,True,Use（引用目的）,True,P13-1145_1_0,2013,Argument Inference from Relevant Event Mentions in Chinese Argument Extraction,Footnote
740,10744," http://nlp.stanford.edu/software/lex-parser.shtml"," ['5 Experimentation', '5.1 Experimental Settings and Baseline']",We use Berkeley Parser and Stanford Parser [Cite_Footnote_3] to create the constituent and dependency parse trees.,3 http://nlp.stanford.edu/software/lex-parser.shtml,"For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser [Cite_Footnote_3] to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.",Method,Tool,True,Use（引用目的）,True,P13-1145_2_0,2013,Argument Inference from Relevant Event Mentions in Chinese Argument Extraction,Footnote
741,10745," http://mallet.cs.umass.edu/"," ['5 Experimentation', '5.1 Experimental Settings and Baseline']","Besides, the ME tool ( Maxent ) [Cite_Footnote_4] is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.",4 http://mallet.cs.umass.edu/,"For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) [Cite_Footnote_4] is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.",Method,Tool,True,Use（引用目的）,True,P13-1145_3_0,2013,Argument Inference from Relevant Event Mentions in Chinese Argument Extraction,Footnote
742,10746," http://lpsolve.sourceforge.net/5.5/"," ['5 Experimentation', '5.1 Experimental Settings and Baseline']","Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver [Cite_Footnote_5] is used to construct our global argument inference model.",5 http://lpsolve.sourceforge.net/5.5/,"For fair comparison, we adopt the same experimental settings as the state-of-the-art event extraction system (Li et al. 2012b) and all the evaluations are experimented on the ACE 2005 Chinese corpus. We randomly select 567 documents as the training set and the remaining 66 documents as the test set. Besides, we reserve 33 documents in the training set as the development set and use the ground truth entities, times and values for our training and testing. As for evaluation, we also follow the standards as defined in Li et al. (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver [Cite_Footnote_5] is used to construct our global argument inference model.",Method,Tool,True,Use（引用目的）,True,P13-1145_4_0,2013,Argument Inference from Relevant Event Mentions in Chinese Argument Extraction,Footnote
743,10747," https://github.com/ShyamSubramanian/HESM"," ['References']",Our source code is available at [Cite] https://github.com/ShyamSubramanian/HESM.,,"Automated fact extraction and verification is a challenging task that involves finding rele-vant evidence sentences from a reliable cor-pus to verify the truthfulness of a claim. Ex-isting models either (i) concatenate all the ev-idence sentences, leading to the inclusion of redundant and noisy information; or (ii) pro-cess each claim-evidence sentence pair sepa-rately and aggregate all of them later, miss-ing the early combination of related sen-tences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple ev-idence sentences), and verify a claim to be supported, refuted or not enough info, by en-coding and attending the claim and evidence sets at different levels of hierarchy. Our ex-perimental results show that HESM outper-forms 7 state-of-the-art methods for fact ex-traction and claim verification. Our source code is available at [Cite] https://github.com/ShyamSubramanian/HESM.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.627_0_0,2020,Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification,Body
744,10748," https://competitions.codalab.org/competitions/18814"," ['5 Experiments', '5.1 Experiment Setting']","Instead, once extracted evidence sets/sentences and predicted labels of the test set by a model are sub-mitted to the online evaluation system [Cite_Footnote_1] , its perfor-mance is measured and displayed at the system.",1 https://competitions.codalab.org/competitions/18814,"Dataset. We evaluate our framework HESM in the FEVER dataset, a large scale fact verifica-tion dataset (Thorne et al., 2018a). The dataset consists of 185, 445 claims with human-annotated evidence sentences from 5, 416, 537 documents. Each claim is labeled as SUPPORTS, REFUTES, or NOT ENOUGH INFO. The dataset consists of training, development, and test sets, as shown in Table 1. The training and development sets, along with their ground truth evidence and labels are available publicly. But, the ground truth evidence and labels of the test set are not publicly available. Instead, once extracted evidence sets/sentences and predicted labels of the test set by a model are sub-mitted to the online evaluation system [Cite_Footnote_1] , its perfor-mance is measured and displayed at the system. In this work, we train and tune our hyper-parameters on training and development sets, respectively.",補足資料,Website,False,Use（引用目的）,True,2020.emnlp-main.627_1_0,2020,Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification,Footnote
745,10749," https://github.com/fred2008/TCMSA"," ['1 Introduction']",We release our code and models at [Cite] https://github.com/fred2008/TCMSA.,,"Experiments on Stanford Sentiment Treebank (SST; Socher et al. 2013) show that our model outperforms standard bottom-up tree-LSTM (Zhu et al., 2015; Looks et al., 2017) and also re-cent work on bidirectional tree-LSTM (Teng and Zhang, 2017). In addition, our model allows a more holistic prediction of phase-level sentiments over the tree with a high degree of node sen-timent consistency. To our knowledge, we are the first to investigate graph NNs for tree senti-ment classification, and the first to discuss phrase level sentiment consistency over a constituent tree for SST. We release our code and models at [Cite] https://github.com/fred2008/TCMSA.",Method,Code,True,Produce（引用目的）,True,P19-1342_0_0,2019,Tree Communication Models for Sentiment Analysis,Body
746,10750," https://rxhui.com"," ['8 Acknowledgments']",This work is supported by a grant from Rxhui Inc [Cite_Footnote_1] .,1 https://rxhui.com,The corresponding author is Yue Zhang. We thank the anonymous reviewers for their valuable com-ments and suggestions. We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [Cite_Footnote_1] .,補足資料,Website,True,Other（引用目的）,True,P19-1342_1_0,2019,Tree Communication Models for Sentiment Analysis,Footnote
747,10751," http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php"," ['4 Resources', '4.1 Corpus']","The French Treebank [Cite_Footnote_2] [FTB] (Abeillé et al., 2003) is a syntactically annotated corpus made up of jour-nalistic articles from Le Monde newspaper.",2 http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php,"The French Treebank [Cite_Footnote_2] [FTB] (Abeillé et al., 2003) is a syntactically annotated corpus made up of jour-nalistic articles from Le Monde newspaper. We used the latest edition of the corpus (June 2010) that we preprocessed with the Stanford Parser pre-processing tools (Green et al., 2011). It contains 473,904 tokens and 15,917 sentences. One benefit of this corpus is that its compounds are marked. Their annotation was driven by linguistic criteria such as the ones in (Gross, 1986). Compounds are identified with a specific non-terminal symbol ”MWX” where X is the part-of-speech of the expression. They have a flat structure made of the part-of-speech of their components as shown in figure 1.",Material,Dataset,True,Introduce（引用目的）,True,P12-1022_0_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
748,10752," http://igm.univ-mlv.fr/~unitex"," ['4 Resources', '4.2 Lexical resources']","We used the distribution freely available in the platform Unitex [Cite_Footnote_3] (Paumier, 2011).",3 http://igm.univ-mlv.fr/˜unitex,"French is a resource-rich language as attested by the existing morphological dictionaries which in-clude compounds. In this paper, we use two large-coverage general-purpose dictionaries: Dela (Cour-tois, 1990; Courtois et al., 1997) and Lefff (Sagot, 2010). The Dela was manually developed in the 90’s by a team of linguists. We used the distribution freely available in the platform Unitex [Cite_Footnote_3] (Paumier, 2011). It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiword nouns). The compounds present in the resources re-spect the linguistic criteria defined in (Gross, 1986). The lefff is a freely available dictionary that has been automatically compiled by drawing from dif-ferent sources and that has been manually validated. We used a version with 553,138 lexical entries in-cluding 26,311 multiword ones (22,673 multiword nouns). Their different modes of acquisition makes those two resources complementary. In both, lexical entries are composed of a inflected form, a lemma, a part-of-speech and morphological features. The Dela has an additional feature for most of the mul-tiword entries: their syntactic surface form. For in-stance, eau de vie (brandy) has the feature NDN be-cause it has the internal flat structure noun – prepo-sition de – noun.",補足資料,Website,True,Introduce（引用目的）,True,P12-1022_1_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
749,10753," http://atoll.inria.fr/~sagot/lefff.html"," ['4 Resources', '4.2 Lexical resources']",The lefff is a freely available dictionary [Cite_Footnote_4] that has been automatically compiled by drawing from dif-ferent sources and that has been manually validated.,"4 http://atoll.inria.fr/˜sagot/lefff.html and the reranker models: n is the current position in the sentence, w(i) is the word at position i; t(i) is the part-of-speech tag of w(i); if the word at absolute position i is part of a compound in the Shortest Path Segmentation, mwt(i) and mws(i) are respectively the part-of-speech tag and the internal structure of the compound, mwpos(i) indicates its relative position in the compound (B or I).","French is a resource-rich language as attested by the existing morphological dictionaries which in-clude compounds. In this paper, we use two large-coverage general-purpose dictionaries: Dela (Cour-tois, 1990; Courtois et al., 1997) and Lefff (Sagot, 2010). The Dela was manually developed in the 90’s by a team of linguists. We used the distribution freely available in the platform Unitex (Paumier, 2011). It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiword nouns). The compounds present in the resources re-spect the linguistic criteria defined in (Gross, 1986). The lefff is a freely available dictionary [Cite_Footnote_4] that has been automatically compiled by drawing from dif-ferent sources and that has been manually validated. We used a version with 553,138 lexical entries in-cluding 26,311 multiword ones (22,673 multiword nouns). Their different modes of acquisition makes those two resources complementary. In both, lexical entries are composed of a inflected form, a lemma, a part-of-speech and morphological features. The Dela has an additional feature for most of the mul-tiword entries: their syntactic surface form. For in-stance, eau de vie (brandy) has the feature NDN be-cause it has the internal flat structure noun – prepo-sition de – noun.",Material,Knowledge,True,Introduce（引用目的）,True,P12-1022_2_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
750,10754," http://alpage.inria.fr/statgram/frdep/fr"," ['6 Evaluation', '6.1 Experiment Setup']","We then combined MWE pregrouping based on this recognizer and the Berkeley parser [Cite_Footnote_5] (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc).","5 We used the version adapted to French in the software Bonsai (Candito and Crabbé, 2009): http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html. The original version is available at: http://code.google.com/p/berkeleyparser/. We trained the parser as follows: right binarization, no parent annotation, six split-merge cycles and default random seed initialisation (8).","We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser [Cite_Footnote_5] (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).",Method,Tool,True,Extend（引用目的）,False,P12-1022_3_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
751,10755," http://code.google.com/p/berkeleyparser/"," ['6 Evaluation', '6.1 Experiment Setup']","We then combined MWE pregrouping based on this recognizer and the Berkeley parser [Cite_Footnote_5] (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc).","5 We used the version adapted to French in the software Bonsai (Candito and Crabbé, 2009): http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html. The original version is available at: http://code.google.com/p/berkeleyparser/. We trained the parser as follows: right binarization, no parent annotation, six split-merge cycles and default random seed initialisation (8).","We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser [Cite_Footnote_5] (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).",Method,Tool,True,Introduce（引用目的）,True,P12-1022_4_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
752,10756," http://wapiti.limsi.fr/"," ['6 Evaluation', '6.1 Experiment Setup']","The CRF recognizer relies on the software Wapiti [Cite_Footnote_6] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources.","6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).","We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [Cite_Footnote_6] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).",Method,Tool,True,Introduce（引用目的）,False,P12-1022_5_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
753,10757," http://igm.univ-mlv.fr/~mconstan/research/software/"," ['6 Evaluation', '6.1 Experiment Setup']","The part-of-speech tagger used to extract POS features was lgtagger [Cite_Footnote_7] (Constant and Sigogne, 2011).",7 Available at http://igm.univ-mlv.fr/˜mconstan/research/software/.,"We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [Cite_Footnote_7] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).",Method,Tool,True,Use（引用目的）,True,P12-1022_6_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
754,10758," http://www.mcs.anl.gov/petsc/"," ['6 Evaluation', '6.1 Experiment Setup']","To train the reranker, we used a MaxEnt al-gorithm [Cite_Footnote_8] as in (Charniak and Johnson, 2005).","8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/","We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [Cite_Footnote_8] as in (Charniak and Johnson, 2005).",Material,Knowledge,True,Use（引用目的）,True,P12-1022_7_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
755,10759," http://www.mcs.anl.gov/research/projects/tao/"," ['6 Evaluation', '6.1 Experiment Setup']","To train the reranker, we used a MaxEnt al-gorithm [Cite_Footnote_8] as in (Charniak and Johnson, 2005).","8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/","We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [Cite_Footnote_8] as in (Charniak and Johnson, 2005).",Material,Knowledge,True,Use（引用目的）,True,P12-1022_8_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
756,10760," http://nlp.cs.nyu.edu/evalb/"," ['6 Evaluation', '6.1 Experiment Setup']","The labeled F 1 score [F1] [Cite_Footnote_9] , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes.",9 Evalb tool available at http://nlp.cs.nyu.edu/evalb/. We also used the evaluation by category implemented in the class EvalbByCat in the Stanford Parser.,"Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] [Cite_Footnote_9] , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence 10 . And finally, the Leaf-Ancestor score [LA] 11 (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree. The global score of a generated parse is equal to the average score of all terminal nodes. Punctuation tokens are ignored in all met-rics. The quality of MWE identification was evalu-ated by computing the F 1 score on MWE nodes. We also evaluated the MWE segmentation by using the unlabeled F 1 score (U). In order to compare both ap-proaches, parse trees generated by BKYc were auto-matically transformed in trees with the same MWE annotation scheme as the trees generated by BKY.",Method,Tool,True,Use（引用目的）,True,P12-1022_9_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
757,10761," http://ilk.uvt.nl/conll/software.html"," ['6 Evaluation', '6.1 Experiment Setup']",The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence [Cite_Footnote_10] .,10 This score is computed by using the tool available at http://ilk.uvt.nl/conll/software.html. The constituent trees are automatically converted into dependency trees with the tool Bonsai.,"Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] 9 , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence [Cite_Footnote_10] . And finally, the Leaf-Ancestor score [LA] (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree. The global score of a generated parse is equal to the average score of all terminal nodes. Punctuation tokens are ignored in all met-rics. The quality of MWE identification was evalu-ated by computing the F 1 score on MWE nodes. We also evaluated the MWE segmentation by using the unlabeled F 1 score (U). In order to compare both ap-proaches, parse trees generated by BKYc were auto-matically transformed in trees with the same MWE annotation scheme as the trees generated by BKY.",Method,Tool,True,Use（引用目的）,True,P12-1022_10_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
758,10762," http://www.grsampson.net/Resources.html"," ['6 Evaluation', '6.1 Experiment Setup']","And finally, the Leaf-Ancestor score [LA] [Cite_Footnote_11] (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree.",11 Leaf-ancestor assessment tool available at http://www.grsampson.net/Resources.html,"Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] 9 , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence . And finally, the Leaf-Ancestor score [LA] [Cite_Footnote_11] (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree. The global score of a generated parse is equal to the average score of all terminal nodes. Punctuation tokens are ignored in all met-rics. The quality of MWE identification was evalu-ated by computing the F 1 score on MWE nodes. We also evaluated the MWE segmentation by using the unlabeled F 1 score (U). In order to compare both ap-proaches, parse trees generated by BKYc were auto-matically transformed in trees with the same MWE annotation scheme as the trees generated by BKY.",Method,Tool,True,Use（引用目的）,True,P12-1022_11_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
759,10763," http://www.cis.upenn.edu/~dbikel/software.html"," ['6 Evaluation', '6.1 Experiment Setup']","In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples [Cite_Footnote_12] .",12 Dan Bikel’s tool available at http://www.cis.upenn.edu/˜dbikel/software.html.,"In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples [Cite_Footnote_12] . The statistical significance between two MWE identification experiments was established by using the McNemar-s test (Gillick and Cox, 1989). The results of the two experiments are considered statistically significant with the com-puted value p < 0.01.",Method,Tool,True,Use（引用目的）,True,P12-1022_12_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Footnote
760,10764," http://igm.univ-mlv.fr/~unitex"," ['4 Resources', '4.2 Lexical resources']","We used the distribution freely available in the platform Unitex (Paumier, 2011) [Cite_Ref] .",S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/˜unitex.,"French is a resource-rich language as attested by the existing morphological dictionaries which in-clude compounds. In this paper, we use two large-coverage general-purpose dictionaries: Dela (Cour-tois, 1990; Courtois et al., 1997) and Lefff (Sagot, 2010). The Dela was manually developed in the 90’s by a team of linguists. We used the distribution freely available in the platform Unitex (Paumier, 2011) [Cite_Ref] . It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiword nouns). The compounds present in the resources re-spect the linguistic criteria defined in (Gross, 1986). The lefff is a freely available dictionary that has been automatically compiled by drawing from dif-ferent sources and that has been manually validated. We used a version with 553,138 lexical entries in-cluding 26,311 multiword ones (22,673 multiword nouns). Their different modes of acquisition makes those two resources complementary. In both, lexical entries are composed of a inflected form, a lemma, a part-of-speech and morphological features. The Dela has an additional feature for most of the mul-tiword entries: their syntactic surface form. For in-stance, eau de vie (brandy) has the feature NDN be-cause it has the internal flat structure noun – prepo-sition de – noun.",補足資料,Document,False,Use（引用目的）,True,P12-1022_13_0,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Reference
761,10765," http://igm.univ-mlv.fr/~unitex"," ['6 Evaluation', '6.1 Experiment Setup']","The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) [Cite_Ref] to apply lexical resources.",S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/˜unitex.,"We carried out 3 different experiments. We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) [Cite_Ref] to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).",補足資料,Document,False,Use（引用目的）,True,P12-1022_13_1,2012,Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing,Reference
762,10766," https://github.com/wasiahmad/PolicyIE"," ['1 Introduction']",We release the code and data to facilitate research. [Cite_Footnote_1],1 https://github.com/wasiahmad/ PolicyIE,"To facilitate fine-grained information extraction, we present PolicyIE, an English corpus consisting of 5,250 intent and 11,788 slot annotations over 31 privacy policies of websites and mobile appli-cations. We perform experiments using sequence tagging and sequence-to-sequence (Seq2Seq) learn-ing models to jointly model intent classification and slot filling. The results show that both modeling approaches perform comparably in intent classifi-cation, while Seq2Seq models outperform the se-quence tagging models in slot filling by a large margin. We conduct a thorough error analysis and categorize the errors into seven types. We observe that sequence tagging approaches miss more slots while Seq2Seq models predict more spurious slots. We further discuss the error cases by considering other factors to help guide future work. We release the code and data to facilitate research. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.340_0_0,2021,Intent Classification and Slot Filling for Privacy Policies,Footnote
763,10767," https://github.com/danieliu/play-scraper"," ['2 Construction of PolicyIE Corpus', '2.1 Privacy Policies Selection']","For mobile application privacy policies, we scrape application information from Google Play Store us-ing play-scraper public API [Cite_Footnote_2] and crawl their privacy policy.",2 https://github.com/danieliu/ play-scraper,"Initial Collection Ramanath et al. (2014) intro-duced a corpus of 1,010 privacy policies of the top websites ranked on Alexa.com. We crawled those websites’ privacy policies in November 2019 since the released privacy policies are outdated. For mobile application privacy policies, we scrape application information from Google Play Store us-ing play-scraper public API [Cite_Footnote_2] and crawl their privacy policy. We ended up with 7,500 mobile applications’ privacy policies.",Method,Code,True,Use（引用目的）,True,2021.acl-long.340_1_0,2021,Intent Classification and Slot Filling for Privacy Policies,Footnote
764,10768," https://github.com/microsoft/unilm"," ['References']","BART 3 (P:0.83, R: 1.0) [Cite_Footnote_7]",7 https://github.com/microsoft/unilm 8 https://github.com/microsoft/MASS,"BART 3 (P:0.83, R: 1.0) [Cite_Footnote_7]",Method,Code,False,Introduce（引用目的）,False,2021.acl-long.340_2_0,2021,Intent Classification and Slot Filling for Privacy Policies,Footnote
765,10769," https://github.com/microsoft/MASS"," ['References']","BART 3 (P:0.83, R: 1.0) [Cite_Footnote_7]",7 https://github.com/microsoft/unilm 8 https://github.com/microsoft/MASS,"BART 3 (P:0.83, R: 1.0) [Cite_Footnote_7]",Method,Code,False,Introduce（引用目的）,False,2021.acl-long.340_3_0,2021,Intent Classification and Slot Filling for Privacy Policies,Footnote
766,10770," https://github.com/INK-USC/KagNet"," ['-']",We open-source our code [Cite_Footnote_1] to the community for future research in knowledge-aware commonsense reasoning.,1 https://github.com/INK-USC/KagNet,"Commonsense CapableOf reasoning aims ent to empower ReceiveAction machines with the human ub ability ev to make glue_stick presumptions about work ordinary HasS situations on in our daily life. In this paper, At we Locati propose AtLocation a textual inference framework office for answer- Schema Graph ing commonsense questions, which effec-Grounding sense knowledge graphs to perform Commonsense explain- Inference Commonsense Inference able inferences Where . The do framework adults use first glue grounds sticks? a question-answer A: classroom pair from B: the office semantic C: desk space drawer to the knowledge-based symbolic space as a schema graph, a related sub-graph of exter-nal knowledge Semantic graphs Space . It represents schema graphs with a novel knowledge-aware graph network module named K AG N ET , and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The interme-diate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for B ERT -based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning. We open-source our code [Cite_Footnote_1] to the community for future research in knowledge-aware commonsense reasoning.",Method,Code,True,Produce（引用目的）,True,D19-1282_0_0,2019,KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning,Footnote
767,10771," https://github.com/TalLinzen/rnn_agreement"," ['2 Number Agreement with LSTM Language Models']","We use the same parsed Wikipedia corpus, verb inflectors, prepro-cessing steps, and dataset split as Linzen et al. (2016). [Cite_Footnote_1]",1 The dataset and scripts are obtained from https://github.com/TalLinzen/rnn_agreement.,"Experimental Settings. We use the same parsed Wikipedia corpus, verb inflectors, prepro-cessing steps, and dataset split as Linzen et al. (2016). [Cite_Footnote_1] Word types beyond the most frequent 10,000 are converted to their respective POS tags. We summarize the corpus statistics of the dataset, along with the test set distribution of the num-ber of attractors, in Table 1. Similar to Linzen et al. (2016), we only include test cases where all intervening nouns are of the opposite number forms than the subject noun. All models are im-plemented using the DyNet library (Neubig et al., 2017).",Mixed,Mixed,True,Use（引用目的）,True,P18-1132_0_0,2018,"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better",Footnote
768,10772," https://github.com/tensorflow/models/tree/master/research/lm_1b"," ['2 Number Agreement with LSTM Language Models']","(2016), we include the results of our repli-cation of the large-scale language model of Joze-fowicz et al. (2016) that was trained on the One Billion Word Benchmark. [Cite_Footnote_4]",4 The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/tree/master/research/lm_1b.,"Training was done using a language modeling objective that predicts the next word given the pre-fix; at test time we compute agreement error rates by comparing the probability of the correct verb form with the incorrect one. We report perfor-mance of a few different LSTM hidden layer con-figurations, while other hyper-parameters are se-lected based on a grid search. 2 Following Linzen ous LSTM language models, broken down by the number of attractors. The top two rows represent the random and majority class baselines, while the next row ( † ) is the reported result from Linzen et al. (2016) for an LSTM language model with 50 hidden units (some entries, denoted by ≈, are approximately derived from a chart, since Linzen et al. (2016) did not provide a full table of results). We report results of our LSTM implementations of various hidden layer sizes, along with our re-run of the Jozefowicz et al. (2016) language model, in the next five rows. We lastly report the performance of a state of the art character LSTM baseline with a large model capacity (Melis et al., 2018). et al. (2016), we include the results of our repli-cation of the large-scale language model of Joze-fowicz et al. (2016) that was trained on the One Billion Word Benchmark. [Cite_Footnote_4] Hyper-parameter tun-ing is based on validation set perplexity.",Material,Knowledge,True,Extend（引用目的）,False,P18-1132_1_0,2018,"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better",Footnote
769,10773," https://github.com/clab/rnng"," ['3 Number Agreement with RNNGs', '3.2 Experiments']","We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [Cite_Footnote_8] trained on the Penn Treebank (Marcus et al., 1993).",8 https://github.com/clab/rnng,"Experimental settings. We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [Cite_Footnote_8] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences. At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols. An example of the stack contents (i.e. the prefix) when predicting the verb is provided in Fig. 3(a). We similarly run a grid search over the same hyper-parameter range as the sequential LSTM and compare the results with the strongest sequential LSTM baseline from §2.",Material,Knowledge,False,Use（引用目的）,True,P18-1132_2_0,2018,"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better",Footnote
770,10774," https://github.com/clab/rnng"," ['3 Number Agreement with RNNGs', '3.3 Further Analysis']","Following Dyer et al. (2016), for each sentence on the validation set we sample 100 candidate trees from a dis-criminative model [Cite_Footnote_11] as our proposal distribution.",11 https://github.com/clab/rnng,"Perplexity. To what extent does the success of RNNGs in the number agreement task with mul-tiple attractors correlate with better performance under the perplexity metric? We answer this ques-tion by using an importance sampling marginal-ization procedure (Dyer et al., 2016) to obtain an estimate of p(x) under both RNNGs and the se-quential syntactic LSTM model. Following Dyer et al. (2016), for each sentence on the validation set we sample 100 candidate trees from a dis-criminative model [Cite_Footnote_11] as our proposal distribution. As demonstrated in Table 3, the LSTM language model has the lowest validation set perplexity de-spite substantially worse performance than RN-NGs in number agreement with multiple attrac-tors, suggesting that lower perplexity is not neces-sarily correlated with number agreement success.",Material,Knowledge,False,Use（引用目的）,True,P18-1132_3_0,2018,"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better",Footnote
771,10775," http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html"," ['3 Corpus, Features, and Data Creation', '3.1 Corpus']","Finally, we automatically an-notated all utterances with part-of-speech tags, us-ing TreeTagger (Schmid, 1994), which we’ve trained on the switchboard corpus of spoken lan-guage (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies. [Cite_Footnote_6]",6 The tagger is available free for academic research from http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html.,"For these pairs we also annotated some more at-tributes, which are summarised in Table 1. Note that the average distance is slightly higher than that reported in (Schlangen and Lascarides, 2003) for (2-party) dialogue (1.8); this is presumably due to the presence of more speakers who are able to re-ply to an utterance. Finally, we automatically an-notated all utterances with part-of-speech tags, us-ing TreeTagger (Schmid, 1994), which we’ve trained on the switchboard corpus of spoken lan-guage (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies. [Cite_Footnote_6]",Method,Tool,True,Use（引用目的）,True,P05-1031_0_0,2005,Towards Finding and Fixing Fragments: Using ML to Identify Non-Sentential Utterances and their Antecedents in Multi-Party Dialogue,Footnote
772,10776," http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html"," ['4 Experiments and Results', '4.1 Experimental Setup']","• M AX E NT , Zhang Le’s C++ implementation [Cite_Footnote_8] of maximum entropy modelling (Berger et al., 1996).",8 Available from http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html.,"• M AX E NT , Zhang Le’s C++ implementation [Cite_Footnote_8] of maximum entropy modelling (Berger et al., 1996). In our experiments, we used L-BFGS parameter es-",Method,Tool,True,Use（引用目的）,True,P05-1031_1_0,2005,Towards Finding and Fixing Fragments: Using ML to Identify Non-Sentential Utterances and their Antecedents in Multi-Party Dialogue,Footnote
773,10777," http://ilk.uvt.nl/downloads/pub/...papers/ilk0310.pdf"," ['4 Experiments and Results', '4.1 Experimental Setup']","• T I MBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003) [Cite_Ref] , which implements a memory-based learning algorithm ( IB 1) which pre-dicts the class of a test data point by looking at its distance to all examples from the training data, us-ing some distance metric.","Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2003. TiMBL: Tilburg memory based learner, version 5.0, reference guide. ILC Technical Report 03-10, Induction of Linguis-tic Knowledge; Tilburg University. Available from http://ilk.uvt.nl/downloads/pub/...papers/ilk0310.pdf.","• T I MBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003) [Cite_Ref] , which implements a memory-based learning algorithm ( IB 1) which pre-dicts the class of a test data point by looking at its distance to all examples from the training data, us-ing some distance metric. In our experiments, we have used the weighted-overlap method, which as-signs weights to all features. timation.",補足資料,Paper,True,Introduce（引用目的）,True,P05-1031_2_0,2005,Towards Finding and Fixing Fragments: Using ML to Identify Non-Sentential Utterances and their Antecedents in Multi-Party Dialogue,Reference
774,10778," https://spacy.io"," ['4 Experiments', '4.2 The “good probes” are good for both']","3 [Cite] https://spacy.io is set to 0, probes with one hidden layer and 40 hidden neurons are better in both criteria.",,"3 [Cite] https://spacy.io is set to 0, probes with one hidden layer and 40 hidden neurons are better in both criteria.",Method,Tool,False,Introduce（引用目的）,False,2020.emnlp-main.744_0_0,2020,An information theoretic view on selecting linguistic probes,Body
775,10779," https://github.com/SPOClab-ca/InfoProbe"," ['B Experiments details']",We open source our codes at [Cite] https: //github.com/SPOClab-ca/InfoProbe .,,"Note that we also swept hyperparameters for FastText, where probes with less parameters do not always outperform more complex probes in either accuracy, loss, selectivity, or information gain. Figures 8 and 9 illustrate these observations. C Reproducibility On a T4 GPU card, training one epoch takes around 20 seconds. Without setting maximum gra-dient steps, 98.6% of experiments finish within 400 epochs. We open source our codes at [Cite] https: //github.com/SPOClab-ca/InfoProbe .",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.744_1_0,2020,An information theoretic view on selecting linguistic probes,Body
776,10780," https://github.com/fxsjy/jieba"," ['5 Processing steps', '5.1 Preprocessing']",We use the third-party tool jieba [Cite_Footnote_3] for word seg-mentation and POS tagging; both steps are cus-tomized in order to achieve a better performance on domain- and task-specific data.,3 https://github.com/fxsjy/jieba,"We use the third-party tool jieba [Cite_Footnote_3] for word seg-mentation and POS tagging; both steps are cus-tomized in order to achieve a better performance on domain- and task-specific data. Specifically, the dictionary provided by the tool is inter-sected with a user-specified dictionary. This user-specified dictionary contains all words from our lexical resources. The user-added words are anno-tated with customized POS tags, such as ‘F’ for feature, ‘EV’ for evaluation etc. The following two examples depict the same sentence as output by jieba without and with customization:",Method,Tool,True,Use（引用目的）,True,P15-4010_3_0,2015,A system for fine-grained aspect-based sentiment analysis of Chinese,Footnote
777,10781," https://www.gutenberg.org/"," ['2 Data Collection', '2.2 Open Vocabulary Condition']",We use public domain books from Project Gutenberg. [Cite_Footnote_1],1 https://www.gutenberg.org/,"The majority of our data was collected with open-vocabulary sentences from books. We use public domain books from Project Gutenberg. [Cite_Footnote_1] Unlike the closed-vocabulary data which is collected in a sin-gle sitting, the open-vocabulary data is broken into multiple sessions where electrodes are reattached before each session and may have minor changes in position between different sessions. In addition to sessions with parallel silent and vocalized utter-ances, we also collect non-parallel sessions with only vocalized utterances. A summary of dataset features is shown in Table 2. We select a validation and test set randomly from the silent parallel EMG data, with 30 and 100 utterances respectively. Note that during testing, we use only the silent EMG recordings E S , so the vocalized recordings of the test utterances are unused.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.445_0_0,2020,Digital Voicing of Silent Speech,Footnote
778,10782," https://pypi.org/project/noisereduce/"," ['2 Data Collection', '2.3 Recording Details']","Background noise is reduced us-ing a spectral gating algorithm, [Cite_Footnote_2] and volume is nor-malized across sessions based on peak root-mean-square levels.",2 https://pypi.org/project/noisereduce/,"Audio is recorded from a built-in laptop micro-phone at 16kHz. Background noise is reduced us-ing a spectral gating algorithm, [Cite_Footnote_2] and volume is nor-malized across sessions based on peak root-mean-square levels.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.445_1_0,2020,Digital Voicing of Silent Speech,Footnote
779,10783," https://github.com/NVIDIA/nv-wavenet"," ['δ CCA [i, j] = P S E 0S [i] − P V E V0 [j]', '3.3 WaveNet Synthesis']",We use a WaveNet implementa-tion from NVIDIA [Cite_Footnote_3] which provides efficient GPU inference.,3 https://github.com/NVIDIA/nv-wavenet,"To synthesize audio from speech features, we use a WaveNet decoder (van den Oord et al., 2016), which generates the audio sample by sample con-ditioned on MFCC speech features A 0 . WaveNet is capable of generating fairly natural sounding speech, in contrast to the vocoder-based synthe-sizer used in previous EMG-to-speech papers, which caused significant degradation in naturalness (Janke and Diener, 2017). Our full synthesis model consists of a bidirectional LSTM of 512 dimen-sions, a linear projection down to 128 dimensions, and finally the WaveNet decoder which generates samples at 16 kHz. We use a WaveNet implementa-tion from NVIDIA [Cite_Footnote_3] which provides efficient GPU inference. WaveNet hyperparameters can be found in Appendix A. During training, the model is given gold speech features as input, which we found to work better than training from EMG-predicted fea-tures. Due to memory constraints we do not use any batching during training, but other optimiza-tion hyperparameters are the same as those from Section 3.1.",補足資料,Website,False,Use（引用目的）,False,2020.emnlp-main.445_2_0,2020,Digital Voicing of Silent Speech,Footnote
780,10784," https://github.com/mozilla/DeepSpeech"," ['4 Experiments', '4.2.2 Automatic Evaluation']","For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [Cite_Footnote_5] (Hannun et al., 2014).",5 https://github.com/mozilla/DeepSpeech,"In addition to the human evaluation, we also per-form an automatic evaluation by transcribing sys-tem outputs with a large-vocabulary automatic speech recognition (ASR) system. Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [Cite_Footnote_5] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.445_3_0,2020,Digital Voicing of Silent Speech,Footnote
781,10785," https://doi.org/10.5281/zenodo.4064408"," ['5 Conclusion']",We hope that our public release of data will encourage others to further improve models for this task. [Cite_Footnote_6],6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.,"Our results show that digital voicing of silent speech, while still challenging in open domain set-tings, shows promise as an achievable technology. We show that it is important to account for differ-ences in EMG signals between silent and vocal-ized speaking modes and demonstrate an effective method of doing so. On silent EMG recordings from closed vocabulary data our speech outputs achieve high intelligibility, with a 3.6% transcrip-tion word error rate and relative error reduction of 95% from our baseline. We also significantly improve intelligibility in an open vocabulary condi-tion, with a relative error reduction over 20%. We hope that our public release of data will encourage others to further improve models for this task. [Cite_Footnote_6]",Material,Dataset,True,Produce（引用目的）,True,2020.emnlp-main.445_4_0,2020,Digital Voicing of Silent Speech,Footnote
782,10786," https://github.com/dgaddy/silent_speech"," ['5 Conclusion']",We hope that our public release of data will encourage others to further improve models for this task. [Cite_Footnote_6],6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.,"Our results show that digital voicing of silent speech, while still challenging in open domain set-tings, shows promise as an achievable technology. We show that it is important to account for differ-ences in EMG signals between silent and vocal-ized speaking modes and demonstrate an effective method of doing so. On silent EMG recordings from closed vocabulary data our speech outputs achieve high intelligibility, with a 3.6% transcrip-tion word error rate and relative error reduction of 95% from our baseline. We also significantly improve intelligibility in an open vocabulary condi-tion, with a relative error reduction over 20%. We hope that our public release of data will encourage others to further improve models for this task. [Cite_Footnote_6]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.445_5_0,2020,Digital Voicing of Silent Speech,Footnote
783,10787," http://www.cog.brown.edu/∼mj/software.htm"," ['packed forest for each sentence. 3', '5.1 Data Preparation']","There are 0.8M fea-tures in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M fea-tures in the updated version. [Cite_Footnote_5]",5 http://www.cog.brown.edu/∼mj/software.htm. We follow this version as it corrects some bugs from their 2005 paper which leads to a 0.4% increase in performance (see Table 4).,"Following Charniak and Johnson (2005), we ex-tracted the features from the 50-best parses on the training set (sec. 02-21), and used a cut-off of 5 to prune away low-count features. There are 0.8M fea-tures in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M fea-tures in the updated version. [Cite_Footnote_5] However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Ta-bles 3 and 4). This result confirms that our feature set design is appropriate, and the averaged percep-tron learner is a reasonable candidate for reranking.",Material,Dataset,True,Compare（引用目的）,True,P08-1067_0_0,2008,Forest Reranking: Discriminative Parsing with Non-Local Features ∗,Footnote
784,10788," http://www.cog.brown.edu/∼mj/papers/ms-uw06talk.pdf"," ['packed forest for each sentence. 3', '5.2 Results and Analysis']","8 The n-best feature extraction already uses relative counts (Johnson, 2006) [Cite_Ref] , which reduced file sizes by at least a factor 4.",Mark Johnson. 2006. Features of statisti-cal parsers. Talk given at the Joint Mi-crosoft Research and Univ. of Washing-ton Computational Linguistics Colloquium. http://www.cog.brown.edu/∼mj/papers/ms-uw06talk.pdf.,"8 The n-best feature extraction already uses relative counts (Johnson, 2006) [Cite_Ref] , which reduced file sizes by at least a factor 4.",補足資料,Media,True,Introduce（引用目的）,True,P08-1067_1_0,2008,Forest Reranking: Discriminative Parsing with Non-Local Features ∗,Reference
785,10789," http://www.cog.brown.edu/∼mj/papers/ms-uw06talk.pdf"," ['packed forest for each sentence. 3', '5.2 Results and Analysis']","type system F 1 % Collins (2000) 89.7 Henderson (2004) 90.1 Charniak and Johnson (2005) 91.0 D updated (Johnson, 2006) [Cite_Ref] 91.4 this work 91.7 Bod (2003) 90.7 G Petrov and Klein (2007) 90.1 S McClosky et al. (2006) 92.1",Mark Johnson. 2006. Features of statisti-cal parsers. Talk given at the Joint Mi-crosoft Research and Univ. of Washing-ton Computational Linguistics Colloquium. http://www.cog.brown.edu/∼mj/papers/ms-uw06talk.pdf.,"type system F 1 % Collins (2000) 89.7 Henderson (2004) 90.1 Charniak and Johnson (2005) 91.0 D updated (Johnson, 2006) [Cite_Ref] 91.4 this work 91.7 Bod (2003) 90.7 G Petrov and Klein (2007) 90.1 S McClosky et al. (2006) 92.1",補足資料,Media,True,Introduce（引用目的）,True,P08-1067_1_1,2008,Forest Reranking: Discriminative Parsing with Non-Local Features ∗,Reference
786,10790," http://claimrank.qcri.org"," ['4 The System in Action']",ClaimRank is available online. [Cite_Footnote_3],3 http://claimrank.qcri.org,ClaimRank is available online. [Cite_Footnote_3] Our systems’ user interface consists of three views:,補足資料,Website,True,Produce（引用目的）,False,N18-5006_0_0,2018,ClaimRank: Detecting Check-Worthy Claims in Arabic and English,Footnote
787,10791," https://github.com/ETIP-team/ETIP-Project/"," ['5 Experiments', '5.1 ETIP Dataset']",The dataset is available on-line ( [Cite] https://github.com/ETIP-team/ETIP-Project/) without author information.,,"We collected 500 Chinese insurance contracts, which include life, disability, health, property, home, and auto insurance, where 350 contract-s are regarded as the corpus for training word embeddings (Mikolov et al., 2013) and the oth-er 150 contracts are manually labeled for ele-ment tagging testing. The maximum nested lev-el is three in ETIP. The dataset is available on-line ( [Cite] https://github.com/ETIP-team/ETIP-Project/) without author information. This project cooperated with an information solu-tion provider of China Pacific Insurance Co., Lt-d. (CPIC). Tab. 1 shows the number (N), aver-age length (L) and average element length ratio (ELR) of seven categories in ETIP dataset. CP and IA are the two largest categories in the dataset. ELR of C, PC and E are 0.12, 0.63 and 0.76 re-spectively, which means that they are usually a phrase or clause embedded in a sentence and C is a 2-3 word phrase. ELR of CP, IA, and T are nearly 1.0, which denotes that they are always sentences.",Material,Dataset,True,Produce（引用目的）,True,N19-2022_0_0,2019,[unextracted],Body
788,10792," https://github.com/fxsjy/"," ['5 Experiments', '5.2 Experimental Settings']","Chinese texts are tokenized with Jieba (Jieba, 2017) [Cite_Ref] or NLPIR (NLPIR, 2018).",Jieba. 2017. https://github.com/fxsjy/jieba. Accessed: 2018-05-28.,"Chinese texts are tokenized with Jieba (Jieba, 2017) [Cite_Ref] or NLPIR (NLPIR, 2018). 300-dimensional word vectors are trained on our insurance corpus. The size of the input layer in the CNN model is 60 × 300, and zeros are padded if the length of the training sample is less than 60. The kernel size of the convolution layer is 5×300, and the size of the feature maps is 36. the fixed length of TOI pooling layer output is 72 = 2 × 36.",Method,Tool,True,Use（引用目的）,True,N19-2022_1_0,2019,[unextracted],Reference
789,10793," https://github.com/NLPIR-team/NLPIR"," ['5 Experiments', '5.2 Experimental Settings']","Chinese texts are tokenized with Jieba (Jieba, 2017) or NLPIR (NLPIR, 2018) [Cite_Ref] .",NLPIR. 2018. https://github.com/NLPIR-team/NLPIR. Accessed: 2018-05- 28.,"Chinese texts are tokenized with Jieba (Jieba, 2017) or NLPIR (NLPIR, 2018) [Cite_Ref] . 300-dimensional word vectors are trained on our insurance corpus. The size of the input layer in the CNN model is 60 × 300, and zeros are padded if the length of the training sample is less than 60. The kernel size of the convolution layer is 5×300, and the size of the feature maps is 36. the fixed length of TOI pooling layer output is 72 = 2 × 36.",Method,Tool,True,Use（引用目的）,True,N19-2022_2_0,2019,[unextracted],Reference
790,10794," https://dumps.wikimedia.org/"," ['5 Experiments', '5.3 Word Embedding Comparison']","The augmented word2vec mod-el trained by our insurance contract corpus can improve the similarities of the insurance syn-onyms compared to the models trained by other corpora, e.g., Baidu Encyclopedia (Baidu, 2018), Wikipedia zh (Wikipedia, 2018) [Cite_Ref] , People’s Daily News (People’s Daily, 2018).",Wikipedia. 2018. Chinese wikipedia. https://dumps.wikimedia.org/.,"350 contracts in ETIP Dataset are regarded as the corpus for training word embeddings (Mikolov et al., 2013). The augmented word2vec mod-el trained by our insurance contract corpus can improve the similarities of the insurance syn-onyms compared to the models trained by other corpora, e.g., Baidu Encyclopedia (Baidu, 2018), Wikipedia zh (Wikipedia, 2018) [Cite_Ref] , People’s Daily News (People’s Daily, 2018). Cosine similarity between word vectors of insurance synonyms is shown in Tab. 2. The Chinese words are trans-lated into English by Google Translate. Tab. 2 shows that the insurance corpus can greatly im-prove the word embedding similarity between in-surance synonyms compared with other corpora.",Material,Knowledge,True,Compare（引用目的）,True,N19-2022_4_0,2019,[unextracted],Reference
791,10795," https://github.com/jflanigan/jamr"," ['3 Data and Comparisons']",We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions. [Cite_Footnote_3],3 https://github.com/jflanigan/jamr,"All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions. [Cite_Footnote_3] We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly dif-ferent data than that used here but in practice we have not seen significant variation in results.",補足資料,Document,True,Use（引用目的）,True,D15-1136_0_0,2015,Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation,Footnote
792,10796," http://amr.isi.edu"," ['8 Results and Discussion']",The parser is available for public download and use at [Cite] http://amr.isi.edu.,,"Our AMR parser’s performance is shown in Ta-ble 3. We progressively show the incremental im-provements and compare to the systems of Flani-gan et al. (2014) and Wang et al. (2015). Purely transforming AMR data into a form that is com-patible with the SBMT pipeline yields suboptimal results, but by adding role-based restructuring, re-labeling, and reordering, as described in Section 4, we are able to surpass Flanigan et al. (2014). Adding an AMR LM and semantic resources in-creases scores further, outperforming Wang et al. (2015). Rule-based alignments are an improve-ment upon unsupervised alignments, but concate-nating the two alignments is even better. We com-pare rule set sizes of the various systems in Ta-ble 5; initially we improve the rule set by remov-ing numerous overly brittle rules but then succes-sive changes progressively add useful rules. The parser is available for public download and use at [Cite] http://amr.isi.edu.",補足資料,Website,False,Introduce（引用目的）,False,D15-1136_1_0,2015,Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation,Body
793,10797," http://www.cs.cmu.edu/~nbach/papers/"," ['9 Related Work']","Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), se-mantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and re-lation finding (Bach and Badaskar, 2007) [Cite_Ref] .",Nguyen Bach and Sameer Badaskar. 2007. A Review of Relation Extraction. Unpublished. http: //www.cs.cmu.edu/˜nbach/papers/ A-survey-on-Relation-Extraction. pdf.,"Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), se-mantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and re-lation finding (Bach and Badaskar, 2007) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,D15-1136_2_0,2015,Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation,Reference
794,10798," http://www.ldc.upenn.edu/Projects/ACE/"," ['3 Experiments and Results', '3.1 Data']","Our proposed graph-based method is evaluated on the ACE corpus [Cite_Footnote_1] , which contains 519 files from sources including broadcast, newswire, and news-paper.",1 http://www.ldc.upenn.edu/Projects/ACE/,"Our proposed graph-based method is evaluated on the ACE corpus [Cite_Footnote_1] , which contains 519 files from sources including broadcast, newswire, and news-paper. A break-down of the tagged data by different relation subtypes is given in Table 1.",Material,Dataset,True,Use（引用目的）,True,N06-2007_0_0,2006,Semi-supervised Relation Extraction with Label Propagation,Footnote
795,10799," http://ilk.uvt.nl/∼sabine/chunklink/"," ['3 Experiments and Results', '3.2 Features']","Most of these features are computed from the parse trees derived from Charniak Parser (Char-niak, 1999) and the Chunklink script [Cite_Footnote_2] written by Sabine Buchholz from Tilburg University.",2 Software available at http://ilk.uvt.nl/∼sabine/chunklink/,"We extract the following lexical and syntactic fea-tures from two entity mentions, and the contexts be-fore, between and after the entity pairs. Especially, we set the mid-context window as everything be-tween the two entities and the pre- and post- context as up to two words before and after the correspond-ing entity. Most of these features are computed from the parse trees derived from Charniak Parser (Char-niak, 1999) and the Chunklink script [Cite_Footnote_2] written by Sabine Buchholz from Tilburg University. three context windows.",Method,Tool,True,Use（引用目的）,True,N06-2007_1_0,2006,Semi-supervised Relation Extraction with Label Propagation,Footnote
796,10800," http://www.csie.ntu.edu.tw/∼cjlin/libsvm"," ['3 Experiments and Results', '3.3 Experimental Evaluation 3.3.1 Relation Detection', '3.3.2 SVM vs. LP']","For SVM, we use LIBSVM tool with linear kernel function [Cite_Footnote_3] .",3 LIBSV M: a library for support vector machines. Soft-ware available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.,"Table 2 reports the performance of relation detec-tion by using SVM and LP with different sizes of labled data. For SVM, we use LIBSVM tool with linear kernel function [Cite_Footnote_3] . And the same sampled la-beled data used in LP is used to train SVM mod-els. From Table 2, we see that both LP Cosine and LP JS achieve higher Recall than SVM. Especially, with small labeled dataset (percentage of labeled data ≤ 25%), this merit is more distinct. When the percentage of labeled data increases from 50% to 100%, LP Cosine is still comparable to SVM in F-measure while LP JS achieves better F-measure than SVM. On the other hand, LP JS consistently outper-forms LP Cosine .",Method,Tool,True,Use（引用目的）,True,N06-2007_2_0,2006,Semi-supervised Relation Extraction with Label Propagation,Footnote
797,10801," http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm"," ['4 Experiment', '4.1 Dataset Description']",We conduct our experiments on the public Short Text Conversation (STC) dataset [Cite_Footnote_1] released in NTCIR-13.,1 http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm,We conduct our experiments on the public Short Text Conversation (STC) dataset [Cite_Footnote_1] released in NTCIR-13. STC maintains a large reposit-ory of post-comment pairs from the Sina Weibo which is one of the popular Chinese social sites.,Material,Dataset,True,Use（引用目的）,True,P18-1102_0_0,2018,Learning to Control the Specificity in Neural Response Generation,Footnote
798,10802," https://pypi.python.org/pypi/jieba"," ['4 Experiment', '4.3 Implementation Details']","The variance σ [Cite_Footnote_2] of the Gaussian Kernel layer is set as 1, and all other trainable parameters are randomly initialized by uniform distribution within [-0.08,0.08].",2 https://pypi.python.org/pypi/jieba,"We implemented our model in Tensorflow . We tuned the hyper-parameters via the development set. Specifically, we use one layer of bi-directional GRU for encoder and another uni-directional GRU for decoder, with the GRU hidden unit size set as 300 in both the encoder and decoder. The dimen-sion of semantic word embeddings in both utter-ances and responses is 300, while the dimension of usage word embeddings in responses is 50. We apply the Adam algorithm (Kingma and Ba, 2015) for optimization, where the parameters of Adam are set as in (Kingma and Ba, 2015). The variance σ [Cite_Footnote_2] of the Gaussian Kernel layer is set as 1, and all other trainable parameters are randomly initialized by uniform distribution within [-0.08,0.08]. The mini-batch size for the update is set as 128. We clip the gradient when its norm exceeds 5.",Material,Knowledge,False,Use（引用目的）,True,P18-1102_1_0,2018,Learning to Control the Specificity in Neural Response Generation,Footnote
799,10803," https://www.tensorflow.org/"," ['4 Experiment', '4.3 Implementation Details']",We implemented our model in Tensorflow [Cite_Footnote_3] .,3 https://www.tensorflow.org/,"We implemented our model in Tensorflow [Cite_Footnote_3] . We tuned the hyper-parameters via the development set. Specifically, we use one layer of bi-directional GRU for encoder and another uni-directional GRU for decoder, with the GRU hidden unit size set as 300 in both the encoder and decoder. The dimen-sion of semantic word embeddings in both utter-ances and responses is 300, while the dimension of usage word embeddings in responses is 50. We apply the Adam algorithm (Kingma and Ba, 2015) for optimization, where the parameters of Adam are set as in (Kingma and Ba, 2015). The variance σ of the Gaussian Kernel layer is set as 1, and all other trainable parameters are randomly initialized by uniform distribution within [-0.08,0.08]. The mini-batch size for the update is set as 128. We clip the gradient when its norm exceeds 5.",補足資料,Website,True,Use（引用目的）,True,P18-1102_2_0,2018,Learning to Control the Specificity in Neural Response Generation,Footnote
800,10804," http://inprotk.sourceforge.net"," ['1 Introduction']","After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities. [Cite_Footnote_1]",1 The code of the toolkit and its iSS component and the demo applications discussed below have been released as open-source at http://inprotk.sourceforge.net.,"After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P12-3018_0_0,2012,I NPRO _iSS: A Component for Just-In-Time Incremental Speech Synthesis,Footnote
801,10805," http://www.speech.cs.cmu.edu/tools/lextool.html"," ['3 Target Recovery', '3.3 Phonetic Lexicon']","If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. [Cite_Footnote_1]",1 http://www.speech.cs.cmu.edu/tools/lextool.html,"The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998). This dictionary has an inventory of 39 phonemes. If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. [Cite_Footnote_1] The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited. But, when it is used to map the phoneme lattice into a word lattice of potential targets then the fixed vocabulary from the language model is used.",Method,Tool,True,Introduce（引用目的）,False,N16-1079_0_0,2016,Phonological Pun-derstanding,Footnote
802,10806," http://ssli.ee.washington.edu/data/puns"," ['4 Experiments', '4.1 Data']","We collected 75 puns from various joke websites such as Tumblr, Reddit, and Twitter and soliciting examples from friends and colleagues. [Cite_Footnote_2]",2 Data available at http://ssli.ee.washington.edu/data/puns.,"We collected 75 puns from various joke websites such as Tumblr, Reddit, and Twitter and soliciting examples from friends and colleagues. [Cite_Footnote_2] These were collected without reference to the sources used by Sobkowiak to assemble the puns used in building the phonetic edit model. This data was used for test data only and is completely separate from the training data used by the language model and the phonetic edit model. (It would be nice to have used some of the 1,182 puns from Sobkowiak for test data but only the isolated pun/target pairs were provided without the necessary word contexts.) Pun locations were marked in each sentence as the minimal set of words that change between the pun and the target.",Material,Dataset,True,Use（引用目的）,False,N16-1079_1_0,2016,Phonological Pun-derstanding,Footnote
803,10807," http://www.speech.cs.cmu.edu/cgibin/cmudict"," ['3 Target Recovery', '3.3 Phonetic Lexicon']","Pronunciations come from the CMU pronunciation dictionary (Weide, 1998) [Cite_Ref] .",Robert L Weide. 1998. The cmu pronouncing dictionary. URL: http://www.speech.cs.cmu.edu/cgibin/cmudict.,"The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998) [Cite_Ref] . This dictionary has an inventory of 39 phonemes. If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited. But, when it is used to map the phoneme lattice into a word lattice of potential targets then the fixed vocabulary from the language model is used.",Material,Knowledge,True,Use（引用目的）,True,N16-1079_2_0,2016,Phonological Pun-derstanding,Reference
804,10808," http://catalog.ldc.upenn.edu/LDC2006T13"," ['3 Method']",We will refer to these substitutes as false substi-tute candidates s f [Cite_Footnote_1] . . .,1 http://catalog.ldc.upenn.edu/ LDC2006T13,"To perform lexical substitution, we follow the delex-icalization framework of Szarvas et al. (2013). We automatically build Distributional Thesauri (DTs) for the medical domain and use features from the Uni-fied Medical Language System (UMLS) ontology. The dataset for supervised lexical substitution consists of sentences, containing an annotated target word t. Con-sidering the sentence being the context for the target word, the target word might have different meanings. Thus annotated substitute candidates s g 1 . . . s g n ∈ s g , need to be provided for each context. The negative ex-amples are substitute candidates that either are incor-rect for the target word, do not fit into the context or both. We will refer to these substitutes as false substi-tute candidates s f [Cite_Footnote_1] . . . s f m ∈ s f with s f ∩ s g = ∅.",Material,Dataset,False,Use（引用目的）,False,D14-1066_0_0,2014,Lexical Substitution for the Medical Domain,Footnote
805,10809," http://www.csie.ntu.edu.tw/~cjlin/liblinear/"," ['3 Method']","Additionally, we use logistic regression (Fan et al., 2008) as classifier [Cite_Footnote_2] .",2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/˜cjlin/liblinear/) available from http://liblinear.bwaldvogel.de/,"For the generation of substitute candidates we do not use WordNet, as done in previous works (Szarvas et al., 2013), but use only substitutes from a DT. To train a single classifier, features that distinguishing the mean-ing of words in different context need to be considered. Such features could be e.g. n-grams, features from dis-tributional semantics or features which are extracted relative to the target word, such as the ratio between frequencies of the substitute candidate and the target word. After training, we apply the algorithm to un-seen substitute candidates and rank them according to their positive probabilities, given by the classifier. Con-trary to Szarvas et al. (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [Cite_Footnote_2] .",Method,Tool,True,Use（引用目的）,True,D14-1066_1_0,2014,Lexical Substitution for the Medical Domain,Footnote
806,10810," http://liblinear.bwaldvogel.de/"," ['3 Method']","Additionally, we use logistic regression (Fan et al., 2008) as classifier [Cite_Footnote_2] .",2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/˜cjlin/liblinear/) available from http://liblinear.bwaldvogel.de/,"For the generation of substitute candidates we do not use WordNet, as done in previous works (Szarvas et al., 2013), but use only substitutes from a DT. To train a single classifier, features that distinguishing the mean-ing of words in different context need to be considered. Such features could be e.g. n-grams, features from dis-tributional semantics or features which are extracted relative to the target word, such as the ratio between frequencies of the substitute candidate and the target word. After training, we apply the algorithm to un-seen substitute candidates and rank them according to their positive probabilities, given by the classifier. Con-trary to Szarvas et al. (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [Cite_Footnote_2] .",補足資料,Website,True,Introduce（引用目的）,True,D14-1066_2_0,2014,Lexical Substitution for the Medical Domain,Footnote
807,10811," http://www.nlm.nih.gov/bsd/licensee/2014_stats/baseline_med_filecount.html"," ['4 Resources', '4.1 Distributional thesauri (DTs)']",The first DT is computed based on Medline [Cite_Footnote_4] ab-stracts.,4 http://www.nlm.nih.gov/bsd/licensee/2014_stats/baseline_med_filecount.html,"The first DT is computed based on Medline [Cite_Footnote_4] ab-stracts. This thesaurus uses the left and the right word as context features. To include multi-word expressions, we allow the number of tokens that form a term to be up to the length of three.",Material,DataSource,False,Use（引用目的）,False,D14-1066_3_0,2014,Lexical Substitution for the Medical Domain,Footnote
808,10812," https://code.google.com/p/jweb1t/"," ['4 Resources', '4.3 Google Web1T']","For accessing the resource, we use JWeb1T [Cite_Footnote_5] (Giuliano et al., 2007).",5 https://code.google.com/p/jweb1t/,"We use the Google Web1T to generate n-gram features as we expect this open domain resource to have consid-erable coverage for most specific domains as well. For accessing the resource, we use JWeb1T [Cite_Footnote_5] (Giuliano et al., 2007).",Method,Tool,True,Use（引用目的）,True,D14-1066_4_0,2014,Lexical Substitution for the Medical Domain,Footnote
809,10813," https://ai.google.com/research/NaturalQuestions/leaderboard"," ['References']","Fur-thermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard [Cite_Footnote_1] .","1 Till our submission time, 29 Nov. 2019. We re-fer readers to https://ai.google.com/research/NaturalQuestions/leaderboard for the latest re-sults.","Reading long documents to answer open-domain questions remains challenging in nat-ural language understanding. In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question an-swering. RikiNet contains a dynamic para-graph dual-attention reader and a multi-level cascaded answer predictor. The reader dynam-ically represents the document and question by utilizing a set of complementary attention mechanisms. The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner. On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks. To our best knowledge, it is the first single model that out-performs the single human performance. Fur-thermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard [Cite_Footnote_1] .",補足資料,Website,True,Introduce（引用目的）,True,2020.acl-main.604_0_0,2020,RikiNet: Reading Wikipedia Pages for Natural Question Answering Dayiheng Liu ♠† Yeyun Gong † Jie Fu ♦ Yu Yan †,Footnote
810,10814," https://ai.google.com/research/atNaturalQuestions/visualization"," ['1 Introduction']","Recently, a new benchmark MRC dataset called Natural Questions [Cite_Footnote_2] (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater chal-lenge for the existing MRC models.",2 NQ provides some visual examples of the data https://ai.google.com/research/atNaturalQuestions/visualization.,"Recently, a new benchmark MRC dataset called Natural Questions [Cite_Footnote_2] (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater chal-lenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives an entire Wikipedia page which is signifi-cantly longer compared to other datasets. Sec-ondly, NQ task not only requires the model to find an answer span (called short answer) to the question like previous MRC tasks but also asks the model to find a paragraph that contains the infor-mation required to answer the question (called long answer).",補足資料,Media,True,Introduce（引用目的）,True,2020.acl-main.604_1_0,2020,RikiNet: Reading Wikipedia Pages for Natural Question Answering Dayiheng Liu ♠† Yeyun Gong † Jie Fu ♦ Yu Yan †,Footnote
811,10815," https://github.com/lancopku/text-autoaugment"," ['References']","Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines. [Cite_Footnote_1]",1 Our code is available at https://github.com/lancopku/text-autoaugment,"Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previ-ous methods, which decreases the diversity of the augmented data and thus restricts the per-formance gain. To overcome the above limi-tations, we propose a framework named Text AutoAugment (TAA) to establish a composi-tional and learnable paradigm for data aug-mentation. We regard a combination of vari-ous operations as an augmentation policy and utilize an efficient Bayesian Optimization al-gorithm to automatically search for the best policy, which substantially improves the gen-eralization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.711_0_0,2021,Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification,Footnote
812,10816," https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md"," ['3 Experiments', '3.2 Baselines']","We utilize WMT’19 English-German translation models (Ng et al., 2019) based on Transformer (Vaswani et al., 2017) to translate the text from English to German, [Cite_Footnote_4] and then trans-late it back to English.",4 https://github.com/pytorch/fairseq/blob/master/examples/translation/README. md,"We compare our TAA method with the following representative baselines: Back Translation (BT) (Sennrich et al., 2016; Imamura et al., 2018). We utilize WMT’19 English-German translation models (Ng et al., 2019) based on Transformer (Vaswani et al., 2017) to translate the text from English to German, [Cite_Footnote_4] and then trans-late it back to English. We use random sampling for decoding as recommended by (Xie et al., 2020; Edunov et al., 2018), and set the temperature to 0.8 to generate more diverse paraphrases.",補足資料,Document,False,Produce（引用目的）,False,2021.emnlp-main.711_1_0,2021,Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification,Footnote
813,10817," http://www.ark.cs.cmu.edu/AD3"," ['5 Experiments', '5.1 Experimental setup']","We trained a struc-tured SVM with stochastic subgradient descent; the cost-augmented inference problems are re-laxed and solved with AD 3 , as described in §3.3. [Cite_Footnote_8]","8 We use the AD 3 implementation in http://www.ark.cs.cmu.edu/AD3, setting the maximum number of iterations to 200 at training time and 1000 at test time. We extended the code to handle the knapsack and budget factors; the modified code will be part of the next release (AD 3 2.1).","We evaluated our compressive summarizers on data from the Text Analysis Conference (TAC) evaluations. We use the same splits as previ-ous work (Berg-Kirkpatrick et al., 2011; Wood-send and Lapata, 2012): the non-update portions of TAC-2009 for training and TAC-2008 for test-ing. In addition, we reserved TAC-2010 as a dev-set. The test partition contains 48 multi-document summarization problems; each provides 10 related news articles as input, and asks for a summary with up to 100 words, which is evaluated against four manually written abstracts. We ignored all the query information present in the TAC datasets. Single-Task Learning. In the single-task exper-iments, we trained a compressive summarizer on the dataset disclosed by Berg-Kirkpatrick et al. (2011), which contains manual compressive sum-maries for the TAC-2009 data. We trained a struc-tured SVM with stochastic subgradient descent; the cost-augmented inference problems are re-laxed and solved with AD 3 , as described in §3.3. [Cite_Footnote_8] We followed the procedure described in Berg-Kirkpatrick et al. (2011) to reduce the number of candidate sentences: scores were defined for each sentence (the sum of the scores of the concepts they cover), and the best-scored sentences were greedily selected up to a limit of 1,000 words. We then tagged and parsed the selected sentences with TurboParser. Our choice of a dependency parser was motivated by our will for a fast system; in par-ticular, TurboParser attains top accuracies at a rate of 1,200 words per second, keeping parsing times below 1 second for each summarization problem. Multi-Task Learning. For the multi-task ex-periments, we also used the dataset of Berg-Kirkpatrick et al. (2011), but we augmented the training data with extractive summarization and sentence compression datasets, to help train the compressive summarizer. For extractive sum-marization, we used the DUC 2003 and 2004 datasets (a total of 80 multi-document summariza-tion problems). We generated oracle extracts by maximizing bigram recall with respect to the man-ual abstracts, as described in Berg-Kirkpatrick et al. (2011). For sentence compression, we adapted the Simple English Wikipedia dataset of Wood-send and Lapata (2011), containing aligned sen-tences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletion-based compressions.",Method,Tool,False,Use（引用目的）,True,P13-1020_0_0,2013,Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning,Footnote
814,10818," http://www.ark.cs.cmu.edu/TurboParser"," ['5 Experiments', '5.1 Experimental setup']",We then tagged and parsed the selected sentences with TurboParser. [Cite_Footnote_9],9 http://www.ark.cs.cmu.edu/TurboParser,"We evaluated our compressive summarizers on data from the Text Analysis Conference (TAC) evaluations. We use the same splits as previ-ous work (Berg-Kirkpatrick et al., 2011; Wood-send and Lapata, 2012): the non-update portions of TAC-2009 for training and TAC-2008 for test-ing. In addition, we reserved TAC-2010 as a dev-set. The test partition contains 48 multi-document summarization problems; each provides 10 related news articles as input, and asks for a summary with up to 100 words, which is evaluated against four manually written abstracts. We ignored all the query information present in the TAC datasets. Single-Task Learning. In the single-task exper-iments, we trained a compressive summarizer on the dataset disclosed by Berg-Kirkpatrick et al. (2011), which contains manual compressive sum-maries for the TAC-2009 data. We trained a struc-tured SVM with stochastic subgradient descent; the cost-augmented inference problems are re-laxed and solved with AD 3 , as described in §3.3. We followed the procedure described in Berg-Kirkpatrick et al. (2011) to reduce the number of candidate sentences: scores were defined for each sentence (the sum of the scores of the concepts they cover), and the best-scored sentences were greedily selected up to a limit of 1,000 words. We then tagged and parsed the selected sentences with TurboParser. [Cite_Footnote_9] Our choice of a dependency parser was motivated by our will for a fast system; in par-ticular, TurboParser attains top accuracies at a rate of 1,200 words per second, keeping parsing times below 1 second for each summarization problem. Multi-Task Learning. For the multi-task ex-periments, we also used the dataset of Berg-Kirkpatrick et al. (2011), but we augmented the training data with extractive summarization and sentence compression datasets, to help train the compressive summarizer. For extractive sum-marization, we used the DUC 2003 and 2004 datasets (a total of 80 multi-document summariza-tion problems). We generated oracle extracts by maximizing bigram recall with respect to the man-ual abstracts, as described in Berg-Kirkpatrick et al. (2011). For sentence compression, we adapted the Simple English Wikipedia dataset of Wood-send and Lapata (2011), containing aligned sen-tences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletion-based compressions.",Method,Tool,True,Use（引用目的）,True,P13-1020_1_0,2013,Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning,Footnote
815,10819," http://lpsolve.sourceforge.net/"," ['3 Experiments']",We used lp solve [Cite_Footnote_3] to solve our ILP optimization problems.,3 From http://lpsolve.sourceforge.net/,"We used lp solve [Cite_Footnote_3] to solve our ILP optimization problems. We ran experiments on two datasets. We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER , but we found that several documents where too long for lp solve to find a solution.",Method,Tool,True,Use（引用目的）,True,P08-2012_0_0,2008,Enforcing Transitivity in Coreference Resolution,Footnote
816,10820," http://www.nsdl.org"," ['1 Introduction']",Our research is motivated by the challenges we encountered in working with the National Science Digital Library (NSDL) collection. [Cite_Footnote_1],1 http://www.nsdl.org,"Our research is motivated by the challenges we encountered in working with the National Science Digital Library (NSDL) collection. [Cite_Footnote_1] Each item in the collection is a scientific resource, such as a re-search paper, an educational video, or perhaps an entire website. In addition to its main content, each resource is annotated with metadata, which provides information such as the author or creator of the re-source, its subject area, format (text/image/video) and intended audience – in all over 90 distinct fields (though some are very related). Making use of such extensive metadata in a digital library paves the way for constructing highly-focused models of the user’s information need. These models have the potential to dramatically improve the user experience in tar-geted applications, such as the NSDL portals. To illustrate this point, suppose that we are running an educational portal targeted at elementary school teachers, and some user requests teaching aids for an introductory class on gravity. An intelligent search system would be able to translate the request into a structured query that might look something like: subject=’gravity’ AND audience=’grades 1-4’ AND format=’image,video’ AND rights=’free-for-academic-use’. Such a query can be efficiently an-swered by a relational database system.",Material,DataSource,False,Use（引用目的）,False,N07-1012_0_0,2007,Information Retrieval On Empty Fields,Footnote
817,10821," http://inex.is.informatik.uni-duisburg.de/index.html"," ['2 Related work']",A number of relevant works are also pub-lished in the proceedings of the INEX workshop. [Cite_Footnote_3],3 http://inex.is.informatik.uni-duisburg.de/index.html,"Our work is related to a number of existing ap-proaches to semi-structured text search. Desai et al (1987) followed by Macleod (1991) proposed us-ing the standard relational approach to searching unstructured texts. The lack of an explicit rank-ing function in their approaches was partially ad-dressed by Blair (1988). Fuhr (1993) proposed the use of Probabilistic Relational Algebra (PRA) over the weights of individual term matches. Vasan-thukumar et al (1996) developed a relational imple-mentation of the inference network retrieval model. A similar approach was taken by de Vries and Wilschut (1999), who managed to improve the ef-ficiency of the approach. De Fazio et al (1995) in-tegrated IR and RDBMS technology using an ap-proached called cooperative indexing. Cohen (2000) describes WHIRL – a language that allows efficient inexact matching of textual fields within SQL state-ments. A number of relevant works are also pub-lished in the proceedings of the INEX workshop. [Cite_Footnote_3]",補足資料,Paper,True,Introduce（引用目的）,True,N07-1012_1_0,2007,Information Retrieval On Empty Fields,Footnote
818,10822," https://github.com/tuetschek/e2e-cleaning"," ['2 Frequency of Surface Forms']",This is enabled by a set of regular expres-sions released by Dušek et al. (2019a) [Cite_Footnote_1] .,1 https://github.com/tuetschek/ e2e-cleaning,"To compare the diversity of the E2E training data with that of the generated text, we looked at the surface forms used to express each attribute-value pair. This is enabled by a set of regular expres-sions released by Dušek et al. (2019a) [Cite_Footnote_1] . The reg-ular expressions capture the entire phrase used to express an attribute-value pair, focusing on the con-tent words and attempting as much as possible to leave out the function words, e.g.",Material,Knowledge,True,Compare（引用目的）,True,2020.emnlp-main.230_0_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Footnote
819,10823," https://github.com/tuetschek/"," ['References']",Download the dataset from [Cite] https://github.com/tuetschek/ e2e-dataset .,,"We thank the anonymous reviewers for their helpful comments. This research is supported by Science Foundation Ireland in the ADAPT Centre for Dig-ital Content Technology. The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional De-velopment Fund. A Replication Instructions Dataset The E2E dataset contains a training set of 42,061 pairs of meaning representations and hu-man authored utterances, 4,672 pairs in the devel-opment set, and 4,693 in the test set. Download the dataset from [Cite] https://github.com/tuetschek/ e2e-dataset .",Material,Dataset,True,Produce（引用目的）,True,2020.emnlp-main.230_1_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
820,10824," https://github.com/UFAL-DSG/tgen/tree/master/e2e-challenge"," ['References']",We used the delexicalization script pro-vided by the organizers in the TGen repository [Cite] https://github.com/UFAL-DSG/tgen/tree/master/e2e-challenge.,,"We used the delexicalization script pro-vided by the organizers in the TGen repository [Cite] https://github.com/UFAL-DSG/tgen/tree/master/e2e-challenge. This module replaces the names of restaurants which appeared in the Name and Near attributes with a generic value, X-Name and X-Near.",補足資料,Website,False,Use（引用目的）,True,2020.emnlp-main.230_2_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
821,10825," https://github.com/Henry-E/reliable_"," ['References']",These are available in our main repos-itory [Cite] https://github.com/Henry-E/reliable_ neural_nlg Experiment steps,,Main experiment repository All the experi-ments are done with python modules and bash scripts. These are available in our main repos-itory [Cite] https://github.com/Henry-E/reliable_ neural_nlg Experiment steps,補足資料,Website,True,Produce（引用目的）,True,2020.emnlp-main.230_3_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
822,10826," https://github.com/tuetschek/e2e-cleaning"," ['References']",[Cite] https://github.com/tuetschek/ e2e-cleaning,,1. First the delexicalised data is converted into source and target files. It uses modified regu-lar expressions from the e2e-cleaning repos-itory. [Cite] https://github.com/tuetschek/ e2e-cleaning,Material,Knowledge,False,Use（引用目的）,False,2020.emnlp-main.230_4_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
823,10827," https://github.com/Henry-E/surface_realization_opennmt-py"," ['References']",[Cite] https://github.com/Henry-E/ surface_realization_opennmt-py,,2. Inside the scripts/ folder of our main repository there are bash scripts for run-ning the preprocessing required by Open-NMT and the actual training. We use our own fork of OpenNMT; the only changes made were to the beam search decod-ing code. [Cite] https://github.com/Henry-E/ surface_realization_opennmt-py,Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.230_5_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
824,10828," https://github.com/tuetschek/e2e-metrics"," ['References']",We calculate n-gram metric scores using the E2E-metrics module [Cite] https://github.com/tuetschek/e2e-metrics,,1. We calculate n-gram metric scores using the E2E-metrics module [Cite] https://github.com/tuetschek/e2e-metrics,Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.230_6_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
825,10829," https://github.com/tuetschek/e2e-cleaning"," ['References']",To calculate semantic accuracy we use a minimally modified version of the slot error.py module from [Cite] https: //github.com/tuetschek/e2e-cleaning .,,"2. To calculate semantic accuracy we use a minimally modified version of the slot error.py module from [Cite] https: //github.com/tuetschek/e2e-cleaning . We noticed it was incorrectly grouped together attributes in a small number of cases (we saw less than 5). This change improved Slug2Slug’s results, as it now showed that it had fewer missing attributes.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.230_7_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
826,10830," https://github.com/tuetschek/e2e-eval"," ['References']",System outputs from the E2E NLG Challenge participants can be found at [Cite] https://github.com/tuetschek/e2e-eval,,System outputs from the E2E NLG Challenge participants can be found at [Cite] https://github.com/tuetschek/e2e-eval,補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.230_8_0,2020,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,Body
827,10831," http://cogcomp.cs.illinois.edu/page/softwareview/POS"," ['2 The System', '2.1 Temporal Expression Extraction']","We use the Illinois POS tagger [Cite_Footnote_1] (Roth and Zelenko, 1998) to provide part-of-speech tags for the input text before passing it to HeidelTime.",1 http://cogcomp.cs.illinois.edu/page/softwareview/POS,"We built the temporal expression extraction module on top of the Heideltime system (Strötgen and Gertz, 2010) to take advantage of a state-of-the-art tempo-ral extraction system in capturing basic expressions. We use the Illinois POS tagger [Cite_Footnote_1] (Roth and Zelenko, 1998) to provide part-of-speech tags for the input text before passing it to HeidelTime. Below is an example of the HeidelTime output of the example in the previous section:",Method,Tool,True,Use（引用目的）,True,N12-3008_0_0,2012,A Robust Shallow Temporal Reasoning System,Footnote
828,10832," http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2006T08"," ['3 Experimental Study', '3.1 Data Preparation']","To do this, we extract all sentences that satisfy the condition from 183 articles in the TimeBank 1.2 corpus [Cite_Footnote_3] .",3 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp? catalogId=LDC2006T08,"We focus on scaling up temporal systems to deal with complex expressions. Therefore, we prepared an evaluation data set that consists of a list of sen-tences containing at least one of the five temporal connectives since, betwen, from, before and after. To do this, we extract all sentences that satisfy the condition from 183 articles in the TimeBank 1.2 corpus [Cite_Footnote_3] . This results in a total of 486 sentences. Each sentence in the data set comes with the doc-ument creation time (DCT) of its corresponding ar-ticle. The second and the third columns of Table 1 summarize the number of sentences and appear-ances of each temporal connective.",Material,DataSource,True,Use（引用目的）,True,N12-3008_1_0,2012,A Robust Shallow Temporal Reasoning System,Footnote
829,10833," http://cogcomp.cs.illinois.edu/page/demoview/TempSys"," ['4 The Demonstration', '4.1 Visualization']",We have implemented our system in a web-based demo [Cite_Footnote_4] .,4 http://cogcomp.cs.illinois.edu/page/demoview/TempSys,"We have implemented our system in a web-based demo [Cite_Footnote_4] . Figure 2 shows a screenshot of the input panel of the system. The input panel includes a main text box that allows users to input the text, and some other input fields that allow users to customize the system’s outputs. Among the fields, the reference date serves as the document creation time (DCT) of the input text. All temporal expressions captured from the text will be normalized based on the ref-erence date and compared also to the reference date as illustrated in Figure 3.",Method,Tool,True,Produce（引用目的）,True,N12-3008_2_0,2012,A Robust Shallow Temporal Reasoning System,Footnote
830,10834," http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03"," ['3 Initial Experiments', '3.1 Data']","The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC [Cite_Footnote_2] , containing about 28 millions of Chinese words.",2 Available at: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03,"The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC [Cite_Footnote_2] , containing about 28 millions of Chinese words. Since punc-tuations are rarely used to construct collocations, they were removed from the corpora. To auto-matically estimate the precision of extracted col-locations on the Chinese corpus, we built a gold set by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations.",Material,DataSource,True,Extend（引用目的）,True,D09-1051_0_0,2009,Collocation Extraction Using Monolingual Word Alignment Method,Footnote
831,10835," http://www.hcu.ox.ac.uk/BNC/"," ['6 Evaluation on English corpus']","We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [Cite_Footnote_6] .",6 Available at: http://www.hcu.ox.ac.uk/BNC/,"We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [Cite_Footnote_6] . The English corpus contains about 20 millions of words.",Material,DataSource,True,Extend（引用目的）,True,D09-1051_1_0,2009,Collocation Extraction Using Monolingual Word Alignment Method,Footnote
832,10836," http://www.cs.umd.edu/~mount/ANN/"," ['6 Experiment 2: NN-based outlier detection']","Nearest neighbors (by Eu-clidean distance) were computed using the ANN tool (Mount and Arya, 2005) [Cite_Ref] .",D. Mount and S. Arya. 2005. ANN: A library for approx-imate nearest neighbor searching. Download from http://www.cs.umd.edu/˜mount/ANN/.,"Modeling. We model unknown sense detection as an outlier detection task, using Tax and Duin’s out-lier detection approach that we have outlined in the previous section. Nearest neighbors (by Eu-clidean distance) were computed using the ANN tool (Mount and Arya, 2005) [Cite_Ref] . We compute one out-lier detection model per lemma. With training and test sets constructed as described in Section 3, the average training set comprises 22.5 sentences.",Method,Tool,True,Use（引用目的）,True,N06-1017_0_0,2006,Unknown word sense detection as outlier detection,Reference
833,10837," https://about.twitter.com/company"," ['1 Introduction']","With the rise in popularity of social media, mes-sage broadcasting sites such as Twitter and other microblogging services have become an important means of communication, with an estimated 500 million tweets being written every day [Cite_Footnote_1] .",1 https://about.twitter.com/company,"With the rise in popularity of social media, mes-sage broadcasting sites such as Twitter and other microblogging services have become an important means of communication, with an estimated 500 million tweets being written every day [Cite_Footnote_1] . In addi-tion to individual users, various organizations and public figures such as newspapers, government officials and entertainers have established them-selves on social media in order to disseminate in-formation or promote their products.",補足資料,Document,True,Introduce（引用目的）,True,D15-1014_0_0,2015,Indicative Tweet Generation: An Extractive Summarization Problem?,Footnote
834,10838," https://pypi.python.org/pypi/newspaper"," ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']",The newspaper package [Cite_Footnote_2] was used to extract article text and the title from the web page.,2 https://pypi.python.org/pypi/newspaper,"We deduplicated the 16,000 extracted URLs into 6,003 unique addressed, then extracted and preprocessed their contents. The newspaper package [Cite_Footnote_2] was used to extract article text and the title from the web page. Since we are interested in text articles that can serve as the source text for summarization algorithms, we needed to re-move photos and video links such as those from Instagram and YouTube. To do so, we removed those links that contained fewer than a threshold of 150 words. After this preprocessing, the number of useful articles was reduced from 6003 to 3066. There were some further tweet-article pairs where the text of the tweets was identical, these were re-moved by further preprocessing and the number of unique tweet-article pairs came down to 2471.",Material,DataSource,True,Use（引用目的）,True,D15-1014_1_0,2015,Indicative Tweet Generation: An Extractive Summarization Problem?,Footnote
835,10839," http://t.co/K542wvSNVz’"," ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']",[Cite] http://t.co/K542wvSNVz’,,"Tweet ‘#RiggsReport: #CA as the #Election-Night exception. Voters rewarded #GOP nationally, but not in the #GoldenState. [Cite] http://t.co/K542wvSNVz’",補足資料,Document,False,Use（引用目的）,False,D15-1014_2_0,2015,Indicative Tweet Generation: An Extractive Summarization Problem?,Body
836,10840," http://t.co/zrRIBxMUYJ"," ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']","Tweet Officer Wilson will be returned to active duty if no indictment, says #Ferguson Police Chief [Cite] http://t.co/zrRIBxMUYJ",,"Tweet Officer Wilson will be returned to active duty if no indictment, says #Ferguson Police Chief [Cite] http://t.co/zrRIBxMUYJ",補足資料,Document,False,Use（引用目的）,False,D15-1014_3_0,2015,Indicative Tweet Generation: An Extractive Summarization Problem?,Body
837,10841," http://t.co/XExWwxmjnk#travel"," ['4 Analysis']",[Cite] http://t.co/XExWwxmjnk #travel,,Tweet Are #Airlines doing enough with #Ebola? [Cite] http://t.co/XExWwxmjnk #travel,補足資料,Document,False,Use（引用目的）,False,D15-1014_4_0,2015,Indicative Tweet Generation: An Extractive Summarization Problem?,Body
838,10842," http://t.co/LRLS2MhPRE#Ebola"," ['4 Analysis', '4.1 Exact Match Calculations']","Renounce punitive and counterproductive measures such as sealing the borders, [Cite] http://t.co/LRLS2MhPRE",,"Tweet @PNHP: 6. Renounce punitive and counterproductive measures such as sealing the borders, [Cite] http://t.co/LRLS2MhPRE #Ebola",補足資料,Document,False,Use（引用目的）,False,D15-1014_5_0,2015,Indicative Tweet Generation: An Extractive Summarization Problem?,Body
839,10843," http://www.politicalforum.com"," ['2 Related Work', '2.4 Data']","The first dataset (politicalforum, henceforth) consists of 5,743 posts collected from a political forum [Cite_Footnote_2] .",2 http://www.politicalforum.com,"In this section, we describe the datasets used in this paper. We use three different datasets. The first dataset (politicalforum, henceforth) consists of 5,743 posts collected from a political forum [Cite_Footnote_2] . All the posts are in English. The posts cover 12 dis-puted political and ideological topics. The discus-sants of each topic were asked to participate in a poll. The poll asked them to determine their stance on the discussion topic by choosing one item from a list of possible arguments. The list of participants who voted for each argument was published with the poll results. Each poll was accompanied by a discussion thread. The people who participated in the poll were allowed to post text to that thread to justify their choices and to argue with other partic-ipants. We collected the votes and the discussion thread of each poll. We used the votes to identify the subgroup membership of each participant.",補足資料,Website,True,Use（引用目的）,True,P12-1042_0_0,2012,Subgroup Detection in Ideological Discussions,Footnote
840,10844," http://www.createdebate.com"," ['2 Related Work', '2.4 Data']","The second dataset (createdebate, henceforth) comes from an online debating site [Cite_Footnote_3] .",3 http://www.createdebate.com,"The second dataset (createdebate, henceforth) comes from an online debating site [Cite_Footnote_3] . It consists of 30 debates containing a total of 2,712 posts. Each debate is about one topic. The description of each debate states two or more positions regarding the de-bate topic. When a new participant enters the discus-sion, she explicitly picks a position and posts text to support it, support a post written by another partici-pant who took the same position, or to dispute a post written by another participant who took an opposing position. We collected the discussion thread and the participant positions for each debate.",補足資料,Website,True,Use（引用目的）,True,P12-1042_1_0,2012,Subgroup Detection in Ideological Discussions,Footnote
841,10845," http://www.wikipedia.com"," ['2 Related Work', '2.4 Data']","The third dataset (wikipedia, henceforth) comes from the Wikipedia [Cite_Footnote_4] discussion section.",4 http://www.wikipedia.com,"The third dataset (wikipedia, henceforth) comes from the Wikipedia [Cite_Footnote_4] discussion section. When a topic on Wikipedia is disputed, the editors of that topic start a discussion about it. We collected 117 Wikipeida discussion threads. The threads contains a total of 1,867 posts.",Material,DataSource,True,Use（引用目的）,True,P12-1042_2_0,2012,Subgroup Detection in Ideological Discussions,Footnote
842,10846," http://github.com/coder352/HAHSum"," ['1 Introduction']",Our source code will be available on Github [Cite_Footnote_1] .,1 http://github.com/coder352/HAHSum,"The contributions of this paper are as below: 1) We propose a hierarchical attentive heteroge-neous graph based model(HAHSum) to guide the redundancy information propagating between sen-tences and learn redundancy-aware sentence rep-resentation; 2) Our architecture is able to extract flexible quantity of sentences with a threshold, in-stead of top-k strategy; 3) We evaluate HAHSum on three popular benchmarks (CNN/DM, NYT, NEWSROOM) and experimental results show that HAHSum outperforms the existing state-of-the-art approaches. Our source code will be available on Github [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.295_0_0,2020,Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,Footnote
843,10847," https://github.com/huggingface/transformers"," ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']","We train our model (with about 400M parameters) one day for 100,000 steps on [Cite_Footnote_2] GPUs(Nvidia",2 https://github.com/huggingface/transformers,"Parameters: We employ pre-trained ‘albert-xxlarge-v2’ 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [Cite_Footnote_2] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set. Adam with β 1 = 0.9, β 2 = 0.999 is used as optimizer and learning rate schedule follows the strategies with warming-up on first 10,000 steps (Vaswani et al., 2017). The final threshold in ex-traction is 0.65 for CNN/DM, 0.58 for NYT and 0.64 for Newsroom, with the highest ROUGE-1 score individually. A higher threshold will be with more concise summary and the lower threshold will return more information.",Method,Tool,False,Use（引用目的）,True,2020.emnlp-main.295_1_0,2020,Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,Footnote
844,10848," https://github.com/nlpyang/PreSumm"," ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']",Parameters: We employ pre-trained ‘albert-xxlarge-v2’ 2 and reuse the implementation of PreSumm [Cite_Footnote_3] .,3 https://github.com/nlpyang/PreSumm,"Parameters: We employ pre-trained ‘albert-xxlarge-v2’ 2 and reuse the implementation of PreSumm [Cite_Footnote_3] . We train our model (with about 400M parameters) one day for 100,000 steps on GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set. Adam with β 1 = 0.9, β 2 = 0.999 is used as optimizer and learning rate schedule follows the strategies with warming-up on first 10,000 steps (Vaswani et al., 2017). The final threshold in ex-traction is 0.65 for CNN/DM, 0.58 for NYT and 0.64 for Newsroom, with the highest ROUGE-1 score individually. A higher threshold will be with more concise summary and the lower threshold will return more information.",Method,Code,False,Use（引用目的）,True,2020.emnlp-main.295_2_0,2020,Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,Footnote
845,10849," https://github.com/neukg/GRTE"," ['References']",[Cite] https://github.com/neukg/GRTE.,,"Table filling based relational triple extraction methods are attracting growing research inter-ests due to their promising performance and their abilities on extracting triples from com-plex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ig-nore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information dur-ing triple extraction. To overcome this defi-ciency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a table feature for each relation. Then two kinds of global associations are mined from the generated ta-ble features. Next, the mined global associ-ations are integrated into the table feature of each relation. This “generate-mine-integrate” process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation’s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the pro-posed model on three benchmark datasets. Ex-perimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: [Cite] https://github.com/neukg/GRTE.",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.208_0_0,2021,A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling,Body
846,10850," https://github.com/gorokoba560/norm-analysis-of-transformer"," ['References']",The codes of our experiments are pub-licly available. [Cite_Footnote_1],1 https://github.com/gorokoba560/ norm-analysis-of-transformer,"Transformer architecture has become ubiqui-tous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer archi-tecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive perfor-mance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual con-nection, and layer normalization. Our anal-ysis of Transformer-based masked language models shows that the token-to-token interac-tion performed via attention has less impact on the intermediate representations than pre-viously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention pat-terns tends not to adversely affect the perfor-mance. The codes of our experiments are pub-licly available. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.373_0_0,2021,Incorporating Residual and Normalization Layers into Analysis of Masked Language Models,Footnote
847,10851," http://stats.wikimedia.org/EN/TablesDatabaseEdits.htm"," ['1 Introduction']","While the number of newly created articles in the online encyclopedia declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable. [Cite_Footnote_1]",1 http://stats.wikimedia.org/EN/TablesDatabaseEdits.htm,"Due to its ever-evolving and collaboratively built content, Wikipedia has been the subject of many NLP studies. While the number of newly created articles in the online encyclopedia declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable. [Cite_Footnote_1] It is reasonable to assume that the latter will not change in the near future. One of the major reasons for the popular-ity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activ-ity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error cor-rection (Cahill et al., 2013), spelling error correc-tion (Zesch, 2012) or paraphrasing (Max and Wis-niewski, 2010). However, they all use different ap-proaches to extract the information needed for their task. Ferschke et al. (2013) outline several appli-cations benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy.",補足資料,Document,True,Introduce（引用目的）,True,D13-1055_0_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
848,10852," http://www.ukp.tu-darmstadt.de/data/edit-classification"," ['1 Introduction']","Additional information necessary to reproduce our results, including word lists and training, development and test data, is re-leased online. [Cite_Footnote_2]",2 http://www.ukp.tu-darmstadt.de/data/ edit-classification,"In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012). This taxonomy enables a fine-grained analysis of edit activity in revision histories. We present the results from an automatic classifica-tion experiment, based on an annotated corpus of ed-its in the English Wikipedia. Additional information necessary to reproduce our results, including word lists and training, development and test data, is re-leased online. [Cite_Footnote_2] To the best of our knowledge, this is the first approach allowing to classify each single edit in Wikipedia into one or more of 21 different edit categories using a supervised machine learning approach.",Material,Knowledge,False,Produce（引用目的）,True,D13-1055_1_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
849,10853," http://sourceforge.net/projects/jazzydicts"," ['3 Experiments', '3.2 Features for Edit Category Classification']","For spell-checking, we use British and US-American English Jazzy dic-tionaries. [Cite_Footnote_3]",3 http://sourceforge.net/projects/ jazzydicts,"Table 1 includes the value of each feature for the example edit from Figure 1. This edit modifies the link [[Dactyl|Dactylic]] by adding a speci-fication to the target of that link. For spell-checking, we use British and US-American English Jazzy dic-tionaries. [Cite_Footnote_3] Markup elements are detected by the Sweble Wikitext parser (Dohrn and Riehle, 2011).",Material,Knowledge,True,Use（引用目的）,True,D13-1055_2_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
850,10854," http://meta.wikimedia.org/wiki/User_classes"," ['3 Experiments', '3.2 Features for Edit Category Classification']","The Wikimedia user group [Cite_Footnote_4] of an author specifies the edit permissions of this user (e.g. bot, administrator, blocked user).",4 http://meta.wikimedia.org/wiki/User_ classes,"Meta data features We consider the comment, author, time stamp or any other flag (“minor change”) of r v as meta data. The Wikimedia user group [Cite_Footnote_4] of an author specifies the edit permissions of this user (e.g. bot, administrator, blocked user). We indicate whether the revision comments or parts of it have been auto-generated. This happens when a page is blanked, i.e. all of its content has been deleted or replaced or when a new page or redirect is created (denoted by the Comment is auto-generated feature). Furthermore, edits within a specific sec-tion of an article are automatically marked by adding a prefix with the name of this section to the com-ment of the revision (denoted by the Auto-generated comment ratio feature). Meta data features have the same value for all edits in a (r v−1 , r v )-pair.",補足資料,Document,True,Introduce（引用目的）,True,D13-1055_3_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
851,10855," http://opennlp.apache.org"," ['3 Experiments', '3.2 Features for Edit Category Classification']",POS tagging was carried out using the OpenNLP POS tagger. [Cite_Footnote_5],"5 Maxent model for English, http://opennlp.apache.org","Language Language features are calculated on the context s v−1 and s v of edits, any wiki markup is deleted. For the Explicit Semantic Analysis, we use Wiktionary (Zesch et al., 2008) and not Wikipedia assuming that the former has a better coverage with respect to different lexical classes. POS tagging was carried out using the OpenNLP POS tagger. [Cite_Footnote_5] The vandalism word list contains a hand-crafted set of around 100 vandalism and spam words from various places in the web.",Method,Tool,True,Use（引用目的）,True,D13-1055_4_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
852,10856," http://meka.sourceforge.net"," ['3 Experiments', '3.3 Experimental Setup']","For the machine learning part, we use Weka (Hall et al., 2009) with the Meka [Cite_Footnote_6] and Mu-lan (Tsoumakas et al., 2010) extensions for multi-label classification.",6 http://meka.sourceforge.net,"We extract features with the help of ClearTK (Ogren et al., 2008). For the machine learning part, we use Weka (Hall et al., 2009) with the Meka [Cite_Footnote_6] and Mu-lan (Tsoumakas et al., 2010) extensions for multi-label classification. We use DKPro Lab (Eckart de Castilho et al., 2011) to test different parameter com-binations. We randomly split the gold standard data from WPEC into 80% training, 10% test and 10% development set, as shown in Table 2.",Method,Tool,True,Use（引用目的）,True,D13-1055_5_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
853,10857," http://en.wikipedia.org/wiki/Wikipedia:FA"," ['5 A closer look at edit sequences: Mining collaboration patterns']","WPQAC consists of 10 fea-tured and 10 non-featured articles [Cite_Footnote_7] , with an over-all number of 21,578 revisions (9,986 revisions from featured articles and 11,592 from non-featured articles), extracted from the April 2011 English Wikipedia dump.",7 http://en.wikipedia.org/wiki/Wikipedia: FA,"An edit category classifier allows us to label en-tire article revision histories. We applied the best-performing model from Section 3.3 trained on the entire WPEC to automatically classify all edits in the Wikipedia Quality Assessment Corpus (WPQAC) as presented in previous work (Daxenberger and Gurevych, 2012). WPQAC consists of 10 fea-tured and 10 non-featured articles [Cite_Footnote_7] , with an over-all number of 21,578 revisions (9,986 revisions from featured articles and 11,592 from non-featured articles), extracted from the April 2011 English Wikipedia dump. The articles in WPQAC are care-fully chosen to form comparable pairs of featured and non-featured articles, which should reduce the noise of external influences on edit activity such as popularity or visibility. In Daxenberger and Gurevych (2012), we have shown significant dif-ferences in the edit category distribution of arti-cles with featured status before and after the articles were featured. We concluded that articles become more stable after being featured, as shown by the higher number of surface edits and lower number of meaning-changing edits.",Material,DataSource,True,Use（引用目的）,True,D13-1055_6_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
854,10858," http://www.philippe-fournier-viger.com/spmf"," ['5 A closer look at edit sequences: Mining collaboration patterns']",Calculations have been carried out within the open-source SPMF Java data mining plat-form. [Cite_Footnote_8],8 http://www.philippe-fournier-viger.com/spmf,"Different to our previous approach which is based on the mere distribution of edit categories, in the present study we include the chronological order of edits and use a 10 times larger amount of data for our experiments. We segmented all adjacent revisions in WPQAC into edits, following the approach ex-plained in Daxenberger and Gurevych (2012). Dur-ing the classification process, we discarded revisions where the classifier could not assign any of the 21 edit categories with a confidence higher than the threshold, cf. Table 3. This resulted in 17,640 re-maining revisions. We applied a sequential pattern mining algorithm with time constraints (Hirate and Yamana, 2006; Fournier-Viger et al., 2008) to the data. The latter is based on the PrefixSpan algorithm (Pei et al., 2004). Calculations have been carried out within the open-source SPMF Java data mining plat-form. [Cite_Footnote_8]",Method,Tool,True,Use（引用目的）,True,D13-1055_7_0,2013,Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger † and Iryna Gurevych †‡,Footnote
855,10859," https://github.com/forkarinda/MFN"," ['1 Introduction']",The extracted ASR-output transcripts and code will be released on [Cite] https://github.com/forkarinda/MFN.,,The extracted ASR-output transcripts and code will be released on [Cite] https://github.com/forkarinda/MFN.,補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.144_0_0,2020,Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,Body
856,10860," https://github.com/Maluuba/nlg-evalcan"," ['A Appendices', 'A.1 Hierarchical Fusion Decoder']","We use the nmtpytorch evaluation library [Cite] https: //github.com/lium-lst/nmtpytorch suggested by the How2 Challenge, which includes BLEU (1, 2, 3, 4), ROUGE-L, METEOR, and CIDEr eval-uation metrics.",,"The final output state reaches through the feed-forward and add&norm layer like the general trans-former, calculated as the following equation: where W 1 , W 2 , b 1 and b 2 are trainable parameters. A.2 Evalution Metrics We use the nmtpytorch evaluation library [Cite] https: //github.com/lium-lst/nmtpytorch suggested by the How2 Challenge, which includes BLEU (1, 2, 3, 4), ROUGE-L, METEOR, and CIDEr eval-uation metrics. As an alternative, nlg-eval https://github.com/Maluuba/nlg-eval can obtain the same evaluation score as nmtpytorch.",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.144_1_0,2020,Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,Body
857,10861," https://github.com/Maluuba/nlg-eval"," ['A Appendices', 'A.1 Hierarchical Fusion Decoder']","As an alternative, nlg-eval [Cite] https://github.com/Maluuba/nlg-eval can obtain the same evaluation score as nmtpytorch.",,"The final output state reaches through the feed-forward and add&norm layer like the general trans-former, calculated as the following equation: where W 1 , W 2 , b 1 and b 2 are trainable parameters. A.2 Evalution Metrics We use the nmtpytorch evaluation library https: //github.com/lium-lst/nmtpytorch suggested by the How2 Challenge, which includes BLEU (1, 2, 3, 4), ROUGE-L, METEOR, and CIDEr eval-uation metrics. As an alternative, nlg-eval [Cite] https://github.com/Maluuba/nlg-eval can obtain the same evaluation score as nmtpytorch.",Method,Code,False,Introduce（引用目的）,False,2020.emnlp-main.144_2_0,2020,Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,Body
858,10862," https://github.com/neural-dialogue-metrics/rouge"," ['A Appendices', 'A.1 Hierarchical Fusion Decoder']","In addition, we also use a ROUGE evaluation library [Cite] https://github.com/neural-dialogue-metrics/rouge , which supports the evaluation of ROUGE series metrics (ROUGE-N, ROUGE-L and ROUGE-W).",,"In addition, we also use a ROUGE evaluation library [Cite] https://github.com/neural-dialogue-metrics/rouge , which supports the evaluation of ROUGE series metrics (ROUGE-N, ROUGE-L and ROUGE-W).",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.144_3_0,2020,Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,Body
859,10863," https://github.com/forkarinda/MFN"," ['A Appendices', 'A.3 Data']",The extracted ASR-output transcript data is avail-able on [Cite] https://github.com/forkarinda/MFN.,,The extracted ASR-output transcript data is avail-able on [Cite] https://github.com/forkarinda/MFN.,補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.144_4_0,2020,Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,Body
860,10864," http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf"," ['4 Experimental Setup', '4.1 Training Data']",Most of the collections in this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly part of the resources specified for the constrained data track of the NIST MT evaluation [Cite_Footnote_4] .,"4 For a list of the NIST MT09 constrained train-ing condition resources, see http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf",All MT training experiments made use of an Arabic-English corpus of approximately 200 mil-lion tokens (English side). Most of the collections in this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly part of the resources specified for the constrained data track of the NIST MT evaluation [Cite_Footnote_4] .,補足資料,Document,False,Introduce（引用目的）,False,D09-1074_0_0,2009,Discriminative Corpus Weight Estimation for Machine Translation,Footnote
861,10865," http://homepages.inf.ed.ac.uk/s0677528/data.html"," ['3 Problem Formulation']","As mentioned earlier, we use the dataset created in Feng and Lapata (2008). [Cite_Footnote_2]",2 Available from http://homepages.inf.ed.ac.uk/s0677528/data.html.,"In this section we give a brief description of the im-age database we employ and also define the image annotation and story picturing tasks we are attempt-ing here. As mentioned earlier, we use the dataset created in Feng and Lapata (2008). [Cite_Footnote_2] It contains 3,361 articles that have been downloaded from the BBC News website . Each article contains a news image which in turn is associated with a caption. The images are usually 203 pixels wide and 152 pix-els high. The average caption length is 5.35 tokens, and the average document length 133.85 tokens. The captions vocabulary is 2,167 words and the docu-ment vocabulary is 6,253. The vocabulary shared between captions and documents is 2,056 words. In contrast to the Corel database, this dataset con-tains more complex images (with many objects) and has a larger vocabulary (Corel’s vocabulary is ap-proximately 300 words). An example of an abridged database entry is shown in Figure 1.",Material,Dataset,True,Use（引用目的）,True,N10-1125_0_0,2010,Topic Models for Image Annotation and Text Illustration,Footnote
862,10866," http://news.bbc.co.uk/"," ['3 Problem Formulation']","It contains 3,361 articles that have been downloaded from the BBC News website [Cite_Footnote_3] .",3 http://news.bbc.co.uk/,"In this section we give a brief description of the im-age database we employ and also define the image annotation and story picturing tasks we are attempt-ing here. As mentioned earlier, we use the dataset created in Feng and Lapata (2008). It contains 3,361 articles that have been downloaded from the BBC News website [Cite_Footnote_3] . Each article contains a news image which in turn is associated with a caption. The images are usually 203 pixels wide and 152 pix-els high. The average caption length is 5.35 tokens, and the average document length 133.85 tokens. The captions vocabulary is 2,167 words and the docu-ment vocabulary is 6,253. The vocabulary shared between captions and documents is 2,056 words. In contrast to the Corel database, this dataset con-tains more complex images (with many objects) and has a larger vocabulary (Corel’s vocabulary is ap-proximately 300 words). An example of an abridged database entry is shown in Figure 1.",補足資料,Website,True,Use（引用目的）,True,N10-1125_1_0,2010,Topic Models for Image Annotation and Text Illustration,Footnote
863,10867," http://code.google.com/p/thebeast"," ['4 Models for Bridging Resolution', '4.2 Markov Logic Networks']",We use thebeast [Cite_Footnote_5] to learn weights for the formulas and to perform inference.,5 http://code.google.com/p/thebeast,"MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich re-lations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast [Cite_Footnote_5] to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP infer-ence for Markov logic.",Method,Tool,True,Use（引用目的）,True,N13-1111_0_0,2013,Global Inference for Bridging Anaphora Resolution,Footnote
864,10868," http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html"," ['5 Features', '5.1 Local features', '5.1.2 Other features']","All raw hit counts are converted into the Dunning Root Loglikelihood association measure, [Cite_Footnote_6] then nor-malized using Formula 2 within all antecedent can-didates of one anaphor.",6 http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html,"First, we extract the three most highly associ-ated prepositions for each anaphor. Then for each anaphor-antecedent candidate pair, we use their head words to create the query ”anaphor preposition an-tecedent”. To improve recall, we take lowercase, uppercase, singular and plural forms of the head word into account, and replace proper names by fine-grained named entity types (using a gazetteer). All raw hit counts are converted into the Dunning Root Loglikelihood association measure, [Cite_Footnote_6] then nor-malized using Formula 2 within all antecedent can-didates of one anaphor.",Method,Code,False,Use（引用目的）,False,N13-1111_1_0,2013,Global Inference for Bridging Anaphora Resolution,Footnote
865,10869," https://github.com/longyuewangdcu/RLFW-NAT"," ['Abstract']","Our code, data, and trained mod-els are available at [Cite] https://github.com/longyuewangdcu/RLFW-NAT.",,"Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on pre-dicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyz-ing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from tar-get to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new train-ing strategy for further boosting NAT perfor-mance. We conduct experiments on five trans-lation benchmarks over two advanced architec-tures. Results demonstrate that the proposed approach can significantly and universally im-prove translation quality by reducing transla-tion errors on low-frequency words. Encour-agingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, re-spectively. Our code, data, and trained mod-els are available at [Cite] https://github.com/longyuewangdcu/RLFW-NAT.",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.266_0_0,2021,Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation,Body
866,10870," http://www.statmt.org/wmt19/translation-task.html"," ['3 Experiment', '3.1 Setup']","To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [Cite_Footnote_4]",4 http://www.statmt.org/wmt19/translation-task.html,"Data Main experiments are conducted on four widely-used translation datasets: WMT14 English-German (En-De, Vaswani et al. 2017), WMT16 Romanian-English (Ro-En, Gu et al. 2018), WMT17 Chinese-English (Zh-En, Hassan et al. 2018), and WAT17 Japanese-English (Ja-En, Mor-ishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [Cite_Footnote_4] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Pa-pineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical sig-nificance test. The translation accuracy of low-frequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established based on the widely-used automatic alignment tool GIZA++ (Och and Ney, 2003).",Material,DataSource,False,Use（引用目的）,True,2021.acl-long.266_1_0,2021,Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation,Footnote
867,10871," https://www.clsp.jhu.edu/workshops/18-workshop/"," ['Acknowledgements']","This work was mostly conducted at the 2018 Fred-erick Jelinek Memorial Summer Workshop on Speech and Language Technologies, [Cite_Footnote_1] hosted and sponsored by Johns Hopkins University.",1 https://www.clsp.jhu.edu/workshops/18-workshop/,"This work was mostly conducted at the 2018 Fred-erick Jelinek Memorial Summer Workshop on Speech and Language Technologies, [Cite_Footnote_1] hosted and sponsored by Johns Hopkins University. Shruti Palaskar received funding from Facebook and Ama-zon grants. Jindřich Libovický received funding from the Czech Science Foundation, grant no. 19- 26934X. This work used the Extreme Science and Engineering Discovery Environment (XSEDE) sup-ported by NSF grant ACI-1548562 and the Bridges system supported by NSF award ACI-1445606, at the Pittsburgh Supercomputing Center.",補足資料,Website,False,Introduce（引用目的）,True,P19-1659_0_0,2019,Multimodal Abstractive Summarization for How2 Videos,Footnote
868,10872," http://www.byronwallace.com/EBM_abstracts_data"," ['2 Data']","There are 18,849 (831), 44,329 (1,808), 41,454 (1,711) variable length sequences for P , I , and O in the training (testing) data. [Cite_Footnote_1]","1 The complete details of the corpus, along with inter-annotator analysis and links for download of the full cor-pus will be described in a forthcoming paper and eventually made available here: http://www.byronwallace.com/EBM_abstracts_data.","For training sequence tagging models we use a corpus of 4,741 medical article abstracts with manual crowd-sourced annotations for P , I , and O sequences. For testing we use a set of 191 ab-stracts annotated for P , I , and O by medical profes-sionals. There are 18,849 (831), 44,329 (1,808), 41,454 (1,711) variable length sequences for P , I , and O in the training (testing) data. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,N18-2060_0_0,2018,Syntactic Patterns Improve Information Extraction for Medical Search,Footnote
869,10873," https://www.nlm.nih.gov/databases/download/pubmed_medline.html"," ['2 Data']",We retrieved the headings and associated sections automatically from abstracts in XML format (downloaded from PubMed [Cite_Footnote_2] ).,2 https://www.nlm.nih.gov/databases/download/pubmed_medline.html,"For minimally supervised extraction of n-gram patterns, we use structured abstracts in which the authors describe different aspects of their work under targeted headings. We retrieved the headings and associated sections automatically from abstracts in XML format (downloaded from PubMed [Cite_Footnote_2] ). In general abstracts are structured id-iosyncratically (often as Introduction, Methods, Results, Discussion). We capitalized on the minor-ity of abstracts that used the explicit Participants, Intervention and Outcome headings. We obtained 50,000 segments for each of these three categories.",Material,DataSource,True,Extend（引用目的）,False,N18-2060_1_0,2018,Syntactic Patterns Improve Information Extraction for Medical Search,Footnote
870,10874," https://github.com/gyauney/domain-specific-lexical-grounding"," ['1 Introduction']","Our second contribution is a simple but per-formant clustering algorithm for this setting, EntSharp. [Cite_Footnote_3]",3 Code is at https://github.com/gyauney/domain-specific-lexical-grounding.,"Our second contribution is a simple but per-formant clustering algorithm for this setting, EntSharp. [Cite_Footnote_3] We intend this method to learn from himage, wordi co-occurrences collected from multi-image, multi-sentence document collections.",Method,Code,True,Produce（引用目的）,False,2020.emnlp-main.160_0_0,2020,Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents,Footnote
871,10875," https://github.com/salesforce/TopicBERT"," ['References']",Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selec-tion and topic disentanglement tasks outper-forming existing methods by a good margin. [Cite_Footnote_1],1 Code will be available at https://github.com/salesforce/TopicBERT.,"While participants in a multi-party multi-turn conversation simultaneously engage in multi-ple conversation topics, existing response se-lection methods are developed mainly focus-ing on a two-party single-conversation sce-nario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and rel-evant conversation context. With this new for-mulation, we propose a novel multi-task learn-ing framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretrain-ing step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selec-tion and topic disentanglement tasks outper-forming existing methods by a good margin. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.533_0_0,2020,Response Selection for Multi-Party Conversations with Dynamic Topic Tracking,Footnote
872,10876," https://github.com/dstc8-track2/NOESIS-II/tree/master/subtask4"," ['5 Experiments', '5.3 Experiment III: Disentanglement']","This is the baseline model [Cite_Footnote_3] from DSTC-8 task organizers that has the best result for task 4 (Kummerfeld et al., 2019), which is trained by employing a two-layer feed-forward neural network on a set of 77 hand engineered features combined with word average embed-dings from pretrained Glove embeddings.",3 https://github.com/dstc8-track2/NOESIS-II/tree/master/subtask4,"• Feed-Forward. This is the baseline model [Cite_Footnote_3] from DSTC-8 task organizers that has the best result for task 4 (Kummerfeld et al., 2019), which is trained by employing a two-layer feed-forward neural network on a set of 77 hand engineered features combined with word average embed-dings from pretrained Glove embeddings.",Material,Knowledge,False,Produce（引用目的）,True,2020.emnlp-main.533_3_0,2020,Response Selection for Multi-Party Conversations with Dynamic Topic Tracking,Footnote
873,10877," https://arxiv.org/abs/1803.07416"," ['7:453–466.']","Our implementation of sliding win-dow local attention is similar to the approach in the local attention 1d layer in Ten-sor2Tensor [Cite_Footnote_8] , but with the addition of flexible mask-ing, relative position encoding, and global tokens as side keys/values.",8 https://arxiv.org/abs/1803.07416,"As future work, we would like to investigate complementary attention mechanisms like those of Reformer (Kitaev et al., 2020) or Routing Trans-former (Roy et al., 2020), push scalability with ideas like those from RevNet (Gomez et al., 2017), and study the performance of ETC in datasets with even richer structure. Kanagal, Ilya Eckstein, Manan Shah, Nich Kwon, Vikram Rao Sudarshan, Joshua Maynez, Manzil Zaheer, Kelvin Guu, Tom Kwiatkowski, Kristina Toutanova, and D. Sivakumar for helpful discus-sions, support, comments, and feedback on earlier versions of this work. We would also like to thank the Longformer authors (Iz Beltagy, Matthew E. Peters, Arman Cohan) for their useful feedback on earlier versions of this paper and for sharing parameter counts. Appendix A: Implementation Details Global-Local Attention Implementation This appendix provides further details on the TPU/GPU-friendly implementation of global-local attention. Our implementation of sliding win-dow local attention is similar to the approach in the local attention 1d layer in Ten-sor2Tensor [Cite_Footnote_8] , but with the addition of flexible mask-ing, relative position encoding, and global tokens as side keys/values. We use a simple example to describe the internal blocking logic. Let’s say the input corresponds to embeddings for the fol-lowing word pieces, each represented by a letter: ABCDEFG.",Method,Code,True,Compare（引用目的）,True,2020.emnlp-main.19_0_0,2020,ETC: Encoding Long and Structured Inputs in Transformers,Footnote
874,10878," https://qangaroo.cs.ucl.ac.uk/"," ['Appendix C: Training Details', 'NQ']",WikiHop Data Download Link: [Cite] https://qangaroo.cs.,,"Inference: In order to make predictions, sup-porting facts are predicted using a single dense layer taking the global input embeddings as input with a threshold over the output logits. Output type is predicted with a single dense layer from the global CLS token. Answer spans where predicted also with dense layers, but using the long input embeddings as inputs, using the following crite-ria: begin/end positions must be in sentences or titles, begin/end must be in the same sentence/title, spans must belong to a supporting fact, begin must be before end, and spans cannot exceed a maxi-mum answer length of 30 tokens. Within spans satisfying those criteria, a single span with top begin prob ∗ end prob is selected. WikiHop Data Download Link: [Cite] https://qangaroo.cs.ucl.ac.uk/",Material,Dataset,True,Produce（引用目的）,True,2020.emnlp-main.19_2_0,2020,ETC: Encoding Long and Structured Inputs in Transformers,Body
875,10879," https://github.com/nweir127/COD3S"," ['4 Experiments']",Appendix B provides details for reproducibility. [Cite_Footnote_6],6 Our code and pretrained models are available at https://github.com/nweir127/COD3S,"We experiment with 16-bit LSH signatures of SBERT embeddings. After prepending target-side bit signatures, pairs are encoded with byte-pair encoding (BPE; Sennrich et al., 2016) using a vocabulary size of 10K. We train Transformer models (Vaswani et al., 2017) using the FAIRSEQ library (Ott et al., 2019). Appendix B provides details for reproducibility. [Cite_Footnote_6]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.421_0_0,2020,COD 3 S : Diverse Generation with Discrete Semantic Signatures Nathaniel Weir 1 João Sedoc 2 Benjamin Van Durme 1,Footnote
876,10880," https://github.com/microsoft/ContextualSP"," ['1 Introduction']","Inspired by the above, in this paper, we propose a novel and exten-sive approach which formulates IUR as semantic segmentation [Cite_Footnote_1] .",1 Our code is available at https://github.com/microsoft/ContextualSP.,"To deal with IUR, a natural idea is to trans-fer models from coreference resolution (Clark and Manning, 2016). However, this idea is not easy to realize, as ellipsis also accounts for a large pro-portion. Despite being different, coreference and ellipsis both can be resolved without introducing out-of-dialogue words in most cases. That is to say, words of the rewritten utterance are nearly from either the context utterances or the incom-plete utterance. Observing it, most previous works employ the pointer network (Vinyals et al., 2015) or the sequence to sequence model with copy mechanism (Gu et al., 2016; See et al., 2017). However, they generate the rewritten utterance from scratch, neglecting a key trait that the main structure of a rewritten utterance is always the same as the incomplete utterance. To highlight it, we imagine the rewritten utterance as the outcome after a series of edit operations (i.e. substitute and insert) on the incomplete utterance. Taking the ex-ample from Table 1, x 3∗ can be obtained by sub-stituting “这样”(this) in x 3 with “阴天”(cloudy) in x 2 and inserting “北京”(Beijing) before “为什 么”(Why), much easier than producing x ∗3 via de-coding word by word. These edit operations are carried out between word pairs of the context ut-terances and the incomplete utterance, analogous to semantic segmentation (a well-known task in computer vision): Given relevance features be-tween word pairs as an image, the model is to pre-dict the edit type for each word pair as a pixel-level mask (elaborated in Section 3). Inspired by the above, in this paper, we propose a novel and exten-sive approach which formulates IUR as semantic segmentation [Cite_Footnote_1] . Our contributions are as follows:",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.227_0_0,2020,Incomplete Utterance Rewriting as Semantic Segmentation,Footnote
877,10881," https://github.com/tsafavi/NegatER"," ['1 Introduction']",Code and data are available at [Cite] https://github.com/tsafavi/NegatER.,,"• Evaluation: In keeping with the novelty of the problem, we conduct multiple evaluations that address the fundamental research ques-tions of negative commonsense. First, we measure the effectiveness of our LM fine-tuning approach and the utility of N EGAT ER-generated negatives in KB completion tasks (§ 4, 5). Next, we study the intrinsic qual-ity of the generated negatives (§ 6). When considering all such factors, N EGAT ER out-performs numerous competitive baselines. Most notably, training KB completion models with highly-ranked negative examples from N EGAT ER results in statistically significant accuracy improvements of up to 1.90 % . Code and data are available at [Cite] https://github.com/tsafavi/NegatER.",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.456_0_0,2021,N EGAT ER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases,Body
878,10882," https://spacy.io/usage/linguistic-features#pos-tagging"," ['5 Task-based evaluation', '5.2 Baselines']","We tag each phrase in the KB as either a verb, noun, or adjective phrase us-ing the SpaCy POS tagger. [Cite_Footnote_1]",1 https://spacy.io/usage/linguistic-features# pos-tagging,"• A NTONYMS : We tag each phrase in the KB as either a verb, noun, or adjective phrase us-ing the SpaCy POS tagger. [Cite_Footnote_1] Then, for each verb (noun, adjective) phrase, we replace the first verb (noun, adjective) token with a ran-",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.456_1_0,2021,N EGAT ER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases,Footnote
879,10883," http://www.wikivs.com/wiki/MainPage"," ['2 Related Work']",Other websites such as WikiVS [Cite_Footnote_1] contain user-contributed comparisons that have been categorized based on the nature of the entities being compared.,1 http://www.wikivs.com/wiki/Main Page,"Recently, the internet has seen a growth in websites offering comparisons for different entities. Prod-uct websites such as eBay maintain comparisons for products. Google also outputs pre-built compar-isons between common entities when queried with the word “vs.” between them. Both of these output purely structured attribute-value information and are unable to compare along more qualitative and de-scriptive dimensions such as ease of living or qual-ity of nightlife when comparing cities, for exam-ple. Other websites such as WikiVS [Cite_Footnote_1] contain user-contributed comparisons that have been categorized based on the nature of the entities being compared. These are manually curated and therefore do not scale to the quadratic number of entity pairs.",補足資料,Document,True,Introduce（引用目的）,True,N16-1009_0_0,2016,Entity-balanced Gaussian pLSA for Automated Comparison,Footnote
880,10884," http://code.google.com/p/word2vec/"," ['3 Task & System Description', '3.2 Building Clusters for Comparison']","We construct a vector representation for each descriptive phrase by averaging the word-vectors of individual words in a phrase (Mikolov et al., 2013) [Cite_Footnote_4] .",4 We use the pre-trained 300 dimension vectors available at http://code.google.com/p/word2vec/,"In response, we exploit the availability of pre-trained word vectors as a source of background se-mantic knowledge for every phrase, and generalize the pLSA model to Gaussian pLSA (G-pLSA). We construct a vector representation for each descriptive phrase by averaging the word-vectors of individual words in a phrase (Mikolov et al., 2013) [Cite_Footnote_4] . Thus, this model is pLSA with each topic-word distribution represented as a Gaussian distribution over descrip-tive phrases in the embedding space. This model is also similar to the recently introduced Gaussian LDA model (Das et al., 2015), but without LDA’s Dirichlet priors as discussed above.",Material,Knowledge,True,Use（引用目的）,True,N16-1009_1_0,2016,Entity-balanced Gaussian pLSA for Automated Comparison,Footnote
881,10885," http://wikitravel.org/en/Wikitravel:Articletemplates/Sections"," ['4 Human Subject Evaluations']","In addition, all articles contain sections [Cite_Footnote_7] describing different as-pects of a city from a tourism point of view (e.g., places to see, transportation, shopping and eating).",7 http://wikitravel.org/en/Wikitravel:Articletemplates/Sections,"In order to evaluate the usefulness of our system we conducted extensive experiments on Amazon Me-chanical Turk (AMT). Our experiments answer the following questions. (1) Are comparisons generated using our clustering methods G-pLSA and EB G-pLSA preferred by users against the entity oblivi-ous baseline of GMM? (2) Are our system-generated comparison tables helpful to people for the task of entity comparison? Datasets & System Settings: We experiment on two datasets – tourism and movies. For tourism, we downloaded a collection of 16,785 travel arti-cles from WikiTravel. The website contains arti-cles that have been collaboratively written by Web users. Each article describes a city or a larger geo-graphic area that is of interest to tourists. In addition, all articles contain sections [Cite_Footnote_7] describing different as-pects of a city from a tourism point of view (e.g., places to see, transportation, shopping and eating). For our proof of concept, we performed IE only on the ‘places to see’ sections.",補足資料,Document,True,Introduce（引用目的）,True,N16-1009_2_0,2016,Entity-balanced Gaussian pLSA for Automated Comparison,Footnote
882,10886," http://snap.stanford.edu/data,June"," ['4 Human Subject Evaluations']","For Movies dataset, we used the Amazon review data set (Leskovec and Krevl, 2014) [Cite_Ref] .","Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data,June.","For Movies dataset, we used the Amazon review data set (Leskovec and Krevl, 2014) [Cite_Ref] . It has over 7.9 million reviews for 250,000 movies. We combined all the reviews for a movie, thus, generating a large review document per movie. This dataset is much noisier compared to WikiTravel due to presence of slang, incorrect grammar, sarcasm, etc. In addition, users also tend to compare and contrast while re-viewing movies so there are even references to other movies. As a result, the descriptive phrases extracted were much more noisy.",Material,Dataset,True,Use（引用目的）,True,N16-1009_3_0,2016,Entity-balanced Gaussian pLSA for Automated Comparison,Reference
883,10887," https://github.com/jungokasai/graph_parser"," ['3 Results and Discussion']","We implement all of our models in TensorFlow (Abadi et al., 2016). [Cite_Footnote_3]",3 Our code is available online for easy replication of our results at https://github.com/jungokasai/graph_parser.,"We follow the protocol of Bangalore et al. (2009), Chung et al. (2016), Kasai et al. (2017), and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005). Following that work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set. The training, dev, and test sets comprise 39832, 1921, and 2415 sentences, respectively. We implement all of our models in TensorFlow (Abadi et al., 2016). [Cite_Footnote_3]",Method,Code,True,Produce（引用目的）,True,N18-1107_0_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Footnote
884,10888," http://www.aclweb.org/anthology/P16-1231"," ['6 Related Work']",Andor et al. (2016) [Cite_Ref] developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search.,"Daniel Andor, Chris Alberti, David Weiss, Aliak-sei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Glob-ally normalized transition-based neural networks. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 2442–2452. http://www.aclweb.org/anthology/P16-1231.","Recent neural network models for transition-based and graph-based parsing can be viewed as remedies for the aforementioned limitations. Andor et al. (2016) [Cite_Ref] developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search. The globally normalized objective ad-dresses the label bias problem and makes global training effective in the transition-based parsing setting. Kiperwasser and Goldberg (2016) incor-porated a dynamic oracle (Goldberg and Nivre, 2013) in a BiLSTM transition-based parser that remedies global error propagation. Kiperwasser and Goldberg (2016) and Dozat and Manning (2017) proposed graph-based parsers that have ac-cess to rich feature representations obtained from BiLSTMs.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_1_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
885,10889," http://www.aclweb.org/anthology/P11-1048"," ['6 Related Work']","Previous work integrated CCG supertagging and parsing using belief propagation and dual de-composition approaches (Auli and Lopez, 2011) [Cite_Ref] .","Michael Auli and Adam Lopez. 2011. A compari-son of loopy belief propagation and dual decompo-sition for integrated CCG supertagging and parsing. In ACL. Association for Computational Linguistics, pages 470–480. http://www.aclweb.org/anthology/P11-1048.","Previous work integrated CCG supertagging and parsing using belief propagation and dual de-composition approaches (Auli and Lopez, 2011) [Cite_Ref] . Nguyen et al. (2017) incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing perfor-mance.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_2_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
886,10890," http://www.aclweb.org/anthology/N/N09/N09-2047"," ['1 Introduction']",This near uniqueness of a parse given a gold sequence of supertags has been con-firmed empirically (TAG: Bangalore et al. (2009) [Cite_Ref] ; Chung et al. (2016); Kasai et al. (2017); CCG: Lewis et al. (2016)).,"Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Benoı̂t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185–188. http://www.aclweb.org/anthology/N/N09/N09-2047.","Tree Adjoining Grammar (TAG, Joshi and Sch-abes (1997)) and Combinatory Categorial Gram-mar (CCG, Steedman and Baldridge (2011)) are both mildly context-sensitive grammar formalisms that are lexicalized: every elementary structure (elementary tree for TAG and category for CCG) is associated with exactly one lexical item, and ev-ery lexical item of the language is associated with a finite set of elementary structures in the gram-mar (Rambow and Joshi, 1994). In TAG and CCG, the task of parsing can be decomposed into two phases (e.g. TAG: Bangalore and Joshi (1999); CCG: Clark and Curran (2007)): supertagging, where elementary units or supertags are assigned to each lexical item and parsing where these su-pertags are combined together. The first phase of supertagging can be considered as “almost pars-ing” because supertags for a sentence almost al-ways determine a unique parse (Bangalore and Joshi, 1999). This near uniqueness of a parse given a gold sequence of supertags has been con-firmed empirically (TAG: Bangalore et al. (2009) [Cite_Ref] ; Chung et al. (2016); Kasai et al. (2017); CCG: Lewis et al. (2016)).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_3_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
887,10891," http://www.aclweb.org/anthology/N/N09/N09-2047"," ['2 Our Models', '2.2 Parsing Model']","For example, Bangalore et al. (2009) [Cite_Ref] proposes the MICA parser, an Earley parser that exploits a TAG grammar that has been trans-formed into a variant of a probabilistic CFG.","Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Benoı̂t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185–188. http://www.aclweb.org/anthology/N/N09/N09-2047.","Until recently, TAG parsers have been grammar based, requiring as input a set of elemenetary trees (supertags). For example, Bangalore et al. (2009) [Cite_Ref] proposes the MICA parser, an Earley parser that exploits a TAG grammar that has been trans-formed into a variant of a probabilistic CFG. One advantage of such a parser is that its parses are guaranteed to be well-formed according to the TAG grammar provided as input.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_3_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
888,10892," http://www.aclweb.org/anthology/N/N09/N09-2047"," ['2 Our Models', '2.3 Joint Modeling']","2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009 [Cite_Ref] ; Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017).","Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Benoı̂t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185–188. http://www.aclweb.org/anthology/N/N09/N09-2047.","2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009 [Cite_Ref] ; Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_3_2,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
889,10893," http://www.aclweb.org/anthology/N/N09/N09-2047"," ['3 Results and Discussion']","We follow the protocol of Bangalore et al. (2009) [Cite_Ref] , Chung et al. (2016), Kasai et al. (2017), and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).","Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Benoı̂t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185–188. http://www.aclweb.org/anthology/N/N09/N09-2047.","We follow the protocol of Bangalore et al. (2009) [Cite_Ref] , Chung et al. (2016), Kasai et al. (2017), and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005). Following that work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set. The training, dev, and test sets comprise 39832, 1921, and 2415 sentences, respectively. We implement all of our models in TensorFlow (Abadi et al., 2016). 3",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_3_3,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
890,10894," http://www.aclweb.org/anthology/N/N09/N09-2047"," ['3 Results and Discussion', '3.3 Joint Models']","Our current implementation pro-cesses 225 sentences per second on a single Tesla K80 GPU, an order of magnitude faster than the MICA system (Bangalore et al., 2009) [Cite_Ref] .","Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Benoı̂t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185–188. http://www.aclweb.org/anthology/N/N09/N09-2047.","Lastly, it is worth noting our joint parsing ar-chitecture has a substantial advantage regarding parsing speed. Since POS tagging, supertagging, and parsing decisions are made independently for each word in a sentence, our system can parallelize computation once the sentence is encoded in the BiLSTM layers. Our current implementation pro-cesses 225 sentences per second on a single Tesla K80 GPU, an order of magnitude faster than the MICA system (Bangalore et al., 2009) [Cite_Ref] . 5",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_3_4,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
891,10895," http://www.aclweb.org/anthology/N/N09/N09-2047"," ['1 Introduction']","This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Ban-galore et al., 2009 [Cite_Ref] ; Chung et al., 2016; Kasai et al., 2017).","Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Benoı̂t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185–188. http://www.aclweb.org/anthology/N/N09/N09-2047.","We focus on TAG parsing in this work. TAG differs from CCG in having a more varied set of supertags. Concretely, the TAG-annotated version of the WSJ Penn Treebank (Marcus et al., 1993) that we use (Chen et al., 2005) includes 4727 dis-tinct supertags (2165 occur once) while the CCG-annotated version (Hockenmaier and Steedman, 2007) only includes 1286 distinct supertags (439 occur once). This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Ban-galore et al., 2009 [Cite_Ref] ; Chung et al., 2016; Kasai et al., 2017).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_3_5,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
892,10896," http://aclanthology.coli.uni-saarland.de/pdf/W/W03/W03-1006.pdf"," ['5 Downstream Tasks']","Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) and semantic role la-beling (Chen and Rambow, 2003) [Cite_Ref] .","John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In EMNLP. pages 41–48. http://aclanthology.coli.uni-saarland.de/pdf/W/W03/W03-1006.pdf. Nasr, Owen Rambow, and Srinivas Bangalore. 2016. Revisiting supertagging and parsing: How to use supertags in transition-based parsing. In TAG+. pages 85–92. http://www.aclweb.org/anthology/W16-3309.","Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) and semantic role la-beling (Chen and Rambow, 2003) [Cite_Ref] . In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance. These re-sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_4_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
893,10897," http://www.aclweb.org/anthology/K17-3002"," ['1 Introduction']","In particular, we use the biaffine graph-based parser proposed by Dozat and Manning (2017) [Cite_Ref] together with our novel techniques for supertagging.","Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","In this work, we present a supertagger and a parser that substantially improve upon previously reported results. We propose crucial modifications to the bidirectional LSTM (BiLSTM) supertagger in Kasai et al. (2017). First, we use character-level Convolutional Neural Networks (CNNs) for en-coding morphological information instead of suf-fix embeddings. Secondly, we perform concatena-tion after each BiLSTM layer. Lastly, we explore the impact of adding additional BiLSTM layers and highway connections. These techniques yield an increase of 1.3% in accuracy. For parsing, since the derivation tree in a lexicalized TAG is a type of dependency tree (Rambow and Joshi, 1994), we can directly apply dependency parsing models. In particular, we use the biaffine graph-based parser proposed by Dozat and Manning (2017) [Cite_Ref] together with our novel techniques for supertagging.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
894,10898," http://www.aclweb.org/anthology/K17-3002"," ['2 Our Models', '2.2 Parsing Model']","Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [Cite_Ref] that allows for global training and inference.","Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017). Kasai et al. (2017) and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [Cite_Ref] that allows for global training and inference.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
895,10899," http://www.aclweb.org/anthology/K17-3002"," ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']","We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) [Cite_Ref] and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017).","Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) [Cite_Ref] and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al. (2017) and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are ran-domly initialized.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_2,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
896,10900," http://www.aclweb.org/anthology/K17-3002"," ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']","Following Dozat and Manning (2017) [Cite_Ref] and Kiperwasser and Goldberg (2016), we use BiLSTMs to obtain features for each word in a sentence.","Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","We train our parser to predict edges between lex-ical items in an LTAG derivation tree. Edges are labeled by the operations together with the deep syntactic roles of substitution sites (0=underlying subject, 1=underlying direct object, 2=underlying indirect object, 3,4=oblique arguments, CO=co-head for prepositional/particle verbs, and adj=all adjuncts). Figure 1 shows our biaffine parsing ar-chitecture. Following Dozat and Manning (2017) [Cite_Ref] and Kiperwasser and Goldberg (2016), we use BiLSTMs to obtain features for each word in a sentence. We add highway connections in the same fashion as our supertagging model.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_3,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
897,10901," http://www.aclweb.org/anthology/K17-3002"," ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']","In the testing phase, we use the heuristics formulated by Dozat and Manning (2017) [Cite_Ref] to ensure that the resulting parse is single-rooted and acyclic.","Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","In training, we simply take the greedy maximum probability to predict the parent of each word. In the testing phase, we use the heuristics formulated by Dozat and Manning (2017) [Cite_Ref] to ensure that the resulting parse is single-rooted and acyclic.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_4,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
898,10902," http://www.aclweb.org/anthology/K17-3002"," ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']",We generally follow the hyperparameters cho-sen in Dozat and Manning (2017) [Cite_Ref] .,"Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","We generally follow the hyperparameters cho-sen in Dozat and Manning (2017) [Cite_Ref] . Specifically, we use BiLSTMs layers with 400 units each. In-put, layer-to-layer, and recurrent dropout rates are all 0.33. The depths of all MLPs are all 1, and the MLPs for unlabeled attachment and those for labeling contain 500 (d arc ) and 100 (d rel ) units re-spectively. For character-level CNNs, we use the hyperparameters from Ma and Hovy (2016).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_5,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
899,10903," http://www.aclweb.org/anthology/K17-3002"," ['6 Related Work']",Kiperwasser and Goldberg (2016) and Dozat and Manning (2017) [Cite_Ref] proposed graph-based parsers that have ac-cess to rich feature representations obtained from BiLSTMs.,"Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","Recent neural network models for transition-based and graph-based parsing can be viewed as remedies for the aforementioned limitations. Andor et al. (2016) developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search. The globally normalized objective ad-dresses the label bias problem and makes global training effective in the transition-based parsing setting. Kiperwasser and Goldberg (2016) incor-porated a dynamic oracle (Goldberg and Nivre, 2013) in a BiLSTM transition-based parser that remedies global error propagation. Kiperwasser and Goldberg (2016) and Dozat and Manning (2017) [Cite_Ref] proposed graph-based parsers that have ac-cess to rich feature representations obtained from BiLSTMs.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_6,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
900,10904," http://www.aclweb.org/anthology/K17-3002"," ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']","In Universal Dependencies, Dozat et al. (2017) [Cite_Ref] empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger.","Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford’s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20–30. http://www.aclweb.org/anthology/K17-3002.","The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) [Cite_Ref] empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al. (2017) and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are ran-domly initialized.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_8_7,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
901,10905," http://www.aclweb.org/anthology/W17-6213"," ['1 Introduction']","We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ).","Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","In addition to these architectural extensions for supertagging and parsing, we also explore multi-task learning approaches for TAG parsing. Specif-ically, we perform POS tagging, supertagging, and parsing using the same feature representations from the BiLSTMs. This joint modeling has the benefit of avoiding a time-consuming and com-plicated pipeline process, and instead produces a full syntactic analysis, consisting of supertags and the derivation that combines them, simultane-ously. Moreover, this multi-task learning frame-work further improves performance in all three tasks. We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ). The end-to-end TAG parser substantially outper-forms the previously reported best results.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
902,10906," http://www.aclweb.org/anthology/W17-6213"," ['2 Our Models', '2.2 Parsing Model']","More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ).","Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ). Kasai et al. (2017) and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
903,10907," http://www.aclweb.org/anthology/W17-6213"," ['2 Our Models', '2.2 Parsing Model']",Kasai et al. (2017) and Friedman et al. (2017) [Cite_Ref] achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.,"Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017). Kasai et al. (2017) and Friedman et al. (2017) [Cite_Ref] achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_2,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
904,10908," http://www.aclweb.org/anthology/W17-6213"," ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']","We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ).","Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al. (2017) and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are ran-domly initialized.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_3,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
905,10909," http://www.aclweb.org/anthology/W17-6213"," ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']","Indeed, Kasai et al. (2017) and Friedman et al. (2017) [Cite_Ref] demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.","Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al. (2017) and Friedman et al. (2017) [Cite_Ref] demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are ran-domly initialized.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_4,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
906,10910," http://www.aclweb.org/anthology/W17-6213"," ['2 Our Models', '2.3 Joint Modeling']","2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ).","Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [Cite_Ref] ).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_5,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
907,10911," http://www.aclweb.org/anthology/W17-6213"," ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']",The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) and Friedman et al. (2017) [Cite_Ref] .,"Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) and Friedman et al. (2017) [Cite_Ref] . For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) during the training phase, yielding the success in the analogy task. For example, a transitive verb su-pertag whose object has been filled by substitution should be treated by the parser in the same way as an intransitive verb supertag. In our graph-based parsing setting, we do not have a notion of parse history or partial derivations that directly connect intransitive and transitive verbs. However, syn-tactic analogies still hold to a considerable degree in the vector representations of supertags induced by our joint models, with average rank of the cor-rect answer nearly the same as that obtained in the transition-based parser.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_6,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
908,10912," http://www.aclweb.org/anthology/W17-6213"," ['5 Downstream Tasks']","Avg. rank is the average position of the correct choice in the ranked list of the closest neighbors; the top line indicates the result of using su-pertag embeddings that are trained jointly with a tran-sition based parser (Friedman et al., 2017) [Cite_Ref] .","Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","Table 4: Syntactic analogy test results on the 300 most frequent supertags. Avg. rank is the average position of the correct choice in the ranked list of the closest neighbors; the top line indicates the result of using su-pertag embeddings that are trained jointly with a tran-sition based parser (Friedman et al., 2017) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_7,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
909,10913," http://www.aclweb.org/anthology/W17-6213"," ['3 Results and Discussion']","We follow the protocol of Bangalore et al. (2009), Chung et al. (2016), Kasai et al. (2017), and Fried-man et al. (2017) [Cite_Ref] ; we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).","Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umeå, Sweden, pages 122–131. http://www.aclweb.org/anthology/W17-6213.","We follow the protocol of Bangalore et al. (2009), Chung et al. (2016), Kasai et al. (2017), and Fried-man et al. (2017) [Cite_Ref] ; we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005). Following that work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set. The training, dev, and test sets comprise 39832, 1921, and 2415 sentences, respectively. We implement all of our models in TensorFlow (Abadi et al., 2016). 3",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_9_8,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
910,10914," https://www.aclweb.org/anthology/D17-1180"," ['1 Introduction']",This near uniqueness of a parse given a gold sequence of supertags has been con-firmed empirically (TAG: Bangalore et al. (2009); Chung et al. (2016); Kasai et al. (2017) [Cite_Ref] ; CCG: Lewis et al. (2016)).,"Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","Tree Adjoining Grammar (TAG, Joshi and Sch-abes (1997)) and Combinatory Categorial Gram-mar (CCG, Steedman and Baldridge (2011)) are both mildly context-sensitive grammar formalisms that are lexicalized: every elementary structure (elementary tree for TAG and category for CCG) is associated with exactly one lexical item, and ev-ery lexical item of the language is associated with a finite set of elementary structures in the gram-mar (Rambow and Joshi, 1994). In TAG and CCG, the task of parsing can be decomposed into two phases (e.g. TAG: Bangalore and Joshi (1999); CCG: Clark and Curran (2007)): supertagging, where elementary units or supertags are assigned to each lexical item and parsing where these su-pertags are combined together. The first phase of supertagging can be considered as “almost pars-ing” because supertags for a sentence almost al-ways determine a unique parse (Bangalore and Joshi, 1999). This near uniqueness of a parse given a gold sequence of supertags has been con-firmed empirically (TAG: Bangalore et al. (2009); Chung et al. (2016); Kasai et al. (2017) [Cite_Ref] ; CCG: Lewis et al. (2016)).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
911,10915," https://www.aclweb.org/anthology/D17-1180"," ['1 Introduction']","This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Ban-galore et al., 2009; Chung et al., 2016; Kasai et al., 2017 [Cite_Ref] ).","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","We focus on TAG parsing in this work. TAG differs from CCG in having a more varied set of supertags. Concretely, the TAG-annotated version of the WSJ Penn Treebank (Marcus et al., 1993) that we use (Chen et al., 2005) includes 4727 dis-tinct supertags (2165 occur once) while the CCG-annotated version (Hockenmaier and Steedman, 2007) only includes 1286 distinct supertags (439 occur once). This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Ban-galore et al., 2009; Chung et al., 2016; Kasai et al., 2017 [Cite_Ref] ).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
912,10916," https://www.aclweb.org/anthology/D17-1180"," ['1 Introduction']",We propose crucial modifications to the bidirectional LSTM (BiLSTM) supertagger in Kasai et al. (2017) [Cite_Ref] .,"Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","In this work, we present a supertagger and a parser that substantially improve upon previously reported results. We propose crucial modifications to the bidirectional LSTM (BiLSTM) supertagger in Kasai et al. (2017) [Cite_Ref] . First, we use character-level Convolutional Neural Networks (CNNs) for en-coding morphological information instead of suf-fix embeddings. Secondly, we perform concatena-tion after each BiLSTM layer. Lastly, we explore the impact of adding additional BiLSTM layers and highway connections. These techniques yield an increase of 1.3% in accuracy. For parsing, since the derivation tree in a lexicalized TAG is a type of dependency tree (Rambow and Joshi, 1994), we can directly apply dependency parsing models. In particular, we use the biaffine graph-based parser proposed by Dozat and Manning (2017) together with our novel techniques for supertagging.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_2,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
913,10917," https://www.aclweb.org/anthology/D17-1180"," ['1 Introduction']","We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017).","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","In addition to these architectural extensions for supertagging and parsing, we also explore multi-task learning approaches for TAG parsing. Specif-ically, we perform POS tagging, supertagging, and parsing using the same feature representations from the BiLSTMs. This joint modeling has the benefit of avoiding a time-consuming and com-plicated pipeline process, and instead produces a full syntactic analysis, consisting of supertags and the derivation that combines them, simultane-ously. Moreover, this multi-task learning frame-work further improves performance in all three tasks. We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017). The end-to-end TAG parser substantially outper-forms the previously reported best results.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_3,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
914,10918," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.1 Supertagging Model']","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) [Cite_Ref] and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models.","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) [Cite_Ref] and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_4,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
915,10919," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.1 Supertagging Model']","We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017 [Cite_Ref] ) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017 [Cite_Ref] ) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_5,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
916,10920," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']","This concatenation af-ter each layer differs from Kasai et al. (2017) [Cite_Ref] and Lewis et al. (2016), where concatenation happens only after the final BiLSTM layer.","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","Here a semicolon ; means concatenation, is element-wise multiplication, and σ is the sigmoid function. In the first BiLSTM layer, the input x t is the vector representation of word t. (The sequence is reversed for the backwards pass.) In all subse-quent layers, x t is the corresponding output from the previous BiLSTM; the output of a BiLSTM at timestep t is equal to [h ft ;h bt ], the concatenation of hidden state corresponding to input t in the for-ward and backward pass. This concatenation af-ter each layer differs from Kasai et al. (2017) [Cite_Ref] and Lewis et al. (2016), where concatenation happens only after the final BiLSTM layer. We will show in a later section that concatenation after each layer contributes to improvement in performance.",補足資料,Paper,True,Compare（引用目的）,True,N18-1107_11_6,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
917,10921," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']",We also extend the models in Kasai et al. (2017) [Cite_Ref] and Lewis et al. (2016) by allowing highway con-nections between LSTM layers.,"Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","We also extend the models in Kasai et al. (2017) [Cite_Ref] and Lewis et al. (2016) by allowing highway con-nections between LSTM layers. A highway con-nection is a gating mechanism that combines the current and previous layer outputs, which can pre-vent the problem of vanishing/exploding gradients (Srivastava et al., 2015). Specifically, in networks with highway connections, we replace Eq. 6 by:",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_7,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
918,10922," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']",We generally follow the hyperparameters cho-sen in Lewis et al. (2016) and Kasai et al. (2017) [Cite_Ref] .,"Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","We generally follow the hyperparameters cho-sen in Lewis et al. (2016) and Kasai et al. (2017) [Cite_Ref] . Specifically, we use BiLSTMs layers with 512 units each. Input, layer-to-layer, and recurrent (Gal and Ghahramani, 2016) dropout rates are all 0.5. For the CNN character-level representation, we used the hyperparameters from Ma and Hovy (2016).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_8,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
919,10923," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.2 Parsing Model']","More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017).","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017). Kasai et al. (2017) and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_9,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
920,10924," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.2 Parsing Model']",Kasai et al. (2017) [Cite_Ref] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.,"Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017). Kasai et al. (2017) [Cite_Ref] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_10,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
921,10925," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']","We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017).","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al. (2017) and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are ran-domly initialized.",補足資料,Paper,True,Compare（引用目的）,False,N18-1107_11_11,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
922,10926," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']","Indeed, Kasai et al. (2017) [Cite_Ref] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al. (2017) [Cite_Ref] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are ran-domly initialized.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_12,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
923,10927," https://www.aclweb.org/anthology/D17-1180"," ['2 Our Models', '2.3 Joint Modeling']","2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017).","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017 [Cite_Ref] ; Friedman et al., 2017).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_13,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
924,10928," https://www.aclweb.org/anthology/D17-1180"," ['3 Results and Discussion']","We follow the protocol of Bangalore et al. (2009), Chung et al. (2016), Kasai et al. (2017) [Cite_Ref] , and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","We follow the protocol of Bangalore et al. (2009), Chung et al. (2016), Kasai et al. (2017) [Cite_Ref] , and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005). Following that work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set. The training, dev, and test sets comprise 39832, 1921, and 2415 sentences, respectively. We implement all of our models in TensorFlow (Abadi et al., 2016). 3",補足資料,Document,True,Use（引用目的）,True,N18-1107_11_14,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
925,10929," https://www.aclweb.org/anthology/D17-1180"," ['3 Results and Discussion', '3.3 Joint Models']","Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [Cite_Ref] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse.","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [Cite_Ref] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions. Most interesting is the fact that the graph-based parser shows less of an effect of dependency length. Since the shift-reduce parser builds a parse sequentially with one parsing action depending on those that come be-fore it, we would expect to find a propogation of errors made in establishing shorter dependencies to the establishment of longer dependencies.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_15,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
926,10930," https://www.aclweb.org/anthology/D17-1180"," ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']","We next analyze the induced vector representa-tions in the output projection matrices of our su-pertagger and joint parsers using the syntactic analogy framework (Kasai et al., 2017) [Cite_Ref] .","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","We next analyze the induced vector representa-tions in the output projection matrices of our su-pertagger and joint parsers using the syntactic analogy framework (Kasai et al., 2017) [Cite_Ref] . Consider, for instance, the analogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a subject relative clause headed by a tran-sitive verb (t99) is to a subject relative headed by an intransitive verb (t109). Following the ideas in Mikolov et al. (2013) for word analogies, we can express this structural analogy as t27 - t81 + t109 = t99 and test it by cosine similarity. Table 4 shows the results of the analogy test with 246 equations involving structural analogies with only the 300 most frequent supertags in the training data. While the embeddings (projection matrix) from the independently trained supertagger do not appear to reflect the syntax, those obtained from the joint models yield linguistic structure despite the fact that the supertag embeddings (projection matrix) is trained without any a priori syntactic knowledge about the elementary trees.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_16,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
927,10931," https://www.aclweb.org/anthology/D17-1180"," ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']",The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [Cite_Ref] and Friedman et al. (2017).,"Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [Cite_Ref] and Friedman et al. (2017). For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) during the training phase, yielding the success in the analogy task. For example, a transitive verb su-pertag whose object has been filled by substitution should be treated by the parser in the same way as an intransitive verb supertag. In our graph-based parsing setting, we do not have a notion of parse history or partial derivations that directly connect intransitive and transitive verbs. However, syn-tactic analogies still hold to a considerable degree in the vector representations of supertags induced by our joint models, with average rank of the cor-rect answer nearly the same as that obtained in the transition-based parser.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_17,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
928,10932," https://www.aclweb.org/anthology/D17-1180"," ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']","For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) [Cite_Ref] during the training phase, yielding the success in the analogy task.","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) and Friedman et al. (2017). For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) [Cite_Ref] during the training phase, yielding the success in the analogy task. For example, a transitive verb su-pertag whose object has been filled by substitution should be treated by the parser in the same way as an intransitive verb supertag. In our graph-based parsing setting, we do not have a notion of parse history or partial derivations that directly connect intransitive and transitive verbs. However, syn-tactic analogies still hold to a considerable degree in the vector representations of supertags induced by our joint models, with average rank of the cor-rect answer nearly the same as that obtained in the transition-based parser.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_11_18,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
929,10933," https://www.aclweb.org/anthology/D17-1180"," ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']","Our joint parser outperforms the other parsers, including the neu-ral network shift-reduce TAG parser (Kasai et al., 2017) [Cite_Ref] .","Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713–1723. https://www.aclweb.org/anthology/D17-1180.","Table 6 shows the results. Our joint parser outperforms the other parsers, including the neu-ral network shift-reduce TAG parser (Kasai et al., 2017) [Cite_Ref] . Our data-driven parsers yield relatively low performance in the ObQ and RNR constructions. Performance on ObQ is low, we expect, because of their rarity in the data on which the parser is trained. 7 For RNR, rarity may be an issue as well as the limits of the TAG analysis of this construc-tion. Nonetheless, we see that the rich structural representations that a TAG parser provides enables substantial improvements in the extraction of un-bounded dependencies. In the future, we hope to evaluate state-of-the-art Stanford dependency parsers automatically.",補足資料,Paper,True,Compare（引用目的）,True,N18-1107_11_19,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
930,10934," http://www.aclweb.org/anthology/P16-1101"," ['2 Our Models', '2.1 Supertagging Model']","[Cite_Ref] ), and has shown that such models substantially improve perfor-mance beyond non-neural models.","Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064–1074. http://www.aclweb.org/anthology/P16-1101.","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016 [Cite_Ref] ), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_13_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
931,10935," http://www.aclweb.org/anthology/P16-1101"," ['2 Our Models', '2.1 Supertagging Model', '2.1.1 Input Representations']","The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016 [Cite_Ref] ).","Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064–1074. http://www.aclweb.org/anthology/P16-1101.","The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016 [Cite_Ref] ). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word. We initialize the word embeddings to be the pre-trained GloVe vectors (Pennington et al., 2014); for words not in GloVe, we initialize their embedding to a zero vec-tor. The other embeddings are randomly initial-ized. We obtain predicted POS tags from a BiL-STM POS tagger with the same configuration as in Ma and Hovy (2016).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_13_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
932,10936," http://www.aclweb.org/anthology/P16-1101"," ['2 Our Models', '2.1 Supertagging Model', '2.1.1 Input Representations']",We obtain predicted POS tags from a BiL-STM POS tagger with the same configuration as in Ma and Hovy (2016) [Cite_Ref] .,"Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064–1074. http://www.aclweb.org/anthology/P16-1101.","The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word. We initialize the word embeddings to be the pre-trained GloVe vectors (Pennington et al., 2014); for words not in GloVe, we initialize their embedding to a zero vec-tor. The other embeddings are randomly initial-ized. We obtain predicted POS tags from a BiL-STM POS tagger with the same configuration as in Ma and Hovy (2016) [Cite_Ref] .",Material,Knowledge,True,Use（引用目的）,True,N18-1107_13_2,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
933,10937," http://www.aclweb.org/anthology/P16-1101"," ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']","For the CNN character-level representation, we used the hyperparameters from Ma and Hovy (2016) [Cite_Ref] .","Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064–1074. http://www.aclweb.org/anthology/P16-1101.","We generally follow the hyperparameters cho-sen in Lewis et al. (2016) and Kasai et al. (2017). Specifically, we use BiLSTMs layers with 512 units each. Input, layer-to-layer, and recurrent (Gal and Ghahramani, 2016) dropout rates are all 0.5. For the CNN character-level representation, we used the hyperparameters from Ma and Hovy (2016) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_13_3,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
934,10938," http://www.aclweb.org/anthology/P16-1101"," ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']","For character-level CNNs, we use the hyperparameters from Ma and Hovy (2016) [Cite_Ref] .","Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064–1074. http://www.aclweb.org/anthology/P16-1101.","We generally follow the hyperparameters cho-sen in Dozat and Manning (2017). Specifically, we use BiLSTMs layers with 400 units each. In-put, layer-to-layer, and recurrent dropout rates are all 0.33. The depths of all MLPs are all 1, and the MLPs for unlabeled attachment and those for labeling contain 500 (d arc ) and 100 (d rel ) units re-spectively. For character-level CNNs, we use the hyperparameters from Ma and Hovy (2016) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_13_4,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
935,10939," http://www.aclweb.org/anthology/P16-1101"," ['3 Results and Discussion', '3.1 Supertaggers']","Our BiLSTM POS tagger yielded 97.37% and 97.53% tagging accuracy on the dev and test sets, performance on par with the state-of-the-art (Ling et al., 2015; Ma and Hovy, 2016 [Cite_Ref] ).","Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064–1074. http://www.aclweb.org/anthology/P16-1101.","Our BiLSTM POS tagger yielded 97.37% and 97.53% tagging accuracy on the dev and test sets, performance on par with the state-of-the-art (Ling et al., 2015; Ma and Hovy, 2016 [Cite_Ref] ). Seen in the middle section of Table 1 is supertagging per-formance obtained from various model configu-rations. “Final concat” in the model name in-dicates that vectors from forward and backward pass are concatenated only after the final layer. Concatenation happens after each layer otherwise. Numbers immediately after BiLSTM indicate the numbers of layers. CNN, HW, and POS denote respectively character-level CNNs, highway con-nections, and pipeline POS input from our BiL-STM POS tagger. Firstly, the differences in per-formance between BiLSTM2 (final concat) and BiLSTM2 and between BiLSTM2 and BiLSTM2- CNN suggest an advantage to performing concate-nation after each layer and adding character-level CNNs. Adding predicted POS to the input some-what helps supertagging though the difference is small. Adding a third BiLSTM layer helps only if there are highway connections, presumably be-cause deeper BiLSTMs are more vulnerable to the vanishing/exploding gradient problem. Our supertagging model (BiLSTM3-HW-CNN-POS) that performs best on the dev set achieves an ac-curacy of 90.81% on the test set, outperforming the previously best result by more than 1.3%.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_13_5,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
936,10940," http://www.aclweb.org/anthology/H/H05/H05-1066"," ['6 Related Work']","Graph-based parsers (e.g. MST (McDonald et al., 2005) [Cite_Ref] ) are trained to directly assign scores to dependency graphs.","Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective depen-dency parsing using spanning tree algorithms. In EMNLP. Association for Computational Lin-guistics, Vancouver, British Columbia, Canada, pages 523–530. http://www.aclweb.org/anthology/H/H05/H05-1066.","The two major classes of data-driven methods for dependency parsing are often called transition-based and graph-based parsing (Kübler et al., 2009). Transition-based parsers (e.g. MALT (Nivre, 2003)) learn to predict the next transition given the input and the parse history. Graph-based parsers (e.g. MST (McDonald et al., 2005) [Cite_Ref] ) are trained to directly assign scores to dependency graphs.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_15_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
937,10941," https://doi.org/10.18653/v1/K17-3014"," ['6 Related Work']","Nguyen et al. (2017) [Cite_Ref] incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging.","Dat Quoc Nguyen, Mark Dras, and Mark John-son. 2017. A novel neural network model for joint pos tagging and graph-based dependency pars-ing. In CoNLL 2017 Shared Task: Multilin-gual Parsing from Raw Text to Universal Depen-dencies. Association for Computational Linguis-tics, pages 134–142. https://doi.org/10.18653/v1/K17-3014.","Previous work integrated CCG supertagging and parsing using belief propagation and dual de-composition approaches (Auli and Lopez, 2011). Nguyen et al. (2017) [Cite_Ref] incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing perfor-mance.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_16_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
938,10942," http://www.aclweb.org/anthology/C10-1094"," ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']","Because of variations across formalisms in their representational format for unbounded depden-dencies, past work has conducted manual evalu-ation on this corpus (Rimell et al., 2009; Nivre et al., 2010 [Cite_Ref] ).","Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-los Gómez Rodrı́guez. 2010. Evaluation of depen-dency parsers on unbounded dependencies. In COL-ING. Coling 2010 Organizing Committee, Beijing, China, pages 833–841. http://www.aclweb.org/anthology/C10-1094.","Because of variations across formalisms in their representational format for unbounded depden-dencies, past work has conducted manual evalu-ation on this corpus (Rimell et al., 2009; Nivre et al., 2010 [Cite_Ref] ). We instead conduct an automatic evaluation using a procedure that converts TAG parses to structures directly comparable to those specified in the unbounded dependency corpus. To this end, we apply two types of structural trans-formation in addition to those used for the PETE task: 1) a more extensive analysis of coordina-tion, 2) resolution of differences in dependency representations in cases involving copula verbs and co-anchors (e.g., verbal particles). See Ap-pendix A for details. After the transformations, we simply check if the resulting dependency graphs contain target labeled arcs given in the dataset.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_17_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
939,10943," http://www.aclweb.org/anthology/C10-1094"," ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']",The results of the first five parsers are taken from Rimell et al. (2009) and Nivre et al. (2010) [Cite_Ref] .,"Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-los Gómez Rodrı́guez. 2010. Evaluation of depen-dency parsers on unbounded dependencies. In COL-ING. Coling 2010 Organizing Committee, Beijing, China, pages 833–841. http://www.aclweb.org/anthology/C10-1094.",corpus. The results of the first five parsers are taken from Rimell et al. (2009) and Nivre et al. (2010) [Cite_Ref] . The Total and Avg columns indicate the percentage of correctly recovered dependencies out of all dependencies and the average of accuracy on the 7 constructions.,補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_17_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
940,10944," http://jmlr.org/proceedings/papers/v32/santos14.pdf"," ['2 Our Models', '2.1 Supertagging Model', '2.1.1 Input Representations']","The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014 [Cite_Ref] ; Chiu and Nichols, 2016; Ma and Hovy, 2016).","Cicero D. Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Tony Jebara and Eric P. Xing, editors, ICML. JMLR Workshop and Conference Proceedings, pages 1818– 1826. http://jmlr.org/proceedings/papers/v32/santos14.pdf.","The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014 [Cite_Ref] ; Chiu and Nichols, 2016; Ma and Hovy, 2016). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word. We initialize the word embeddings to be the pre-trained GloVe vectors (Pennington et al., 2014); for words not in GloVe, we initialize their embedding to a zero vec-tor. The other embeddings are randomly initial-ized. We obtain predicted POS tags from a BiL-STM POS tagger with the same configuration as in Ma and Hovy (2016).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_18_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
941,10945," http://www.aclweb.org/anthology/N16-1027"," ['2 Our Models', '2.1 Supertagging Model']","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016 [Cite_Ref] ; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models.","Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. 2016. Supertagging with LSTMs. In NAACL. Association for Computational Linguistics, San Diego, California, pages 232–237. http://www.aclweb.org/anthology/N16-1027.","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016 [Cite_Ref] ; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_19_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
942,10946," http://www.aclweb.org/anthology/W17-6214"," ['5 Downstream Tasks']","Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) [Cite_Ref] and semantic role la-beling (Chen and Rambow, 2003).","Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umeå, Sweden, pages 132–141. http://www.aclweb.org/anthology/W17-6214.","Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) [Cite_Ref] and semantic role la-beling (Chen and Rambow, 2003). In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance. These re-sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_20_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
943,10947," http://www.aclweb.org/anthology/W17-6214"," ['5 Downstream Tasks', '5.1 PETE']","Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017) [Cite_Ref] .","Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umeå, Sweden, pages 132–141. http://www.aclweb.org/anthology/W17-6214.","Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation (Yuret et al., 2010). The task was intended to evaluate syn-tactic parsers across different formalisms, focus-ing on entailments that could be determined en-tirely on the basis of the syntactic representa-tions of the sentences that are involved, with-out recourse to lexical semantics, logical reason-ing, or world knowledge. For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017) [Cite_Ref] . Xu et al. (2017) provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_20_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
944,10948," http://www.aclweb.org/anthology/W17-6214"," ['5 Downstream Tasks', '5.1 PETE']",Xu et al. (2017) [Cite_Ref] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task.,"Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umeå, Sweden, pages 132–141. http://www.aclweb.org/anthology/W17-6214.","Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation (Yuret et al., 2010). The task was intended to evaluate syn-tactic parsers across different formalisms, focus-ing on entailments that could be determined en-tirely on the basis of the syntactic representa-tions of the sentences that are involved, with-out recourse to lexical semantics, logical reason-ing, or world knowledge. For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017). Xu et al. (2017) [Cite_Ref] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_20_2,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
945,10949," http://www.aclweb.org/anthology/W17-6214"," ['A.1 Transformations from PETE']",We apply three types of transformation from Xu et al. (2017) [Cite_Ref] to interpret the TAG parses.,"Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umeå, Sweden, pages 132–141. http://www.aclweb.org/anthology/W17-6214.",We apply three types of transformation from Xu et al. (2017) [Cite_Ref] to interpret the TAG parses.,補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_20_3,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
946,10950," http://www.aclweb.org/anthology/W17-6214"," ['A.2 Coordination']",We roughly follow the method presented in Xu et al. (2017) [Cite_Ref] with extensions.,"Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umeå, Sweden, pages 132–141. http://www.aclweb.org/anthology/W17-6214.","We roughly follow the method presented in Xu et al. (2017) [Cite_Ref] with extensions. Under the TAG analysis, VP coordination involves a VP-recursive auxiliary tree headed by the coordinator that in-cludes a VP substitution node (for the second con-junct) with label 1. In order to allow the first clauses subject argument (as well as modal verbs and negations) to be shared by the second verb, we add the relevant relations to the second verb. In ad-dition, we analyze sentential coordination cases. Sentence coordination in our TAG grammar usu-ally happens between two complete sentences and no modifiers or arguments are shared, and there-fore it can be analyzed via substituting a sentence int the coordinator with label 1. However, when sentential coordination happens between two rela-tive clause modifiers, our TAG grammar analyzes the second clause as a complete sentence, meaning that we need to recover the extracted argument by consulting the property of the first clause. Further-more, the deep syntactic role of the extracted argu-ment can be different in the two relative clauses. For instance, in the sentence, “... the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have re-moved,” we need to recover an arc from removed to stump with label 1 whereas the arc from im-paled to stump has label 0. To resolve this issue, when there is coordination of two relative clause modifiers, we add an edge from the head of the second clause to the modified noun with the same label as the label that under which the relative pro-noun is attached to the head.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_20_4,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
947,10951," http://www.aclweb.org/anthology/P15-2041"," ['2 Our Models', '2.1 Supertagging Model']","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015 [Cite_Ref] ; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models.","Wenduan Xu, Michael Auli, and Stephen Clark. 2015. CCG supertagging with a recurrent neural network. In ACL. Association for Computational Linguistics, Beijing, China, pages 250–255. http://www.aclweb.org/anthology/P15-2041.","Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015 [Cite_Ref] ; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_22_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
948,10952," http://www.aclweb.org/anthology/S10-1009"," ['1 Introduction']","Finally, we apply our new parsers to the down-stream tasks of Parsing Evaluation using Tex-tual Entailements (PETE, Yuret et al. (2010) [Cite_Ref] ) and Unbounded Dependency Recovery (Rimell et al., 2009).","Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. SemEval-2010 task 12: Parser evaluation using tex-tual entailments. In SemEval. Association for Com-putational Linguistics, Uppsala, Sweden, pages 51– 56. http://www.aclweb.org/anthology/S10-1009.","Finally, we apply our new parsers to the down-stream tasks of Parsing Evaluation using Tex-tual Entailements (PETE, Yuret et al. (2010) [Cite_Ref] ) and Unbounded Dependency Recovery (Rimell et al., 2009). We demonstrate that our end-to-end parser outperforms the best results in both tasks. These results illustrate that TAG is a viable formalism for tasks that benefit from the assignment of rich structural descriptions to sentences.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_23_0,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
949,10953," http://www.aclweb.org/anthology/S10-1009"," ['5 Downstream Tasks', '5.1 PETE']","Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation (Yuret et al., 2010) [Cite_Ref] .","Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. SemEval-2010 task 12: Parser evaluation using tex-tual entailments. In SemEval. Association for Com-putational Linguistics, Uppsala, Sweden, pages 51– 56. http://www.aclweb.org/anthology/S10-1009.","Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation (Yuret et al., 2010) [Cite_Ref] . The task was intended to evaluate syn-tactic parsers across different formalisms, focus-ing on entailments that could be determined en-tirely on the basis of the syntactic representa-tions of the sentences that are involved, with-out recourse to lexical semantics, logical reason-ing, or world knowledge. For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017). Xu et al. (2017) provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1107_23_1,2018,End-to-end Graph-based TAG Parsing with Neural Networks Jungo Kasai ♣ Robert Frank ♣ Pauli Xu ♣,Reference
950,10954," https://github.com/timvdc/poetry"," ['5 Conclusion']",The current version can be downloaded at [Cite] https://github.com/timvdc/poetry.,,"In order to facilitate reproduction of the results and encourage further research, the poetry genera-tion system is made available as open source soft-ware. The current version can be downloaded at [Cite] https://github.com/timvdc/poetry.",Method,Tool,True,Produce（引用目的）,True,2020.acl-main.223_0_0,2020,Automatic Poetry Generation from Prosaic Text,Body
951,10955," http://tdil-dc.in/index.php?lang=en"," ['4 Experimental Setup']","The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010) [Cite_Footnote_1] spans multiple Indian languages from the health and tourism domains.",1 The corpus is available on request from http://tdil-dc.in/index.php?lang=en,"Datasets: For training English-Hindi NMT sys-tems, we use the IITB English-Hindi parallel cor-pus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010) [Cite_Footnote_1] spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB par-allel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set.",Material,Dataset,True,Introduce（引用目的）,False,N19-1387_0_0,2019,Addressing Word-order Divergence in Multilingual Neural Machine Translation for Extremely Low Resource Languages,Footnote
952,10956," https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md"," ['4 Experimental Setup']","Text embeddings (Grave et al., 2018) [Cite_Footnote_2] .",2 https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md,"Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the stan-dard encoder-attention-decoder architecture (Bah-danau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fast- Text embeddings (Grave et al., 2018) [Cite_Footnote_2] .",Material,Knowledge,False,Produce（引用目的）,True,N19-1387_1_0,2019,Addressing Word-order Divergence in Multilingual Neural Machine Translation for Extremely Low Resource Languages,Footnote
953,10957," https://github.com/anoopkunchukuttan/cfilt_preorder"," ['4 Experimental Setup']",Pre-ordering: We use CFILT-preorder [Cite_Footnote_3] for pre-reordering English sentences.,3 https://github.com/anoopkunchukuttan/ cfilt_preorder,"Pre-ordering: We use CFILT-preorder [Cite_Footnote_3] for pre-reordering English sentences. It contains two pre-ordering configurations: (1) generic rules (G) that apply to all Indian languages (Ramanathan et al., 2008), and (2) hindi-tuned rules (HT) which im-proves generic rules by incorporating improve-ments found through error analysis of English-Hindi reordering (Patel et al., 2013). The Hindi-tuned rules improve translation for other English to Indian language pairs too (Kunchukuttan et al., 2014).",Method,Tool,True,Use（引用目的）,True,N19-1387_2_0,2019,Addressing Word-order Divergence in Multilingual Neural Machine Translation for Extremely Low Resource Languages,Footnote
954,10958," http://research.microsoft.com/mct"," ['5 Experiments']","(1) First is the MCTest-500 dataset [Cite_Footnote_1] , a freely available set of 500 stories (split into 300 train, 50 dev and 150 test) and associated questions (Richardson et al., 2013).",1 http://research.microsoft.com/mct,"Datasets: We use two datasets for our evaluation. (1) First is the MCTest-500 dataset [Cite_Footnote_1] , a freely available set of 500 stories (split into 300 train, 50 dev and 150 test) and associated questions (Richardson et al., 2013). The stories are fictional so the answers can be found only in the story it-self. The stories and questions are carefully lim-ited, thereby minimizing the world knowledge re-quired for this task. Yet, the task is challenging for most modern NLP systems. Each story in MCTest has four multiple choice questions, each with four answer choices. Each question has only one cor-rect answer. Furthermore, questions are also anno-tated with ‘single’ and ‘multiple’ labels. The ques-tions annotated ‘single’ only require one sentence in the story to answer them. For ‘multiple’ ques-tions it should not be possible to find the answer to the question in any individual sentence of the passage. In a sense, the ‘multiple’ questions are harder than the ‘single’ questions as they typically require complex lexical analysis, some inference and some form of limited reasoning. Cucerzan-converted questions can also be downloaded from the MCTest website.",Material,Dataset,True,Use（引用目的）,True,P15-1024_0_0,2015,Learning Answer-Entailing Structures for Machine Comprehension,Footnote
955,10959," https://research.facebook.com/researchers/1543934539189348"," ['5 Experiments']","The second dataset is a synthetic dataset released under the bAbI project [Cite_Footnote_2] (Weston et al., 2015).",2 https://research.facebook.com/researchers/1543934539189348 sider two evaluation metrics: accuracy (propor-,"(2) The second dataset is a synthetic dataset released under the bAbI project [Cite_Footnote_2] (Weston et al., 2015). The dataset presents a set of 20 ‘tasks’, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner. For each ‘task’, 1000 questions are used for training and 1000 for testing. The ‘tasks’ refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg. relations, yes/no questions, counting ques-tions, etc. Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appear-ing in the text, etc. We write simple rules to con-vert the question and answer candidate pairs to hy-potheses. Baselines: We have five baselines. (1) The first three baselines are inspired from Richardson et al. (2013). The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text. (2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well. As far as we know, SW+D is the best previ-ously published result on this task. 4 (3) The third baseline (called RTE) uses textual entail-ment to answer MCTest questions. For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015). The base-line uses LSTMs (Hochreiter and Schmidhuber, 1997) to accomplish the task. LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) is taken from Iyyer et al. (2014). QANTA too uses a recursive neural net-work for question answering. Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments. First, we look at question classification. We use a simple question classi-fication based on the question word (what, why, what, etc.). We call this QClassification. Next, we also use a question/answer classification from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought. We call this QAClassification. Finally, we also learn a clas-sifier for the 20 tasks in the Machine Compre-hension gamut described in Weston et al. (2015). The classification algorithm (called TaskClassifi-cation) was built on the bAbI training set. It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the ques-tion and answer. The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting. In our ex-periments we will see that learning these strategies is better than learning the question answer classi-fication which is in turn better than learning the question classification.",Material,Dataset,True,Use（引用目的）,True,P15-1024_1_0,2015,Learning Answer-Entailing Structures for Machine Comprehension,Footnote
956,10960," http://hltfbk.github.io/Excitement-Open-Platform/"," ['5 Experiments']","For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. [Cite_Footnote_5]","5 The BIUTEE system (Stern and Dagan, 2012) available under the Excitement Open Platform http://hltfbk.github.io/Excitement-Open-Platform/ was used for recognizing textual entailment.","(2) The second dataset is a synthetic dataset released under the bAbI project (Weston et al., 2015). The dataset presents a set of 20 ‘tasks’, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner. For each ‘task’, 1000 questions are used for training and 1000 for testing. The ‘tasks’ refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg. relations, yes/no questions, counting ques-tions, etc. Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appear-ing in the text, etc. We write simple rules to con-vert the question and answer candidate pairs to hy-potheses. Baselines: We have five baselines. (1) The first three baselines are inspired from Richardson et al. (2013). The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text. (2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well. As far as we know, SW+D is the best previ-ously published result on this task. 4 (3) The third baseline (called RTE) uses textual entail-ment to answer MCTest questions. For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. [Cite_Footnote_5] (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015). The base-line uses LSTMs (Hochreiter and Schmidhuber, 1997) to accomplish the task. LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) is taken from Iyyer et al. (2014). QANTA too uses a recursive neural net-work for question answering. Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments. First, we look at question classification. We use a simple question classi-fication based on the question word (what, why, what, etc.). We call this QClassification. Next, we also use a question/answer classification from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought. We call this QAClassification. Finally, we also learn a clas-sifier for the 20 tasks in the Machine Compre-hension gamut described in Weston et al. (2015). The classification algorithm (called TaskClassifi-cation) was built on the bAbI training set. It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the ques-tion and answer. The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting. In our ex-periments we will see that learning these strategies is better than learning the question answer classi-fication which is in turn better than learning the question classification.",補足資料,Website,False,Use（引用目的）,False,P15-1024_2_0,2015,Learning Answer-Entailing Structures for Machine Comprehension,Footnote
957,10961," http://cs.umd.edu/miyyer/qblearn/"," ['5 Experiments']",(5) The fifth base-line (called QANTA) [Cite_Footnote_6] is taken from Iyyer et al. (2014).,6 http://cs.umd.edu/miyyer/qblearn/,"(2) The second dataset is a synthetic dataset released under the bAbI project (Weston et al., 2015). The dataset presents a set of 20 ‘tasks’, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner. For each ‘task’, 1000 questions are used for training and 1000 for testing. The ‘tasks’ refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg. relations, yes/no questions, counting ques-tions, etc. Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appear-ing in the text, etc. We write simple rules to con-vert the question and answer candidate pairs to hy-potheses. Baselines: We have five baselines. (1) The first three baselines are inspired from Richardson et al. (2013). The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text. (2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well. As far as we know, SW+D is the best previ-ously published result on this task. 4 (3) The third baseline (called RTE) uses textual entail-ment to answer MCTest questions. For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015). The base-line uses LSTMs (Hochreiter and Schmidhuber, 1997) to accomplish the task. LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) [Cite_Footnote_6] is taken from Iyyer et al. (2014). QANTA too uses a recursive neural net-work for question answering. Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments. First, we look at question classification. We use a simple question classi-fication based on the question word (what, why, what, etc.). We call this QClassification. Next, we also use a question/answer classification from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought. We call this QAClassification. Finally, we also learn a clas-sifier for the 20 tasks in the Machine Compre-hension gamut described in Weston et al. (2015). The classification algorithm (called TaskClassifi-cation) was built on the bAbI training set. It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the ques-tion and answer. The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting. In our ex-periments we will see that learning these strategies is better than learning the question answer classi-fication which is in turn better than learning the question classification.",Material,Knowledge,False,Introduce（引用目的）,False,P15-1024_3_0,2015,Learning Answer-Entailing Structures for Machine Comprehension,Footnote
958,10962," http://cogcomp.cs.illinois.edu/Data/QA/QC/"," ['5 Experiments']","Next, we also use a question/answer classification [Cite_Footnote_7] from Li and Roth (2002).",7 http://cogcomp.cs.illinois.edu/Data/QA/QC/,"(2) The second dataset is a synthetic dataset released under the bAbI project (Weston et al., 2015). The dataset presents a set of 20 ‘tasks’, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner. For each ‘task’, 1000 questions are used for training and 1000 for testing. The ‘tasks’ refer to question categories such as questions requiring reasoning over single/two/three supporting facts or two/three arg. relations, yes/no questions, counting ques-tions, etc. Candidate answers are not provided but the answers are typically constrained to a small set: either yes or no or entities already appear-ing in the text, etc. We write simple rules to con-vert the question and answer candidate pairs to hy-potheses. Baselines: We have five baselines. (1) The first three baselines are inspired from Richardson et al. (2013). The first baseline (called SW) uses a sliding window and matches a bag of words constructed from the question and hypothesized answer to the text. (2) Since this ignores long range dependencies, the second baseline (called SW+D) accounts for intra-word distances as well. As far as we know, SW+D is the best previ-ously published result on this task. 4 (3) The third baseline (called RTE) uses textual entail-ment to answer MCTest questions. For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015). The base-line uses LSTMs (Hochreiter and Schmidhuber, 1997) to accomplish the task. LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) is taken from Iyyer et al. (2014). QANTA too uses a recursive neural net-work for question answering. Task Classification for MultiTask Learning: We consider three alternative task classifications for our experiments. First, we look at question classification. We use a simple question classi-fication based on the question word (what, why, what, etc.). We call this QClassification. Next, we also use a question/answer classification [Cite_Footnote_7] from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought. We call this QAClassification. Finally, we also learn a clas-sifier for the 20 tasks in the Machine Compre-hension gamut described in Weston et al. (2015). The classification algorithm (called TaskClassifi-cation) was built on the bAbI training set. It is essentially a Naive-Bayes classifier and uses only simple unigram and bigram features for the ques-tion and answer. The tasks typically correspond to different strategies when looking for an answer in the machine comprehension setting. In our ex-periments we will see that learning these strategies is better than learning the question answer classi-fication which is in turn better than learning the question classification.",Material,Knowledge,True,Use（引用目的）,True,P15-1024_4_0,2015,Learning Answer-Entailing Structures for Machine Comprehension,Footnote
959,10963," https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum"," ['A.1 Data']",WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. [Cite_Footnote_3],3 We take the processed Wikipedia articles from https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum released on April 25th 2018.,"WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. [Cite_Footnote_3] We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).",Material,DataSource,True,Use（引用目的）,True,P19-1504_1_0,2019,Generating Summaries with Topic Templates and Structured Convolutional Decoders,Footnote
960,10964," http://wiki.dbpedia.org/downloads-2016-10"," ['A.1 Data']","We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [Cite_Footnote_4]",4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).,"WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [Cite_Footnote_4] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).",Material,Knowledge,False,Use（引用目的）,True,P19-1504_2_0,2019,Generating Summaries with Topic Templates and Structured Convolutional Decoders,Footnote
961,10965," http://mappings.dbpedia.org/server/ontology/classes/"," ['A.1 Data']","We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [Cite_Footnote_4]",4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).,"WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [Cite_Footnote_4] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).",Material,Knowledge,False,Use（引用目的）,True,P19-1504_3_0,2019,Generating Summaries with Topic Templates and Structured Convolutional Decoders,Footnote
962,10966," http://barbar.cs.lth.se:8081"," ['1 Introduction']","Ad-ditionally, a considerable amount of training data is available for training SRL models (Table [Cite_Footnote_2] in Sec. 3), which made neural SRL models success-ful (Zhou and Xu, 2015; Yang and Mitchell, 2017).",2 http://barbar.cs.lth.se:8081,"SRL4ORL. The semantic roles of the predicate fear (marked blue bold) correspond to the opin-ion roles H and T, according to MPQA. For this reason, the output of SRL systems has been com-monly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Ad-ditionally, a considerable amount of training data is available for training SRL models (Table [Cite_Footnote_2] in Sec. 3), which made neural SRL models success-ful (Zhou and Xu, 2015; Yang and Mitchell, 2017).",Material,Knowledge,False,Introduce（引用目的）,False,N18-1054_0_0,2018,SRL4ORL: Improving Opinion Role Labeling Using Multi-Task Learning With Semantic Role Labeling,Footnote
963,10967," https://github.com/amarasovic/naacl-mpqa-srl4orl/blob/master/generate_mpqa_jsons.py"," ['3 Experimental setup', '3.1 Datasets']",We report detailed pre-processing of MPQA [Cite_Footnote_4] and data statistics in the Supplementary Material.,4 Examples how to use our scripts can be found at https://github.com/amarasovic/naacl-mpqa-srl4orl/blob/master/generate_mpqa_jsons.py.,We report detailed pre-processing of MPQA [Cite_Footnote_4] and data statistics in the Supplementary Material.,補足資料,Document,True,Produce（引用目的）,True,N18-1054_1_0,2018,SRL4ORL: Improving Opinion Role Labeling Using Multi-Task Learning With Semantic Role Labeling,Footnote
964,10968," https://github.com/neulab/cmu-multinlp"," ['1 Introduction']","We have released our code and the General Language Analysis Datasets (GLAD) benchmark with 8 datasets covering 10 tasks in the BRAT format at [Cite] https://github.com/neulab/cmu-multinlp , and provide a leaderboard to facilitate future work on generalized models for NLP.",,"The simple hypothesis behind our paper is: if humans can perform natural language analysis in a single unified format, then perhaps machines can as well. Fortunately, there already exist NLP mod-els that perform span prediction and prediction of relations between pairs of spans, such as the end-to-end coreference model of Lee et al. (2017). We extend this model with minor architectural mod-ifications (which are not our core contributions) and pre-trained contextualized representations (e.g., Guo et al. (2016) 7 3 7 Swayamdipta et al. (2018) 7 7 3 BERT; Devlin et al. (2019) 1 ) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference reso-lution (Coref.), open information extraction (Ope-nIE), part-of-speech tagging (POS), dependency parsing (Dep.), constituency parsing (Consti.), se-mantic role labeling (SRL), aspect based sentiment analysis (ABSA), and opinion role labeling (ORL). While previous work has used similar formalisms to understand the representations learned by pre-trained embeddings (Tenney et al., 2019a,b), to the best of our knowledge this is the first work that uses such a unified model to actually perform analysis. Moreover, we demonstrate that despite the model’s simplicity, it can achieve comparable performance with special-purpose state-of-the-art models on the tasks above (Table 1). We also demonstrate that this framework allows us to easily perform multi-task learning (MTL), leading to improvements when there are related tasks to be learned from or data is sparse. Further analysis shows that dissimilar tasks exhibit divergent attention patterns, which explains why MTL is harmful on certain tasks. We have released our code and the General Language Analysis Datasets (GLAD) benchmark with 8 datasets covering 10 tasks in the BRAT format at [Cite] https://github.com/neulab/cmu-multinlp , and provide a leaderboard to facilitate future work on generalized models for NLP.",Material,Dataset,True,Produce（引用目的）,True,2020.acl-main.192_0_0,2020,Generalizing Natural Language Analysis through Span-relation Representations,Body
965,10969," http://www.iesl.cs.umass.edu/data/bibtex"," ['4 Experiments: Author Coreference']","For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries. [Cite_Footnote_3]",3 http://www.iesl.cs.umass.edu/data/bibtex,"For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries. [Cite_Footnote_3] We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset labeled for coref-erence, in order to assess accuracy. We also include ∼5 million mentions from DBLP.",Material,Dataset,True,Use（引用目的）,True,P12-1040_0_0,2012,A Discriminative Hierarchical Model for Fast Coreference at Large Scale,Footnote
966,10970," http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html"," ['4 Experiments: Author Coreference']","In addition we include 2,833 mentions from the REXA dataset [Cite_Footnote_4] labeled for coref-erence, in order to assess accuracy.",4 http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html,"For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries. We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset [Cite_Footnote_4] labeled for coref-erence, in order to assess accuracy. We also include ∼5 million mentions from DBLP.",Material,DataSource,True,Extend（引用目的）,True,P12-1040_1_0,2012,A Discriminative Hierarchical Model for Fast Coreference at Large Scale,Footnote
967,10971," https://github.com/Lambda-3/underDiscourseSimplification"," ['3 Recursive Sentence Splitting']","We present D IS S IM , a recursive sentence splitting approach that creates a semantic hierarchy of sim-plified sentences. [Cite_Footnote_1]",1 The source code of our framework is avail-able https://github.com/Lambda-3/underDiscourseSimplification.,"We present D IS S IM , a recursive sentence splitting approach that creates a semantic hierarchy of sim-plified sentences. [Cite_Footnote_1] The goal of our approach is to generate an intermediate representation that presents a simple and more regular structure which is easier to process for downstream se-mantic applications and may support a faster generalization in ML tasks. For this purpose, we cover a wider range of syntactic constructs (10 in total) than state-of-the-art rule-based syn-tactic frameworks. In particular, our approach is not limited to breaking up clausal components, but also splits and rephrases a variety of phrasal elements, resulting in a much more fine-grained output where each proposition represents a mini-mal semantic unit that is typically composed of a simple subject-predicate-object structure. Though tackling a larger set of linguistic constructs, our framework operates on a much smaller set of only 35 manually defined rules as compared to existing syntax-driven rule-based approaches.",Method,Code,True,Produce（引用目的）,True,P19-1333_0_0,2019,Transforming Complex Sentences into a Semantic Hierarchy,Footnote
968,10972," https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material"," ['3 Recursive Sentence Splitting', '3.1 Transformation Stage']","Table 2 displays some examples of our transformation patterns, [Cite_Footnote_2] which are specified in terms of Tregex patterns.","2 For reproducibility purposes, the complete set of trans-formation patterns is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.","The transformation patterns are based on syn-tactic and lexical features that can be derived from a sentence’s phrase structure. They were heuris-tically determined in a rule engineering process whose main goal was to provide a best-effort set of patterns, targeting the challenge of being applied in a recursive fashion and to overcome biased or incorrectly structured parse trees. We empirically determined a fixed execution order of the rules by examining which sequence achieved the best sim-plification results in a manual qualitative analysis conducted on a development test set of 100 ran-domly sampled Wikipedia sentences. The gram-mar rules are applied recursively in a top-down fashion on the source sentence, until no more sim-plification pattern matches. In that way, the in-put is turned into a discourse tree, consisting of a set of hierarchically ordered and semantically interconnected sentences that present a simpli-fied syntax. Table 2 displays some examples of our transformation patterns, [Cite_Footnote_2] which are specified in terms of Tregex patterns.",Material,Knowledge,True,Produce（引用目的）,True,P19-1333_1_0,2019,Transforming Complex Sentences into a Semantic Hierarchy,Footnote
969,10973," https://github.com/senisioi/NeuralTextSimplification"," ['4 Experimental Setup']","Furthermore, in accor-dance with prior work on TS, we report average BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016) scores for the rephrasings of each system. [Cite_Footnote_6]",6 For the computation of the BLEU and SARI scores we used the implementation of Nisioi et al. (2017) which is available under https://github.com/senisioi/NeuralTextSimplification.,"Automatic Evaluation. The automatic metrics that were calculated in the evaluation procedure comprise a number of basic statistics, including (i) the average sentence length of the simplified sentences in terms of the average number of to-kens per output sentence (#T/S); (ii) the average number of simplified output sentences per com-plex input (#S/C); (iii) the percentage of sentences that are copied from the source without perform-ing any simplification operation (%SAME), serv-ing as an indicator for system conservatism; and (iv) the averaged Levenshtein distance from the in-put (LD SC ), which provides further evidence for a system’s conservatism. Furthermore, in accor-dance with prior work on TS, we report average BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016) scores for the rephrasings of each system. [Cite_Footnote_6] Finally, we computed the SAMSA and SAMSA abl score of each system, which are the first met-rics that explicitly target syntactic aspects of TS (Sulem et al., 2018b).",Method,Tool,True,Use（引用目的）,True,P19-1333_2_0,2019,Transforming Complex Sentences into a Semantic Hierarchy,Footnote
970,10974," https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material"," ['References']",These analyses were carried out on a dataset which we compiled. [Cite_Footnote_10],10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.,"Tables 9 and 10 show the results of the recall-based qualitative analysis of the transformation patterns, together with the findings of the error analysis. These analyses were carried out on a dataset which we compiled. [Cite_Footnote_10] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.",Material,Dataset,True,Produce（引用目的）,True,P19-1333_3_0,2019,Transforming Complex Sentences into a Semantic Hierarchy,Footnote
971,10975," https://github.com/valentinhofmann/dagobert"," ['1 Introduction']",We also publish the largest dataset of derivatives in context to date. [Cite_Footnote_1],1 We make our code and data publicly available at https: //github.com/valentinhofmann/dagobert.,"Contributions. We develop the first frame-work for generating derivationally complex English words with a PLM, specifically BERT, and ana-lyze BERT’s performance in different settings. Our best model, DagoBERT (Derivationally and gener-atively optimized BERT), clearly outperforms an LSTM-based model, the previous state of the art. We find that DagoBERT’s errors are mainly due to syntactic and semantic overlap between affixes. Furthermore, we show that the input segmentation impacts how much derivational knowledge is avail-able to BERT, both during training and inference. This suggests that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used. We also publish the largest dataset of derivatives in context to date. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.316_0_0,2020,DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,Footnote
972,10976," https://files.pushshift.io/reddit/comments/"," ['3 Dataset of Derivatives']","While Vylo-mova et al. (2017) use Wikipedia, we extract the dataset from Reddit. [Cite_Footnote_3]","3 We draw upon the entire Baumgartner Reddit Corpus, a collection of all public Reddit posts available at https: //files.pushshift.io/reddit/comments/.","We base our study on a new dataset of derivatives in context similar in form to the one released by Vylomova et al. (2017), i.e., it is based on sen-tences with a derivative (e.g., this jacket is unwearable .) that are altered by masking the derivative (this jacket is .). Each item in the dataset consists of (i) the altered sen-tence, (ii) the derivative (unwearable) and (iii) the base (wear). The task is to generate the cor-rect derivative given the altered sentence and the base. We use sentential contexts rather than tags to represent derivational meanings because they better reflect the semantic variability inherent in derivational morphology (Section 2). While Vylo-mova et al. (2017) use Wikipedia, we extract the dataset from Reddit. [Cite_Footnote_3] Since productively formed derivatives are not part of the language norm ini-tially (Bauer, 2001), social media is a particularly fertile ground for our study.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.316_1_0,2020,DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,Footnote
973,10977," https://github.com/ivri/dmorph"," ['4 Experiments', '4.3 Models']","In order to provide a strict comparison to Vy-lomova et al. (2017), we also evaluate our LSTM and best BERT-based model on the suffix dataset released by Vylomova et al. (2017) against the reported performance of their encoder-decoder model. [Cite_Footnote_8]","8 The dataset is available at https://github.com/ivri/dmorph. While Vylomova et al. (2017) take morpho-orthographic changes into account, we only predict affixes, not the accompanying changes in orthography (Section 4.1).","In order to provide a strict comparison to Vy-lomova et al. (2017), we also evaluate our LSTM and best BERT-based model on the suffix dataset released by Vylomova et al. (2017) against the reported performance of their encoder-decoder model. [Cite_Footnote_8] Notice Vylomova et al. (2017) show that providing the LSTM with the POS of the deriva-tive increases performance. Here, we focus on the more general case where the POS is not known and hence do not consider this setting.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.316_2_0,2020,DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,Footnote
974,10978," http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html"," ['1 Introduction']",Cold Start task [Cite_Footnote_1] requires systems to take set of documents and produce a comprehensive set of triples that encode relationships between and at-tributes of the named-entities that are mentioned in the corpus.,1 See details at http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html,"The Text Analysis Conference (TAC) Knowledge Base Population (KBP) Cold Start task [Cite_Footnote_1] requires systems to take set of documents and produce a comprehensive set of triples that encode relationships between and at-tributes of the named-entities that are mentioned in the corpus. Systems are evaluated based on the fi-delity of the constructed knowledge base. For the 2012 evaluation, a fixed schema of 42 relations (or slots), and their logical inverses was provided, for example:",補足資料,Website,True,Introduce（引用目的）,True,N13-3008_0_0,2013,KELVIN: a tool for automated,Footnote
975,10979," http://en.wikipedia.org/"," ['1 Introduction']","To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia [Cite_Footnote_2] , or DBpedia.",2 http://en.wikipedia.org/,"To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia [Cite_Footnote_2] , or DBpedia. Only facts that are as-serted in and gleaned from the source documents are posited.",Material,DataSource,True,Introduce（引用目的）,True,N13-3008_1_0,2013,KELVIN: a tool for automated,Footnote
976,10980," http://www.dbpedia.org/"," ['1 Introduction']","To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia , or DBpedia. [Cite_Footnote_3]",3 http://www.dbpedia.org/,"To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia , or DBpedia. [Cite_Footnote_3] Only facts that are as-serted in and gleaned from the source documents are posited.",Material,DataSource,True,Introduce（引用目的）,True,N13-3008_2_0,2013,KELVIN: a tool for automated,Footnote
977,10981," http://www.itl.nist.gov/iad/mig/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdf"," ['3 Pipeline Components', '3.1 SERIF']","The functions SERIF can provide are based largely on the NIST ACE specification, [Cite_Footnote_9] and include: (a) identifying named-entities and classifying them by type and subtype; (b) performing intra-document co-reference analysis, including named mentions, as well as co-referential nominal and pronominal mentions; (c) parsing sentences and extracting intra-sentential relations between entities; and, (d) detect-ing certain types of events.","9 The principal types of ACE named-entities are per-sons, organizations, and geo-political entities (GPEs). GPEs are inhabited locations with a government. See http://www.itl.nist.gov/iad/mig/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdf.","BBN’s SERIF tool 8 (Boschee et al., 2005) provides a considerable suite of document annotations that are an excellent basis for building a knowledge base. The functions SERIF can provide are based largely on the NIST ACE specification, [Cite_Footnote_9] and include: (a) identifying named-entities and classifying them by type and subtype; (b) performing intra-document co-reference analysis, including named mentions, as well as co-referential nominal and pronominal mentions; (c) parsing sentences and extracting intra-sentential relations between entities; and, (d) detect-ing certain types of events.",補足資料,Document,True,Introduce（引用目的）,True,N13-3008_3_0,2013,KELVIN: a tool for automated,Footnote
978,10982," http://dblp.uni-trier.de/db/journals/corr/corr1412.html"," ['2 Proposed Methodology']","We employ 20% Dropout (Srivastava et al., 2014) in the fully connected layers as a measure of regularization and Adam optimizer (Kingma and Ba, 2014) [Cite_Ref] for optimization.",Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimiza-tion. CoRR abs/1412.6980. http://dblp.uni-trier.de/db/journals/corr/corr1412.html.,"Network parameters for CNN, LSTM & GRU: In the fully connected layers we use 50 and 10 neurons , respectively for the two hidden layers. We use Relu activations (Glorot et al., 2011) for intermediate layers and tanh activation in the final layer. We employ 20% Dropout (Srivastava et al., 2014) in the fully connected layers as a measure of regularization and Adam optimizer (Kingma and Ba, 2014) [Cite_Ref] for optimization.",補足資料,Paper,True,Introduce（引用目的）,True,D17-1057_1_0,2017,A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis,Reference
979,10983," http://nlp.stanford.edu/software"," ['1 Introduction']","On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components.","2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp","The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language pro-cessing components, including part-of-speech tag-gers, chunkers, and parsers. There remain many differences in how these components are built, re-sulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differ-ently-trained linguistic component, such as tokeni-zation. The community recognizes this difficulty, and shared task organizers are now providing ac-companying parses and other analyses of the shared task data. For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components.",Method,Tool,True,Introduce（引用目的）,True,N12-3006_0_0,2012,"MSR SPLAT, a language analysis toolkit",Footnote
980,10984," http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp"," ['1 Introduction']","On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components.","2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp","The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language pro-cessing components, including part-of-speech tag-gers, chunkers, and parsers. There remain many differences in how these components are built, re-sulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differ-ently-trained linguistic component, such as tokeni-zation. The community recognizes this difficulty, and shared task organizers are now providing ac-companying parses and other analyses of the shared task data. For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components.",Method,Tool,True,Introduce（引用目的）,True,N12-3006_1_0,2012,"MSR SPLAT, a language analysis toolkit",Footnote
981,10985," http://incubator.apache.org/opennlp"," ['1 Introduction']","On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components.","2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp","The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language pro-cessing components, including part-of-speech tag-gers, chunkers, and parsers. There remain many differences in how these components are built, re-sulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differ-ently-trained linguistic component, such as tokeni-zation. The community recognizes this difficulty, and shared task organizers are now providing ac-companying parses and other analyses of the shared task data. For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [Cite_Footnote_2] to increase ac-cessibility and replicability of core components.",Method,Tool,True,Introduce（引用目的）,True,N12-3006_2_0,2012,"MSR SPLAT, a language analysis toolkit",Footnote
982,10986," https://github.com/jvking/reddit-RL-simulator"," ['1 Introduction']","First, we propose a novel reinforcement learning task with both states and combinatorial actions de-fined by natural language, [Cite_Footnote_1] which is introduced in section 2.",1 Simulator code and Reddit discussion identifiers are re-leased at https://github.com/jvking/reddit-RL-simulator,"There are two main contributions in this paper. First, we propose a novel reinforcement learning task with both states and combinatorial actions de-fined by natural language, [Cite_Footnote_1] which is introduced in section 2. This task, which is based on comment popularity prediction using data from the Reddit dis-cussion forum, can serve as a benchmark in social media recommendation and trend spotting. The sec-ond contribution is the development of a novel deep reinforcement learning architecture for handling a combinatorial action space associated with natural language. Prior work related to both the task and deep reinforcement learning is reviewed in section 3, Details for the new models and baseline architec-tures are described in section 4. Experimental re-sults in section 5 show the proposed methods outper-form baseline models and that a bidirectional LSTM is effective for characterizing the combined utility of sub-actions. A brief summary of findings and open questions are in section 6.",Mixed,Mixed,True,Produce（引用目的）,True,D16-1189_0_0,2016,Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads,Footnote
983,10987," http://www.reddit.com"," ['2 Popularity Prediction and Tracking']","Our experiments are based on Reddit [Cite_Footnote_2] , one of the world’s largest public discussion forums.",2 http://www.reddit.com,"Our experiments are based on Reddit [Cite_Footnote_2] , one of the world’s largest public discussion forums. On Red-dit, registered users initiate a post and people re-spond with comments, either to the original post or one of its associated comments. Together, the com-ments and the original post form a discussion tree, which grows as new comments are contributed. It has been show that discussions tend to have a hier-archical topic structure (Weninger et al., 2013), i.e. different branches of the discussion reflect narrow-ing of higher level topics. Reddit discussions are grouped into different domains, called subreddits, according to different topics or themes. Depending on the popularity of the subreddit, a post can receive hundreds of comments.",補足資料,Website,True,Use（引用目的）,True,D16-1189_1_0,2016,Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads,Footnote
984,10988," http://www.darpa.mil/ipto/programs/gale/index.htm"," ['1 Introduction']","More recently, within the GALE [Cite_Footnote_1] project, multiple MT systems have been developed in each consortium, thus system combination be-comes more important.",1 http://www.darpa.mil/ipto/programs/gale/index.htm,"Many machine translation (MT) frameworks have been developed, including rule-based transfer MT, corpus-based MT (statistical MT and example-based MT), syntax-based MT and the hybrid, statistical MT augmented with syntactic structures. Different MT paradigms have their strengths and weaknesses. Systems adopting the same framework usually pro-duce different translations for the same input, due to their differences in training data, preprocessing, alignment and decoding strategies. It is beneficial to design a framework that combines the decoding strategies of multiple systems as well as their out-puts and produces translations better than any single system output. More recently, within the GALE [Cite_Footnote_1] project, multiple MT systems have been developed in each consortium, thus system combination be-comes more important.",補足資料,Website,True,Introduce（引用目的）,True,D07-1029_0_0,2007,Hierarchical System Combination for Machine Translation,Footnote
985,10989," https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT"," ['1 Introduction']","In summary, our main con-tributions are three-fold [Cite_Footnote_1] :",1 The source code can be found in this repos-itory: https://github.com/jiaweiw/ Extract-Edit-Unsupervised-NMT,"Empirical results on popular benchmarks show that exact-edit consistently outperforms the state-of-the-art unsupervised NMT system (Lample et al., 2018b) with back-translation across four dif-ferent languages pairs. In summary, our main con-tributions are three-fold [Cite_Footnote_1] :",Method,Code,True,Produce（引用目的）,True,N19-1120_0_0,2019,Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation,Footnote
986,10990," https://github.com/facebookresearch/faiss"," ['4 Experiments', '4.2 Implementation Details']","For efficient nearest neighbor search in the extracting step, we use the open-source Faiss library (Johnson et al., 2017) [Cite_Footnote_2] .",2 https://github.com/facebookresearch/ faiss,"Model Structure In this work, the NMT mod-els can be built upon long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) cells. For LSTM cells, both the encoder and decoder have 3 layers. As for Transformer, we use 4 lay-ers both in the encoder and the decoder. As for both LSTM and Transformer, all encoder param-eters are shared across two languages. Similarly, we share all decoder parameters across two lan-guages. Both two model structure are optimized using Adam (Kingma and Ba, 2014) with a batch size of 32. The rate for LSTM cell is 0.0003 while Transformer’s is set as 0.0001. The weights in Equation 7 are ω lm = ω ext = 1. The λ for cal-culating ranking scores is 0.5. As for the evalu-ation network R, we use a multilayer perceptron with two hidden layers of size 512. For efficient nearest neighbor search in the extracting step, we use the open-source Faiss library (Johnson et al., 2017) [Cite_Footnote_2] . We calculate the similarity of sentences in each episode instead of each batch for computa-tional efficiency. At decoding time, sentences are generated using greedy decoding.",Material,DataSource,False,Use（引用目的）,True,N19-1120_1_0,2019,Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation,Footnote
987,10991," https://github.com/vid-koci/bert-commonsense"," ['1 Introduction']","We instantiate our method on English Wikipedia and generate the Wikipedia Co-REferences Masked (W IKI CREM) dataset with 2.4M examples, which we make publicly available for further usage [Cite_Footnote_1] .",1 The code can be found at https://github.com/vid-koci/bert-commonsense. The dataset and the models can be obtained from https://ora.ox.ac.uk/objects/uuid: c83e94bb-7584-41a1-aef9-85b0e764d9e3,"In this work, we address the lack of large train-ing sets for pronoun disambiguation by introduc-ing a large dataset that can be easily extended. To generate this dataset, we find passages of text where a personal name appears at least twice and mask one of its non-first occurrences. To make the disambiguation task more challenging, we also ensure that at least one other distinct personal name is present in the text in a position before the masked occurrence. We instantiate our method on English Wikipedia and generate the Wikipedia Co-REferences Masked (W IKI CREM) dataset with 2.4M examples, which we make publicly available for further usage [Cite_Footnote_1] . We show its value by using it to fine-tune the B ERT language model (Devlin et al., 2018) for pronoun resolution.",Method,Code,True,Produce（引用目的）,True,D19-1439_0_0,2019,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,Footnote
988,10992," https://ora.ox.ac.uk/objects/uuid:c83e94bb-7584-41a1-aef9-85b0e764d9e3"," ['1 Introduction']","We instantiate our method on English Wikipedia and generate the Wikipedia Co-REferences Masked (W IKI CREM) dataset with 2.4M examples, which we make publicly available for further usage [Cite_Footnote_1] .",1 The code can be found at https://github.com/vid-koci/bert-commonsense. The dataset and the models can be obtained from https://ora.ox.ac.uk/objects/uuid: c83e94bb-7584-41a1-aef9-85b0e764d9e3,"In this work, we address the lack of large train-ing sets for pronoun disambiguation by introduc-ing a large dataset that can be easily extended. To generate this dataset, we find passages of text where a personal name appears at least twice and mask one of its non-first occurrences. To make the disambiguation task more challenging, we also ensure that at least one other distinct personal name is present in the text in a position before the masked occurrence. We instantiate our method on English Wikipedia and generate the Wikipedia Co-REferences Masked (W IKI CREM) dataset with 2.4M examples, which we make publicly available for further usage [Cite_Footnote_1] . We show its value by using it to fine-tune the B ERT language model (Devlin et al., 2018) for pronoun resolution.",Mixed,Mixed,True,Produce（引用目的）,True,D19-1439_1_0,2019,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,Footnote
989,10993," https://dumps.wikimedia.org/enwiki/"," ['3 The W IKI CREM Dataset']","Starting from English Wikipedia [Cite_Footnote_2] , we search for sentences and pairs of sentences with the following properties: at least two distinct per-sonal names appear in the text, and one of them is repeated.",2 https://dumps.wikimedia.org/enwiki/ dump id: enwiki-20181201,"In this section, we describe how we obtained W I - KI CREM. Starting from English Wikipedia [Cite_Footnote_2] , we search for sentences and pairs of sentences with the following properties: at least two distinct per-sonal names appear in the text, and one of them is repeated. We do not use pieces of text with more than two sentences to collect concise examples only. Personal names in the text are called “can-didates”. One non-first occurrence of the repeated candidate is masked, and the goal is to predict the masked name, given the correct and one incorrect candidate. In case of more than one incorrect can-didate in the sentence, several datapoints are con-structed, one for each incorrect candidate.",Material,DataSource,True,Use（引用目的）,True,D19-1439_2_0,2019,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,Footnote
990,10994," https://spacy.io/usage/linguistic-features#named-entities"," ['3 The W IKI CREM Dataset']",We used the Spacy Named Entity Recognition library [Cite_Footnote_3] to find the occurrences of names in the text.,3 https://spacy.io/usage/ linguistic-features#named-entities,"We used the Spacy Named Entity Recognition library [Cite_Footnote_3] to find the occurrences of names in the text. The resulting dataset consists of 2, 438, 897 samples. 10, 000 examples are held out to serve as the validation set. Two examples from our dataset can be found on Figure 1.",Material,Knowledge,True,Use（引用目的）,True,D19-1439_3_0,2019,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,Footnote
991,10995," https://pypi.org/project/gender-guesser/"," ['3 The W IKI CREM Dataset']",We use the Gender guesser li-brary [Cite_Footnote_4] to determine the gender of the candidates.,4 https://pypi.org/project/gender-guesser/,"W IKI CREM statistics. We analyze our dataset for gender bias. We use the Gender guesser li-brary [Cite_Footnote_4] to determine the gender of the candidates. To mimic the analysis of pronoun genders per-formed in the related works (Webster et al., 2018; Rudinger et al., 2018; Zhao et al., 2018), we ob-serve the gender of the correct candidates only. There were 0.8M “male” or “mostly male” names and 0.42M “female” or “mostly female” names, the rest were classified as “unknown”. The ratio between female and male candidates is thus esti-mated around 0.53 in favour of male candidates. We will see that this gender imbalance does not have any negative impact on bias, as shown in Sec-tion 6.2.",Material,Knowledge,True,Use（引用目的）,True,D19-1439_4_0,2019,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,Footnote
992,10996," https://github.com/huggingface/pytorch-pretrained-BERT"," ['4 Model', '4.1 B ERT']",We use the PyTorch implementation of B ERT [Cite_Footnote_5] and the pre-trained weights for B ERT -large released by Devlin et al. (2018).,5 https://github.com/huggingface/ pytorch-pretrained-BERT,"In this work, we only focus on the masked token prediction. We use the PyTorch implementation of B ERT [Cite_Footnote_5] and the pre-trained weights for B ERT -large released by Devlin et al. (2018).",Method,Tool,True,Use（引用目的）,True,D19-1439_5_0,2019,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,Footnote
993,10997," https://github.com/AIPHES/ACL20-Reference-Free-MT-Evaluation"," ['References']",We make our MT evaluation code available. [Cite_Footnote_1],1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation,"Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual simi-larity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system trans-lations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systemati-cally investigate a range of metrics based on state-of-the-art cross-lingual semantic repre-sentations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limita-tions, namely, (a) a semantic mismatch be-tween representations of mutual translations and, more prominently, (b) the inability to punish “translationese”, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 cor-relation points. We make our MT evaluation code available. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.acl-main.151_0_0,2020,On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation,Footnote
994,10998," https://github.com/AIPHES/"," ['6 Conclusion']",[Cite] https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation .,,We release our metrics under the name XMover-Score publicly: [Cite] https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation .,Method,Code,True,Produce（引用目的）,True,2020.acl-main.151_1_0,2020,On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation,Body
995,10999," http://github.com/tuvuumass/task-transferability"," ['1 Introduction']","We publicly release our task library, which con-sists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability. [Cite_Footnote_2]",2 Library and code available at http://github.com/tuvuumass/task-transferability.,"We publicly release our task library, which con-sists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability. [Cite_Footnote_2]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.635_0_0,2020,Exploring and Predicting Transferability across NLP Tasks,Footnote
996,11000," https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs"," ['References Christopher Clark, Kenton Lee, Ming-Wei Chang,']","Treebank (POS-PTB; Marcus et al., 1993) and B; Cer et al., 2017) and Quora Question the Universal Dependencies English Web Tree-Pairs [Cite_Footnote_20] (QQP); natural language inference (NLI) bank (POS-EWT; Silveira et al., 2014); syntactic with Multi-Genre NLI (MNLI; Williams et al., constituency ancestor tagging, i.e., predicting the 2018), SQuAD (Rajpurkar et al., 2016) con-constituent label of the parent (Parent), grandpar-verted into Question-answering NLI (QNLI; Wang ent (GParent), and great-grandparent (GGParent) et al., 2019b), Recognizing Textual Entailment of each word in the PTB phrase-structure tree; 1,2,3,5 (RTE; Dagan et al., 2005, et seq.), and the semantic tagging task (ST; Bjerva et al., 2016; Winograd Schema Challenge (Levesque, 2011) re-",20 https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs,"Treebank (POS-PTB; Marcus et al., 1993) and B; Cer et al., 2017) and Quora Question the Universal Dependencies English Web Tree-Pairs [Cite_Footnote_20] (QQP); natural language inference (NLI) bank (POS-EWT; Silveira et al., 2014); syntactic with Multi-Genre NLI (MNLI; Williams et al., constituency ancestor tagging, i.e., predicting the 2018), SQuAD (Rajpurkar et al., 2016) con-constituent label of the parent (Parent), grandpar-verted into Question-answering NLI (QNLI; Wang ent (GParent), and great-grandparent (GGParent) et al., 2019b), Recognizing Textual Entailment of each word in the PTB phrase-structure tree; 1,2,3,5 (RTE; Dagan et al., 2005, et seq.), and the semantic tagging task (ST; Bjerva et al., 2016; Winograd Schema Challenge (Levesque, 2011) re-",Material,Knowledge,False,Use（引用目的）,False,2020.emnlp-main.635_1_0,2020,Exploring and Predicting Transferability across NLP Tasks,Footnote
997,11001," https://github.com/alontalmor/MultiQA"," ['References Christopher Clark, Kenton Lee, Ming-Wei Chang,']","; and conjunct identification, i.e., identify-ing the tokens that comprise the conjuncts in a Question answering (eleven tasks): We use coordination construction, with the coordination eleven QA datasets from the MultiQA (Tal-annotated PTB dataset (Conj; Ficler and Goldberg, mor and Berant, 2019a) repository [Cite_Footnote_21] , includ-",21 https://github.com/alontalmor/MultiQA,"2016); and conjunct identification, i.e., identify-ing the tokens that comprise the conjuncts in a Question answering (eleven tasks): We use coordination construction, with the coordination eleven QA datasets from the MultiQA (Tal-annotated PTB dataset (Conj; Ficler and Goldberg, mor and Berant, 2019a) repository [Cite_Footnote_21] , includ-",Material,Knowledge,False,Use（引用目的）,False,2020.emnlp-main.635_2_0,2020,Exploring and Predicting Transferability across NLP Tasks,Footnote
998,11002," http://answers.yahoo.com/"," ['1 Introduction']","An-swers, [Cite_Footnote_1] Answers.com, etc.), which became very popular resources for question answering.",1 http://answers.yahoo.com/,"One particularly interesting source of unstruc-tured text data is CQA websites (e.g. Yahoo! An-swers, [Cite_Footnote_1] Answers.com, etc.), which became very popular resources for question answering. The in-formation expressed there can be very useful, for example, to answer future questions (Shtok et al., 2012), which makes it attractive for knowledge base population. Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb 3 about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents). There are certain limitations in applying existing re-lation extraction algorithms to CQA data, i.e., they typically consider sentences independently and ig-nore the discourse of QnA pair text. However, of-ten it is impossible to understand the answer without knowing the question. For example, in many cases users simply give the answer to the question with-out stating it in a narrative sentence (e.g. “What does ”xoxo” stand for? Hugs and kisses.“), in some other cases the answer contains a statement, but some im-portant information is omitted (e.g. “What’s the cap-ital city of Bolivia? Sucre is the legal capital, though the government sits in La Paz“).",補足資料,Website,False,Introduce（引用目的）,True,N15-2013_0_0,2015,Relation Extraction from Community Generated Question-Answer Pairs,Footnote
999,11003," http://www.answers.com"," ['1 Introduction']","An-swers, Answers.com, [Cite_Footnote_2] etc.), which became very popular resources for question answering.",2 http://www.answers.com,"One particularly interesting source of unstruc-tured text data is CQA websites (e.g. Yahoo! An-swers, Answers.com, [Cite_Footnote_2] etc.), which became very popular resources for question answering. The in-formation expressed there can be very useful, for example, to answer future questions (Shtok et al., 2012), which makes it attractive for knowledge base population. Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb 3 about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents). There are certain limitations in applying existing re-lation extraction algorithms to CQA data, i.e., they typically consider sentences independently and ig-nore the discourse of QnA pair text. However, of-ten it is impossible to understand the answer without knowing the question. For example, in many cases users simply give the answer to the question with-out stating it in a narrative sentence (e.g. “What does ”xoxo” stand for? Hugs and kisses.“), in some other cases the answer contains a statement, but some im-portant information is omitted (e.g. “What’s the cap-ital city of Bolivia? Sucre is the legal capital, though the government sits in La Paz“).",補足資料,Website,False,Introduce（引用目的）,True,N15-2013_1_0,2015,Relation Extraction from Community Generated Question-Answer Pairs,Footnote
1000,11004," http://www.lemurproject.org/clueweb12/"," ['1 Introduction']","Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb [Cite_Footnote_3] about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents).",3 http://www.lemurproject.org/clueweb12/,"One particularly interesting source of unstruc-tured text data is CQA websites (e.g. Yahoo! An-swers, 1 Answers.com, 2 etc.), which became very popular resources for question answering. The in-formation expressed there can be very useful, for example, to answer future questions (Shtok et al., 2012), which makes it attractive for knowledge base population. Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb [Cite_Footnote_3] about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents). There are certain limitations in applying existing re-lation extraction algorithms to CQA data, i.e., they typically consider sentences independently and ig-nore the discourse of QnA pair text. However, of-ten it is impossible to understand the answer without knowing the question. For example, in many cases users simply give the answer to the question with-out stating it in a narrative sentence (e.g. “What does ”xoxo” stand for? Hugs and kisses.“), in some other cases the answer contains a statement, but some im-portant information is omitted (e.g. “What’s the cap-ital city of Bolivia? Sucre is the legal capital, though the government sits in La Paz“).",Material,Dataset,True,Introduce（引用目的）,True,N15-2013_2_0,2015,Relation Extraction from Community Generated Question-Answer Pairs,Footnote
1001,11005," http://webscope.sandbox.yahoo.com/catalog.php?datatype=l"," ['4 Experiments', '4.1 Datasets']","Answers Comprehensive Ques-tions and Answers [Cite_Footnote_4] and a crawl of WikiAnswers (Fader et al., 2014).",4 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,"For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers [Cite_Footnote_4] and a crawl of WikiAnswers (Fader et al., 2014). The Yahoo! Answers dataset contains 4,483,032 questions (3,894,644 in English) with the corresponding answers collected on 10/25/2007. The crawl of WikiAnswers has 30,370,994 question clusters, tagged by WikiAn-swers users as paraphrases, and only 3,386,256 them have answers. From these clusters we used all possi-ble pairs of questions and answers (19,629,443 pairs in total).",Material,Dataset,True,Use（引用目的）,True,N15-2013_3_0,2015,Relation Extraction from Community Generated Question-Answer Pairs,Footnote
1002,11006," http://wiki.answers.com"," ['4 Experiments', '4.1 Datasets']","Answers Comprehensive Ques-tions and Answers and a crawl of WikiAnswers [Cite_Footnote_5] (Fader et al., 2014).",5 http://wiki.answers.com,"For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers and a crawl of WikiAnswers [Cite_Footnote_5] (Fader et al., 2014). The Yahoo! Answers dataset contains 4,483,032 questions (3,894,644 in English) with the corresponding answers collected on 10/25/2007. The crawl of WikiAnswers has 30,370,994 question clusters, tagged by WikiAn-swers users as paraphrases, and only 3,386,256 them have answers. From these clusters we used all possi-ble pairs of questions and answers (19,629,443 pairs in total).",Material,Dataset,True,Use（引用目的）,True,N15-2013_4_0,2015,Relation Extraction from Community Generated Question-Answer Pairs,Footnote
1003,11007," https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf"," ['3 Experiments', '3.1 Data Preprocessing']","For transliterating the Urdu and Konkani to a com-mon script, we used the Indic Trans library (Bhat et al., 2014), and for the others, we used Indic NLP library (Kunchukuttan, 2020) [Cite_Ref] (as Urdu and Konkani not supported).",Anoop Kunchukuttan. 2020. The IndicNLP Library. https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf.,"For transliterating the Urdu and Konkani to a com-mon script, we used the Indic Trans library (Bhat et al., 2014), and for the others, we used Indic NLP library (Kunchukuttan, 2020) [Cite_Ref] (as Urdu and Konkani not supported). In addition, there is an exception with Urdu because it follows a right to left writing system and all other Indian languages follow left to right writing order. Hence, in the pro-cessing step, we also changed the order of Urdu to maintain consistency among all languages, and do-ing this also made our string similarity algorithms work more efficiently.",Method,Code,False,Use（引用目的）,True,2021.acl-srw.12_0_0,2021,How do different factors Impact the Inter-language Similarity? A Case Study on Indian languages,Reference
1004,11008," https://www.alexa.com/siteinfo/reddit.com"," ['2 Model', '2.1 Motivating Dataset']","We evaluate on a corpus mined from Reddit, an internet forum which ranks as the fourth most traf-ficked site in the US (Alexa, 2018) [Cite_Ref] and sees mil-lions of daily comments (Reddit, 2015).","Alexa. 2018. Reddit.com traffic, demographics and competitors. https://www.alexa.com/siteinfo/reddit.com. Accessed: 2018-02- 22.","We evaluate on a corpus mined from Reddit, an internet forum which ranks as the fourth most traf-ficked site in the US (Alexa, 2018) [Cite_Ref] and sees mil-lions of daily comments (Reddit, 2015). Discourse on Reddit follows a branching pattern, shown in Figure 1. The largest unit of discourse is a thread, beginning with a link to external content or a natu-ral language prompt, posted to a relevant subreddit based on its subject matter. Users comment in re-sponse to the original post (OP), or to any other comment. The result is a structure which splits at many points into more specific or tangential discussions that while locally coherent may dif-fer substantially from each other. The data reflect features of the underlying memory and network structure of the generating process; comments are serially correlated and highly cross-referential. We treat individual comments as “documents” under the standard topic modeling paradigm, but use ob-served reply structure to induce a tree of documents for every thread.",補足資料,Document,True,Introduce（引用目的）,True,D18-1496_0_0,2018,Modeling Online Discourse with Coupled Distributed Topics,Reference
1005,11009," https://www.cs.jhu.edu/~jason/tutorials/variational.html"," ['3 Learning and Inference']","Follow-ing the approach described for undirected models by Eisner (2011) [Cite_Ref] , we approximate these quantities and their gradients with respect to the model pa-rameters θ as we will now describe (thread-level embeddings are omitted in this section for clarity).",Jason Eisner. 2011. High-level explanation of varia-tional inference. https://www.cs.jhu.edu/~jason/tutorials/variational.html. Accessed: 2018-02-22.,"Inference in this model class in intractable, so as has been done in previous work on topic modeling (Ng and Jordan, 2003) we rely on variational meth-ods to approximate the gradients needed during training as well as the posteriors over the topic bit vectors. Specifically, we will need the gradients of the normalizer and the sum of the energy function over the hidden variables which we refer to as the marginal energy. Follow-ing the approach described for undirected models by Eisner (2011) [Cite_Ref] , we approximate these quantities and their gradients with respect to the model pa-rameters θ as we will now describe (thread-level embeddings are omitted in this section for clarity).",補足資料,Paper,True,Introduce（引用目的）,True,D18-1496_1_0,2018,Modeling Online Discourse with Coupled Distributed Topics,Reference
1006,11010," https://github.com/jialuli-luka/SyntaxVLN"," ['References']",We also show that our agent is better at aligning instructions with the cur-rent visual information via qualitative visual-izations. [Cite_Footnote_1],1 Code and models: https://github.com/jialuli-luka/SyntaxVLN,"Vision language navigation is the task that re-quires an agent to navigate through a 3D en-vironment based on natural language instruc-tions. One key challenge in this task is to ground instructions with the current visual in-formation that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction re-quired for the next action. However, differ-ent words have different functions in a sen-tence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like de-pendencies and phrase structures can aid the agent to locate important parts of the instruc-tion. Hence, in this paper, we propose a naviga-tion agent that utilizes syntax information de-rived from a dependency tree to enhance align-ment between the instruction and the current visual scenes. Empirically, our agent outper-forms the baseline model that does not use syn-tax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the cur-rent visual information via qualitative visual-izations. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.82_0_0,2021,Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information,Footnote
1007,11011," https://github.com/airsplay/R2R-EnvDrop"," ['5 Results and Analysis', '5.1 Room-to-Room Dataset']","We compare our agent with the baseline agent (Tan et al., 2019) [Cite_Footnote_3] on the R2R test leaderboard.",3 The baseline is the re-implementation of their model without back translation based on code: https://github.com/airsplay/R2R-EnvDrop,"We compare our agent with the baseline agent (Tan et al., 2019) [Cite_Footnote_3] on the R2R test leaderboard. Our syntax-aware agent achieves 47.8% in success rate and 0.45 in SPL, improving the baseline model by 2.1% in success rate and 2% in SPL.",Material,Knowledge,True,Compare（引用目的）,True,2021.naacl-main.82_1_0,2021,Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information,Footnote
1008,11012," http://openke.thunlp.org/"," ['References']","The toolkit, documentation, and pre-trained embeddings are all released on [Cite] http://openke.thunlp.org/.",,"We release an open toolkit for knowledge em-bedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continu-ous low-dimensional space. OpenKE prior-itizes operational efficiency to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibil-ity to easily incorporate new models into the framework. Besides the toolkit, the embed-dings of some existing large-scale knowledge graphs pre-trained by OpenKE are also avail-able, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on [Cite] http://openke.thunlp.org/.",Mixed,Mixed,True,Produce（引用目的）,True,D18-2024_0_0,2018,OpenKE: An Open Toolkit for Knowledge Embedding,Body
1009,11013," http://openke.thunlp.org/"," ['1 Introduction']",Other related resources and details can be found on [Cite] http://openke.,,"Besides the toolkit, we also provide the pre-trained embeddings of several well-known large-scale KGs, which can be used directly for other relevant works without repeatedly spending much time for embedding KGs. In this paper, we mainly present the architecture design and implementa-tion of OpenKE, as well as the benchmark eval-uation results of some typical KE models im-plemented with OpenKE. Other related resources and details can be found on [Cite] http://openke.thunlp.org/.",Mixed,Mixed,True,Produce（引用目的）,True,D18-2024_1_0,2018,OpenKE: An Open Toolkit for Knowledge Embedding,Body
1010,11014," http://github.com/thunlp/OpenKE"," ['4 Implementations']",OpenKE has been available to the public on GitHub [Cite_Footnote_1] and is open-source under the MIT license.,1 http://github.com/thunlp/OpenKE,"In this section, we mainly present the implemen-tations of acceleration modules and special sam-pling algorithm in OpenKE. OpenKE has been available to the public on GitHub [Cite_Footnote_1] and is open-source under the MIT license.",Method,Tool,False,Produce（引用目的）,True,D18-2024_2_0,2018,OpenKE: An Open Toolkit for Knowledge Embedding,Footnote
1011,11015," https://github.com/thunlp/Fast-TransX"," ['4 Implementations', '4.2 Parallel Learning']","Hence, we enable OpenKE to adapt models for parallel learning on CPU [Cite_Footnote_2] be-sides employing GPU learning, which allow users to make full use of all available computing re-sources.",2 https://github.com/thunlp/Fast-TransX,"Abundant computing resources (e.g Servers with multiple GPUs) do not exist all the time. In fact, we often rely on simple personal computers for model validation. Hence, we enable OpenKE to adapt models for parallel learning on CPU [Cite_Footnote_2] be-sides employing GPU learning, which allow users to make full use of all available computing re-sources. The parallel learning method is shown in Algorithm 1. The main idea of parallel learning method is based on data parallelism mechanism, which divides training triples into several parts and trains each part of triples with a corresponding thread. In parallel learning, there are two strate-gies implemented to update gradients. One of the methods is the lock-free strategy, which means all threads share the unified embedding space and update embeddings directly without synchronized operations. We also implement a central syn-chronized method, where each thread calculates its own gradient and results will be updated after summing up the gradients from all threads.",Method,Tool,False,Produce（引用目的）,True,D18-2024_3_0,2018,OpenKE: An Open Toolkit for Knowledge Embedding,Footnote
1012,11016," https://everest.hds.utc.fr/doku.php?id=en:transe"," ['5 Evaluations']",[Cite_Footnote_3] .,3 https://everest.hds.utc.fr/doku.php? id=en:transe,"Some datasets are usually used as benchmarks for link prediction, such as FB15K and WN18. FB15K is the relatively dense subgraph of Free-base; WN18 is the subset of WordNet. These pub-lic datasets are available online [Cite_Footnote_3] . Following pre-vious works, We adopt them in our experiments. The statistics of FB15K and WN18 are listed in Table 2, including the number of entities, rela-tions, and facts.",Material,Dataset,True,Introduce（引用目的）,True,D18-2024_4_0,2018,OpenKE: An Open Toolkit for Knowledge Embedding,Footnote
1013,11017," https://github.com/thunlp/KB2E"," ['5 Evaluations']","To demonstrate the efficiency of OpenKE, we select TransE as a rep-resentative and implement it with both OpenKE and KB2E [Cite_Footnote_4] , and then compare their training time.",4 https://github.com/thunlp/KB2E,"As mentioned above, OpenKE supports mod-els with efficient learning on both CPU and GPU. For CPU, the benchmarks are run on an Intel(R) Core(TM) i7-6700K @ 3.70GHz, with 4 cores and 8 threads. For GPU, the models in both TensorFlow and PyTorch versions are trained by GeForce GTX 1070 (Pascal), with CUDA v.8.0 (driver 384.111) and cuDNN v.6.5. To compare with the previous works, we simply follow the pa-rameter settings used before and traverse all train-ing triples for 1000 rounds. Other detailed pa-rameters and training strategies are shown in our source code. We show these results in Table 3 and Table 4. In these tables, the difference between our implementations and the paper reported results are listed in the parentheses. To demonstrate the efficiency of OpenKE, we select TransE as a rep-resentative and implement it with both OpenKE and KB2E [Cite_Footnote_4] , and then compare their training time. KB2E is a widely-used toolkit for KE models on GitHub. These results can be found in Table 5.",Method,Tool,True,Compare（引用目的）,True,D18-2024_5_0,2018,OpenKE: An Open Toolkit for Knowledge Embedding,Footnote
1014,11018," http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012"," ['References']",Our re-sults demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing. [Cite_Footnote_1],1 The source code and data for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012,"We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hy-pothesis is that a valid mapping yields POS annotations with coherent linguistic proper-ties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteris-tics of the derived mapping. Given the ex-ponential size of the mapping space, we pro-pose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our re-sults demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D12-1125_0_0,2012,Learning to Map into a Universal POS Tagset,Footnote
1015,11019," http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012"," ['5 Parameter Estimation', '5.1 Optimizing the Mapping A']",A detailed derivation is provided in the supplementary file. [Cite_Footnote_8],8 The supplementary file is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012.,"While we do not attempt to solve the problem globally, we do have a simple update scheme that monotonically decreases the objective. The update can be derived in a similar manner to expectation maximization (EM) (Neal and Hinton, 1999) and convex concave procedures (Yuille and Rangarajan, 2003). Figure 2 describes our optimization algo-rithm. The key ideas in deriving it are using pos-terior distributions as in EM, and using a variational formulation of entropy. The term F c (A) is handled in a similar way to the posterior regularization algo-rithm derivation. A detailed derivation is provided in the supplementary file. [Cite_Footnote_8]",補足資料,Document,True,Produce（引用目的）,True,D12-1125_1_0,2012,Learning to Map into a Universal POS Tagset,Footnote
1016,11020," http://cs.stanford.edu/∼pliang/software/"," ['7 Results']","In our experiments, we in-duce such clusters using Brown clustering, [Cite_Footnote_13] which has been successfully used for similar purposes in parsing research (Koo et al., 2008).","13 In our experiments, we employ Liang’s implementation http://cs.stanford.edu/∼pliang/software/. The number of clus-ters is set to 30.","Application to Automatically Induced POS Tags A potential benefit of the proposed method is to re-late automatically induced clusters in the target lan-guage to universal tags. In our experiments, we in-duce such clusters using Brown clustering, [Cite_Footnote_13] which has been successfully used for similar purposes in parsing research (Koo et al., 2008). We then map these clusters to the universal tags using our algo-rithm.",Method,Tool,True,Use（引用目的）,True,D12-1125_2_0,2012,Learning to Map into a Universal POS Tagset,Footnote
1017,11021," http://alchemy.cs.washington.edu/"," ['3 Markov Logic']","The open-source Alchemy package (Kok et al., 2007) [Cite_Ref] provides implementations of existing algo-rithms for Markov logic.","Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.","The open-source Alchemy package (Kok et al., 2007) [Cite_Ref] provides implementations of existing algo-rithms for Markov logic. In Section 5, we develop the first general-purpose unsupervised learning al-gorithm for Markov logic by extending the existing algorithms to handle hidden predicates.",Method,Tool,False,Extend（引用目的）,False,D08-1068_0_0,2008,Joint Unsupervised Coreference Resolution with Markov Logic,Reference
1018,11022," http://alchemy.cs.washington.edu/"," ['5 Learning and Inference']","In the existing implementation in Alchemy (Kok et al., 2007) [Cite_Ref] , SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m,c!)).","Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.","To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al., 2007) [Cite_Ref] , SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m,c!)). Such predicates can be viewed as multi-valued predi-cates (e.g., Head(m) with value ranging over all c’s) and are prevalent in NLP applications. We adapted SampleSAT to flip two or more atoms in each step so that the unique-value constraints are automatically satisfied. By default, MC-SAT treats each ground clause as a separate factor while de-termining the slice. This can be very inefficient for highly correlated clauses. For example, given a non-pronoun mention m currently in cluster c and with head t, among the mixture prior rules involv-ing m InClust(m, c) is the only one that is satisfied, and among those head-prediction rules involving m, ¬IsPrn(m)∧InClust(m, c)∧Head(m, t) is the only one that is satisfied; the factors for these rules mul-tiply to φ = exp(w m,c + w m,c,t ), where w m,c is the weight for InClust(m, c), and w m,c,t is the weight for ¬IsPrn(m) ∧ InClust(m, c) ∧ Head(m, t), since an unsatisfied rule contributes a factor of e 0 = 1. We extended MC-SAT to treat each set of mutually ex-clusive and exhaustive rules as a single factor. E.g., for the above m, MC-SAT now samples u uniformly from (0, φ), and requires that in the next state φ 0 be no less than u. Equivalently, the new cluster and head for m should satisfy w m,c 0 + w m,c 0 ,t 0 ≥ log(u). We extended SampleSAT so that when it consid-ers flipping any variable involved in such constraints (e.g., c or t above), it ensures that their new values still satisfy these constraints.",補足資料,Paper,False,Introduce（引用目的）,False,D08-1068_0_1,2008,Joint Unsupervised Coreference Resolution with Markov Logic,Reference
1019,11023," http://alchemy.cs.washington.edu/"," ['6 Experiments', '6.1 System']","We implemented our method as an extension to the Alchemy system (Kok et al., 2007) [Cite_Ref] .","Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.","We implemented our method as an extension to the Alchemy system (Kok et al., 2007) [Cite_Ref] . Since our learn-ing uses sampling, all results are the average of five runs using different random seeds. Our optimiza-tion problem is not convex, so initialization is im-portant. The core of our model (head mixture) tends to cluster non-pronouns with the same head. There-fore, we initialized by setting all weights to zero, and running the same learning algorithm on the base MLN, while assuming that in the ground truth, non-pronouns are clustered by their heads. (Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.) We used 30 iterations of PSCG for learning. (In preliminary experiments, additional iterations had little effect on coreference accuracy.) We generated 100 samples using MC-SAT for each expectation approximation.",補足資料,Paper,True,Introduce（引用目的）,True,D08-1068_0_2,2008,Joint Unsupervised Coreference Resolution with Markov Logic,Reference
1020,11024," http://alchemy.cs.washington.edu/"," ['3 Markov Logic']","Kok & Domingos (2007) [Cite_Ref] applied Markov logic to relational clustering, but they used hard EM.","Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.","2 Alchemy includes a discriminative EM algorithm, but it as-sumes that only a few values are missing, and cannot handle completely hidden predicates. Kok & Domingos (2007) [Cite_Ref] applied Markov logic to relational clustering, but they used hard EM.",補足資料,Paper,True,Introduce（引用目的）,True,D08-1068_0_3,2008,Joint Unsupervised Coreference Resolution with Markov Logic,Reference
1021,11025," https://github.com/heartexlabs/label-studio"," ['6 Results and Discussion', '6.3.2 Automated Conversion']","To assess this nuance, we manually annotate a subset of the MCRC datasets using Label Studio (Tkachenko et al., 2020) [Cite_Ref] , with a random set of examples annotated by each of the authors.","Maxim Tkachenko, Mikhail Malyuk, Nikita Shevchenko, Andrey Holmanyuk, and Nikolai Liubimov. 2020. Label Studio: Data labeling software. Open source software available from https://github.com/heartexlabs/label-studio.","SizeOf(Vocab j ) mechanism itself becomes a confounding factor, enabling the RACE converted model to perform bet-ter on the MCRC task. To assess this nuance, we manually annotate a subset of the MCRC datasets using Label Studio (Tkachenko et al., 2020) [Cite_Ref] , with a random set of examples annotated by each of the authors. To create a setting where the differ-ence is vivid, we design the annotation subsets such that the RACE converted model gives an accuracy of around 50% using the hybrid conversion strategy. The independent manual annotations prevent any exploitable signal from leaking into the training data of the model through the conversion mech-anism. We compare the performance of models trained on converted forms of the RACE dataset using both our hybrid strategy as well as manual annotation. †† We manually annotate 100 examples from MultiRC and 50 each from ComsosQA and DREAM. MultiRC is evaluated at an option-level with each question-answer pair considered an in-dividual example. On the other hand, CosmosQA and DREAM are evaluated at a question-level, with each example consisting of three question-answer pairs, and one label corresponding to the correct answer option.",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.104_1_0,2021,Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization,Reference
1022,11026," https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html"," ['C Reproducibility Checklist', 'C.2 Neural Conversion']",we calculated by tuning for best balanced accuaracy [Cite] https://scikit-learn.,,• The threshold for CFCS as classifica-tion experiments (Section 6.2 (1)) we calculated by tuning for best balanced accuaracy [Cite] https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html.,Material,Knowledge,False,Produce（引用目的）,True,2021.naacl-main.104_4_0,2021,Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization,Body
1023,11027," https://github.com/Mayer123/CS_Model_Adaptation"," ['3 Experimental Setup', '3.2 Strategies']","We describe how we adapt pre-trained GPT-2 and BART models to a target task with three methods [Cite_Footnote_1] : (S1) Fine-tuning is the classic model adaptation ap-proach, where all its parameters are updated using the training signal from the ground truth.","1 Our code is available at https://github.com/Mayer123/CS_Model_Adaptation wards human-like linguistic generalization? In Pro-ceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5210–5217, On-line. Association for Computational Linguistics.","We describe how we adapt pre-trained GPT-2 and BART models to a target task with three methods [Cite_Footnote_1] : (S1) Fine-tuning is the classic model adaptation ap-proach, where all its parameters are updated using the training signal from the ground truth. (S2) Prefix-tuning (Li and Liang, 2021) is a method which fixes the pre-trained model’s parameters dur-ing adaptation. This method adds trainable param-eters, called prefix states, to the self-attention com-ponent (Vaswani et al., 2017) of every transformer layer in the model; only these prefix states are up-dated during training. Essentially, the prefix states act as conditioning variables that contextualize the representation of the inputs, such that the model can generate the desired outputs.",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.445_0_0,2021,Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models,Footnote
1024,11028," http://www.itl.nist.gov/iad/mig/tests/mt/2006/"," ['1 Introduction']","We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-lation (OpenMT) 2006 evaluation [Cite_Footnote_2] .",2 http://www.itl.nist.gov/iad/mig/tests/mt/2006/,"However, the EM algorithm for WA is well-known for introducing “garbage collector ef-fects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erro-neously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Graça et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE Chinese-English Word Alignment and Tagging Training corpus (GALE WA corpus) with it’s human-annotated word alignment. The Chinese word “HE ZHANG,” denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-lation (OpenMT) 2006 evaluation [Cite_Footnote_2] . The resulting alignment is shown in Figure 1(b). It can be seen that w r is erroneously aligned to multiple English words.",補足資料,Website,True,Introduce（引用目的）,True,D15-1209_0_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Footnote
1025,11029," http://www.phontron.com/kftt/"," ['4 Experiments', '4.1 Experimental Settings']","The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [Cite_Footnote_6] .",6 http://www.phontron.com/kftt/,"The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT cor-pus. They are from the same domain, both con-tain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [Cite_Footnote_6] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.",補足資料,Website,True,Introduce（引用目的）,True,D15-1209_1_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Footnote
1026,11030," http://nlp.stanford.edu/software/segmenter.shtml"," ['4 Experiments', '4.1 Experimental Settings']","The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) [Cite_Footnote_7] ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ).",7 http://nlp.stanford.edu/software/segmenter.shtml,"The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) [Cite_Footnote_7] ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.",Method,Tool,True,Use（引用目的）,True,D15-1209_2_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Footnote
1027,11031," http://www.phontron.com/kytea/"," ['4 Experiments', '4.1 Experimental Settings']","The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) 7 ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea [Cite_Footnote_8] ).",8 http://www.phontron.com/kytea/,"The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) 7 ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea [Cite_Footnote_8] ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.",Method,Tool,True,Use（引用目的）,True,D15-1209_3_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Footnote
1028,11032," http://www2.nict.go.jp/univ-com/multitrans/cicada/"," ['4 Experiments', '4.1 Experimental Settings']","Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watan-abe, 2012; Tamura et al., 2013) [Cite_Footnote_9] .",9 http://www2.nict.go.jp/univ-com/multitrans/cicada/,"The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watan-abe, 2012; Tamura et al., 2013) [Cite_Footnote_9] . We named the implemented aligner AGRIPPA, to support our in-house decoders OCTAVIAN and AUGUSTUS.",Method,Tool,True,Extend（引用目的）,True,D15-1209_4_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Footnote
1029,11033," http://www.isi.edu/~avaswani/giza-pp-l0.html"," ['4 Experiments', '4.5 Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods']","The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) [Cite_Footnote_11] and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) .",11 http://www.isi.edu/˜avaswani/giza-pp-l0.html,"The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) [Cite_Footnote_11] and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem. This enables the probability distributions on rare words to be estimated more effectively. In this way, these two GIZA++ variants are related to the proposed method. smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES. For the settings of l 0 -normalized GIZA++ that are not in common with GIZA++ were the default settings. As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1 – 4 and HMM model were turned on.",Method,Code,True,Compare（引用目的）,True,D15-1209_5_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Footnote
1030,11034," https://github.com/hznlp/giza-kn"," ['4 Experiments', '4.5 Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods']","The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) [Cite_Footnote_12] .",12 https://github.com/hznlp/giza-kn,"The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) [Cite_Footnote_12] . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem. This enables the probability distributions on rare words to be estimated more effectively. In this way, these two GIZA++ variants are related to the proposed method. smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES. For the settings of l 0 -normalized GIZA++ that are not in common with GIZA++ were the default settings. As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1 – 4 and HMM model were turned on.",Method,Code,True,Compare（引用目的）,True,D15-1209_6_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Footnote
1031,11035," http://www.phontron.com/kftt"," ['2 Related Work']","Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 2011 [Cite_Ref] ), also attempt to address the issue of over-fitting, however EM algorithms related to the pro-posed method have been shown to be more effi-cient (Wang et al., 2014).",Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.,"Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 2011 [Cite_Ref] ), also attempt to address the issue of over-fitting, however EM algorithms related to the pro-posed method have been shown to be more effi-cient (Wang et al., 2014).",補足資料,Website,False,Introduce（引用目的）,True,D15-1209_7_0,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Reference
1032,11036," http://www.phontron.com/kftt"," ['4 Experiments', '4.1 Experimental Settings']","The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [Cite_Ref] .",Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.,"The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT cor-pus. They are from the same domain, both con-tain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [Cite_Ref] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.",補足資料,Website,False,Introduce（引用目的）,True,D15-1209_7_1,2015,Leave-one-out Word Alignment without Garbage Collector Effects,Reference
1033,11037," https://aka.ms/KaggleDBQA"," ['1 Introduction']","KaggleDBQA We introduce KaggleDBQA, a new dataset and evaluation setting for text-to-SQL parsers to bridge the gap between SOTA DBQA research and its real-life deployment. [Cite_Footnote_1]",1 Available at https://aka.ms/KaggleDBQA.,"KaggleDBQA We introduce KaggleDBQA, a new dataset and evaluation setting for text-to-SQL parsers to bridge the gap between SOTA DBQA research and its real-life deployment. [Cite_Footnote_1] It systemati-cally addresses three aforementioned challenges:",Mixed,Mixed,True,Produce（引用目的）,False,2021.acl-long.176_1_0,2021,KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers,Footnote
1034,11038," https://github.com/ryanzhumich/editsql"," ['4 Experiments', '4.1 Baseline Results']","EditSQL (Zhang et al., 2019): EditSQL (with BERT) is the highest-performing model on the Spi-der dataset that also provides an open-source im-plementation along with a downloadable trained model. [Cite_Footnote_3]",3 https://github.com/ryanzhumich/ editsql,"We first evaluate KaggleDBQA using models that were developed for the Spider dataset. EditSQL (Zhang et al., 2019): EditSQL (with BERT) is the highest-performing model on the Spi-der dataset that also provides an open-source im-plementation along with a downloadable trained model. [Cite_Footnote_3] The model was built for edit-based multi-turn parsing tasks, but can also be used as a single-turn parser for Spider or KaggleDBQA. It employs a sequence-to-sequence model with a question-table co-attention encoder for schema encoding.",Material,Knowledge,True,Produce（引用目的）,False,2021.acl-long.176_3_0,2021,KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers,Footnote
1035,11039," https://aka.ms/KaggleDBQA"," ['5 Conclusion & Future Work']",The KaggleDBQA dataset is available at [Cite] https://aka.ms/KaggleDBQA.,,"KaggleDBQA provides two resources to facili-tate real-world applications of text-to-SQL pars-ing. First, it encourages an evaluation regime that bridges the gap between academic and indus-trial settings, leveraging in-domain knowledge and more realistic database distribution. We encour-age adopting this regime for established text-to- SQL benchmarks. Second, it is a new dataset of more realistic databases and questions, present-ing a challenge to state-of-the-art parsers. De-spite the addition of domain knowledge in the form of database documentation, our baselines reach only 26.77% accuracy, struggling to generalize to harder questions. We hope that better use of docu-mentation and new modeling and domain adapta-tion techniques will help further advance state of the art. The KaggleDBQA dataset is available at [Cite] https://aka.ms/KaggleDBQA.",Material,Dataset,True,Produce（引用目的）,True,2021.acl-long.176_5_0,2021,KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers,Body
1036,11040," https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip"," ['A.3 Implementation Details']",For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [Cite_Footnote_6],"6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip","For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [Cite_Footnote_6] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 ± 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting. We keep the learning rate of non-BERT parameters the same at 7.44e-4. We also increase the dropout rate of the transformers from 0.1 to 0.3 to provide further regularization.",Material,Knowledge,True,Use（引用目的）,True,2021.acl-long.176_6_0,2021,KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers,Footnote
1037,11041," http://nlp.stanford.edu/data/glove.twitter.27B.zip"," ['3 Methods', '3.2 Model']","Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter [Cite_Footnote_5] for each token of ex-tracted discourse arguments from messages.",5 http://nlp.stanford.edu/data/glove.twitter.27B.zip,"Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter [Cite_Footnote_5] for each token of ex-tracted discourse arguments from messages. For the distributional representation of discourse ar-guments, we run a Word-level LSTM on the words’ embeddings within each discourse argu-ment and concatenate−→last hidden state vectors of←−forward LSTM (h) and backward LSTM ( h ) which−→is←−suggested by (Ji and Smith, 2017) (DA = [ h ; h ]). Then, we feed the sequence of the vector representation of discourse arguments to the Discourse-argument-level LSTM (DA-level LSTM) to make a final prediction with log soft-max function. With this structure, the model can learn the representation of interaction of tokens inside each discourse argument, then capture dis-course relations across all of the discourse argu-ments in each message (Figure 2). In order to prevent the overfitting, we added a dropout layer between the Word-level LSTM and the DA-level LSTM layer.",Material,Knowledge,True,Use（引用目的）,True,D18-1372_0_0,2018,Causal Explanation Analysis on Social Media,Footnote
1038,11042," http://www.sle.sharp.co.uk/senseval2/"," ['2 Objective Functions: Naive-Bayes']",We took as data the col-lection of S ENSEVAL -2 English lexical sample WSD corpora. [Cite_Footnote_2],2 http://www.sle.sharp.co.uk/senseval2/,"We performed the following WSD experiments with Naive-Bayes models. We took as data the col-lection of S ENSEVAL -2 English lexical sample WSD corpora. [Cite_Footnote_2] We set the NB model parameters in several ways. We optimized JL (using the RFE s). We also optimized SCL and (the log of) CL , using a conju-gate gradient ( CG ) method (Press et al., 1988). For CL and SCL , we optimized each objective both over the space of all distributions and over the subspace of non-deficient models (giving CL ∗ and SCL ∗ ). Acc was not directly optimized.",Material,Knowledge,True,Use（引用目的）,True,W02-1002_0_0,2002,Proceedings of the,Footnote
1039,11043," http://github.com/MiuLab/CLUSE"," ['References']",The proposed approach shows the su-perior quality of sense embeddings evaluated in both monolingual and bilingual spaces. [Cite_Footnote_1],1 The code and dataset are available at http://github.com/MiuLab/CLUSE.,"This paper proposes a modularized sense in-duction and representation learning model that jointly learns bilingual sense embeddings that align well in the vector space, where the cross-lingual signal in the English-Chinese parallel corpus is exploited to capture the collocation and distributed characteristics in the language pair. The model is evaluated on the Stanford Contextual Word Similarity (SCWS) dataset to ensure the quality of monolingual sense em-beddings. In addition, we introduce Bilingual Contextual Word Similarity (BCWS), a large and high-quality dataset for evaluating cross-lingual sense embeddings, which is the first attempt of measuring whether the learned em-beddings are indeed aligned well in the vector space. The proposed approach shows the su-perior quality of sense embeddings evaluated in both monolingual and bilingual spaces. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D18-1025_0_0,2018,CLUSE: Cross-Lingual Unsupervised Sense Embeddings,Footnote
1040,11044," https://github.com/rekriz11/sockeye-recipes"," ['References']",We report standard automatic and human evalua-tion metrics. [Cite_Footnote_1],"1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes.","Sentence simplification is the task of rewriting texts so they are easier to understand. Recent research has applied sequence-to-sequence (Seq2Seq) models to this task, focusing largely on training-time improvements via reinforce-ment learning and memory augmentation. One of the main problems with applying generic Seq2Seq models for simplification is that these models tend to copy directly from the origi-nal sentence, resulting in outputs that are rel-atively long and complex. We aim to alle-viate this issue through the use of two main techniques. First, we incorporate content word complexities, as predicted with a leveled word complexity model, into our loss function dur-ing training. Second, we generate a large set of diverse candidate simplifications at test time, and rerank these to promote fluency, adequacy, and simplicity. Here, we measure simplicity through a novel sentence complexity model. These extensions allow our models to per-form competitively with state-of-the-art sys-tems while generating simpler sentences. We report standard automatic and human evalua-tion metrics. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,N19-1317_0_0,2019,Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification,Footnote
1041,11045," https://newsela.com/data/"," ['3 Seq2Seq Approach', '3.1 Complexity-Weighted Loss Function', '3.1.1 Word Complexity Prediction']","3 We ex-tract word counts in each of the five levels; in this dataset, we denote 4 as the original complex doc-ument, [Cite_Footnote_3] as the least simplified re-write, and 0 as the most simplified re-write.",3 Newsela is an education company that provides reading materials for students in elementary through high school. The Newsela corpus can be requested at https://newsela.com/data/,"Extending the complex word identification model of Kriz et al. (2018), we train a linear regression model using length, number of syllables, and word frequency; we also include Word2Vec embeddings (Mikolov et al., 2013). To collect data for this task, we consider the Newsela corpus, a collection of 1,840 news articles written by professional edi-tors at 5 reading levels (Xu et al., 2015). 3 We ex-tract word counts in each of the five levels; in this dataset, we denote 4 as the original complex doc-ument, [Cite_Footnote_3] as the least simplified re-write, and 0 as the most simplified re-write. We propose using Al-gorithm 1 to obtain the complexity label for each word w, where l w represents the level given to the word, and c w i represents the number of times that word occurs in level i.",Material,Dataset,True,Introduce（引用目的）,True,N19-1317_1_0,2019,Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification,Footnote
1042,11046," http://www.cs.cmu.edu/"," ['1 Introduction']",Adaptation experiments (§5) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [Cite_Footnote_1],1 Data/code publicly available at http://www.cs.cmu.edu/ ∼ pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.,"We construct a new dataset of Speaker Anno-tated TED talks (SATED, §4) to validate our ap-proach. Adaptation experiments (§5) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,False,P18-2050_0_0,2018,Extreme Adaptation for Personalized Neural Machine Translation,Footnote
1043,11047," https://github.com/neulab/and"," ['1 Introduction']",Adaptation experiments (§5) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [Cite_Footnote_1],1 Data/code publicly available at http://www.cs.cmu.edu/ ∼ pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.,"We construct a new dataset of Speaker Anno-tated TED talks (SATED, §4) to validate our ap-proach. Adaptation experiments (§5) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,False,P18-2050_1_0,2018,Extreme Adaptation for Personalized Neural Machine Translation,Footnote
1044,11048," https://www.ted.com/talks"," ['4 Speaker Annotated TED Talks Dataset']","The dataset consists of transcripts directly col-lected from [Cite] https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks.",,"The dataset consists of transcripts directly col-lected from [Cite] https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks. We pre-process the data by removing sentences that don’t have any translation or are longer than 60 words, lower-casing, and tokenizing (using the Moses tokenizer (Koehn et al., 2007)).",補足資料,Media,True,Extend（引用目的）,True,P18-2050_2_0,2018,Extreme Adaptation for Personalized Neural Machine Translation,Body
1045,11049," https://www.ted.com"," ['4 Speaker Annotated TED Talks Dataset']","In order to evaluate the effectiveness of our proposed methods, we construct a new dataset, Speaker Annotated TED (SATED) based on TED talks, [Cite_Footnote_4] with three language pairs, English-French (en-fr), English-German (en-de) and English-Spanish (en-es) and speaker annotation.",4 https://www.ted.com,"In order to evaluate the effectiveness of our proposed methods, we construct a new dataset, Speaker Annotated TED (SATED) based on TED talks, [Cite_Footnote_4] with three language pairs, English-French (en-fr), English-German (en-de) and English-Spanish (en-es) and speaker annotation.",補足資料,Website,True,Use（引用目的）,True,P18-2050_3_0,2018,Extreme Adaptation for Personalized Neural Machine Translation,Footnote
1046,11050," https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode"," ['4 Speaker Annotated TED Talks Dataset']","This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, [Cite] https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks.",,"This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, [Cite] https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks. The data it-self can be found at http://www.cs.cmu.edu/ ∼ pmichel1/sated/.",補足資料,Document,True,Introduce（引用目的）,True,P18-2050_4_0,2018,Extreme Adaptation for Personalized Neural Machine Translation,Body
1047,11051," http://www.cs.cmu.edu/"," ['4 Speaker Annotated TED Talks Dataset']",The data it-self can be found at [Cite] http://www.cs.cmu.edu/ ∼ pmichel1/sated/.,,"This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks. The data it-self can be found at [Cite] http://www.cs.cmu.edu/ ∼ pmichel1/sated/.",Material,Dataset,True,Produce（引用目的）,False,P18-2050_5_0,2018,Extreme Adaptation for Personalized Neural Machine Translation,Body
1048,11052," https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1"," ['5 Experiments', '5.3 Further experiments on the Europarl corpus']","Specifically, we train our models on a speaker annotated version of the Europarl corpus (Rabinovich et al., 2017), on the en-de language pair [Cite_Footnote_6] .",6 available here: https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1,"One of the quirks of the TED talks is that the speaker annotation correlates with the topic of their talk to a high degree. Although the topics that a speaker talks about can be considered as a man-ifestation of speaker traits, we also perform a con-trol experiment on a different dataset to verify that our model is indeed learning more than just topical information. Specifically, we train our models on a speaker annotated version of the Europarl corpus (Rabinovich et al., 2017), on the en-de language pair [Cite_Footnote_6] .",Material,Knowledge,True,Use（引用目的）,True,P18-2050_6_0,2018,Extreme Adaptation for Personalized Neural Machine Translation,Footnote
1049,11053," http://github.com/noa/naacl2021"," ['1 Introduction']","Our code, data splits, and scripts to repro-duce our experiments are available at [Cite] http://github.com/noa/naacl2021.",,"Our code, data splits, and scripts to repro-duce our experiments are available at [Cite] http://github.com/noa/naacl2021.",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.415_0_0,2021,A Deep Metric Learning Approach to Account Linking,Body
1050,11054," http://www.ee.columbia.edu/dvmm/newDownloads.htm"," ['1 Introduction']","With the crawler, we created a bench-mark dataset which is fully annotated with cross-document coreferential events [Cite_Footnote_1] .",1 Dataset can be found at http://www.ee.columbia.edu/dvmm/newDownloads.htm,"In this paper, we propose to jointly incorporate features from both speech (textual) and video (vi-sual) channels for the first time. We also build a newscast crawling system that can automatically accumulate video records and transcribe closed captions. With the crawler, we created a bench-mark dataset which is fully annotated with cross-document coreferential events [Cite_Footnote_1] .",Material,Dataset,True,Produce（引用目的）,True,D15-1020_0_0,2015,Cross-document Event Coreference Resolution based on Cross-media Features,Footnote
1051,11055," http://www.itl.nist.gov/iad/mig/tests/ace/ace05/doc/ace05-evaplan.v3.pdf"," ['2 Approach', '2.1 Event Extraction']","We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005) [Cite_Ref] :",NIST. 2005. The ace 2005 evaluation plan. http: //www.itl.nist.gov/iad/mig/tests/ ace/ace05/doc/ace05-evaplan.v3.pdf.,"Given unstructured transcribed CC, we extract en-tities and events and present them in structured forms. We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005) [Cite_Ref] :",補足資料,Website,True,Introduce（引用目的）,True,D15-1020_1_0,2015,Cross-document Event Coreference Resolution based on Cross-media Features,Reference
1052,11056," https://github.com/freesunshine0316/neural-graph-to-seq-mp"," ['1 Introduction']",We release our code and models at [Cite] https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.,,We release our code and models at [Cite] https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.,Mixed,Mixed,True,Produce（引用目的）,True,P18-1150_0_0,2018,A Graph-to-Sequence Model for AMR-to-Text Generation,Body
1053,11057," http://www.statmt.org/moses/"," ['6 Related work']","Pourdamghani et al. (2016) linearize input graphs by breadth-first traversal, and then use a phrase-based machine translation system [Cite_Footnote_2] to gen-erate results by translating linearized sequences.",2 http://www.statmt.org/moses/,"Among early statistical methods for AMR-to-text generation, Flanigan et al. (2016b) convert input graphs to trees by splitting re-entrances, and then translate the trees into sentences with a tree-to-string transducer. Song et al. (2017) use a syn-chronous node replacement grammar to parse in-put AMRs and generate sentences at the same time. Pourdamghani et al. (2016) linearize input graphs by breadth-first traversal, and then use a phrase-based machine translation system [Cite_Footnote_2] to gen-erate results by translating linearized sequences.",Method,Tool,True,Introduce（引用目的）,True,P18-1150_1_0,2018,A Graph-to-Sequence Model for AMR-to-Text Generation,Footnote
1054,11058," http://paraphrase.org"," ['1 Introduction']","In this work, we release version 1.0 of the Para-Phrase DataBase PPDB, [Cite_Footnote_1] a collection of ranked En-glish and Spanish paraphrases derived by:",1 Freely available at http://paraphrase.org.,"In this work, we release version 1.0 of the Para-Phrase DataBase PPDB, [Cite_Footnote_1] a collection of ranked En-glish and Spanish paraphrases derived by:",Material,Dataset,True,Produce（引用目的）,True,N13-1092_0_0,2013,PPDB: The Paraphrase Database,Footnote
1055,11059," http://projects.ldc.upenn.edu/gale/data/Catalog.html"," ['4 English Paraphrases – PPDB:Eng']","Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [Cite_Footnote_2] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.",2 http://projects.ldc.upenn.edu/gale/data/Catalog.html,"We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [Cite_Footnote_2] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.",Material,Dataset,True,Introduce（引用目的）,False,N13-1092_1_0,2013,PPDB: The Paraphrase Database,Footnote
1056,11060," http://www.translationautomation.com/"," ['4 English Paraphrases – PPDB:Eng']","Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS. [Cite_Footnote_4]",4 http://www.translationautomation.com/,"We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS. [Cite_Footnote_4]",補足資料,Website,True,Introduce（引用目的）,True,N13-1092_2_0,2013,PPDB: The Paraphrase Database,Footnote
1057,11061," https://github.com/snipsco/nlu-benchmark/"," ['3 Experiment Setup']","Datasets For each task, we evaluate our proposed models by applying it on two real-word datasets: SNIPS Natural Language Understanding bench-mark [Cite_Footnote_1] (SNIPS-NLU) and the Airline Travel Infor-mation Systems (ATIS) dataset (Tur et al., 2010).",1 https://github.com/snipsco/nlu-benchmark/,"Datasets For each task, we evaluate our proposed models by applying it on two real-word datasets: SNIPS Natural Language Understanding bench-mark [Cite_Footnote_1] (SNIPS-NLU) and the Airline Travel Infor-mation Systems (ATIS) dataset (Tur et al., 2010). The statistical information on two datasets are shown in Table 1.",Material,Dataset,True,Use（引用目的）,True,P19-1519_0_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1058,11062," https://dialogflow.com/"," ['3 Experiment Setup']","[Cite_Footnote_2] , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex .",2 https://dialogflow.com/,"We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) [Cite_Footnote_2] , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex .",Method,Tool,False,Compare（引用目的）,True,P19-1519_1_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1059,11063," https://www.ibm.com/cloud/watson-assistant/"," ['3 Experiment Setup']","We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant [Cite_Footnote_3] , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex .",3 https://www.ibm.com/cloud/watson-assistant/,"We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant [Cite_Footnote_3] , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex .",Method,Tool,True,Compare（引用目的）,True,P19-1519_2_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1060,11064," https://www.luis.ai/"," ['3 Experiment Setup']","We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis [Cite_Footnote_4] , wit.ai , snips.ai , recast.ai , and Amazon Lex .",4 https://www.luis.ai/,"We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis [Cite_Footnote_4] , wit.ai , snips.ai , recast.ai , and Amazon Lex .",Method,Tool,True,Compare（引用目的）,True,P19-1519_3_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1061,11065," https://wit.ai/"," ['3 Experiment Setup']","We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai [Cite_Footnote_5] , snips.ai , recast.ai , and Amazon Lex .",5 https://wit.ai/,"We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai [Cite_Footnote_5] , snips.ai , recast.ai , and Amazon Lex .",Method,Tool,True,Compare（引用目的）,True,P19-1519_4_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1062,11066," https://snips.ai/"," ['3 Experiment Setup']","We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai [Cite_Footnote_6] , recast.ai , and Amazon Lex .",6 https://snips.ai/,"We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai [Cite_Footnote_6] , recast.ai , and Amazon Lex .",Method,Tool,True,Compare（引用目的）,True,P19-1519_5_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1063,11067," https://recast.ai/"," ['3 Experiment Setup']","[Cite_Footnote_7] , and Amazon Lex .",7 https://recast.ai/,"We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai [Cite_Footnote_7] , and Amazon Lex .",Method,Tool,True,Compare（引用目的）,True,P19-1519_6_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1064,11068," https://aws.amazon.com/lex/"," ['3 Experiment Setup']","We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex [Cite_Footnote_8] .",8 https://aws.amazon.com/lex/,"We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex [Cite_Footnote_8] .",Method,Tool,True,Compare（引用目的）,True,P19-1519_7_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1065,11069," https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017"," ['4 Results']","Also, we benchmark the intent detection perfor-mance of the proposed model with existing natu-ral language understanding services [Cite_Footnote_9] in Figure 3.",9 https://www.slideshare.net/KonstantinSavenkov/nlu- intent-detection-benchmark-by-intento-august-2017,"Also, we benchmark the intent detection perfor-mance of the proposed model with existing natu-ral language understanding services [Cite_Footnote_9] in Figure 3.",補足資料,Document,True,Compare（引用目的）,True,P19-1519_8_0,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,Footnote
1066,11070," https://github.com/UKPLab/acl2017-non-factoid-qa"," ['References']",The source-code of our system is publicly available. [Cite_Footnote_1],1 https://github.com/UKPLab/ acl2017-non-factoid-qa,"Advanced attention mechanisms are an im-portant part of successful neural network approaches for non-factoid answer selec-tion because they allow the models to focus on few important segments within rather long answer texts. Analyzing attention mechanisms is thus crucial for understand-ing strengths and weaknesses of particular models. We present an extensible, highly modular service architecture that enables the transformation of neural network mod-els for non-factoid answer selection into fully featured end-to-end question answer-ing systems. The primary objective of our system is to enable researchers a way to in-teractively explore and compare attention-based neural networks for answer selec-tion. Our interactive user interface helps researchers to better understand the capa-bilities of the different approaches and can aid qualitative analyses. The source-code of our system is publicly available. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,P17-4004_0_0,2017,End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights,Footnote
1067,11071," https://archive.org/details/stackexchange"," ['3 Candidate Retrieval']","Our implementation con-tains data readers that allow to index InsuranceQA (Feng et al., 2015) and all publicly available dumps of the StackExchange platform. [Cite_Footnote_2]",2 https://archive.org/details/ stackexchange,"The service implementation is based on Scala and the Play Framework. Our implementation con-tains data readers that allow to index InsuranceQA (Feng et al., 2015) and all publicly available dumps of the StackExchange platform. [Cite_Footnote_2] Researchers can easily add new datasets by implementing a single data reader class.",補足資料,Website,True,Introduce（引用目的）,True,P17-4004_1_0,2017,End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights,Footnote
1068,11072," https://doi.org/10.1109/ASRU.2015.7404872"," ['1 Introduction']","Models usually learn to generate dense vector representations for questions and candidates, where representations of a question and an associated correct answer should lie closely together within the vector space (Feng et al., 2015) [Cite_Ref] .","Minwei Feng, Bing Xiang, Michael R. Glass, Li-dan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding. pages 813– 820. https://doi.org/10.1109/ASRU.2015.7404872.","Attention-based neural networks are especially successful in answer selection for non-factoid ques-tions, where approaches have to deal with complex multi-sentence texts. The objective of this task is to re-rank a list of candidate answers according to a non-factoid question, where the best-ranked candidate is selected as an answer. Models usually learn to generate dense vector representations for questions and candidates, where representations of a question and an associated correct answer should lie closely together within the vector space (Feng et al., 2015) [Cite_Ref] . Accordingly, the ranking score can be determined with a simple similarity met-ric. Attention in this scenario works by calculating weights for each individual segment in the input (attention vector), where segments with a higher weight should have a stronger impact on the result-ing representation. Several approaches have been recently proposed, achieving state-of-the-art results on different datasets (Dos Santos et al., 2016; Tan et al., 2016; Wang et al., 2016).",補足資料,Paper,True,Introduce（引用目的）,True,P17-4004_4_0,2017,End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights,Reference
1069,11073," https://doi.org/10.1109/ASRU.2015.7404872"," ['3 Candidate Retrieval']","Our implementation con-tains data readers that allow to index InsuranceQA (Feng et al., 2015) [Cite_Ref] and all publicly available dumps of the StackExchange platform.","Minwei Feng, Bing Xiang, Michael R. Glass, Li-dan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding. pages 813– 820. https://doi.org/10.1109/ASRU.2015.7404872.","The service implementation is based on Scala and the Play Framework. Our implementation con-tains data readers that allow to index InsuranceQA (Feng et al., 2015) [Cite_Ref] and all publicly available dumps of the StackExchange platform. Researchers can easily add new datasets by implementing a single data reader class.",補足資料,Paper,True,Introduce（引用目的）,True,P17-4004_4_1,2017,End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights,Reference
1070,11074," http://aclweb.org/anthology/J11-2003"," ['2 System Overview']","Our architecture is similar to the pipelined struc-tures of earlier work in question answering that rely on a retrieval step followed by a more expen-sive supervised ranking approach (Surdeanu et al., 2011 [Cite_Ref] ; Higashinaka and Isozaki, 2008).","Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank an-swers to non-factoid questions from web collections. Computational Linguistics 37(2). http://aclweb.org/anthology/J11-2003.","Our architecture is similar to the pipelined struc-tures of earlier work in question answering that rely on a retrieval step followed by a more expen-sive supervised ranking approach (Surdeanu et al., 2011 [Cite_Ref] ; Higashinaka and Isozaki, 2008). We primar-ily chose this architecture because it allows the user to directly relate the results of the system to the an-swer selection model. The use of more advanced components (e.g. query expansion or answer merg-ing) would negate this possibility due to the added complexity.",補足資料,Paper,True,Introduce（引用目的）,True,P17-4004_7_0,2017,End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights,Reference
1071,11075," http://sarathchandar.in/bridge-corrnet"," ['1 Introduction']",Code and data used in this paper can be downloaded from [Cite] http: //sarathchandar.in/bridge-corrnet.,,"We evaluate our approach using two downstream applications. First, we employ our model to facil-itate transfer learning between multiple languages using English as the pivot language. For this, we do an extensive evaluation using 110 source-target language pairs and clearly show that we out-perform the current state-of-the art approach (Her-mann and Blunsom, 2014b). Second, we em-ploy our model to enable cross modal access be-tween images and French/German captions using English as the pivot view. For this, we created a test dataset consisting of images and their cap-tions in French and German in addition to the En-glish captions which were publicly available. To the best of our knowledge, this task of retrieving im-ages given French/German captions (and vice versa) without direct parallel training data between them has not been addressed in the past. Even on this task we report promising results. Code and data used in this paper can be downloaded from [Cite] http: //sarathchandar.in/bridge-corrnet.",Mixed,Mixed,True,Produce（引用目的）,True,N16-1021_0_0,2016,Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning,Body
1072,11076," http://www.clg.ox.ac.uk/tedcorpus"," ['4 Datasets', '4.1 Multlingual TED corpus']","We used the same pre-processed splits [Cite_Footnote_1] as provided by (Hermann and Blunsom, 2014b).",1 http://www.clg.ox.ac.uk/tedcorpus,"Hermann and Blunsom (2014b) provide a multilin-gual corpus based on the TED corpus for IWSLT 2013 (Cettolo et al., 2012). It contains English tran-scriptions of several talks from the TED conference and their translations in multiple languages. We use the parallel data between English and other lan-guages for training Bridge Corrnet (English, thus, acts as the pivot langauge). Hermann and Blunsom (2014b) also propose a multlingual document classi-fication task using this corpus. The idea is to use the keywords associated with each talk (document) as class labels and then train a classifier to predict these classes. There are one or more such keywords asso-ciated with each talk but only the 15 most frequent keywords across all documents are considered as class labels. We used the same pre-processed splits [Cite_Footnote_1] as provided by (Hermann and Blunsom, 2014b). The training corpus consists of a total of 12,078 par-allel documents distributed across 12 language pairs.",Material,Knowledge,True,Use（引用目的）,True,N16-1021_1_0,2016,Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning,Footnote
1073,11077," http://mscoco.org/dataset/"," ['4 Datasets', '4.2 Multilingual Image Caption dataset']",The MSCOCO dataset [Cite_Footnote_2] contains images and their English captions.,2 http://mscoco.org/dataset/,"The MSCOCO dataset [Cite_Footnote_2] contains images and their English captions. On an average there are 5 cap-tions per image. The standard train/valid/test splits for this dataset are also available online. However, the reference captions for the images in the test split are not provided. Since we need such reference cap-tions for evaluations, we create a new train/valid/test of this dataset. Specifically, we take 80K images from the standard train split and 40K images from the standard valid split. We then randomly split the merged 120K images into train(118K), valida-tion (1K) and test set (1K).",Mixed,Mixed,True,Introduce（引用目的）,True,N16-1021_2_0,2016,Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning,Footnote
1074,11078," http://sarathchandar.in/bridge-corrnet"," ['4 Datasets', '4.2 Multilingual Image Caption dataset']",This multilingual image caption test data (MIC test data) will be made publicly available [Cite_Footnote_3] and will hopefully assist further research in this area.,3 http://sarathchandar.in/bridge-corrnet,"We then create a multilingual version of the test data by collecting French and German translations for all the 5 captions for each image in the test set. We use crowdsourcing to do this. We used the CrowdFlower platform and solicited one French and one German translation for each of the 5000 cap-tions using native speakers. We got each transla-tion verified by 3 annotators. We restricted the ge-ographical location of annotators based on the tar-get language. We found that roughly 70% of the French translations and 60% of the German trans-lations were marked as correct by a majority of the verifiers. On further inspection with the help of in-house annotators, we found that the errors were mainly syntactic and the content words are trans-lated correctly in most of the cases. Since none of the approaches described in this work rely on syn-tax, we decided to use all the 5000 translations as test data. This multilingual image caption test data (MIC test data) will be made publicly available [Cite_Footnote_3] and will hopefully assist further research in this area.",補足資料,Document,False,Introduce（引用目的）,False,N16-1021_3_0,2016,Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning,Footnote
1075,11079," http://www.statmt.org/moses/"," ['6 Experiment 2: Cross modal access using a pivot language']","Here, we train an En-Image Cor-rNet using Z 1 and an Fr/De-En MT system [Cite_Footnote_4] using Z 2 .",4 http://www.statmt.org/moses/,"5. CorrNet + MT: Here, we train an En-Image Cor-rNet using Z 1 and an Fr/De-En MT system [Cite_Footnote_4] using Z 2 . For the task of retrieving images given a French",Method,Tool,True,Use（引用目的）,True,N16-1021_4_0,2016,Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning,Footnote
1076,11080," http://sarathchandar.in/bridge-corrnet"," ['7 Conclusion']",We also release a new multilingual image caption benchmark (MIC bench-mark) which will help in further research in this field [Cite_Footnote_5] .,5 Details about the MIC benchmark and performance of var-ious state-of-the-art models will be maintained at http://sarathchandar.in/bridge-corrnet,"In this paper, we propose Bridge Correlational Neu-ral Networks which can learn common representa-tions for multiple views even when parallel data is available only between these views and a pivot view. Our method performs better than the existing state of the art approaches on the cross language clas-sification task and gives very promising results on the cross modal access task. We also release a new multilingual image caption benchmark (MIC bench-mark) which will help in further research in this field [Cite_Footnote_5] .",補足資料,Document,True,Produce（引用目的）,False,N16-1021_5_0,2016,Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning,Footnote
1077,11081," https://nlp.stanford.edu/software/srparser.html"," ['3 Models', '3.1 Chunkings and Chunk Embeddings']","We chunk the source and target sentences using constituent parsing (Bauer, 2014) [Cite_Ref] .",John Bauer. 2014. Shift-reduce constituency parser. https://nlp.stanford.edu/software/srparser.html. Accessed: 2018-11-30.,"We chunk the source and target sentences using constituent parsing (Bauer, 2014) [Cite_Ref] . We consider all nodes with phrase-level tags (XP) to be con-stituents. Beginning with the leaves, we move up the tree, deleting any node that is wholly contained in a larger constituent but that is neither a con-stituent itself, nor the sibling of a constituent. Fig-ure 2 shows a simplified constituent tree.",Method,Tool,True,Use（引用目的）,True,P19-1467_0_0,2019,Neural Network Alignment for Sentential Paraphrases,Reference
1078,11082," https://github.com/bzhangXMU/transformer-aan"," ['References']","We conduct a series of experiments on WMT17 translation tasks, where on 6 dif-ferent language pairs, we obtain robust and consistent speed-ups in decoding. [Cite_Footnote_1]",1 Source code is available at https://github.com/bzhangXMU/transformer-aan.,"With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive archi-tecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average atten-tion network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to en-hance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Trans-former to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model en-ables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 dif-ferent language pairs, we obtain robust and consistent speed-ups in decoding. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,P18-1166_0_0,2018,Accelerating Neural Transformer via an Average Attention Network,Footnote
1079,11083," https://github.com/thumt/THUMT"," ['5 Experiments', '5.1 WMT14 English-German Translation', '5.1.1 Model Settings']","We implemented our model with masking tricks based on the open-sourced thumt (Zhang et al., 2017b) [Cite_Footnote_2] , and trained and evaluated all models on a single NVIDIA GeForce GTX 1080 GPU.",2 https://github.com/thumt/THUMT,"The maximum number of training steps was set to 100K. Weights of target-side embedding and out-put weight matrix were tied for all models. We implemented our model with masking tricks based on the open-sourced thumt (Zhang et al., 2017b) [Cite_Footnote_2] , and trained and evaluated all models on a single NVIDIA GeForce GTX 1080 GPU. For evalua-tion, we averaged last five models saved with an interval of 1500 training steps.",Method,Tool,False,Use（引用目的）,True,P18-1166_1_0,2018,Accelerating Neural Transformer via an Average Attention Network,Footnote
1080,11084," http://data.statmt.org/wmt17/translation-task/preprocessed/"," ['5 Experiments', '5.2 WMT17 Translation Tasks']","All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [Cite_Footnote_3]",3 http://data.statmt.org/wmt17/translation-task/preprocessed/,"Interestingly, these translation tasks involves train-ing corpora with different scales (ranging from 0.21M to 52M sentence pairs). This help us thor-oughly examine the ability of our model on differ-ent sizes of training data. All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [Cite_Footnote_3]",Material,Dataset,True,Use（引用目的）,True,P18-1166_2_0,2018,Accelerating Neural Transformer via an Average Attention Network,Footnote
1081,11085," http://matrix.statmt.org/matrix"," ['5 Experiments', '5.2 WMT17 Translation Tasks', '5.2.1 Translation Results']",We also provide the results from WMT17 winning systems [Cite_Footnote_4] .,4 http://matrix.statmt.org/matrix,"Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [Cite_Footnote_4] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus.",Method,Tool,False,Use（引用目的）,False,P18-1166_3_0,2018,Accelerating Neural Transformer via an Average Attention Network,Footnote
1082,11086," https://github.com/successar/FRESH"," ['References']","In both automatic and manual evaluations we find that variants of this simple framework yield predic-tive performance superior to ‘end-to-end’ ap-proaches, while being more general and easier to train. [Cite_Footnote_1]",1 Code is available at https://github.com/successar/FRESH,"In many settings it is important for one to be able to understand why a model made a partic-ular prediction. In NLP this often entails ex-tracting snippets of an input text ‘responsible for’ corresponding model output; when such a snippet comprises tokens that indeed informed the model’s prediction, it is a faithful explana-tion. In some settings, faithfulness may be crit-ical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful ratio-nales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method com-plicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary fea-ture importance scores (e.g., gradients from a trained model) are used to induce binary la-bels over token inputs, which an extractor can be trained to predict. An independent classi-fier module is then trained exclusively on snip-pets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predic-tive performance superior to ‘end-to-end’ ap-proaches, while being more general and easier to train. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.acl-main.409_0_0,2020,Learning to Faithfully Rationalize by Construction,Footnote
1083,11087," https://github.com/bastings/interpretable_predictions"," ['References']",We use the Hard Kumaraswamy distribution as provided by the authors here [Cite] https://github.com/bastings/interpretable_predictions.,,Bastings et al. (2019) do not require hyper-parameter search since it uses a Lagrangian relaxation based optimisation for its regu-larizers. We use the same initial hyperpa-rameter settings used by the authors in their codebase. We use the Hard Kumaraswamy distribution as provided by the authors here [Cite] https://github.com/bastings/interpretable_predictions.,補足資料,Paper,False,Introduce（引用目的）,False,2020.acl-main.409_1_0,2020,Learning to Faithfully Rationalize by Construction,Body
1084,11088," https://github.com/xiaochang13/AMR-generation"," ['2 Method', '2.3 Rule Acquisition']","Since incorporating unaligned words will introduce noise, we rank the translation candi-dates for each AMR fragment by their counts in the training data, and select the top N candidates. [Cite_Footnote_1]",1 Our code for grammar induction can be downloaded from https://github.com/xiaochang13/AMR-generation,"We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC pro-cedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discard-ing function words, while our task is AMR-to-text generation, and we need to be able to fill in these un-aligned words. Since incorporating unaligned words will introduce noise, we rank the translation candi-dates for each AMR fragment by their counts in the training data, and select the top N candidates. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D16-1224_0_0,2016,AMR-to-text generation as a Traveling Salesman Problem,Footnote
1085,11089," http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt"," ['2 Method', '2.3 Rule Acquisition']","Besides that, we use a verbalization list [Cite_Footnote_2] for concept rule generation.",2 http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt,"Some concepts (such as “have-rel-role-91”) in an AMR graph do not contribute to the final translation, and we skip them when generating concept rules. Besides that, we use a verbalization list [Cite_Footnote_2] for concept rule generation. For rule “VERBALIZE peacekeep-ing TO keep-01 :ARG1 peace”, we will create a con-cept rule “(k/keep-01 :ARG1 (p/peace)) ||| peace-keeping” if the left-hand-side fragment appears in the target graph.",Material,Knowledge,True,Use（引用目的）,True,D16-1224_1_0,2016,AMR-to-text generation as a Traveling Salesman Problem,Footnote
1086,11090," https://developers.google.com/optimization/"," ['3 Experiments', '3.1 Setup']","To solve the AGTSP, we use Or-tool [Cite_Footnote_3] .",3 https://developers.google.com/optimization/,"We use the dataset of SemEval-2016 Task8 (Mean-ing Representation Parsing), which contains 16833 training instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same mean-ing. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [Cite_Footnote_3] .",Method,Tool,True,Use（引用目的）,True,D16-1224_2_0,2016,AMR-to-text generation as a Traveling Salesman Problem,Footnote
1087,11091," http://www.statmt.org/moses/"," ['3 Experiments', '3.1 Setup']","We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses [Cite_Footnote_4] to translate the linearized AMR into a sen-tence.",4 http://www.statmt.org/moses/,"Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses [Cite_Footnote_4] to translate the linearized AMR into a sen-tence. To traverse the children of an AMR con-cept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMR-gen (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).",Method,Tool,True,Compare（引用目的）,True,D16-1224_3_0,2016,AMR-to-text generation as a Traveling Salesman Problem,Footnote
1088,11092," https://github.com/jflanigan/jamr/tree/Generator"," ['3 Experiments', '3.1 Setup']","We also compare with JAMR-gen [Cite_Footnote_5] (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).",5 https://github.com/jflanigan/jamr/tree/Generator,"Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses to translate the linearized AMR into a sen-tence. To traverse the children of an AMR con-cept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMR-gen [Cite_Footnote_5] (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).",Method,Tool,True,Compare（引用目的）,True,D16-1224_4_0,2016,AMR-to-text generation as a Traveling Salesman Problem,Footnote
1089,11093," https://code.google.com/archive/p/word2vec/"," ['2 The PACRR Model', '2.1 Relevance Matching']","The similar-ity between a query term q and document term d is calculated by taking the cosine similarity using the pre-trained [Cite_Footnote_1] word2vec (Mikolov et al., 2013).",1 https://code.google.com/archive/p/word2vec/,"We first encode the query-document relevance matching via query-document similarity matri-ces sim |q|×|d| that encodes the similarity be-tween terms from a query q and a document d, where sim ij corresponds to the similarity be-tween the i-th term from q and the j-th term from d. When using cosine similarity, we have sim ∈ [−1,1] |q|×|d| . As suggested in (Hui et al., 2017), query-document similarity matrices pre-serve a rich signal that can be used to perform relevance matching beyond unigram matches. In particular, n-gram matching corresponds to con-secutive document terms that are highly similar to at least one of the query terms. Query coverage is reflected in the number of rows in sim that include at least one cell with high similarity. The similar-ity between a query term q and document term d is calculated by taking the cosine similarity using the pre-trained [Cite_Footnote_1] word2vec (Mikolov et al., 2013).",Method,Tool,True,Compare（引用目的）,True,D17-1110_0_0,2017,PACRR: A Position-Aware Neural IR Model for Relevance Matching,Footnote
1090,11094," http://trec.nist.gov/tracks.html"," ['3 Evaluation', '3.1 Experimental Setup']",We rely on the widely-used 2009–2014 T REC Web Track ad-hoc task benchmarks [Cite_Footnote_3] .,3 http://trec.nist.gov/tracks.html,"We rely on the widely-used 2009–2014 T REC Web Track ad-hoc task benchmarks [Cite_Footnote_3] . The benchmarks are based on the C LUE W EB 09 and C LUE W EB 12 datasets as document collections. In total, there are 300 queries and more than 100k judgments (qrels). Three years (2012–14) of query-likelihood baselines provided by T REC serve as baseline runs in the R ERANK S IMPLE benchmark. In the R ERANK ALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55 (2010), 62 (2011), 48 (2012), 50 (2013), and 27 (2014) runs. ERR@20 (Chapelle et al., 2009) and nDCG@20 (Järvelin and Kekäläinen, 2002) are employed as evaluation measures, and both are computed with the script from T REC 6 .",Material,Dataset,False,Use（引用目的）,True,D17-1110_1_0,2017,PACRR: A Position-Aware Neural IR Model for Relevance Matching,Footnote
1091,11095," https://github.com/trec-web/trec-web-2014"," ['3 Evaluation', '3.1 Experimental Setup']",Three years (2012–14) of query-likelihood baselines provided by T REC [Cite_Footnote_5] serve as baseline runs in the R ERANK S IMPLE benchmark.,5 https://github.com/trec-web/ trec-web-2014,"We rely on the widely-used 2009–2014 T REC Web Track ad-hoc task benchmarks . The benchmarks are based on the C LUE W EB 09 and C LUE W EB 12 datasets as document collections. In total, there are 300 queries and more than 100k judgments (qrels). Three years (2012–14) of query-likelihood baselines provided by T REC [Cite_Footnote_5] serve as baseline runs in the R ERANK S IMPLE benchmark. In the R ERANK ALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55 (2010), 62 (2011), 48 (2012), 50 (2013), and 27 (2014) runs. ERR@20 (Chapelle et al., 2009) and nDCG@20 (Järvelin and Kekäläinen, 2002) are employed as evaluation measures, and both are computed with the script from T REC 6 .",補足資料,Website,True,Introduce（引用目的）,True,D17-1110_2_0,2017,PACRR: A Position-Aware Neural IR Model for Relevance Matching,Footnote
1092,11096," http://trec.nist.gov/data/web/12/gdeval.pl"," ['3 Evaluation', '3.1 Experimental Setup']","ERR@20 (Chapelle et al., 2009) and nDCG@20 (Järvelin and Kekäläinen, 2002) are employed as evaluation measures, and both are computed with the script from T REC [Cite_Footnote_6] .",6 http://trec.nist.gov/data/web/12/gdeval.pl,"We rely on the widely-used 2009–2014 T REC Web Track ad-hoc task benchmarks 3 . The benchmarks are based on the C LUE W EB 09 and C LUE W EB 12 datasets as document collections. In total, there are 300 queries and more than 100k judgments (qrels). Three years (2012–14) of query-likelihood baselines 4 provided by T REC 5 serve as baseline runs in the R ERANK S IMPLE benchmark. In the R ERANK ALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55 (2010), 62 (2011), 48 (2012), 50 (2013), and 27 (2014) runs. ERR@20 (Chapelle et al., 2009) and nDCG@20 (Järvelin and Kekäläinen, 2002) are employed as evaluation measures, and both are computed with the script from T REC [Cite_Footnote_6] .",Material,Knowledge,False,Use（引用目的）,True,D17-1110_3_0,2017,PACRR: A Position-Aware Neural IR Model for Relevance Matching,Footnote
1093,11097," https://github.com/fchollet/keras"," ['3 Evaluation', '3.1 Experimental Setup']","All models are implemented with Keras (Chol-let et al., 2015) [Cite_Ref] using Tensorflow as backend, and are trained on servers with multiple CPU cores.",François Chollet et al. 2015. Keras. https://github.com/fchollet/keras.,"All models are implemented with Keras (Chol-let et al., 2015) [Cite_Ref] using Tensorflow as backend, and are trained on servers with multiple CPU cores. In particular, the training of PACRR takes 35 seconds per iteration on average, and in total at most 150 iterations are trained for each model variant.",Method,Code,True,Use（引用目的）,True,D17-1110_4_0,2017,PACRR: A Position-Aware Neural IR Model for Relevance Matching,Reference
1094,11098," http://www.statmt.org/moses/"," ['5 Experiments']","We validated our simple implementation using a phrase table of 38,488,777 lines created with the Moses toolkit [Cite_Footnote_3] (Koehn et al., 2007) phrase-based SMT system, corresponding to 15,764,069 entries for distinct source phrases .",3 http://www.statmt.org/moses/,"We validated our simple implementation using a phrase table of 38,488,777 lines created with the Moses toolkit [Cite_Footnote_3] (Koehn et al., 2007) phrase-based SMT system, corresponding to 15,764,069 entries for distinct source phrases .",Method,Tool,True,Use（引用目的）,True,P12-2005_0_0,2012,Private Access to Phrase Tables for Statistical Machine Translation,Footnote
1095,11099," http://www.statmt.org/wmt08/shared-task.html"," ['5 Experiments']",[Cite_Footnote_5] .,5 http://www.statmt.org/wmt08/shared-task.html,"This PT was obtained processing the training data of the English-Spanish Europarl corpus used in the WMT 2008 shared task [Cite_Footnote_5] . We used a 2,000 sentence test set of the same shared evaluation for experi-menting with the querying phase.",補足資料,Website,True,Introduce（引用目的）,True,P12-2005_1_0,2012,Private Access to Phrase Tables for Statistical Machine Translation,Footnote
1096,11100," http://www.cis.upenn.edu/~treebank/tokenization.html"," ['2 Common Conventions']","Accordingly, the for-mal definition of PTB tokenization [Cite_Footnote_2] has received lit-tle attention, but reproducing PTB tokenization au-tomatically actually is not a trivial task (see § 3).",2 See http://www.cis.upenn.edu/~treebank/tokenization.html for available ‘documentation’ and a sed script for PTB-style tokenization.,"Due to the popularity of the PTB, its tokenization has been a de-facto standard for two decades. Ap-proximately, this means splitting off punctuation into separate tokens, disambiguating straight quotes, and separating contractions such as can’t into ca and n’t. There are, however, many special cases— documented and undocumented. In much tagging and parsing work, PTB data has been used with gold-standard tokens, to a point where many re-searchers are unaware of the existence of the orig-inal ‘raw’ (untokenized) text. Accordingly, the for-mal definition of PTB tokenization [Cite_Footnote_2] has received lit-tle attention, but reproducing PTB tokenization au-tomatically actually is not a trivial task (see § 3).",補足資料,Document,True,Introduce（引用目的）,True,P12-2074_0_0,2012,"Tokenization: Returning to a Long Solved Problem A Survey, Contrastive Experiment, Recommendations, and Toolkit",Footnote
1097,11101," http://nlp.stanford.edu/software/corenlp.shtml"," ['3 A Contrastive Experiment']",PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools [Cite_Footnote_5] ; and (c) tokenization from the parser of Charniak & Johnson (2005).,"5 See http://nlp.stanford.edu/software/corenlp.shtml, run in ‘strictTreebank3’ mode.","PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools [Cite_Footnote_5] ; and (c) tokenization from the parser of Charniak & Johnson (2005). Table 1 shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance (Leven-shtein, 1966) over tokens (for a total of 49,208 sen-tences and 1,173,750 gold-standard tokens).",Method,Tool,True,Use（引用目的）,False,P12-2074_1_0,2012,"Tokenization: Returning to a Long Solved Problem A Survey, Contrastive Experiment, Recommendations, and Toolkit",Footnote
1098,11102," http://www.cog.brown.edu/mj/Software.htm"," ['3 System Description', '3.5 Inference']","Our implementation of adaptor grammars is a mod-ified version of the Pitman-Yor adaptor grammar sampler [Cite_Footnote_2] , altered to deal with the infinite number of entities.",2 Available at http://www.cog.brown.edu/mj/Software.htm,"Our implementation of adaptor grammars is a mod-ified version of the Pitman-Yor adaptor grammar sampler [Cite_Footnote_2] , altered to deal with the infinite number of entities. It carries out inference using a Metropolis-within-Gibbs algorithm (Johnson et al., 2007), in which it repeatedly parses each input line using the CYK algorithm, samples a parse, and proposes this as the new tree.",Method,Tool,True,Extend（引用目的）,True,N09-1019_0_0,2009,Structured Generative Models for Unsupervised Named-Entity Clustering,Footnote
1099,11103," http://cogcomp.cs.illinois.edu"," ['2 The Models', '2.1 Averaged Perceptron']","We use the regularized version of AP in Learn-ing Based Java [Cite_Footnote_4] (LBJ, (Rizzolo and Roth, 2007)).",4 LBJ can be downloaded from http://cogcomp.cs.illinois.edu.,"We use the regularized version of AP in Learn-ing Based Java [Cite_Footnote_4] (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with a generaliza-tion bound related to the margin of the data, Aver-aged Perceptron also comes with a PAC-like gener-alization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoreti-cally and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, while being more efficient in training. It also has been shown to produce state-of-the-art results on many natural language applica-tions (Punyakanok et al., 2008).",Material,Knowledge,True,Use（引用目的）,True,P11-1093_0_0,2011,Algorithm Selection and Model Adaptation for ESL Correction Tasks,Footnote
1100,11104," http://cogcomp.cs.illinois.edu"," ['3 Comparison of Algorithms', '3.1 Evaluation Data']","We evaluate the models using a corpus of ESL es-says, annotated [Cite_Footnote_7] by native English speakers (Ro-zovskaya and Roth, 2010a).",7 The annotation of the ESL corpus can be downloaded from http://cogcomp.cs.illinois.edu.,"We evaluate the models using a corpus of ESL es-says, annotated [Cite_Footnote_7] by native English speakers (Ro-zovskaya and Roth, 2010a). For each preposition (article) used incorrectly, the annotator indicated the correct choice. The data include sentences by speak-ers of five first languages. Table 3 shows statistics by the source language of the writer.",Material,Knowledge,True,Produce（引用目的）,True,P11-1093_1_0,2011,Algorithm Selection and Model Adaptation for ESL Correction Tasks,Footnote
1101,11105," https://en.wikipedia.org/wiki/Friends"," ['5 Datasets', '5.3 Television Series Transcripts']","Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends [Cite_Footnote_2] and The Big Bang Theory, available from Internet Movie Script Database (IMSDb).",2 https://en.wikipedia.org/wiki/Friends,"Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends [Cite_Footnote_2] and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.",補足資料,Document,True,Introduce（引用目的）,True,P16-1094_0_0,2016,A Persona-Based Neural Conversation Model,Footnote
1102,11106," https://en.wikipedia.org/wiki/The_Big_Bang_Theory"," ['5 Datasets', '5.3 Television Series Transcripts']","Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, [Cite_Footnote_3] available from Internet Movie Script Database (IMSDb).",3 https://en.wikipedia.org/wiki/The_ Big_Bang_Theory,"Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, [Cite_Footnote_3] available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.",補足資料,Document,True,Introduce（引用目的）,True,P16-1094_1_0,2016,A Persona-Based Neural Conversation Model,Footnote
1103,11107," http://www.imsdb.com"," ['5 Datasets', '5.3 Television Series Transcripts']","Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). [Cite_Footnote_4]",4 http://www.imsdb.com,"Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). [Cite_Footnote_4] We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.",Material,DataSource,True,Extend（引用目的）,True,P16-1094_2_0,2016,A Persona-Based Neural Conversation Model,Footnote
1104,11108," http://lit.eecs.umich.edu/downloads.html"," ['7 Conclusion and Recommendations']",The code used in the experiments described in this paper is publicly available from [Cite] http: //lit.eecs.umich.edu/downloads.html.,,The code used in the experiments described in this paper is publicly available from [Cite] http: //lit.eecs.umich.edu/downloads.html.,Method,Code,True,Produce（引用目的）,True,N18-1190_0_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Body
1105,11109," https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285"," ['4 Factors Influencing Stability', '4.2 Word Properties']","For a finer-grained repre-sentation, we use the number of different Word-Net senses associated with the word (Miller, 1995; Fellbaum, 1998 [Cite_Ref] ).",Christiane Fellbaum. 1998. WordNet. Wiley Online Library. https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285.,"To get a coarse-grained representation of the polysemy of the word, we consider the number of different POS present. For a finer-grained repre-sentation, we use the number of different Word-Net senses associated with the word (Miller, 1995; Fellbaum, 1998 [Cite_Ref] ).",Material,Knowledge,False,Use（引用目的）,True,N18-1190_5_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1106,11110," https://www.isca-speech.org/archive/"," ['1 Introduction']","Stability is measured across ten randomized embedding spaces trained on the training portion of the PTB (determined using language model-ing splits (Mikolov et al., 2010) [Cite_Ref] ).","Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.","Figure 1: Stability of word2vec as a property of fre-quency in the PTB. Stability is measured across ten randomized embedding spaces trained on the training portion of the PTB (determined using language model-ing splits (Mikolov et al., 2010) [Cite_Ref] ). Each word is placed in a frequency bucket (x-axis), and each column (fre-quency bucket) is normalized.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_16_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1107,11111," https://www.isca-speech.org/archive/"," ['3 Defining Stability']","Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [Cite_Ref] ).","Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.","Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [Cite_Ref] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right y-axis). Each row is normalized, and boxes with more than 0.01 of the row’s mass are outlined.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_16_1,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1108,11112," https://www.isca-speech.org/archive/"," ['5 Lessons Learned: What Contributes to the Stability of an Embedding']","Stability is measured across ten ran-domized embedding spaces trained on the training data of the PTB (determined using language modeling splits (Mikolov et al., 2010) [Cite_Ref] ).","Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.","Figure 3: Stability of both word2vec and GloVe as properties of the starting word position in the training data of the PTB. Stability is measured across ten ran-domized embedding spaces trained on the training data of the PTB (determined using language modeling splits (Mikolov et al., 2010) [Cite_Ref] ). Boxes with more than 0.02% of the total vocabulary mass are outlined.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_16_2,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1109,11113," https://www.isca-speech.org/archive/"," ['5 Lessons Learned: What Contributes to the Stability of an Embedding']","Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [Cite_Ref] ).","Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.","1 2 1.0 ) 32 scale 1024 9998 10 2 log-based 0. Bucket 8 32 ) 1024 scale 9998 Frequency 50 2 log . approx 32 0.6 ( Neighbors ( 1024 9998 Corpus 100 2 Particular 32 9998 Nearest 1024 0.4 Training 200 in 2 Words 32 of in 1024 # 0.2 % Frequency 800 99982 32 1024 max 9998 0 20 40 60 80 100 % Stability Figure 6: Stability of word2vec on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [Cite_Ref] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_16_3,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1110,11114," http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases"," ['2 Related Work']","Here, we explore three different embedding methods: PPMI (Bullinaria and Levy, 2007), word2vec (Mikolov et al., 2013b) [Cite_Ref] , and GloVe (Pennington et al., 2014).","Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Informa-tion Processing Systems (NIPS). pages 3111–3119. http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases.","Here, we explore three different embedding methods: PPMI (Bullinaria and Levy, 2007), word2vec (Mikolov et al., 2013b) [Cite_Ref] , and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the ef-fect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the param-eters of the embedding algorithm word2vec influ-ence the geometry of word vectors and their con-text vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sam-pling and the number of epochs to train for. They also demonstrate that in addition to parameter set-tings, word properties, such as word ambiguity, af-fect embedding quality.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_18_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1111,11115," http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases"," ['4 Factors Influencing Stability', '4.4 Algorithm Properties']","(Mikolov et al., 2013b) [Cite_Ref] uses a shallow neural network to learn word embeddings by predicting context words.","Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Informa-tion Processing Systems (NIPS). pages 3111–3119. http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases.","PPMI creates embeddings by first building a positive pointwise mutual information word-context matrix, and then reducing the dimension-ality of this matrix using SVD (Bullinaria and Levy, 2007). A more recent word embedding al-gorithm, word2vec (skip-gram model) (Mikolov et al., 2013b) [Cite_Ref] uses a shallow neural network to learn word embeddings by predicting context words. Another recent method for creating word embeddings, GloVe, is based on factoring a matrix of ratios of co-occurrence probabilities (Penning-ton et al., 2014).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_18_1,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1112,11116," https://catalog.ldc.upenn.edu/ldc2008t19"," ['4 Factors Influencing Stability', '4.3 Data Properties']","For this model, we gather data from two sources: New York Times (NYT) (Sandhaus, 2008) [Cite_Ref] and Eu-roparl (Koehn, 2005).","Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consor-tium, Philadelphia 6(12):e26752. https://catalog.ldc.upenn.edu/ldc2008t19.","Data features capture properties of the training data (and the word in relation to the training data). For this model, we gather data from two sources: New York Times (NYT) (Sandhaus, 2008) [Cite_Ref] and Eu-roparl (Koehn, 2005). Overall, we consider seven domains of data: (1) NYT - U.S., (2) NYT - New York and Region, (3) NYT - Business, (4) NYT - Arts, (5) NYT - Sports, (6) All of the data from domains 1-5 (denoted “All NYT”), and (7) All of English Europarl. Table 3 shows statistics about these datasets. The first five domains are chosen because they are the top five most common cate-gories of news articles present in the NYT corpus. They are smaller than “All NYT” and Europarl, and they have a narrow topical focus. The “All NYT” domain is more diverse topically and larger than the first five domains. Finally, the Europarl domain is the largest domain, and it is focused on a single topic (European Parliamentary politics). These varying datasets allow us to consider how data-dependent properties affect stability.",Material,DataSource,True,Use（引用目的）,True,N18-1190_26_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1113,11117," http://www.aclweb.org/anthology/P15-2108"," ['2 Related Work']","At a higher level of granularity, Tan et al. (2015) [Cite_Ref] analyze word embedding spaces by comparing two spaces.","Luchen Tan, Haotian Zhang, Charles LA Clarke, and Mark D Smucker. 2015. Lexical compari-son between Wikipedia and Twitter corpora by us-ing word embeddings. In Assocation for Computa-tional Linguistics (ACL). pages 657–661. http: //www.aclweb.org/anthology/P15-2108.","At a higher level of granularity, Tan et al. (2015) [Cite_Ref] analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in different domains (in their case, Twitter and Wikipedia).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_27_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1114,11118," http://anthology.aclweb.org/P/P16/P16-1013.pdf"," ['1 Introduction']","Recently, they have gained tremendous popularity in Natural Language Pro-cessing (NLP) and have been used in tasks as diverse as text similarity (Kenter and De Rijke, 2015), part-of-speech tagging (Tsvetkov et al., 2016) [Cite_Ref] , sentiment analysis (Faruqui et al., 2015), and machine translation (Mikolov et al., 2013a).","Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian MacWhinney, and Chris Dyer. 2016. Learning the curriculum with Bayesian optimization for task-specific word representation learning. Associa-tion for Computational Linguistics (ACL) pages 130–139. http://anthology.aclweb.org/P/P16/P16-1013.pdf.","Word embeddings are low-dimensional, dense vector representations that capture semantic prop-erties of words. Recently, they have gained tremendous popularity in Natural Language Pro-cessing (NLP) and have been used in tasks as diverse as text similarity (Kenter and De Rijke, 2015), part-of-speech tagging (Tsvetkov et al., 2016) [Cite_Ref] , sentiment analysis (Faruqui et al., 2015), and machine translation (Mikolov et al., 2013a). Although word embeddings are widely used across NLP, their stability has not yet been fully evaluated and understood. In this paper, we ex-plore the factors that play a role in the stability of word embeddings, including properties of the data, properties of the algorithm, and properties of the words. We find that word embeddings exhibit substantial instabilities, which can have implica-tions for downstream tasks.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_28_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1115,11119," http://anthology.aclweb.org/P/P16/P16-1013.pdf"," ['4 Factors Influencing Stability', '4.3 Data Properties']","Curriculum learning has been previously explored for word2vec, where it has been found that optimizing training data order can lead to small improvements on common NLP tasks (Tsvetkov et al., 2016) [Cite_Ref] .","Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian MacWhinney, and Chris Dyer. 2016. Learning the curriculum with Bayesian optimization for task-specific word representation learning. Associa-tion for Computational Linguistics (ACL) pages 130–139. http://anthology.aclweb.org/P/P16/P16-1013.pdf.","Our final data-level features explore the role of curriculum learning in stability. It has been posited that the order of the training data affects the performance of certain algorithms, and previ-ous work has shown that for some neural network-based tasks, a good training data order (curricu-lum learning strategy) can improve performance (Bengio et al., 2009). Curriculum learning has been previously explored for word2vec, where it has been found that optimizing training data order can lead to small improvements on common NLP tasks (Tsvetkov et al., 2016) [Cite_Ref] . Of the embedding algorithms we consider, curriculum learning only affects word2vec. Because GloVe and PPMI use the data to learn a complete matrix before build-ing embeddings, the order of the training data will not affect their performance. To measure the ef-fects of training data order, we include as features the first appearance of word W in the dataset for embedding space A and the first appearance of W in the dataset for embedding space B (represented as percentages of the total number of training sen-tences) 4 . We further include the absolute differ-ence between these percentages.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1190_28_1,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1116,11120," http://www.speech.cs.cmu.edu/cgibin/cmudict"," ['4 Factors Influencing Stability', '4.2 Word Properties']","We also consider the number of syllables in a word, determined using the CMU Pronuncing Dic-tionary (Weide, 1998) [Cite_Ref] .",Robert L Weide. 1998. The CMU pronouncing dictionary http://www.speech.cs.cmu.edu/cgibin/cmudict.,"We also consider the number of syllables in a word, determined using the CMU Pronuncing Dic-tionary (Weide, 1998) [Cite_Ref] . If the word is not present in the dictionary, then this is set to zero.",Material,Knowledge,True,Use（引用目的）,True,N18-1190_29_0,2018,Factors Influencing the Surprising Instability of Word Embeddings,Reference
1117,11121," http://www.di.unito.it/~tutreeb/"," ['1 Introduction']","Meanwhile, motivated by different syntactic theories and practices, major languages in the world often possess multiple large-scale hetero-geneous treebanks, e.g., Tiger (Brants et al., 2002) and TüBa-D/Z (Telljohann et al., 2004) treebanks for German, Talbanken (Einarsson, 1976) and Syntag (Järborg, 1986) treebanks for Swedish, ISST (Montemagni et al., 2003) and TUT [Cite_Footnote_1] treebanks for Italian, etc.",1 http://www.di.unito.it/~tutreeb/,"Meanwhile, motivated by different syntactic theories and practices, major languages in the world often possess multiple large-scale hetero-geneous treebanks, e.g., Tiger (Brants et al., 2002) and TüBa-D/Z (Telljohann et al., 2004) treebanks for German, Talbanken (Einarsson, 1976) and Syntag (Järborg, 1986) treebanks for Swedish, ISST (Montemagni et al., 2003) and TUT [Cite_Footnote_1] treebanks for Italian, etc. Ta-ble 1 lists several large-scale Chinese tree-banks. In this work, we take HIT-CDT as a case study. Our next-step plan is to annotate bi-tree aligned data for PKU-CDT and then convert PKU-CDT to our guideline. For non-dependency treebanks, the straight-forward choice is to convert such treebanks to dependency treebanks based on heuris-tic head-finding rules. The second choice is to directly extend our proposed approaches by adapting the patterns and treeLSTMs for non-dependency structures, which should be straightforward as well.",Material,Dataset,True,Introduce（引用目的）,True,P18-1252_0_0,2018,Supervised Treebank Conversion: Data and Approaches,Footnote
1118,11122," http://hlt.suda.edu.cn/index.php/SUCDT"," ['1 Introduction']",We release the annotation guideline and the newly annotated data in [Cite] http://hlt.suda.edu.cn/index.php/SUCDT.,,"Experimental results show that 1) the two conversion approaches achieve nearly the same conversion accuracy, and 2) direct treebank conversion is superior to indirect multi-task learning in exploiting multiple treebanks in methodology simplicity and performance, yet with the cost of manual annotation. We release the annotation guideline and the newly annotated data in [Cite] http://hlt.suda.edu.cn/index.php/SUCDT.",Mixed,Mixed,True,Produce（引用目的）,True,P18-1252_1_0,2018,Supervised Treebank Conversion: Data and Approaches,Body
1119,11123," http://universaldependencies.org"," ['2 Annotation of Bi-tree Aligned Data', '2.1 Data Annotation']","The UD (universal dependencies) project [Cite_Footnote_3] releases a more detailed language-generic guideline to facilitate cross-linguistically consistent annotation, containing 37 relation labels.",3 http://universaldependencies.org,"The UD (universal dependencies) project [Cite_Footnote_3] releases a more detailed language-generic guideline to facilitate cross-linguistically consistent annotation, containing 37 relation labels. However, after in-depth study, we find that the UD guideline is very useful and comprehensive, but may not be completely compact for realistic annotation of Chinese-specific syntax. After many months’ investigation and trial, we have developed a systematic and detailed annotation guideline for Chinese dependency treebank construction. Our 60-page guideline employs 20 relation labels and gives detailed illustrations for annotation, in order to improve consistency and quality.",補足資料,Website,True,Introduce（引用目的）,True,P18-1252_2_0,2018,Supervised Treebank Conversion: Data and Approaches,Footnote
1120,11124," https://github.com/tdozat/Parser-v1"," ['5 Experiments', '5.1 Experiment Settings']","On the Chinese CoNLL-2009 data, our parser achieves 85.80% in LAS, whereas the origi-nal tensorflow-based parser [Cite_Footnote_6] achieves 85.54% (85.38% reported in their paper) under the same parameter settings and external word embedding.",6 https://github.com/tdozat/Parser-v1,"Implementation. In order to more flexibly realize our ideas, we re-implement the baseline biaffine parser in C++ based on the lightweight neural network library of Zhang et al. (2016). On the Chinese CoNLL-2009 data, our parser achieves 85.80% in LAS, whereas the origi-nal tensorflow-based parser [Cite_Footnote_6] achieves 85.54% (85.38% reported in their paper) under the same parameter settings and external word embedding.",Method,Tool,True,Compare（引用目的）,True,P18-1252_3_0,2018,Supervised Treebank Conversion: Data and Approaches,Footnote
1121,11125," https://www.uexpress.com/dearabby/archives"," ['3 S OCIAL -C HEM -101 Dataset', '3.1 Situations']","We gather a total of 104k real life situations from four domains: scraped titles of posts in the subreddits r/confessions (32k) and r/amitheasshole (r/AITA, 30k), which largely focus on moral quandaries and interper-sonal conflicts; 30k sentences from the ROCSto-ries corpus (rocstories, Mostafazadeh et al., 2016); and scraped titles from the Dear Abby ad-vice column archives [Cite_Footnote_3] (dearabby, 12k).",3 https://www.uexpress.com/dearabby/archives,"We use a situation to denote the one-sentence prompt given to a worker as the basis for writ-ing RoTs. We gather a total of 104k real life situations from four domains: scraped titles of posts in the subreddits r/confessions (32k) and r/amitheasshole (r/AITA, 30k), which largely focus on moral quandaries and interper-sonal conflicts; 30k sentences from the ROCSto-ries corpus (rocstories, Mostafazadeh et al., 2016); and scraped titles from the Dear Abby ad-vice column archives [Cite_Footnote_3] (dearabby, 12k).",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.48_0_0,2020,S OCIAL C HEMISTRY 101: Learning to Reason about Social and Moral Norms,Footnote
1122,11126," https://mediabiasfactcheck.com"," ['6 Morality & Political Bias']","Specifically, we generate ROTs and attributes for 50,000 news headlines randomly selected from Nørregaard et al. (2019), a large corpus of political headlines from 2018 paired with news source rat-ings of political leaning (5-point scale from left- to right-leaning) and factual reliability (5-point scale from least reliable to most reliable). [Cite_Footnote_10]","10 We use the MediaBias/FactCheck ratings: https://mediabiasfactcheck.com. Shoumen, BULGARIA. know” – Follows: “It’s devastating to be excluded from a wedding you were invited to”","To demonstrate a use case of our proposed formal-ism, we analyze the social norms and expectations evoked in news headlines from news sources of for headlines and the news source’s political leaning (left: neg., right: pos.) and reliability (controlled for political leaning). Results shown are significant after Holm-correction for multiple comparisons (p < 0.001: ∗∗∗ , p < 0.01: ∗∗ , p < 0.05: ∗ , p > 0.05: n.s.) . Takeaway: We see evidence that a model trained on the S OCIAL -C HEM -101 Dataset can naturally uncover moral and topical leanings in news sources, mirroring results found in previous news studies. various political leanings and trustworthiness, us-ing the N EURAL N ORM T RANSFORMER (GPT-2 XL). Specifically, we generate ROTs and attributes for 50,000 news headlines randomly selected from Nørregaard et al. (2019), a large corpus of political headlines from 2018 paired with news source rat-ings of political leaning (5-point scale from left- to right-leaning) and factual reliability (5-point scale from least reliable to most reliable). [Cite_Footnote_10]",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.48_1_0,2020,S OCIAL C HEMISTRY 101: Learning to Reason about Social and Moral Norms,Footnote
1123,11127," https://cs.rochester.edu/nlp/rocstories/"," ['2 Approach']","rocstories (30k) — The ROCStories corpus from (Mostafazadeh et al., 2016) [Cite_Ref] .","Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839– 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https: //cs.rochester.edu/nlp/rocstories/ .","Punching a friend who stole from me. RoT 1: It is unacceptable to injure a person. RoT 2: People should not steal from others. RoT 3: It is bad to betray a friend. RoT 4: It is OK to want to take revenge. often with interpersonal conflicts, such as “I feel threat-ened by women prettier than me.” As with r/AITA, we scrape only the titles of these posts. This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) — The ROCStories corpus from (Mostafazadeh et al., 2016) [Cite_Ref] . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., “They weren’t sure either so he started asking friends.”. We select a subset of the sentences from ROCStories which are likely to column. These titles are usually information dense sum-maries of interpersonal situations written in the style of news headlines, e.g., “Pushy Party Guests Make Them-selves Too Much at Home.” We scrape all of the titles found in the archives, and use heuristics to attempt to filter out all posts that do not match this style, such as announcements and holiday greetings. once. For example, in a sentence like “We went to the park.” we would pick we. Or for a sentence like “They spent hours talking to us and we had a good time.” we would pick they and us. underlying expectations. RoTs that are too vague often do describe norms, but the link to the situa-tion can be so distant as to be misleading. Good RoTs may be somewhat specific, but explain both the underlying norms at play, and apply to other situations. – Distinct ideas. When multiple RoTs are provided for a situation, each should contain a distinct idea. This includes inversions of the same idea. – Example situation: Never taking out the trash – Violates: “It’s irresponsible to avoid the chores you are assigned” with “It’s bad to not do chores you’re supposed to do”",Material,Dataset,True,Introduce（引用目的）,True,2020.emnlp-main.48_2_0,2020,S OCIAL C HEMISTRY 101: Learning to Reason about Social and Moral Norms,Reference
1124,11128," https://github.com/Liang-Qiu/SVRNN-dialogues"," ['References']","While on multi-party dia-logue datasets, our model learns an interac-tive structure demonstrating its capability of distinguishing speakers or addresses, automat-ically disentangling dialogues without explicit human annotation. [Cite_Footnote_1]",1 The code is released at https://github.com/Liang-Qiu/SVRNN-dialogues.,"Inducing a meaningful structural representa-tion from one or a set of dialogues is a cru-cial but challenging task in computational lin-guistics. Advancement made in this area is critical for dialogue system design and dis-course analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured atten-tion layers into a Variational Recurrent Neu-ral Network (VRNN) model with discrete la-tent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sen-tence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with struc-tured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dia-logue datasets, our model learns an interac-tive structure demonstrating its capability of distinguishing speakers or addresses, automat-ically disentangling dialogues without explicit human annotation. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.148_0_0,2020,"Structured Attention for Unsupervised Dialogue Structure Induction Liang Qiu 1 , Yizhou Zhao 1 , Weiyan Shi 2 , Yuan Liang 3 , Feng Shi 1 ,",Footnote
1125,11129," https://radimrehurek.com/gensim/"," ['5 Experimental Setup']","Word embeddings were pre-trained with the Gensim [Cite_Footnote_2] implementation of word2vec (Mikolov et al., 2013) on the English GigaWord corpus (with case left intact).",2 https://radimrehurek.com/gensim/,"Training Details Word embeddings were pre-trained with the Gensim [Cite_Footnote_2] implementation of word2vec (Mikolov et al., 2013) on the English GigaWord corpus (with case left intact). The di-mensionality of the word embeddings was set to 50. Following Li et al. (2016), the embed-dings were fine-tuned using a mapping matrix W ∈ R 50×50 trained with the following criterion: where LT tuned , and LT pre are lookup tables for fine-tuned and pre-trained word embeddings in the training set. Matrix W can be subsequently used to to estimate fine-tuned embeddings for words in the test set.",Method,Tool,True,Use（引用目的）,True,D17-1133_0_0,2017,nucleus satellite,Footnote
1126,11130," https://nlp.stanford.edu/projects/glove/"," ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']",• Genericembeddingswordusedembeddingsare GloVe: Generic [Cite_Footnote_1] from wordboth Wikipedia and common crawl and the word2vec,1 https://nlp.stanford.edu/projects/glove/,"• Genericembeddingswordusedembeddingsare GloVe: Generic [Cite_Footnote_1] from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings . These generic embeddings will be denoted as Glv, GlvCC and w2v.",Method,Code,False,Use（引用目的）,False,N18-4007_0_0,2018,Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets,Footnote
1127,11131," https://code.google.com/archive/p/word2vec/"," ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']",• Genericembeddingswordusedembeddingsare GloVe: Generic from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings [Cite_Footnote_2] .,2 https://code.google.com/archive/p/word2vec/,"• Genericembeddingswordusedembeddingsare GloVe: Generic from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings [Cite_Footnote_2] . These generic embeddings will be denoted as Glv, GlvCC and w2v.",Method,Tool,True,Use（引用目的）,False,N18-4007_1_0,2018,Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets,Footnote
1128,11132," https://radimrehurek.com/gensim/"," ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']",• DSobtainedwordviaembeddingsLatent Semantic: DS embeddingsAnalysis (LSAare) and via retraining word2vec on the test data sets using the implementation in gensim [Cite_Footnote_3] .,3 https://radimrehurek.com/gensim/,• DSobtainedwordviaembeddingsLatent Semantic: DS embeddingsAnalysis (LSAare) and via retraining word2vec on the test data sets using the implementation in gensim [Cite_Footnote_3] . DS embeddings via LSA are denoted by LSA and DS embeddings via word2vec are de-noted by DSw2v.,Method,Code,False,Use（引用目的）,False,N18-4007_2_0,2018,Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets,Footnote
1129,11133," https://github.com/atschneid/DebugSL"," ['6 Demonstration']",The source code is available at [Cite] https://github.com/atschneid/DebugSL.,,"We present two main scenarios to demonstrate the practical usefulness of DebugSL, screenshots of which are shown in Figure 3. The source code is available at [Cite] https://github.com/atschneid/DebugSL.",Method,Code,True,Produce（引用目的）,True,N18-5008_0_0,2018,DebugSL: An Interactive Tool for Debugging Sentiment Lexicons,Body
1130,11134," https://github.com/atschneid/DebugSL"," ['7 Conclusion']",The project source code is available at [Cite] https://github.com/atschneid/DebugSL and a screencast can be viewed at https://cis.temple.,,In this paper we have presented the system DebugSL and described its usage. The project source code is available at [Cite] https://github.com/atschneid/DebugSL and a screencast can be viewed at https://cis.temple.edu/˜edragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.,Method,Code,True,Produce（引用目的）,True,N18-5008_1_0,2018,DebugSL: An Interactive Tool for Debugging Sentiment Lexicons,Body
1131,11135," https://cis.temple.edu/~edragut/DebugSL.webm"," ['7 Conclusion']",The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [Cite] https://cis.temple.,,In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [Cite] https://cis.temple.edu/˜edragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.,補足資料,Media,True,Produce（引用目的）,True,N18-5008_2_0,2018,DebugSL: An Interactive Tool for Debugging Sentiment Lexicons,Body
1132,11136," https://spacy.io/"," ['-', '8.1 Definitions of Complexity Measures']","Parse tree depth: dependency parse tree depth using spaCy (Honnibal and Montani, 2017) [Cite_Ref] .","Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed-dings, convolutional neural networks and incremen-tal parsing. https://spacy.io/.","4. Parse tree depth: dependency parse tree depth using spaCy (Honnibal and Montani, 2017) [Cite_Ref] .",Method,Tool,False,Introduce（引用目的）,False,2021.naacl-main.352_0_0,2021,Linguistic Complexity Loss in Text-Based Therapy,Reference
1133,11137," https://www.talkspace.com/"," ['2 Dataset']","In this work, we study text-based mes-sages from Talkspace, an online therapy platform with thousands of licensed therapists serving more than one-million users (Talkspace, 2020) [Cite_Ref] .","Talkspace. 2020. Talkspace - #1 rated online therapy, 1 million+ users. https://www.talkspace.com/ , accessed Oct 1, 2020.","Talkspace. In this work, we study text-based mes-sages from Talkspace, an online therapy platform with thousands of licensed therapists serving more than one-million users (Talkspace, 2020) [Cite_Ref] . Anyone seeking therapy, henceforth clients, can sign up for a Talkspace plan and get matched with a licensed therapist who will respond 5ˆ a week through a chat room accessible by clients 24-7.",補足資料,Website,True,Use（引用目的）,True,2021.naacl-main.352_1_0,2021,Linguistic Complexity Loss in Text-Based Therapy,Reference
1134,11138," https://textinspector.com/help/lexical-diversity/"," ['-', '8.1 Definitions of Complexity Measures']","samples of different length, is merely a com-plex approximation (R “ 0.971) of a hypergeo-metric distribution, which they use in an index called HD-D. [Cite_Footnote_1]",1 See https://textinspector.com/help/lexical-diversity/ for McCarthy’s recommendation on vocd-D vs HD-D.,"Here, we describe in detail the linguistic complex-ity measures we used, which span lexical diversity ( ), syntactic simplicity ( ), readability ( ), and prototypicality ( ). samples of different length, is merely a com-plex approximation (R “ 0.971) of a hypergeo-metric distribution, which they use in an index called HD-D. [Cite_Footnote_1] HD-D measures the mean con-tribution that each type makes to the TTR of all possible combinations of a samples of size 35-",Method,Code,False,Use（引用目的）,False,2021.naacl-main.352_2_0,2021,Linguistic Complexity Loss in Text-Based Therapy,Footnote
1135,11139," https://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php"," ['-', '8.1 Definitions of Complexity Measures']","¨ 100q ` 0.0496 WPS , where DWR is the ratio of difficult words [Cite_Footnote_2] and","2 Words not on a list of 3,000 familiar words at https: //www.readabilityformulas.com/articles/ dale-chall-readability-word-list.php","6. Dale-Chall readability score (Dale and Chall, 1948, 1995): DCRS “ 0.1579p DWR ¨ 100q ` 0.0496 WPS , where DWR is the ratio of difficult words [Cite_Footnote_2] and WPS is the average words per sen-",Material,Knowledge,True,Introduce（引用目的）,True,2021.naacl-main.352_3_0,2021,Linguistic Complexity Loss in Text-Based Therapy,Footnote
1136,11140," http://katfuji.lab.tuat.ac.jp/nlp_datasets/"," ['References']","The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [Cite_Footnote_1]",1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa,"In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persua-sive strategy directly. However, existing re-search lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a persuasion strategy us-ing EUs. Our contributions are as follows: (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) re-vealing both intuitive and non-intuitive strate-gic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing base-line neural models that identify the EU bound-ary and type. Our observations imply that EUs definitively characterize online persua-sion strategies. The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D19-1653_0_0,2019,Revealing and Predicting Online Persuasion Strategy with Elementary Units,Footnote
1137,11141," https://github.com/EdoFrank/emnlp2019_morio_egawa"," ['References']","The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [Cite_Footnote_1]",1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa,"In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persua-sive strategy directly. However, existing re-search lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a persuasion strategy us-ing EUs. Our contributions are as follows: (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) re-vealing both intuitive and non-intuitive strate-gic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing base-line neural models that identify the EU bound-ary and type. Our observations imply that EUs definitively characterize online persua-sion strategies. The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D19-1653_1_0,2019,Revealing and Predicting Online Persuasion Strategy with Elementary Units,Footnote
1138,11142," https://github.com/clab/lstm-parser"," ['4 Training Procedure']","Using the dimen-sions discussed in the next section, we required between 8 and 12 hours to reach convergence on a held-out dev set. [Cite_Footnote_6]",6 Software for replicating the experiments is available from https://github.com/clab/lstm-parser.,"We trained our parser to maximize the conditional log-likelihood (Eq. 1) of treebank parses given sentences. Our implementation constructs a com-putation graph for each sentence and runs forward-and backpropagation to obtain the gradients of this objective with respect to the model parameters. The computations for a single parsing model were run on a single thread on a CPU. Using the dimen-sions discussed in the next section, we required between 8 and 12 hours to reach convergence on a held-out dev set. [Cite_Footnote_6]",Method,Tool,True,Produce（引用目的）,False,P15-1033_0_0,2015,Transition-Based Dependency Parsing with Stack Long Short-Term Memory,Footnote
1139,11143," https://github.com/naist-cl-parsing/"," ['1 Introduction']","To pursue this direction further, we construct a corpus such that dependency structures are consis-tent with MWEs, by extending Kato et al. (2016)’s corpus [Cite_Footnote_2] .","2 We release our dependency corpus at https: //github.com/naist-cl-parsing/ mwe-aware-dependency. MWE-aware phrase struc-tures will be distributed from LDC as a part of LDC2017T01. Computational Linguistics (Short Papers), pages 427–432","To pursue this direction further, we construct a corpus such that dependency structures are consis-tent with MWEs, by extending Kato et al. (2016)’s corpus [Cite_Footnote_2] . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware de-pendency structure from our corpus (Figure 1b). Moreover, our corpus includes not only functional MWEs but also NEs. Because NEs are highly pro-ductive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary.",Material,Dataset,True,Extend（引用目的）,True,P17-2068_0_0,2017,English Multiword Expression-aware Dependency Parsing Including Named Entities,Footnote
1140,11144," https://catalog.ldc.upenn.edu/LDC2017T01"," ['2 MWE-aware Dependency Corpus']","We construct our corpus by extending Kato et al. (2016)’s corpus [Cite_Footnote_5] , which is itself built on a corpus by Shigeto et al. (2013).",5 https://catalog.ldc.upenn.edu/LDC2017T01,"To ensure consistency between MWE annotations and dependency structures, we first integrate NE annotations on Ontonotes into phrase structures such that functional MWEs are established as sub-trees. Subsequently, we convert phrase structures to dependency structures. We construct our corpus by extending Kato et al. (2016)’s corpus [Cite_Footnote_5] , which is itself built on a corpus by Shigeto et al. (2013). Regarding MWE annotations, Shigeto et al. (2013) first constructed an MWE dictionary by extract-ing functional MWEs from the English-language Wiktionary , and classified their occurrences in Ontonotes into either MWE or literal usage. Kato et al. (2016) integrated these MWE annotations into phrase structures and established functional MWEs as subtrees.",Material,Dataset,True,Extend（引用目的）,True,P17-2068_1_0,2017,English Multiword Expression-aware Dependency Parsing Including Named Entities,Footnote
1141,11145," https://en.wiktionary.org"," ['2 MWE-aware Dependency Corpus']","Regarding MWE annotations, Shigeto et al. (2013) first constructed an MWE dictionary by extract-ing functional MWEs from the English-language Wiktionary [Cite_Footnote_6] , and classified their occurrences in Ontonotes into either MWE or literal usage.",6 https://en.wiktionary.org,"To ensure consistency between MWE annotations and dependency structures, we first integrate NE annotations on Ontonotes into phrase structures such that functional MWEs are established as sub-trees. Subsequently, we convert phrase structures to dependency structures. We construct our corpus by extending Kato et al. (2016)’s corpus , which is itself built on a corpus by Shigeto et al. (2013). Regarding MWE annotations, Shigeto et al. (2013) first constructed an MWE dictionary by extract-ing functional MWEs from the English-language Wiktionary [Cite_Footnote_6] , and classified their occurrences in Ontonotes into either MWE or literal usage. Kato et al. (2016) integrated these MWE annotations into phrase structures and established functional MWEs as subtrees.",Material,DataSource,True,Extend（引用目的）,True,P17-2068_2_0,2017,English Multiword Expression-aware Dependency Parsing Including Named Entities,Footnote
1142,11146," http://members.unine.ch/jacques.savoy/clef/englishST.txt"," ['3 Models for MWE identification and MWE-aware dependency parsing', '3.1 Pipeline Model']","Meanwhile, we create a dictionary of NEs from a title list of English Wikipedia articles, excepting stop words, provided by UniNE [Cite_Footnote_9] .",9 http://members.unine.ch/jacques.savoy/clef/englishST.txt,"The pipeline model involves the following three steps. First, BIO tags encoding MWE-spans and MWE POS tags, such as “B NNP” and “I DT” are predicted by a sequential labeler based on Con-ditional Random Fields (CRFs) (Lafferty et al., 2001). Second, tokens belonging to each pre-dicted MWE-span are concatenated into a sin-gle node. Finally, an MWE-based dependency structure (Figure 1b) is predicted by an arc-eager transition-based parser. For the CRFs, in addi-tion to word-form and character-based features, we use 1- to 3-gram features based on dictionaries of functional MWEs and NEs within 5-word win-dows from a target token. For a dictionary of func-tional MWEs, we use the dictionary by Shigeto et al. (2013) (Section 2). Meanwhile, we create a dictionary of NEs from a title list of English Wikipedia articles, excepting stop words, provided by UniNE [Cite_Footnote_9] . Regarding parsing features, we use baseline features and rich non-local features pro-posed by Zhang and Nivre (2011).",Material,Knowledge,True,Use（引用目的）,True,P17-2068_3_0,2017,English Multiword Expression-aware Dependency Parsing Including Named Entities,Footnote
1143,11147," http://www.aclweb.org/anthology/N10-1089"," ['6 Related Work']",Korkontzelos and Manandhar (2010) [Cite_Ref] reports an improvement in base-phrase chunking by pre-grouping MWEs as words-with-spaces.,"Ioannis Korkontzelos and Suresh Manandhar. 2010. Can recognising multiword expressions improve shallow parsing? In Human Language Technolo-gies: The 2010 Annual Conference of the North American Chapter of the Association for Compu-tational Linguistics. Association for Computational Linguistics, Los Angeles, California, pages 636– 644. http://www.aclweb.org/anthology/N10-1089.","Korkontzelos and Manandhar (2010) [Cite_Ref] reports an improvement in base-phrase chunking by pre-grouping MWEs as words-with-spaces. They fo-cus on compound nouns, adjective-noun construc-tions, and named entities. However, they use gold MWE-spans, and this is not a realistic setting. By contrast, we use predicted MWE-spans.",補足資料,Paper,True,Introduce（引用目的）,True,P17-2068_7_0,2017,English Multiword Expression-aware Dependency Parsing Including Named Entities,Reference
1144,11148," https://www.github.com/"," ['3 A Bayesian Approach to Information 3.2 Bayesian Mutual Information', 'Proof. See App. D. 3.4.2 No Data-Processing Inequality']",We then expose the probe 5 Experiments and Results [Cite_Footnote_8] agent to increasingly larger sets of data from the 5.1 Data and Representations task.,"8 Our code is available in https://www.github.com/ 10 L2 weight decay regularisation is equivalent to a Gaussian rycolab/bayesian-mi. prior on the parameter space (pg. 350, Bishop, 1995).","MI learning curves. In this regard, our analysis As previously discussed, the Gaussian and is similar to the learning curves used by Talmor Dirichlet priors on the parameters will cause these et al. (2020) and the complexity–accuracy trade- models to initially place a uniform distribution on offs from Pimentel et al. (2020a). the output classes—as such, they will have an ini-tial Bayesian MI of zero. We then expose the probe 5 Experiments and Results [Cite_Footnote_8] agent to increasingly larger sets of data from the 5.1 Data and Representations task. Unfortunately, the posterior of eq. (28) has no closed form solution, so we approximate it with We focus on part-of-speech (POS) tagging and dependency-arc labelling in our experiments. the maximum-a-posteriori probability p θ (t | r, θ ), With this in mind, we make use of the universal where θ ∗ = argmax θ∈Θ p θ (θ | d n ). We ob-tain this MAP estimate using the gradient descent dependencies (UD 2.6; Zeman et al., 2020); method AdamW (Loshchilov and Hutter, 2019) analysing the treebanks of four typologically with a cross-entropy loss and L2 norm regularisa-diverse languages, namely: Basque, English, tion. 10 The posterior predictive belief of eq. (29) Marathi, and Turkish. As our object of analysis, has a closed-form solution 11 we look at the contextual representations from ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019) and BERT (Devlin et al., 2019), using as a count(d N , t) + 1",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.229_0_0,2021,A Bayesian Framework for Information-Theoretic Probing,Footnote
1145,11149," https://about.Twitter.com/company"," ['1 Introduction']","For example, about 500 million tweets are published per day on Twitter [Cite_Footnote_1] , one of the most popular online social networking services.",1 See https://about.Twitter.com/company,"In recent years, short texts are increasingly preva-lent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter [Cite_Footnote_1] , one of the most popular online social networking services. Proba-bilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, s-ince the low-dimensional semantic representation is crucial for many applications, such as prod-uct recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis (Si et al., 2013). However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts.",補足資料,Website,True,Introduce（引用目的）,True,P15-2080_0_0,2015,User Based Aggregation for Biterm Topic Model,Footnote
1146,11150," http://scikit-learn.org/stable/"," ['4 Experiments', '4.2 Document Representation']","We classify these selected tweets by Random Forest classifier (Breiman, 2001) implemented in sklearn [Cite_Footnote_2] python module with 10-fold cross valida-tion.",2 See http://scikit-learn.org/stable/,"We classify these selected tweets by Random Forest classifier (Breiman, 2001) implemented in sklearn [Cite_Footnote_2] python module with 10-fold cross valida-tion. Using accuracy as the evaluation metric, we report the classification performance of different topic models in Figure 2. With the increase of the topic number K, all the models’ accuracies are tending to increase. BTM is worse than all other models, which confirms the effectiveness of user based aggregation. Twitter-BTM and BTM-U always outperform LDA-U, Twitter-LDA and TwitterUB-LDA. Twitter-BTM’s accuracy is a lit-tle higher than BTM-U, which demonstrates that the background topic is helpful to capture more accurate topic representation of documents.",Method,Tool,False,Introduce（引用目的）,True,P15-2080_1_0,2015,User Based Aggregation for Biterm Topic Model,Footnote
1147,11151," http://www.marekrei.com/projects/mltagger"," ['5 Implementation Details']",The code used for performing these experi-ments is made available online. [Cite_Footnote_1],1 http://www.marekrei.com/projects/mltagger,The code used for performing these experi-ments is made available online. [Cite_Footnote_1],Method,Code,True,Produce（引用目的）,True,N18-1027_0_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Footnote
1148,11152," https://doi.org/10.1038/nn.3331"," ['5 Implementation Details']","The model was implemented using Tensorflow (Abadi et al., 2016) [Cite_Ref] .","Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Viegas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: Large-Scale Machine Learning on Heterogeneous Dis-tributed Systems. Arxiv preprint arXiv:1603.04467 https://doi.org/10.1038/nn.3331.","The model was implemented using Tensorflow (Abadi et al., 2016) [Cite_Ref] . The network weights were randomly initialized using the uniform Glorot initialization method (Glorot and Bengio, 2010) and optimization was performed using AdaDelta (Zeiler, 2012) with learning rate 1.0. Dropout (Srivastava et al., 2014) with probability 0.5 was applied to word representations w i and the com-posed representations h i after the LSTMs. The training was performed in batches of 32 sentences. Sentence-level performance was observed on the development data and the training was stopped if performance did not improve for 7 epochs. The best overall model on the development set was then used to report performance on the test data, both for sentence classification and sequence la-beling. In order to avoid random outliers, we per-formed each experiment with 5 random seeds and report here the averaged results.",Method,Tool,True,Use（引用目的）,True,N18-1027_1_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1149,11153," https://doi.org/10.1146/annurev.neuro.26.041002.131047"," ['2 Network Architecture']","Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015) [Cite_Ref] , sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rocktäschel et al., 2016), and error cor-rection (Ji et al., 2017).","Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Rep-resentations. https://doi.org/10.1146/annurev.neuro.26.041002.131047.","The main system takes as input a sentence, sep-arated into tokens, and outputs a binary predic-tion as the label of the sentence. We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015) [Cite_Ref] , sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rocktäschel et al., 2016), and error cor-rection (Ji et al., 2017). In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1027_3_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1150,11154," https://doi.org/10.1146/annurev.neuro.26.041002.131047"," ['2 Network Architecture']","For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015) [Cite_Ref] ; in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015).","Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Rep-resentations. https://doi.org/10.1146/annurev.neuro.26.041002.131047.","Next, we include an attention mechanism that allows the network to dynamically control how much each word position contributes to the com-bined representation. In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015) [Cite_Ref] ; in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015). In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value. This method is equivalente to calculating the attention weights in reference to a fixed weight vector, which is optimized during training. Shen and Lee (2016) proposed an architecture for dialogue act detec-tion where the attention values are found based on a separate set of word embeddings. We found that the method described above was consistently equivalent or better in development experiments, while requiring a smaller number of parameters.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1027_3_1,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1151,11155," https://doi.org/10.1109/72.410363"," ['2 Network Architecture']","For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015); in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015) [Cite_Ref] .","Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom. 2015. Teaching Ma-chines to Read and Comprehend. In Advances in Neural Information Processing Systems (NIPS 2015). pages 1–14. https://doi.org/10.1109/72.410363.","Next, we include an attention mechanism that allows the network to dynamically control how much each word position contributes to the com-bined representation. In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015); in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015) [Cite_Ref] . In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value. This method is equivalente to calculating the attention weights in reference to a fixed weight vector, which is optimized during training. Shen and Lee (2016) proposed an architecture for dialogue act detec-tion where the attention values are found based on a separate set of word embeddings. We found that the method described above was consistently equivalent or better in development experiments, while requiring a smaller number of parameters.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1027_7_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1152,11156," https://doi.org/10.1.1.56.7752"," ['2 Network Architecture']","We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) [Cite_Ref] architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations.",Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-term Memory. Neural Computation 9. https://doi.org/10.1.1.56.7752.,"The main system takes as input a sentence, sep-arated into tokens, and outputs a binary predic-tion as the label of the sentence. We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) [Cite_Ref] architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015), sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rocktäschel et al., 2016), and error cor-rection (Ji et al., 2017). In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1027_8_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1153,11157," https://doi.org/10.18653/v1/P17-1070"," ['2 Network Architecture']","Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015), sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rocktäschel et al., 2016), and error cor-rection (Ji et al., 2017) [Cite_Ref] .","Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen Gong, Steven Truong, and Jianfeng Gao. 2017. A Nested Attention Neural Hybrid Model for Gram-matical Error Correction. In ACL 2017. pages 753–762. https://doi.org/10.18653/v1/P17-1070.","The main system takes as input a sentence, sep-arated into tokens, and outputs a binary predic-tion as the label of the sentence. We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015), sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rocktäschel et al., 2016), and error cor-rection (Ji et al., 2017) [Cite_Ref] . In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.",補足資料,Paper,True,Compare（引用目的）,False,N18-1027_9_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1154,11158," https://doi.org/10.18653/v1/P17-1161"," ['1 Introduction']","Recent state-of-the-art models make use of bidirectional LSTM architectures (Ir-soy and Cardie, 2014), character-based represen-tations (Lample et al., 2016), and additional ex-ternal features (Peters et al., 2017) [Cite_Ref] .","Matthew E. Peters, Waleed Ammar, Chandra Bhaga-vatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language mod-els. In ACL 2017. https://doi.org/10.18653/v1/P17-1161.","Sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence. Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detec-tion, are often formulated as variations of se-quence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Ir-soy and Cardie, 2014), character-based represen-tations (Lample et al., 2016), and additional ex-ternal features (Peters et al., 2017) [Cite_Ref] . Optimiza-tion of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1027_12_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1155,11159," https://doi.org/10.1186/1471-2105-9-S11-S9"," ['4 Datasets', '4.1 CoNLL 2010 Uncertainty Detection']",Vincze et al. (2008) [Cite_Ref] showed that 19.44% of sentences in the biomedi-cal papers of the BioScope corpus contain hedge cues.,"Veronika Vincze, György Szarvas, Richárd Farkas, György Móra, and János Csirik. 2008. The Bio-Scope corpus: biomedical texts annotated for un-certainty, negation and their scopes. BMC Bioin-formatics 9(Suppl 11):S9. https://doi.org/10.1186/1471-2105-9-S11-S9.","The CoNLL 2010 shared task (Farkas et al., 2010) investigated the detection of uncertainty in nat-ural language texts. The use of uncertain lan-guage (also known as hedging) is a common tool in scientific writing, allowing scientists to guide research beyond the evidence without overstating what follows from their work. Vincze et al. (2008) [Cite_Ref] showed that 19.44% of sentences in the biomedi-cal papers of the BioScope corpus contain hedge cues. Automatic detection of these cues is impor-tant for downstream tasks such as information ex-traction and literature curation, as typically only definite information should be extracted and cu-rated.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1027_18_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1156,11160," https://doi.org/10.1007/978-3-319-10590-1_53"," ['3 Alternative Methods', '3.1 Labeling Through Backpropagation']","Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014) [Cite_Ref] .",Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and Understanding Convolutional Networks. Com-puter VisionECCV 2014 8689. https://doi.org/10.1007/978-3-319-10590-1_53.,"We experiment with an alternative method for inducing token-level labels, based on visualiza-tion methods using gradient analysis. Research in computer vision has shown that interpretable visualizations of convolutional networks can be obtained by analyzing the gradient after a single backpropagation pass through the network (Zeiler and Fergus, 2014) [Cite_Ref] . Denil et al. (2014) extended this approach to natural language processing, in order to find and visualize the most important sen-tences in a text. Recent work has also used the gradient-based approach for visualizing the deci-sions of text classification models on the token level (Li et al., 2016; Alikaniotis et al., 2016). In this section we propose an adaptation that can be used for sequence labeling tasks.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1027_21_0,2018,Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens,Reference
1157,11161," https://research-lab.yahoo.co.jp/en/software/"," ['3 Experiments', '3.1 Datasets']","While redistribution of tweets is prohibited, we are planning to publi-cize tweet IDs of this dataset for reproducibility. [Cite_Footnote_2]",2 The tweet IDs will be provided from https://research-lab.yahoo.co.jp/en/software/.,"The dialog dataset contains about 22.3 million tweet-reply pairs extracted from Twitter Firehose data. In its preprocessing, we filtered out spam and bot posts by using user-level signals such as the follower count, the friend count, the favorite count, and whether a profile image is set or not. Also, we replaced all the URLs in the text with “[u]” and all the user mentions with “[m]”, consid-ering them as noise. The rest of the text was used as it was. On average, source and target (or re-ply) tweets after preprocessing were 31.5 and 27.8 characters long, respectively. While redistribution of tweets is prohibited, we are planning to publi-cize tweet IDs of this dataset for reproducibility. [Cite_Footnote_2]",Method,Tool,True,Introduce（引用目的）,True,P18-2121_0_0,2018,Pretraining Sentiment Classifiers with Unlabeled Dialog Data,Footnote
1158,11162," http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf"," ['1 Introduction']","To overcome this problem, Dai and Le (2015) [Cite_Ref] recently proposed a semi-supervised sequence learning framework, where a sentiment classifier based on recurrent neural networks (RNNs) is trained with labeled data after initializing it with the parameters of an RNN-based language model pretrained with a large amount of unlabeled data.","Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079–3087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.","To overcome this problem, Dai and Le (2015) [Cite_Ref] recently proposed a semi-supervised sequence learning framework, where a sentiment classifier based on recurrent neural networks (RNNs) is trained with labeled data after initializing it with the parameters of an RNN-based language model pretrained with a large amount of unlabeled data.",補足資料,Paper,True,Introduce（引用目的）,True,P18-2121_8_0,2018,Pretraining Sentiment Classifiers with Unlabeled Dialog Data,Reference
1159,11163," http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf"," ['2 Proposed Method']","Note that the encoder of the classifier is fine-tuned with labeled data, as in (Dai and Le, 2015) [Cite_Ref] .","Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079–3087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.","Our classifier forms an encoder-labeler struc-ture, which consists of the above encoder and a la-beler that predicts a sentiment label from the con-text. Note that the encoder of the classifier is fine-tuned with labeled data, as in (Dai and Le, 2015) [Cite_Ref] . The main difference between their approach and ours is that we examine paired (dialog) data for pretraining, while they only showed the usefulness of pretraining with unpaired data.",補足資料,Paper,True,Introduce（引用目的）,True,P18-2121_8_1,2018,Pretraining Sentiment Classifiers with Unlabeled Dialog Data,Reference
1160,11164," http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf"," ['3 Experiments', '3.3 Compared Models']","• Lang, SeqAE: Pretrained with the language model and autoencoder model proposed in (Dai and Le, 2015) [Cite_Ref] .","Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079–3087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.","• Lang, SeqAE: Pretrained with the language model and autoencoder model proposed in (Dai and Le, 2015) [Cite_Ref] . The language model is the de-coder part of the encoder-decoder model using a zero vector as the initial hidden layer value, and the autoencoder model is the same structure of the encoder-decoder model, where input and output are the same. To make the comparison as fair as possible, we used the reply-side of the dialog dataset for pretraining Lang and SeqAE so that the same supervision information on the basis of the same tweet-reply pairs would be ap-plied to Lang, SeqAE, and Dial. The num-ber of their pretraining epochs was also equal to that of Dial.",補足資料,Paper,True,Introduce（引用目的）,True,P18-2121_8_2,2018,Pretraining Sentiment Classifiers with Unlabeled Dialog Data,Reference
1161,11165," http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf"," ['4 Related Work']","After Dai and Le (2015) [Cite_Ref] proposed the framework of semi-supervised sequence learning, there have been several attempts to extend sequence learn-ing models for different tasks to semi-supervised settings.","Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079–3087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.","After Dai and Le (2015) [Cite_Ref] proposed the framework of semi-supervised sequence learning, there have been several attempts to extend sequence learn-ing models for different tasks to semi-supervised settings. Cheng et al. (2016) and Ramachandran et al. (2017) studied semi-supervised training of machine translation models via an autoencoder model and language model, respectively. They also used paired data (parallel corpora), but un-supervised training was conducted with reason-able monolingual corpora to compensate for costly parallel corpora, which is opposite to our set-ting. Zhou et al. (2016a,b) proposed to use par-allel corpora for adapting the sentiment resources in a resource-rich language to a resource-poor lan-guage. Their purpose was completely different from ours, since making parallel corpora is also costly. The other studies include semi-supervised extensions for predicting the property values of Wikipedia (Hewlett et al., 2017), detecting medi-cal conditions from heart rate data (Ballinger et al., 2018), and morphological reinflection of inflected words (e.g., “playing” to “played”). They did not use paired-text data to leverage their tasks.",補足資料,Paper,True,Introduce（引用目的）,True,P18-2121_8_3,2018,Pretraining Sentiment Classifiers with Unlabeled Dialog Data,Reference
1162,11166," https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf"," ['4 Related Work']","There have been many studies about distant supervision for sentiment analysis (Read, 2005; Go et al., 2009 [Cite_Ref] ; Davidov et al., 2010; Purver and Battersby, 2012; Mohammad et al., 2013; Tang et al., 2014; dos Santos and Gatti, 2014; Severyn and Moschitti, 2015; Deriu et al., 2016; Müller et al., 2017), but they basically fo-cused on how to use emoticons and hashtags to leverage performance.","Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification using Dis-tant Supervision. Technical report, Stan-ford Digital Library Technologies Project. https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf.","Our method can be regarded as a general ver-sion of distant supervision since we assume that a reply includes the label information of the cor-responding tweet. There have been many studies about distant supervision for sentiment analysis (Read, 2005; Go et al., 2009 [Cite_Ref] ; Davidov et al., 2010; Purver and Battersby, 2012; Mohammad et al., 2013; Tang et al., 2014; dos Santos and Gatti, 2014; Severyn and Moschitti, 2015; Deriu et al., 2016; Müller et al., 2017), but they basically fo-cused on how to use emoticons and hashtags to leverage performance. One exception is the study by (Pool and Nissim, 2016), in which Facebook reactions were used for distant supervision. Their approach is similar to ours using tweet-reply pairs, but our method is more general since they only used six reply categories (i.e., like, love, haha, wow, sad, and angry), not text replies.",補足資料,Paper,True,Introduce（引用目的）,True,P18-2121_12_0,2018,Pretraining Sentiment Classifiers with Unlabeled Dialog Data,Reference
1163,11167," http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"," ['2 Proposed Method']","The encoder-decoder model is a conditional lan-guage model that predicts a correct output se-quence from an input sequence (Sutskever et al., 2014) [Cite_Ref] .","Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural In-formation Processing Systems 27 (NIPS 2014), Curran Associates, Inc., pages 3104–3112. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.","The encoder-decoder model is a conditional lan-guage model that predicts a correct output se-quence from an input sequence (Sutskever et al., 2014) [Cite_Ref] . This model consists of two RNNs: an en-coder and decoder. The encoder extracts a context of the input sequence as a real-valued vector, and the decoder predicts the output sequences from the context individually.",補足資料,Paper,True,Introduce（引用目的）,True,P18-2121_24_0,2018,Pretraining Sentiment Classifiers with Unlabeled Dialog Data,Reference
1164,11168," http://u.cs.biu.ac.il/~nlp/resources/downloads/"," ['2 Experiment Setup']","We ob-serve similar trends to previously published results, and make the dataset splits available for replication. [Cite_Footnote_1]",1 http://u.cs.biu.ac.il/~nlp/resources/downloads/,"Due to various differences (e.g. corpora, train/test splits), we do not list previously reported results, but apply a large space of state-of-the-art supervised methods and review them comparatively. We ob-serve similar trends to previously published results, and make the dataset splits available for replication. [Cite_Footnote_1]",Material,Dataset,True,Compare（引用目的）,True,N15-1098_0_0,2015,Do Supervised Distributional Methods Really Learn Lexical Inference Relations?,Footnote
1165,11169," http://bitbucket.org/yoavgo/word2vecf"," ['2 Experiment Setup', '2.1 Word Representations', '2.1.2 Representation Models']","SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014). [Cite_Footnote_3]",3 http://bitbucket.org/yoavgo/word2vecf,"SVD We reduced M’s dimensionality to k = 500 using Singular Value Decomposition (SVD). SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 nega-tive samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014). [Cite_Footnote_3]",Method,Tool,True,Use（引用目的）,True,N15-1098_1_0,2015,Do Supervised Distributional Methods Really Learn Lexical Inference Relations?,Footnote
1166,11170," http://jobimtext.org"," ['3 Negative Results']","In addition to our setup, these results were also reproduced in preliminary exper-iments by applying the JoBimText framework [Cite_Footnote_4] for scalable distributional thesauri (Biemann and Riedl, 2013) using Google’s syntactic N-grams (Goldberg and Orwant, 2013) as a corpus.",4 http://jobimtext.org,"Based on the above setup, we present three nega-tive empirical results, which challenge the claim that the methods presented in §2.3 are learning a rela-tion between x and y. In addition to our setup, these results were also reproduced in preliminary exper-iments by applying the JoBimText framework [Cite_Footnote_4] for scalable distributional thesauri (Biemann and Riedl, 2013) using Google’s syntactic N-grams (Goldberg and Orwant, 2013) as a corpus.",Method,Code,False,Use（引用目的）,False,N15-1098_2_0,2015,Do Supervised Distributional Methods Really Learn Lexical Inference Relations?,Footnote
1167,11171," http://people.csail.mit.edu/jacobe/naacl09.html"," ['1 Introduction']",Source code is available at [Cite] http://people.,,Source code is available at [Cite] http://people.csail.mit.edu/jacobe/naacl09.html.,Method,Code,True,Produce（引用目的）,True,N09-1040_0_0,2009,Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion,Body
1168,11172," http://onlinebooks.library.upenn.edu"," ['5 Experimental Setup']","Corpora The dataset for evaluation is drawn from a medical textbook (Walker et al., 1990). [Cite_Footnote_2]","2 The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. Malioutov, and Masao Utiyama for making their topic segmentation systems publicly available, and to the anonymous reviewers for useful feedback. This research is supported by the Beckman Postdoctoral Fellowship.","Corpora The dataset for evaluation is drawn from a medical textbook (Walker et al., 1990). [Cite_Footnote_2] The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections. Evaluation is performed sep-arately on each of the twelve parts, with the task of correctly identifying the chapter and section bound-aries. Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries.",Material,DataSource,True,Extend（引用目的）,True,N09-1040_1_0,2009,Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion,Footnote
1169,11173," http://www.freelang.net/dictionary"," ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']","In addition, we used large Eng-lish-Italian and English-Portuguese bilingual lexi-cons available from FreeLang site ( [Cite] http://www.freelang.net/dictionary) as well as an English-Chinese bilingual word list available from LDC (Linguistic Data Consortium).",,"In order to translate the English semantic lexi-cons into other languages, we needed a bilingual lexicon for each of the target languages, Italian, Chinese and Portuguese in our particular case. For this purpose, we first used two corpus-based fre-quency dictionaries compiled for Chinese (Xiao et al., 2009) and Portuguese (Davies and Preto-Bay, 2007), which cover the 5,000 most frequent Chi-nese and Portuguese words respectively. These dictionaries provided high-quality manually edited word translations. In addition, we used large Eng-lish-Italian and English-Portuguese bilingual lexi-cons available from FreeLang site ( [Cite] http://www.freelang.net/dictionary) as well as an English-Chinese bilingual word list available from LDC (Linguistic Data Consortium). Compiled without professional editing, these bilingual word lists contain errors and inaccurate translations, and hence they introduced noise into the mapping pro-cess. However, they provided wider lexical cover-age of the languages involved and complemented the limited sizes of the high-quality dictionaries used in our experiment. Table 1 lists the bilingual lexical resources employed for translating the Eng-lish lexicons into each of the three languages in-volved in our experiment.",Material,Knowledge,True,Use（引用目的）,True,N15-1137_0_0,2015,Development of the Multilingual Semantic Annotation System,Body
1170,11174," http://ucrel.lancs.ac.uk/claws7tags.html"," ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']",USAS semantic categories [Cite_Footnote_1] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1,"1 For definitions of the POS and semantic tags, see websites http://ucrel.lancs.ac.uk/claws7tags.html and http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf",USAS semantic categories [Cite_Footnote_1] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1,Material,Knowledge,True,Introduce（引用目的）,True,N15-1137_1_0,2015,Development of the Multilingual Semantic Annotation System,Footnote
1171,11175," http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf"," ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']",USAS semantic categories [Cite_Footnote_1] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1,"1 For definitions of the POS and semantic tags, see websites http://ucrel.lancs.ac.uk/claws7tags.html and http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf",USAS semantic categories [Cite_Footnote_1] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1,Material,Knowledge,True,Introduce（引用目的）,True,N15-1137_2_0,2015,Development of the Multilingual Semantic Annotation System,Footnote
1172,11176," http://ucrel.lancs.ac.uk/usas/"," ['6 Conclusion and Future Work']","In this paper, we have investigated the feasibility of rapidly bootstrapping semantic annotation tools for new target languages [Cite_Footnote_2] by mapping an existing semantic lexicon and software architecture.",2 The results are available at http://ucrel.lancs.ac.uk/usas/,"In this paper, we have investigated the feasibility of rapidly bootstrapping semantic annotation tools for new target languages [Cite_Footnote_2] by mapping an existing semantic lexicon and software architecture. In par-ticular, we tested the possibility of automatically translating existing English semantic lexicons into other languages, Italian, Chinese and Brazilian Portuguese in this particular case. Our experiment demonstrates that, if appropriate high-quality bi-lingual lexicons are available, it is feasible to rap-idly generating prototype systems with a good lexical coverage with our automatic approach. On the other hand, our experiment also shows that, in order to achieve a high precision, paral-lel/comparable corpus based disambiguation is needed for identifying precise translation equiva-lents, and a certain amount of manual cleaning and improvement of the automatically generated se-mantic lexicons is indispensible. We are continu-ing to improve the multilingual semantic taggers and extend them to cover more languages, such as Spanish and Dutch, aiming to develop a large-scale multilingual semantic annotation and analysis sys-tem. We also intend to perform task-based evalua-tion of the manually checked versus automatically generated lexicons.",補足資料,Document,True,Produce（引用目的）,True,N15-1137_3_0,2015,Development of the Multilingual Semantic Annotation System,Footnote
1173,11177," http://www.cs.ucsb.edu/~william/"," ['4 Information About the Presenters']",[Cite] http://www.cs.ucsb.edu/˜william/,,"William Wang is an Assistant Professor at the De-partment of Computer Science, University of Cali-fornia, Santa Barbara. He received his PhD from School of Computer Science, Carnegie Mellon Uni-versity. He focuses on information extraction and he is the faculty author of DeepPath—the first deep re-inforcement learning system for multi-hop reasoning. He has published more than 50 papers at leading con-ferences and journals including ACL, EMNLP, NAACL, CVPR, COLING, IJCAI, CIKM, ICWSM, SIGDIAL, IJCNLP, INTERSPEECH, ICASSP, ASRU, SLT, Ma-chine Learning, and Computer Speech & Language, and he has received paper awards and honors from CIKM, ASRU, and EMNLP. Website: [Cite] http://www.cs.ucsb.edu/˜william/",補足資料,Website,True,Introduce（引用目的）,True,P18-5007_0_0,2018,Deep Reinforcement Learning for NLP,Body
1174,11178," https://web.stanford.edu/~jiweil/"," ['4 Information About the Presenters']",Web-site: [Cite] https://web.stanford.edu/˜jiweil/,,"Jiwei Li recently spent three years and received his PhD in Computer Science from Stanford University. His research interests are deep learning and dialogue. He is the most prolific NLP/ML first author during 2012-2016, and the lead author of the first study in deep reinforcement learning for dialogue generation. He is the recipient of a Facebook Fellowship in 2015. Web-site: [Cite] https://web.stanford.edu/˜jiweil/ Xiaodong He is the Deputy Managing Director of JD AI Research and Head of the Deep learning, NLP and Speech Lab, and a Technical Vice President of JD.com. He is also an Affiliate Professor at the Uni-versity of Washington (Seattle), serves in doctoral su-pervisory committees. Before joining JD.com, He was with Microsoft for about 15 years, served as Princi-pal Researcher and Research Manager of the DLTC at Microsoft Research, Redmond. His research inter-ests are mainly in artificial intelligence areas includ-ing deep learning, natural language, computer vision, speech, information retrieval, and knowledge represen-tation. He has published more than 100 papers in ACL, EMNLP, NAACL, CVPR, SIGIR, WWW, CIKM, NIPS, ICLR, ICASSP, Proc. IEEE, IEEE TASLP, IEEE SPM, and other venues. He received several awards including the Outstanding Paper Award at ACL 2015. Website: http://air.jd.com/people2.html",補足資料,Website,True,Introduce（引用目的）,True,P18-5007_1_0,2018,Deep Reinforcement Learning for NLP,Body
1175,11179," http://air.jd.com/people2.html"," ['4 Information About the Presenters']",Website: [Cite] http://air.jd.com/people2.html,,"Jiwei Li recently spent three years and received his PhD in Computer Science from Stanford University. His research interests are deep learning and dialogue. He is the most prolific NLP/ML first author during 2012-2016, and the lead author of the first study in deep reinforcement learning for dialogue generation. He is the recipient of a Facebook Fellowship in 2015. Web-site: https://web.stanford.edu/˜jiweil/ Xiaodong He is the Deputy Managing Director of JD AI Research and Head of the Deep learning, NLP and Speech Lab, and a Technical Vice President of JD.com. He is also an Affiliate Professor at the Uni-versity of Washington (Seattle), serves in doctoral su-pervisory committees. Before joining JD.com, He was with Microsoft for about 15 years, served as Princi-pal Researcher and Research Manager of the DLTC at Microsoft Research, Redmond. His research inter-ests are mainly in artificial intelligence areas includ-ing deep learning, natural language, computer vision, speech, information retrieval, and knowledge represen-tation. He has published more than 100 papers in ACL, EMNLP, NAACL, CVPR, SIGIR, WWW, CIKM, NIPS, ICLR, ICASSP, Proc. IEEE, IEEE TASLP, IEEE SPM, and other venues. He received several awards including the Outstanding Paper Award at ACL 2015. Website: [Cite] http://air.jd.com/people2.html",補足資料,Website,True,Introduce（引用目的）,True,P18-5007_2_0,2018,Deep Reinforcement Learning for NLP,Body
1176,11180," http://www.elsevier.com"," ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']",Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website [Cite_Footnote_2] .,2 http://www.elsevier.com,"Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website [Cite_Footnote_2] . The documents were taken from the medical domain within the sub-domain of “breast cancer”. We have auto-matically selected the documents published between 2001 and 2008 where the title or the keywords contain the term cancer du sein in French and breast cancer in English. We col-lected 130 French documents (about 530,000 words) and 1,640 English documents (about",Material,DataSource,True,Introduce（引用目的）,True,P14-1121_0_0,2014,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,Footnote
1177,11181," http://www.ncbi.nlm.nih.gov/pubmed/"," ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']","The English part has been extracted from the medical website PubMed [Cite_Footnote_3] using the keywords: dia-betes, nutrition and feeding.",3 http://www.ncbi.nlm.nih.gov/pubmed/,"Diabetes corpus The documents making up the French part of the comparable corpus have been craweled from the web using three keywords: diabète (diabetes), alimentation (food), and obésité (obesity). After a man-ual selection, we only kept the documents which were relative to the medical domain. As a result, 65 French documents were ex-tracted (about 257,000 words). The English part has been extracted from the medical website PubMed [Cite_Footnote_3] using the keywords: dia-betes, nutrition and feeding. We only kept the free fulltext available documents. As a re-sult, 2,339 English documents were extracted (about 3,5 million words). We also split the English documents into 14 parts each con-taining about 250,000 words.",Material,DataSource,True,Use（引用目的）,True,P14-1121_1_0,2014,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,Footnote
1178,11182," http://code.google.com/p/ttc-project"," ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']",These steps were car-ried out using the TTC TermSuite [Cite_Footnote_4] that applies the same method to several languages including French and English.,4 http://code.google.com/p/ttc-project,"The French and English documents were then normalised through the following linguistic pre-processing steps: tokenisation, part-of-speech tag-ging, and lemmatisation. These steps were car-ried out using the TTC TermSuite [Cite_Footnote_4] that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded. Table 3 shows the number of dis-tinct words (# words) after these steps. It also indicates the comparability degree in percentage (comp.) between the French part and each English part of each comparable corpus. The comparabil-ity measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable. We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora. In the remainder of this article, [breast cancer corpus i] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i (i ∈ [1, 14]).",Method,Tool,True,Use（引用目的）,True,P14-1121_2_0,2014,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,Footnote
1179,11183," http://www.elra.info/"," ['3 Linguistic Resources', '3.2 Bilingual Dictionary']",ELRA-M0033 available from the ELRA catalogue [Cite_Footnote_5] .,5 http://www.elra.info/,The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M0033 available from the ELRA catalogue [Cite_Footnote_5] . This re-source is a general language dictionary which con-tains only a few terms related to the medical do-main.,Material,DataSource,True,Introduce（引用目的）,True,P14-1121_3_0,2014,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,Footnote
1180,11184," http://www.nlm.nih.gov/research/umls"," ['3 Linguistic Resources', '3.3 Terminology Reference Lists']",We selected all French/English single words from the UMLS [Cite_Footnote_6] meta-thesaurus.,6 http://www.nlm.nih.gov/research/umls,"To evaluate the quality of terminology extrac-tion, we built a bilingual terminology reference list for each comparable corpus. We selected all French/English single words from the UMLS [Cite_Footnote_6] meta-thesaurus. We kept only i) the French sin-gle words which occur more than four times in the French part and ii) the English single words which occur more than four times in each English part i . As a result of filtering, 169 French/English single words were extracted for the breast can-cer corpus and 244 French/English single words were extracted for the diabetes corpus. It should be noted that the evaluation of terminology ex-traction using specialized comparable corpora of-ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2007), 125 and 79 in Bouamor et al. (2013).",Material,DataSource,True,Use（引用目的）,True,P14-1121_4_0,2014,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,Footnote
1181,11185," http://L2R.cs.uiuc.edu/~cogcomp/data.php"," ['4 Corpus Annotation']","For our corpus, we selected 1,000 sentences con-taining at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manu-ally annotated them with comma information [Cite_Footnote_3] .",3 The guidelines and annotations are available at http://L2R.cs.uiuc.edu/˜cogcomp/data.php. 19: Remove all examples from p that are covered by r 20: end for 21: end for,"For our corpus, we selected 1,000 sentences con-taining at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manu-ally annotated them with comma information [Cite_Footnote_3] . This annotated corpus served as both training and test datasets (using cross-validation).",Mixed,Mixed,True,Produce（引用目的）,True,P08-1117_0_0,2008,Extraction of Entailed Semantic Relations Through Syntax-based Comma Resolution,Footnote
1182,11186," http://L2R.cs.uiuc.edu/~cogcomp/demos.php"," ['6 Evaluation', '6.1 Experimental Setup']","On the CoNLL-03 shared task, its f-score is about 90% [Cite_Footnote_4] .",4 A web demo of the NER is at http://L2R.cs.uiuc.edu/˜cogcomp/demos.php.,"In gold standard parse trees the syntactic cate-gories are annotated with functional tags. Since cur-rent statistical parsers do not annotate sentences with such tags, we augment the syntactic trees with the output of a Named Entity tagger. For the Named Entity information, we used a publicly available NE Recognizer capable of recognizing a range of cat-egories including Person, Location and Organiza-tion. On the CoNLL-03 shared task, its f-score is about 90% [Cite_Footnote_4] . We evaluate our system from different points of view, as described below. For all the eval-uation methods, we performed five-fold cross vali-dation and report the average precision, recall and f-scores.",補足資料,Website,False,Produce（引用目的）,True,P08-1117_1_0,2008,Extraction of Entailed Semantic Relations Through Syntax-based Comma Resolution,Footnote
1183,11187," https://github.com/monologg/KoBERT-Transformers"," ['3 Backchannel Prediction Model', '3.3 Experimental Setup']","The pre-training language model used in BPM was KoBERT [Cite_Footnote_2] , and ReLU was used as the activa-tion function for each hidden layer; the dropout rate was 0.3.",2 https://github.com/monologg/KoBERT-Transformers,"The pre-training language model used in BPM was KoBERT [Cite_Footnote_2] , and ReLU was used as the activa-tion function for each hidden layer; the dropout rate was 0.3. The batch size was 64, and the number of epochs was 60. Optimization was performed us-ing SGD as the parameter of the Transformers and Adam for the other parameters. The learning rate was 0.0005. The data were divided into training, validation, and test sets at a ratio of 3:1:1. The best model 3 was saved by early stopping regularization based on the validation result.",Material,Knowledge,False,Use（引用目的）,True,2021.emnlp-main.277_0_0,2021,BPM_MT: Enhanced Backchannel Prediction Model using Multi-Task Learning,Footnote
1184,11188," https://code.google.com/p/word2vec/"," ['5 Experiments', '5.2 Training Details and Implementation']","As an example, this amounts to [Cite_Footnote_1] minute per epoch on the TREC dataset, converging within 50 epochs.",1 https://code.google.com/p/word2vec/,"As for training cost, our system processes around 4000 tokens per second on a single GTX 670 GPU. As an example, this amounts to [Cite_Footnote_1] minute per epoch on the TREC dataset, converging within 50 epochs.",Method,Tool,False,Use（引用目的）,False,N16-1177_0_0,2016,Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents,Footnote
1185,11189," http://nlp.stanford.edu/projects/glove/"," ['5 Experiments', '5.2 Training Details and Implementation']","We use two sets of 300-dimensional pre-trained embeddings, word2vec 1 and GloVe [Cite_Footnote_2] , forming two channels for our network.",2 http://nlp.stanford.edu/projects/glove/,"We use two sets of 300-dimensional pre-trained embeddings, word2vec 1 and GloVe [Cite_Footnote_2] , forming two channels for our network. For all datasets, we use 100 convolution filters each for window sizes of 3, 4, 5. Rectified Linear Units (ReLU) is chosen as the nonlinear function in the convolutional layer.",Method,Tool,True,Use（引用目的）,True,N16-1177_1_0,2016,Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents,Footnote
1186,11190," http://people.mpi-inf.mpg.de/~corrogg/"," ['2 Overview of Werdy']","For instance, in the sentence “He takes a deep and long breath”, the set of potential entries includes take (verb, 44 candidate senses), take a breath (verb, [Cite_Footnote_1] candidate sense), and breath (noun, 5 candidate senses).","1 The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people.mpi-inf.mpg.de/˜corrogg/.","In the entry-recognition step (Sec. 3), Werdy obtains for the input sentence a set of potential KB entries along with their part-of-speech tags. The candidate senses of each entry are obtained from WordNet. For instance, in the sentence “He takes a deep and long breath”, the set of potential entries includes take (verb, 44 candidate senses), take a breath (verb, [Cite_Footnote_1] candidate sense), and breath (noun, 5 candidate senses). Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified.",Mixed,Mixed,True,Produce（引用目的）,True,D14-1042_0_0,2014,Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning,Footnote
1187,11191," http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html"," ['4 Syntactic Pruning']",There are 35 different frames in to-tal. [Cite_Footnote_3],3 See http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html.,"WordNet provides an important resource for ob-taining the set of clause types that are compatible with each sense of a verb. In particular, each verb sense in WordNet is annotated with a set of frames (e.g., “somebody verb something”) in which they may occur, capturing both syntactic and semantic constraints. There are 35 different frames in to-tal. [Cite_Footnote_3] We manually assigned a set of clause types to each frame (e.g., SVO to frame “somebody verb something”). Table 1 shows an example frame for each of the seven clause types. On average, each WordNet-3.0 verb sense is associated with 1.57 frames; the maximum number of frames per sense is 9. The distribution of frames is highly skewed: More than 61% of the 21,649 frame annotations belong to one of four simple SVO frames (num-bers 8, 9, 10 and 11), and 22 out of the 35 frames have less than 100 instances. This skew makes the syntactic pruning step effective for non-SVO clauses, but less effective for SVO clauses.",Material,Knowledge,False,Introduce（引用目的）,False,D14-1042_1_0,2014,Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning,Footnote
1188,11192," http://wordnet.princeton.edu/glosstag.shtml"," ['6 Verb-Object Sense Repository']","In particular, we harness the sense-annotated WordNet glosses [Cite_Footnote_4] as well as the sense-annotated SemCor corpus (Landes et al., 1998).",4 http://wordnet.princeton.edu/glosstag.shtml,"We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses [Cite_Footnote_4] as well as the sense-annotated SemCor corpus (Landes et al., 1998).",Material,Knowledge,False,Use（引用目的）,False,D14-1042_2_0,2014,Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning,Footnote
1189,11193," http://web.eecs.umich.edu/~mihalcea/downloads.html"," ['6 Verb-Object Sense Repository']","In particular, we harness the sense-annotated WordNet glosses as well as the sense-annotated SemCor corpus (Landes et al., 1998). [Cite_Footnote_5]",5 http://web.eecs.umich.edu/˜mihalcea/downloads.html,"We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses as well as the sense-annotated SemCor corpus (Landes et al., 1998). [Cite_Footnote_5]",Material,Dataset,True,Use（引用目的）,False,D14-1042_3_0,2014,Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning,Footnote
1190,11194," http://www.sussex.ac.uk/Users/drh21/"," ['7 Evaluation']","There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004), [Cite_Footnote_8] which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled.",8 http://www.sussex.ac.uk/Users/drh21/,"Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the en-try’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Baner-jee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experi-ments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004), [Cite_Footnote_8] which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled.",Method,Tool,False,Extend（引用目的）,True,D14-1042_4_0,2014,Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning,Footnote
1191,11195," http://www.neo4j.org/"," ['7 Evaluation']",We imple-mented this algorithm using the Neo4j library. [Cite_Footnote_9],9 http://www.neo4j.org/,Degree Centrality. Proposed by Navigli and La-pata (2010). The method collects all paths con-necting each candidate sense of an entry to the set of candidate senses of the words the entry’s con-text. The candidate sense with the highest degree in the resulting subgraph is selected. We imple-mented this algorithm using the Neo4j library. [Cite_Footnote_9] We used a fixed threshold of 1 and vary the search depth in range 1–20. We used the candidate senses of all nouns and verbs in a sentence as context.,Method,Code,True,Use（引用目的）,True,D14-1042_5_0,2014,Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning,Footnote
1192,11196," http://people.mpi-inf.mpg.de/~corrogg/"," ['7 Evaluation']",Our source code and the results of our evaluation are publicly available [Cite_Footnote_10] .,10 http://people.mpi-inf.mpg.de/˜corrogg/,Methodology. The disambiguation was per-formed with respect to coarse-grained sense clus-ters. The score of a cluster is the sum of the indi-vidual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected. Our source code and the results of our evaluation are publicly available [Cite_Footnote_10] .,Mixed,Mixed,True,Produce（引用目的）,True,D14-1042_6_0,2014,Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning,Footnote
1193,11197," https://xjtu-intsoft.github.io/chase"," ['1 Introduction']","The dataset, benchmark approaches, and our annotation tools are available at [Cite] https://xjtu-intsoft.github.io/chase.",,"To understand the characteristics of C HASE , we conduct a detailed data analysis and experi-ment with three state-of-the-art (SOTA) XDTS approaches, namely, EditSQL (Zhang et al., 2019), IGSQL (Cai and Wan, 2020), and our extension of RAT-SQL (Wang et al., 2020a). The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that C HASE presents significant chal-lenges for future research. The dataset, benchmark approaches, and our annotation tools are available at [Cite] https://xjtu-intsoft.github.io/chase.",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.180_0_0,2021,C HASE : A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL,Body
1194,11198," https://github.com/google-research/bert"," ['A Appendix', 'A.3.1 Adaptation to Chinese Inputs']","Since all the benchmark approaches use BERT for encodings and C HASE is constructed for Chinese, we replace BERT with Chinese-BERT. [Cite_Footnote_7]",7 https://github.com/google-research/bert,"Since all the benchmark approaches use BERT for encodings and C HASE is constructed for Chinese, we replace BERT with Chinese-BERT. [Cite_Footnote_7] During the adaptations of EditSQL and IGSQL, we iden-tified and fixed 3 bugs in their pre-process and post-process procedures. The string-match based schema linking method in RAT-SQL utilizes the Stanford CoreNLP Toolkit (Manning et al., 2014) to tokenize a question, and the method performs string matches between the resulting words and schema items. To adapt this method to Chinese, we try to use the Chinese package of CoreNLP to tokenize questions. However, we find that doing so fails to link a lot of schema items. Consider the question “这首歌曲的名字是?” and the col-umn “歌名” which is an abbreviation for “歌曲 的名字”. The question is tokenized by CoreNLP into h 这, 首, 歌曲, 的, 名字, 是, ? i. None of the resulting words can be matched with “歌名”. Consequently, the method cannot link the column to the question. To solve this problem, we simply tokenize a Chinese question character by charac-ter. In this way, the character ‘歌’ and ‘名’ can be partly matched to “歌名”. Although this solution would introduce a lot of noises, our experimental results show that this solution outperforms the one using CoreNLP. It would be very useful to explore the ways to conduct schema linking in Chinese.",Method,Tool,True,Use（引用目的）,False,2021.acl-long.180_1_0,2021,C HASE : A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL,Footnote
1195,11199," http://github.com/andreasvc/discodop"," ['3 Experiments', '3.2 Experimental Setup']","For the evaluation, we use the corresponding module of discodop. [Cite_Footnote_4]",4 http://github.com/andreasvc/discodop,"For the evaluation, we use the corresponding module of discodop. [Cite_Footnote_4] We report several metrics (as implemented in discodop):",Method,Tool,False,Use（引用目的）,True,P15-1116_0_0,2015,Discontinuous Incremental Shift-Reduce Parsing,Footnote
1196,11200," http://github.com/wmaier/rparse"," ['3 Experiments', '3.2 Experimental Setup']","We run further experiments with rparse [Cite_Footnote_5] (Kallmeyer and Maier, 2013) to facilitate a com-parison with a grammar-based parser.",5 http://github.com/wmaier/rparse,"We run further experiments with rparse [Cite_Footnote_5] (Kallmeyer and Maier, 2013) to facilitate a com-parison with a grammar-based parser.",Method,Tool,True,Compare（引用目的）,True,P15-1116_1_0,2015,Discontinuous Incremental Shift-Reduce Parsing,Footnote
1197,11201," http://nlp.stanford.edu/software/srparser.shtml"," ['1 Introduction']","In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014) [Cite_Ref] ) and extend it such that it can create trees with crossing branches (following Versley (2014)).",John Bauer. 2014. Stanford shift-reduce parser. http://nlp.stanford.edu/software/srparser.shtml.,"In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014) [Cite_Ref] ) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on dis-continuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluation, where we pay particular attention to the parser performance on discontinuous structures, we show among other things that surprisingly, a grammar-based parser has an edge over a shift-reduce ap-proach concerning the reconstruction of discontin-uous constituents.",補足資料,Paper,True,Introduce（引用目的）,True,P15-1116_2_0,2015,Discontinuous Incremental Shift-Reduce Parsing,Reference
1198,11202," http://nlp.stanford.edu/software/srparser.shtml"," ['2 Discontinuous Shift-Reduce Parsing']","Our parser architecture follows previous work, particularly Zhu et al. (2013) and Bauer (2014) [Cite_Ref] .",John Bauer. 2014. Stanford shift-reduce parser. http://nlp.stanford.edu/software/srparser.shtml.,"Our parser architecture follows previous work, particularly Zhu et al. (2013) and Bauer (2014) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,P15-1116_2_1,2015,Discontinuous Incremental Shift-Reduce Parsing,Reference
1199,11203," https://github.com/google-research/bert"," ['3 Method', '3.1 Contextual Utterance Embedding']","The pre-trained uncased BERT-Base [Cite_Footnote_1] model converts the token embeddings into con-textualized token representations, which can be converted to the vector representations via max pooling, so that they are regarded as the contex-tual utterance embeddings h (",1 See https://github.com/google-research/bert for details.,"We generate contextual utterance features from the tokens by following the method in (Luo and Wang, 2019). First, every utterance u 1 , u 2 , · · · , u N is tokenized by the BPE tokenizer (Sennrich et al., 2015), i.e., u i = (u i,1 , u i,2 , · · · , u i,T i ), where T i denotes the number of tokens. The tokens are embedded through WordPiece embeddings (Wu et al., 2016). The pre-trained uncased BERT-Base [Cite_Footnote_1] model converts the token embeddings into con-textualized token representations, which can be converted to the vector representations via max pooling, so that they are regarded as the contex-tual utterance embeddings h (i0) ∈ R D m for i = 1, · · · , M, where D m denotes the dimension of the utterance embeddings. This BERT model is fine-tuned through a training process.",Method,Tool,True,Use（引用目的）,False,2020.emnlp-main.597_0_0,2020,Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations,Footnote
1200,11204," http://www.retrevo.com/s/camera"," ['2 Customer Behavior Study']",he/she would purchase using a camera review blog [Cite_Footnote_1] to inform his/her decision.,1 http://www.retrevo.com/s/camera,"In order to study how online product reviews are used to make purchasing decisions, we conducted a user study. The study involved 16 pair of gradu-ate students. In each pair there was a customer and an observer. The goal of the customer was to de-cide which camera he/she would purchase using a camera review blog [Cite_Footnote_1] to inform his/her decision. As the customer read through the reviews, he/she was asked to think aloud and the observer recorded their observations.",補足資料,Document,True,Use（引用目的）,True,N09-2010_0_0,2009,Identifying Types of Claims in Online Customer Reviews,Footnote
1201,11205," http://en.wikipedia.org/wiki/English_grammar#Semantic_gradability"," ['3 Annotation Scheme']","usually [Cite_Footnote_3] wordsmake sucha claimas bald, as there is no qualified definition of being good or better.",3 http://en.wikipedia.org/wiki/English_ grammar#Semantic_gradability,"• Notgood,quantifiablebetter, best etcgradable. usually [Cite_Footnote_3] wordsmake sucha claimas bald, as there is no qualified definition of being good or better.",補足資料,Document,False,Introduce（引用目的）,False,N09-2010_1_0,2009,Identifying Types of Claims in Online Customer Reviews,Footnote
1202,11206," http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip"," ['5 Data and Annotation Procedure']",We applied our annotation scheme to the product re-view dataset [Cite_Footnote_4] released by Hu and Liu (2004).,4 http://www.cs.uic.edu/˜liub/FBS/CustomerReviewData.zip,We applied our annotation scheme to the product re-view dataset [Cite_Footnote_4] released by Hu and Liu (2004). We annotated the data for 3 out of 5 products. Each comment in the review is evaluated as being quali-fied or bald claim. The data has been made available for research purposes .,Material,Dataset,True,Use（引用目的）,True,N09-2010_2_0,2009,Identifying Types of Claims in Online Customer Reviews,Footnote
1203,11207," http://minorthird.sourceforge.net/"," ['6 Experiments and Results']","For our supervised machine learning experiments on automatic classification of comments as qualified or bald, we used the Support Vector Machine classifier in the MinorThird toolkit (Cohen, 2004) [Cite_Ref] with the de-fault linear kernel.",William Cohen. 2004. Minorthird: Methods for Iden-tifying Names and Ontological Relations in Text us-ing Heuristics for Inducing Regularities from Data. http://minorthird.sourceforge.net/,"For our supervised machine learning experiments on automatic classification of comments as qualified or bald, we used the Support Vector Machine classifier in the MinorThird toolkit (Cohen, 2004) [Cite_Ref] with the de-fault linear kernel. We report average classification accuracy and average Cohen’s Kappa using 10-fold cross-validation.",Method,Tool,True,Use（引用目的）,True,N09-2010_3_0,2009,Identifying Types of Claims in Online Customer Reviews,Reference
1204,11208," https://github.com/tuzhaopeng/NMT-Coverage"," ['References']",Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [Cite_Footnote_1],1 Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.,"Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vec-tor is fed to the attention model to help ad-just future attention, which lets NMT sys-tem to consider more about untranslated source words. Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,P16-1008_0_0,2016,Modeling Coverage for Neural Machine Translation Zhaopeng Tu † Zhengdong Lu † Yang Liu ‡ Xiaohua Liu † Hang Li †,Footnote
1205,11209," http://nlp.stanford.edu/software/dependenciesmanual.pdf"," ['3 Answer Grading System', '3.1 Node to Node Matching']",[Cite_Footnote_2] .,"2 For more information on the relations used in this experi-ment, consult the Stanford Typed Dependencies Manual at http://nlp.stanford.edu/software/dependenciesmanual.pdf","If we consider the dependency graphs output by the Stanford parser as directed (minimally cyclic) graphs, we can define for each node x a set of nodes N x that are reachable from x using a subset of the relations (i.e., edge types) [Cite_Footnote_2] . We variously define “reachable” in four ways to create four subgraphs defined for each node. These are as follows:",補足資料,Document,True,Produce（引用目的）,True,P11-1076_0_0,2011,Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments,Footnote
1206,11210," http://www.csie.ntu.edu.tw/~cjlin/libsvm/"," ['6 http://Infomap-nlp.sourceforge.net/']","We use the libSVM [Cite_Footnote_7] im-plementation of SVR, with tuned parameters.",7 http://www.csie.ntu.edu.tw/˜cjlin/libsvm/,"An input pair (A i ,A s ) is then associated with a grade g(A i , A s ) = u T ψ(A i , A s ) computed as a lin-ear combination of features. The weight vector u is trained to optimize performance in two scenarios: Regression: An SVM model for regression (SVR) is trained using as target function the grades as-signed by the instructors. We use the libSVM [Cite_Footnote_7] im-plementation of SVR, with tuned parameters.",Method,Tool,True,Use（引用目的）,True,P11-1076_2_0,2011,Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments,Footnote
1207,11211," http://svmlight.joachims.org/"," ['6 http://Infomap-nlp.sourceforge.net/']",We use the SVMLight [Cite_Footnote_8] implemen-tation of SVMRank with tuned parameters.,8 http://svmlight.joachims.org/,"Ranking: An SVM model for ranking (SVMRank) is trained using as ranking pairs all pairs of stu-dent answers (A s ,A t ) such that grade(A i ,A s ) > grade(A i ,A t ), where A i is the corresponding in-structor answer. We use the SVMLight [Cite_Footnote_8] implemen-tation of SVMRank with tuned parameters.",Method,Tool,True,Use（引用目的）,True,P11-1076_3_0,2011,Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments,Footnote
1208,11212," https://www.eccox.io/"," ['References']",Ecco is available at [Cite] https: //www.eccox.io/ .,,"Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco – an open-source 1 li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models. This includes (1) gradient-based fea-ture attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and exami-nation tools for neuron activations in the under-explored Feed-Forward Neural Network sub-layer of Transformer layers. (4) convenient ex-amination of activation vectors via canonical correlation analysis (CCA), non-negative ma-trix factorization (NMF), and probing classi-fiers. We find that syntactic information can be retrieved from BERT’s FFNN representa-tions in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntac-tic information. Ecco is available at [Cite] https: //www.eccox.io/ . 2",Method,Code,True,Produce（引用目的）,True,2021.acl-demo.30_0_0,2021,Ecco: An Open Source Library for the Explainability of Transformer Language Models,Body
1209,11213," https://github.com/jalammar/ecco"," ['References']","To increase the trans-parency of Transformer-based language mod-els, we present Ecco – an open-source [Cite_Footnote_1] li-brary for the explainability of Transformer-based NLP models.",1 The code is available at https://github.com/jalammar/ecco,"Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco – an open-source [Cite_Footnote_1] li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models. This includes (1) gradient-based fea-ture attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and exami-nation tools for neuron activations in the under-explored Feed-Forward Neural Network sub-layer of Transformer layers. (4) convenient ex-amination of activation vectors via canonical correlation analysis (CCA), non-negative ma-trix factorization (NMF), and probing classi-fiers. We find that syntactic information can be retrieved from BERT’s FFNN representa-tions in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntac-tic information. Ecco is available at https: //www.eccox.io/ .",Method,Code,True,Produce（引用目的）,True,2021.acl-demo.30_1_0,2021,Ecco: An Open Source Library for the Explainability of Transformer Language Models,Footnote
1210,11214," https://youtu.be/bcEysXmR09c"," ['References']",. [Cite_Footnote_2],2 Video demo available at https://youtu.be/ bcEysXmR09c,"Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco – an open-source li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models. This includes (1) gradient-based fea-ture attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and exami-nation tools for neuron activations in the under-explored Feed-Forward Neural Network sub-layer of Transformer layers. (4) convenient ex-amination of activation vectors via canonical correlation analysis (CCA), non-negative ma-trix factorization (NMF), and probing classi-fiers. We find that syntactic information can be retrieved from BERT’s FFNN representa-tions in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntac-tic information. Ecco is available at https: //www.eccox.io/ . [Cite_Footnote_2]",補足資料,Media,True,Produce（引用目的）,True,2021.acl-demo.30_2_0,2021,Ecco: An Open Source Library for the Explainability of Transformer Language Models,Footnote
1211,11215," http://worrydream.com/ExplorableExplanations/"," ['1 Introduction']",Ecco provides tools and interactive explorable explanations [Cite_Footnote_3] aiding the examination and intuition of:,3 http://worrydream.com/ExplorableExplanations/ Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th,Ecco provides tools and interactive explorable explanations [Cite_Footnote_3] aiding the examination and intuition of:,補足資料,Document,True,Introduce（引用目的）,True,2021.acl-demo.30_3_0,2021,Ecco: An Open Source Library for the Explainability of Transformer Language Models,Footnote
1212,11216," https://github.com/google/svcca"," ['6 System Design']","Canonical Correla-tion Analysis is calculated using the code open-sourced [Cite_Footnote_4] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019).",4 https://github.com/google/svcca,"Ecco is built on top of open source libraries including Scikit-Learn (Pedregosa et al., 2011), Matplotlib (Hunter, 2007), NumPy (Walt et al., 2011), PyTorch (Paszke et al., 2019) and Trans-formers (Wolf et al., 2020). Canonical Correla-tion Analysis is calculated using the code open-sourced [Cite_Footnote_4] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019).",Method,Code,True,Use（引用目的）,True,2021.acl-demo.30_4_0,2021,Ecco: An Open Source Library for the Explainability of Transformer Language Models,Footnote
1213,11217," https://github.com/jalammar/ecco"," ['8 Conclusion']",Ecco is open-source software [Cite_Footnote_5] and contributions are welcome.,5 https://github.com/jalammar/ecco,"Ecco is open-source software [Cite_Footnote_5] and contributions are welcome. Acknowledgments This work was improved thanks to feedback pro-vided by Abdullah Almaatouq, Anfal Alatawi, Christopher Olah, Fahd Alhazmi, Hadeel Al-Negheimish, Hend Al-Khalifa, Isabelle Augen-stein, Jasmijn Bastings, Najwa Alghamdi, Pepa Atanasova, and Sebastian Gehrmann.",Method,Tool,True,Produce（引用目的）,True,2021.acl-demo.30_5_0,2021,Ecco: An Open Source Library for the Explainability of Transformer Language Models,Footnote
1214,11218," https://lucene.apache.org/"," ['3 Method', '3.1 Initial retrieval']",To this end we retrieve several ranked lists from an Apache Lucene [Cite_Footnote_1] index using various state-of-the-art IR similarities.,1 https://lucene.apache.org/,"We first obtain for each query a reasonable pool of candidate documents to be re-ranked using our weakly-supervised models. To this end we retrieve several ranked lists from an Apache Lucene [Cite_Footnote_1] index using various state-of-the-art IR similarities. that are available in Lucene. The various retrieved lists are then combined to generate a single pool of top-k candidates for re-ranking by employing the PoolRank (Roitman, 2018) fusion method. We refer to this IR pipeline as IR-Base.",補足資料,Website,True,Extend（引用目的）,False,2020.emnlp-main.343_0_0,2020,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,Footnote
1215,11219," https://bit.ly/2ApmLcz"," ['4 Evaluation', '4.1 Datasets and Indexing']","The first benchmark, TREC-COVID [Cite_Footnote_2] , is based on the CORD-19 dataset , which contains scientific documents related to the recent Coronavirus pandemic.",2 https://bit.ly/2ApmLcz,"We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID [Cite_Footnote_2] , is based on the CORD-19 dataset , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels). Documents in this dataset have three fields (title, abstract and content). The two other bench-marks are based on news articles datasets: AP (As-sociation Press, about 242K docs) and WSJ (Wall Street Journal, about 160K docs). These datasets are part of the TREC ad-hoc retrieval newswire collection . Here we used topics 51-150 and topics 151-200 (with their respective qrels) for the AP and WSJ datasets, respectively. Those two datasets have only title and content so we created the ab-stract by taking the first 512 tokens of the content.",Material,Dataset,False,Produce（引用目的）,False,2020.emnlp-main.343_1_0,2020,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,Footnote
1216,11220," https://bit.ly/3dxyZ1i"," ['4 Evaluation', '4.1 Datasets and Indexing']","The first benchmark, TREC-COVID , is based on the CORD-19 dataset [Cite_Footnote_3] , which contains scientific documents related to the recent Coronavirus pandemic.",3 https://bit.ly/3dxyZ1i,"We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID , is based on the CORD-19 dataset [Cite_Footnote_3] , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels). Documents in this dataset have three fields (title, abstract and content). The two other bench-marks are based on news articles datasets: AP (As-sociation Press, about 242K docs) and WSJ (Wall Street Journal, about 160K docs). These datasets are part of the TREC ad-hoc retrieval newswire collection . Here we used topics 51-150 and topics 151-200 (with their respective qrels) for the AP and WSJ datasets, respectively. Those two datasets have only title and content so we created the ab-stract by taking the first 512 tokens of the content.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.343_2_0,2020,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,Footnote
1217,11221," https://bit.ly/3gJcF6X"," ['4 Evaluation', '4.1 Datasets and Indexing']",These datasets are part of the TREC ad-hoc retrieval newswire collection [Cite_Footnote_5] .,5 https://bit.ly/3gJcF6X,"We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID , is based on the CORD-19 dataset , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels). Documents in this dataset have three fields (title, abstract and content). The two other bench-marks are based on news articles datasets: AP (As-sociation Press, about 242K docs) and WSJ (Wall Street Journal, about 160K docs). These datasets are part of the TREC ad-hoc retrieval newswire collection [Cite_Footnote_5] . Here we used topics 51-150 and topics 151-200 (with their respective qrels) for the AP and WSJ datasets, respectively. Those two datasets have only title and content so we created the ab-stract by taking the first 512 tokens of the content.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.343_3_0,2020,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,Footnote
1218,11222," https://bit.ly/2Me0Gk1"," ['4 Evaluation', '4.2 Experimental Setup']",We used the pytorch huggingface implementation of BERT and GPT2 [Cite_Footnote_6] .,6 https://bit.ly/2Me0Gk1,"IR-Base. The following Lucene similarities config-urations were used: i) BM25Similarity (Robertson and Zaragoza, 2009) with k1 = 1.2 and b = 0.7. ii) LMDirichletSimilarity (Zhai, 2009) with Dirichlet-smoothing parameter µ = 200 and µ = 1000 for TREC-COVID and news datasets, respectively. iii) DFRSimilarity (Amati and Van Rijsbergen, 2002) with BasicModelIF, AfterEffectB and Normaliza-tionH3. iv) AxiomaticF1LOG (Fang and Zhai, 2005) with growth parameter s = 0.25 and s = 0.1 for TREC-COVID and news datasets, respectively. BERT models. We used the pytorch huggingface implementation of BERT and GPT2 [Cite_Footnote_6] . For the two BERT models we used bert-base-uncased (12-layers, 768-hidden, 12-heads, 110M parameters). Fine-tuning was done with a learning rate of 2e- 5 and 3 training epochs. For training BERT-Q-a on each of the three datasets, we used a subset of their first 20K documents. For TREC-COVID, we used SciBERT model (Beltagy et al., 2019) (that was pre-trained on 1M scientific documents), as it yields better results than using the vanilla pre-trained BERT model. This is mainly due to the scientific nature of the documents in this bench-mark.",Method,Tool,False,Use（引用目的）,True,2020.emnlp-main.343_4_0,2020,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,Footnote
1219,11223," https://github.com/YosiMass/ad-hoc-retrieval"," ['4 Evaluation', '4.2 Experimental Setup']","After filtering the generated para-phrases, we were left with 18K, 4.5K and 3.5K paraphrases for TREC-COVID, WSJ and AP re-spectively. [Cite_Footnote_7]",7 The filtered paraphrases can be downloaded from https://github.com/YosiMass/ad-hoc-retrieval,"GPT2. For generating title paraphrases we used GPT2 small model (12-layers, 768-hidden, 12-heads, 110M parameters). For fine-tuning we used (title, abstract) pairs from all documents of TREC-COVID and a subset of the first 20K documents of the other two datasets. We generated 10 para-phrases for the first 20K documents of each of the three datasets. After filtering the generated para-phrases, we were left with 18K, 4.5K and 3.5K paraphrases for TREC-COVID, WSJ and AP re-spectively. [Cite_Footnote_7] Fusion. We fine-tuned the PoolRank (Roitman, 2018) method’s parameters for all datasets as fol-lows: For Base fusion we used CombSUM (Nuray and Can, 2006) with sum-normalization. The other parameters were set as: Pseudo-relevance set size: 5 documents. Term clip size: 100. Document re-ranking using KL-score (equally interpolated with the CombSUM score) with Dirichlet-smoothing pa-rameter µ = 200 and µ = 1000 for TREC-COVID and news datasets, respectively.",Material,Knowledge,True,Produce（引用目的）,True,2020.emnlp-main.343_5_0,2020,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,Footnote
1220,11224," https://bit.ly/2XjkE2T"," ['4 Evaluation', '4.3 Results']","On TREC-COVID, we directly compared against the three best automatic performing systems [Cite_Footnote_8] (out of 141 system runs submitted to the Round-1 challenge by 56 different teams), namely: sabir, IRIT markers and unipd.it.",8 The details of these systems as well as other competing systems are available in https://bit.ly/2XjkE2T,"To demonstrate the relative effectiveness of our proposed approach, we compared its quality to state-of-the-art alternative baselines. On TREC-COVID, we directly compared against the three best automatic performing systems [Cite_Footnote_8] (out of 141 system runs submitted to the Round-1 challenge by 56 different teams), namely: sabir, IRIT markers and unipd.it.",Method,Tool,True,Compare（引用目的）,True,2020.emnlp-main.343_6_0,2020,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,Footnote
1221,11225," https://iep.utm.edu/ethics/"," ['1 Introduction']","Ethics (or ethical theory), is a the-oretical and applied branch of philosophy which studies what is good and right, especially as it per-tains to how humans ought to behave in the most general sense (Fieser, 1995) [Cite_Ref] .",James Fieser. 1995. Ethics. https://iep.utm.edu/ethics/ (accessed: 11-03-2020).,"Our work bridges this gap by illustrating how a philosophical theory of ethics can be applied to NLP research. Ethics (or ethical theory), is a the-oretical and applied branch of philosophy which studies what is good and right, especially as it per-tains to how humans ought to behave in the most general sense (Fieser, 1995) [Cite_Ref] . As NLP research qual-ifies as a human activity, it is within the purview of ethics. In particular, we are using a prescriptive, rather than descriptive, theory of ethics; prescrip-tive theories define and recommend ethical behav-ior whereas descriptive theories merely report how people generally conceive of ethical behavior.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.297_0_0,2021,Case Study: Deontological Ethics in NLP,Reference
1222,11226," http://www.chokkan.org/software/crfsuite/"," ['4 Experimental setup', '4.1 MT and NLU systems']","For building NLU models, we use Conditional Random Fields (Lafferty et al., 2001; Okazaki, 2007 [Cite_Ref] ) for Named Entity Recognition and a Max-imum Entropy classifier (Berger et al., 1996) for Intent Classification; we keep the sets of features, hyper-parameters and configuration constant for our experiments.",Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs). http://www.chokkan.org/software/crfsuite/.,"For building NLU models, we use Conditional Random Fields (Lafferty et al., 2001; Okazaki, 2007 [Cite_Ref] ) for Named Entity Recognition and a Max-imum Entropy classifier (Berger et al., 1996) for Intent Classification; we keep the sets of features, hyper-parameters and configuration constant for our experiments.",Method,Tool,True,Use（引用目的）,True,N18-3017_0_0,2018,Selecting Machine-Translated Data for Quick Bootstrapping of a Natural Language Understanding System,Reference
1223,11227," http://sourceforge.net/projects/mstparser"," ['2 Unlabeled Dependency Parsing using Global Features', '2.3 Local Features']",The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [Cite_Footnote_3] .,3 http://sourceforge.net/projects/mstparser,"The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [Cite_Footnote_3] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent. The direction and the distance from a parent to its child, and the FEATS fields of the parent and the child which are split into elements and then combined are also included. Features that appeared less than 5 times in training data are ignored.",Method,Tool,True,Compare（引用目的）,True,D07-1100_0_0,2007,Multilingual Dependency Parsing using Global Features,Footnote
1224,11228," https://github.com/sanketvmehta/efficient-meta-lifelong-learning"," ['4 Experiments', '4.2 Experimental Setup']","For our framework, Meta-MbPA [Cite_Footnote_1] , unless stated otherwise, we set the number of neighbors K = 32 and control the memory size through a write rate r M = 1%.",1 Source code is available at https://github.com/sanketvmehta/efficient-meta-lifelong-learning.,"For our framework, Meta-MbPA [Cite_Footnote_1] , unless stated otherwise, we set the number of neighbors K = 32 and control the memory size through a write rate r M = 1%. We use L = 30 local adaptation steps and perform local adaptation for whole testing set. That is, we randomly draw K = 32 examples from the memory and perform a local adaptation step. Through this, the computational cost is equivalent to MbPA++ but we only need to perform the whole process once while MbPA++ requires conducting local adaptation independently for each testing ex-ample. We set α = 1e −5 (in Eq. (5), (6)), β = 10 (in Eq. (7)) and λ l = 0.001 (in Eq. (4)). All of the experiments are performed using PyTorch (Paszke et al., 2017), which allows for automatic differentiation through the gradient update as re-quired for optimizing the meta-task loss Eq. (5) and meta-replay loss Eq. (6).",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.39_0_0,2020,Efficient Meta Lifelong-Learning with Limited Memory,Footnote
1225,11229," https://cogcomp.cs.illinois.edu/page/publication_view/790"," ['1 Introduction']",The truncated word embeddings are available from the papers web page at [Cite] https://cogcomp.cs.illinois.,,"The results we present are quite surprising. We show that it is possible to reduce the memory con-sumption by an order of magnitude both when word embeddings are being used and in training. In the first case, as we show, simply truncating the resulting representations after training and us-ing a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits. Moreover, we provide t-wo ways to train existing algorithms (Mikolov et al., 2013a; Mikolov et al., 2013b) when the memory is limited during training and show that, here, too, an order of magnitude saving in mem-ory is possible without degrading performance. We conduct comprehensive experiments on ex-isting word and phrase similarity and relatedness datasets as well as on dependency parsing, to e-valuate these results. Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space re-quired by a factor of 16. The truncated word embeddings are available from the papers web page at [Cite] https://cogcomp.cs.illinois.edu/page/publication_view/790.",補足資料,Paper,True,Introduce（引用目的）,True,P16-2063_0_0,2016,Word Embeddings with Limited Memory,Body
1226,11230," https://dumps.wikimedia.org/"," ['4 Experiments on Word/Phrase Similarity']","We train the word embedding algo-rithms, word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), based on the Oct. 2013 Wikipedi-a dump. [Cite_Footnote_1]",1 https://dumps.wikimedia.org/,"In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algo-rithms, word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), based on the Oct. 2013 Wikipedi-a dump. [Cite_Footnote_1] We first compare levels of truncation of word2vec embeddings, and then evaluate the s-tochastic rounding and the auxiliary vectors based methods for training word2vec vectors.",Material,DataSource,True,Use（引用目的）,True,P16-2063_1_0,2016,Word Embeddings with Limited Memory,Footnote
1227,11231," http://www.wordvectors.org/"," ['4 Experiments on Word/Phrase Similarity', '4.1 Datasets']","We use the datasets summarized by Faruqui and Dyer (Faruqui and Dyer, 2014): wordsim-353, wordsim-sim, wordsim-rel, MC-30, RG-65, MTurk-287, MTurk-771, MEN 3000, YP-130, Rare-Word, Verb-143, and SimLex-999. [Cite_Footnote_2]",2 http://www.wordvectors.org/,"Word Similarity. Word similarity datasets have been widely used to evaluate word embed-ding results. We use the datasets summarized by Faruqui and Dyer (Faruqui and Dyer, 2014): wordsim-353, wordsim-sim, wordsim-rel, MC-30, RG-65, MTurk-287, MTurk-771, MEN 3000, YP-130, Rare-Word, Verb-143, and SimLex-999. [Cite_Footnote_2] We compute the similarities between pairs of words and check the Spearman’s rank correlation coeffi-cient (Myers and Well., 1995) between the com-puter and the human labeled ranks.",Material,Dataset,True,Use（引用目的）,True,P16-2063_2_0,2016,Word Embeddings with Limited Memory,Footnote
1228,11232," https://github.com/jiangfeng1124/acl15-clnndep"," ['5 Experiments on Dependency Parsing']","We follow the setup of (Guo et al., 2015) in a monolingual setting [Cite_Footnote_3] .",3 https://github.com/jiangfeng1124/ acl15-clnndep,"We also incorporate word embedding results into a downstream task, dependency parsing, to eval-uate whether the truncated embedding results are still good features compared to the original fea-tures. We follow the setup of (Guo et al., 2015) in a monolingual setting [Cite_Footnote_3] . We train the parser with 5,000 iterations using different truncation set-tings for word2vec embedding. The data used to train and evaluate the parser is the English data in the CoNLL-X shared task (Buchholz and Mar-si, 2006). We follow (Guo et al., 2015) in using the labeled attachment score (LAS) to evaluate the different parsing results. Here we only show the word embedding results for 200 dimensions, since empirically we found 25-dimension results were not as stable as 200 dimensions.",補足資料,Document,False,Use（引用目的）,False,P16-2063_3_0,2016,Word Embeddings with Limited Memory,Footnote
1229,11233," https://ntunlpsg.github.io/project/rst-parser"," ['1 Introduction']",We make our code available at [Cite] https://ntunlpsg.github.io/project/rst-parser,,We make our code available at [Cite] https://ntunlpsg.github.io/project/rst-parser,Method,Code,True,Produce（引用目的）,True,2021.naacl-main.128_0_0,2021,RST Parsing from Scratch,Body
1230,11234," https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum"," ['2 Curriculum Learning for Adaptation', '2.2 Curriculum Learning Training Strategy']","In this paper, we use the same probabilistic cur-riculum strategy and code base [Cite_Footnote_1] as Zhang et al. (2018).",1 https://github.com/kevinduh/sockeye-recipes/tree/master/egs/ curriculum,"In this paper, we use the same probabilistic cur-riculum strategy and code base [Cite_Footnote_1] as Zhang et al. (2018). The main difference here is the applica-tion to domain adaptation. The proposed strategy is summarized as follows:",Method,Code,True,Use（引用目的）,True,N19-1189_0_0,2019,Curriculum Learning for Domain Adaptation in Neural Machine Translation,Footnote
1231,11235," https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum"," ['3 Experiments and Results']",The code base is provided to ensure reproducibility. [Cite_Footnote_2],2 https://github.com/kevinduh/sockeye-recipes/tree/master/egs/ curriculum,We evaluate on four domain adaptation tasks. The code base is provided to ensure reproducibility. [Cite_Footnote_2],Method,Code,True,Produce（引用目的）,True,N19-1189_1_0,2019,Curriculum Learning for Domain Adaptation in Neural Machine Translation,Footnote
1232,11236," https://www.paracrawl.eu/"," ['3 Experiments and Results', '3.1 Data and Setup']","Unlabeled-domain For additionalData unlabeled-domain data, we use web-crawled bitext from the Paracrawl project. [Cite_Footnote_4]",4 https://www.paracrawl.eu/,"We randomly sample 15k parallel sentences from the original corpora as our in-domain bitext. We also have around 2k sentences of de-velopment and test data for TED and 3k for patent. Unlabeled-domain For additionalData unlabeled-domain data, we use web-crawled bitext from the Paracrawl project. [Cite_Footnote_4] We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English. Using different data selection methods, we include up to the 4096k and 2048k sentence-pairs for our German and Russian experiments, respectively.",Material,DataSource,True,Use（引用目的）,True,N19-1189_2_0,2019,Curriculum Learning for Domain Adaptation in Neural Machine Translation,Footnote
1233,11237," http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/"," ['3 Experiments and Results', '3.1 Data and Setup']",• TED talks: data-split from Duh (2018) [Cite_Ref] .,Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/˜kevinduh/a/multitarget-tedtalks/.,• TED talks: data-split from Duh (2018) [Cite_Ref] .,補足資料,Website,False,Use（引用目的）,True,N19-1189_3_0,2019,Curriculum Learning for Domain Adaptation in Neural Machine Translation,Reference
1234,11238," https://github.com/HKUST-KnowComp/RINANTE"," ['1 Introduction']",Our code is available at [Cite] https://github.com/HKUST-KnowComp/RINANTE.,,Our code is available at [Cite] https://github.com/HKUST-KnowComp/RINANTE.,Method,Code,True,Produce（引用目的）,True,P19-1520_0_0,2019,Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision,Body
1235,11239," https://www.yelp.com/dataset/challenge"," ['4 Experiments', '4.1 Datasets']","Besides the above datasets, we also use a Yelp dataset [Cite_Footnote_1] and an Amazon Electronics dataset (He and McAuley, 2016) as auxiliary data to be anno-tated with the mined rules.",1 https://www.yelp.com/dataset/challenge,"Besides the above datasets, we also use a Yelp dataset [Cite_Footnote_1] and an Amazon Electronics dataset (He and McAuley, 2016) as auxiliary data to be anno-tated with the mined rules. They are also used to train word embeddings. The Yelp dataset is used for the restaurant datasets SE14-R and SE15-R. It includes 4,153,150 reviews that are for 144,072 different businesses. Most of the businesses are restaurants. The Amazon Electronics dataset is used for the laptop dataset SE14-L. It includes 1,689,188 reviews for 63,001 products such as lap-tops, TV, cell phones, etc.",Material,Dataset,True,Use（引用目的）,True,P19-1520_1_0,2019,Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision,Footnote
1236,11240," http://jmcauley.ucsd.edu/data/amazon/"," ['4 Experiments', '4.1 Datasets']","Besides the above datasets, we also use a Yelp dataset and an Amazon Electronics dataset (He and McAuley, 2016) [Cite_Footnote_2] as auxiliary data to be anno-tated with the mined rules.",2 http://jmcauley.ucsd.edu/data/amazon/,"Besides the above datasets, we also use a Yelp dataset and an Amazon Electronics dataset (He and McAuley, 2016) [Cite_Footnote_2] as auxiliary data to be anno-tated with the mined rules. They are also used to train word embeddings. The Yelp dataset is used for the restaurant datasets SE14-R and SE15-R. It includes 4,153,150 reviews that are for 144,072 different businesses. Most of the businesses are restaurants. The Amazon Electronics dataset is used for the laptop dataset SE14-L. It includes 1,689,188 reviews for 63,001 products such as lap-tops, TV, cell phones, etc.",Material,Dataset,True,Use（引用目的）,True,P19-1520_2_0,2019,Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision,Footnote
1237,11241," http://en.wiktionary.org"," ['1 Introduction']",The English Wik-tionary [Cite_Footnote_1] is a crowd-sourced lexical resource that in-cludes complete inflection tables for many lexical items in many languages.,1 http://en.wiktionary.org,"For natural languages with rich morphology, knowl-edge of how to inflect base forms is critical for both text generation and analysis. Hand-engineered, rule-based methods for predicting inflections can offer extremely high accuracy, but they are laborious to construct and do not exist with full lexical cover-age in all languages. By contrast, a large number of example inflections are freely available in a semi-structured format on the Web. The English Wik-tionary [Cite_Footnote_1] is a crowd-sourced lexical resource that in-cludes complete inflection tables for many lexical items in many languages. We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language’s in-flectional morphology. 2",Material,Knowledge,True,Introduce（引用目的）,False,N13-1138_0_0,2013,Supervised Learning of Complete Morphological Paradigms,Footnote
1238,11242," http://eecs.berkeley.edu/~gdurrett"," ['1 Introduction']","We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language’s in-flectional morphology. [Cite_Footnote_2]",2 See http://eecs.berkeley.edu/~gdurrett for our datasets and code.,"For natural languages with rich morphology, knowl-edge of how to inflect base forms is critical for both text generation and analysis. Hand-engineered, rule-based methods for predicting inflections can offer extremely high accuracy, but they are laborious to construct and do not exist with full lexical cover-age in all languages. By contrast, a large number of example inflections are freely available in a semi-structured format on the Web. The English Wik-tionary 1 is a crowd-sourced lexical resource that in-cludes complete inflection tables for many lexical items in many languages. We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language’s in-flectional morphology. [Cite_Footnote_2]",Mixed,Mixed,True,Produce（引用目的）,True,N13-1138_1_0,2013,Supervised Learning of Complete Morphological Paradigms,Footnote
1239,11243," http://eecs.berkeley.edu/~gdurrett"," ['8 Conclusion']",Our Wiktionary datasets and an open-source version of our code are available at [Cite] http://eecs.berkeley.edu/~gdurrett,,Our Wiktionary datasets and an open-source version of our code are available at [Cite] http://eecs.berkeley.edu/~gdurrett,Mixed,Mixed,True,Produce（引用目的）,True,N13-1138_2_0,2013,Supervised Learning of Complete Morphological Paradigms,Body
1240,11244," https://github.com/emorynlp/nlp4j"," ['3 Corpus', '3.1 Passage Generation']","To break down the episode-level summaries into scene-level, they are segmented into sentences by the tokenizer in NLP4J. [Cite_Footnote_2]",2 https://github.com/emorynlp/nlp4j,"The plot summaries collected from the fan sites are associated with episodes, not scenes. To break down the episode-level summaries into scene-level, they are segmented into sentences by the tokenizer in NLP4J. [Cite_Footnote_2] Each sentence in the plot summaries is then queried to Elasticsearch that has indexed the selected scenes, and the scene with the highest relevance is retrieved. Finally, the retrieved scene along with the queried sentence are sent to a crowd worker who is asked to determine whether or not they are relevant, and perform anaphora resolution to replace all pronouns in the sentence with the corresponding character names. The sentence that is checked for the relevancy and processed by the anaphora resolution is considered a passage.",Method,Tool,True,Use（引用目的）,True,N18-1185_0_0,2018,Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog,Footnote
1241,11245," https://github.com/emorynlp/reading-comprehension"," ['7 Conclusion']",All our resources including the annotated corpus and source codes of the models are avail-able at: [Cite] https://github.com/emorynlp/reading-comprehension.,,"We introduce a new corpus consisting of multiparty dialogs and crowdsourced annotation for the task of passage completion. To the best of our knowl-edge, this is the first corpus that can challenge deep learning models for passage completion on this genre. We also present a deep learning architec-ture combining convolutional and recurrent neural networks, coupled with utterance-level and dialog-level attentions. Models trained by our architec-ture significantly outperform the one trained by the pure bidirectional LSTM, especially on longer dialogs. Our analysis demonstrates the compre-hension of our model using the attention matrix. For the future work, we will expand the annotation for more entity types and automatically link men-tions with respect to their entities using an entity linker. All our resources including the annotated corpus and source codes of the models are avail-able at: [Cite] https://github.com/emorynlp/reading-comprehension.",Mixed,Mixed,True,Produce（引用目的）,True,N18-1185_1_0,2018,Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog,Body
1242,11246," http://www.aclweb.org/anthology/D14-1162"," ['5 Experiments']","The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [Cite_Ref] are used for all experiments (d = 100).","Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532–1543. http://www.aclweb.org/anthology/D14-1162.","The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [Cite_Ref] are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25. For the 2/1D convolutions in Sections 4.1 and 4.3, f = e = 50 filters are used, and the ReLu acti-vation is applied to all convolutional layers. The dimension of the LSTM outputs ~h ↓↑ ∗ is 32, and the tanh activation is applied to all hidden states of LSTMs. Finally, the Adam optimizer with the learning rate of 0.001 is used to learn the weights of all models. Table 4 shows the dataset split for our experiments that roughly gives 80/10/10% for training/development/evaluation sets.",Material,Knowledge,True,Use（引用目的）,True,N18-1185_12_0,2018,Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog,Reference
1243,11247," https://github.com/facebookresearch/UnsupervisedQA"," ['1 Introduction']","The first approach for unsu-pervised QA, reducing the problem to unsuper-vised cloze translation, using methods from unsu-pervised machine translation ii) Extensive experi-ments testing the impact of various cloze question translation algorithms and assumptions iii) Ex-periments demonstrating the application of our method for few-shot learning in EQA. [Cite_Footnote_1]",1 Synthetic EQA training data and models that generate it will be made publicly available at https://github.com/facebookresearch/UnsupervisedQA,"To summarize, this paper makes the follow-ing contributions: i) The first approach for unsu-pervised QA, reducing the problem to unsuper-vised cloze translation, using methods from unsu-pervised machine translation ii) Extensive experi-ments testing the impact of various cloze question translation algorithms and assumptions iii) Ex-periments demonstrating the application of our method for few-shot learning in EQA. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P19-1484_0_0,2019,Unsupervised Question Answering by Cloze Translation,Footnote
1244,11248," http://commoncrawl.org/"," ['2 Unsupervised Extractive QA', '2.4 Unsupervised Cloze Translation']","Question Corpus We mine questions from En-glish pages from a recent dump of common crawl using simple selection criteria: [Cite_Footnote_3] We select sen-tences that start in one of a few common wh* words, (“how much”, “how many”, “what”, “when”, “where” and “who”) and end in a ques-tion mark.",3 http://commoncrawl.org/,"Question Corpus We mine questions from En-glish pages from a recent dump of common crawl using simple selection criteria: [Cite_Footnote_3] We select sen-tences that start in one of a few common wh* words, (“how much”, “how many”, “what”, “when”, “where” and “who”) and end in a ques-tion mark. We reject questions that have repeated question marks or “?!”, or are longer than 20 to-kens. This process yields over 100M english ques-tions when deduplicated. Corpus Q is created by sampling 5M questions such that there are equal numbers of questions starting in each wh* word.",Material,DataSource,True,Use（引用目的）,True,P19-1484_1_0,2019,Unsupervised Question Answering by Cloze Translation,Footnote
1245,11249," https://github.com/huggingface/"," ['3 Experiments', '3.1 Unsupervised QA Experiments']","For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [Cite_Footnote_5]","5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa","For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [Cite_Footnote_5] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.",Method,Tool,False,Use（引用目的）,True,P19-1484_2_0,2019,Unsupervised Question Answering by Cloze Translation,Footnote
1246,11250," https://github.com/allenai/document-qa"," ['3 Experiments', '3.1 Unsupervised QA Experiments']","For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [Cite_Footnote_5]","5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa","For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [Cite_Footnote_5] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.",Method,Tool,False,Use（引用目的）,True,P19-1484_3_0,2019,Unsupervised Question Answering by Cloze Translation,Footnote
1247,11251," http://bit.ly/semi-supervised-qa"," ['3 Experiments', '3.1 Unsupervised QA Experiments']","To enable fair comparison, we re-implement their approach using their publicly available data, and train a variant with BERT-Large. [Cite_Footnote_6]",6 http://bit.ly/semi-supervised-qa,"We shall compare our results to some published baselines. Rajpurkar et al. (2016) use a super-vised logistic regression model with feature en-gineering, and a sliding window approach that finds answers using word overlap with the ques-tion. Kaushik and Lipton (2018) train (supervised) models that disregard the input question and sim-ply extract the most likely answer span from the context. To our knowledge, ours is the first work to deliberately target unsupervised QA on SQuAD. Dhingra et al. (2018) focus on semi-supervised QA, but do publish an unsupervised evaluation. To enable fair comparison, we re-implement their approach using their publicly available data, and train a variant with BERT-Large. [Cite_Footnote_6] Their approach also uses cloze questions, but without translation, and heavily relies on the structure of wikipedia ar-ticles.",Method,Tool,False,Compare（引用目的）,True,P19-1484_4_0,2019,Unsupervised Question Answering by Cloze Translation,Footnote
1248,11252," https://catalog.ldc.upenn.edu/LDC2013T19"," ['5 Discussion']","It is worth noting that to attain our best perfor-mance, we require the use of both an NER system, indirectly using labelled data from OntoNotes 5, and a constituency parser for extracting sub-clauses, trained on the Penn Treebank (Marcus et al., 1994). [Cite_Footnote_7]",7 Ontonotes 5: https://catalog.ldc.upenn.edu/LDC2013T19,"It is worth noting that to attain our best perfor-mance, we require the use of both an NER system, indirectly using labelled data from OntoNotes 5, and a constituency parser for extracting sub-clauses, trained on the Penn Treebank (Marcus et al., 1994). [Cite_Footnote_7] Moreover, a language-specific wh* heuristic was used for training the best perform-ing NMT models. This limits the applicability and flexibility of our best-performing approach to do-mains and languages that already enjoy extensive linguistic resources (named entity recognition and treebank datasets), as well as requiring some hu-man engineering to define new heuristics.",Material,Knowledge,True,Use（引用目的）,True,P19-1484_5_0,2019,Unsupervised Question Answering by Cloze Translation,Footnote
1249,11253," https://github.com/glample/fastBPE"," ['-', 'Details']","We use the English tokenizer from Moses (Koehn et al., 2007), and use FastBPE ( [Cite] https://github.com/glample/fastBPE) to split into subword units, with a vocabulary size of 60000.",,"Here we describe experimental details for un-supervised NMT setup. We use the English tokenizer from Moses (Koehn et al., 2007), and use FastBPE ( [Cite] https://github.com/glample/fastBPE) to split into subword units, with a vocabulary size of 60000. The architec-ture uses a 4-layer transformer encoder and 4-layer transformer decoder, where one layer is language specific for both the encoder and decoder, the rest are shared. We use the standard hyperparameter settings recommended by Lample et al. (2018). The models are initialised with random weights, and the input word embedding matrix is initialised using FastText vectors (Bojanowski et al., 2016) trained on the concatenation of the C and Q cor-pora. Initially, the auto-encoding loss and back-translation loss have equal weight, with the auto-encoding loss coefficient reduced to 0.1 by 100K steps and to 0 by 300k steps. We train using 5M cloze questions and natural questions, and cease training when the BLEU scores between back-translated and input questions stops improving, usually around 300K optimisation steps. When generating, we decode greedily, and note that de-coding with a beam size of 5 did not significantly change downstream QA performance, or greatly change the fluency of generations.",Method,Tool,True,Use（引用目的）,True,P19-1484_6_0,2019,Unsupervised Question Answering by Cloze Translation,Body
1250,11254," https://github.com/facebookresearch/XLM"," ['A.6 Language Model Pretraining']",We use the XLM implementation ( [Cite] https://github.com/facebookresearch/XLM) and use de-fault hyperparameters for both MLM pretraining and and unsupervised NMT fine-tuning.,,"We experimented with Masked Language Model (MLM) pretraining of the translation mod-els, p s→t (q|q 0 ) and p t→s (q 0 |q). We use the XLM implementation ( [Cite] https://github.com/facebookresearch/XLM) and use de-fault hyperparameters for both MLM pretraining and and unsupervised NMT fine-tuning. The UNMT encoder is initialized with the MLM model’s parameters, and the decoder is randomly initialized. We find translated questions to be qualitatively more fluent and abstractive than the those from the models used in the main paper. Table 6 supports this observation, demonstrating that questions produced by models with MLM pre-training are classified as well-formed 10.5% more often than those without pretraining, surpassing the rule-based question generator of Heilman and Smith (2010). However, using MLM pretraining did not lead to significant differences for question answering performance (the main focus of this pa-per), so we leave a thorough investigation into lan-guage model pretraining for unsupervised ques-tion answering as future work.",Material,Knowledge,False,Use（引用目的）,True,P19-1484_7_0,2019,Unsupervised Question Answering by Cloze Translation,Body
1251,11255," https://arxiv.org/help/api"," ['3 Experiments', '3.2 Multi-Label Classification']","We collected abstracts and categories of papers submitted to arXiv from January 1st, 2019 to June 4th, 2019 using arXiv API. [Cite_Footnote_6]",6 https://arxiv.org/help/api,"Because the arXiv dataset released by Yang et al. (2018) removed all line breaks, we created one ourselves. We collected abstracts and categories of papers submitted to arXiv from January 1st, 2019 to June 4th, 2019 using arXiv API. [Cite_Footnote_6]",Method,Code,True,Use（引用目的）,True,2020.acl-main.33_2_0,2020,Text Classification with Negative Supervision,Footnote
1252,11256," https://github.com/google-research/bert"," ['3 Experiments', '3.3 Settings']","For BERT, we used the pre-trained BERT-base [Cite_Footnote_7] (d = 768).",7 https://github.com/google-research/bert,"As a text encoder, we employed BERT and a Hi-erarchical Attention Network (HAN) (Yang et al., 2016) for generating sentence and document rep-resentation, respectively. For BERT, we used the pre-trained BERT-base [Cite_Footnote_7] (d = 768). We imple-mented the HAN following Yang et al. (2016) who used the bi-directional Gated Recurrent Unit as the encoder with the hidden size of 50 (d = 50). The embedding layer of the HAN was initialised using CBOW (Mikolov et al., 2013) embeddings (with dimensions of 200), which were trained using nega-tive sampling on the training and development sets of each task.",Method,Code,False,Use（引用目的）,True,2020.acl-main.33_3_0,2020,Text Classification with Negative Supervision,Footnote
1253,11257," http://gordonscruton.blogspot.in/2011/08/"," ['1 Introduction']",Table 1: Examples of coherence and cohesion [Cite_Footnote_2] .,2 We took the examples from this site for explaining coherence and cohesion: http: //gordonscruton.blogspot.in/2011/08/ what-is-cohesion-coherence-cambridge. html,Table 1: Examples of coherence and cohesion [Cite_Footnote_2] .,Material,Knowledge,True,Use（引用目的）,True,P18-1219_0_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1254,11258," http://www.cfilt.iitb.ac.in/cognitive-nlp/"," ['1 Introduction']","Thirdly, we also release our dataset [Cite_Footnote_3] to help in further research in using gaze features in other tasks involving predicting the quality of texts.",3 The dataset can be downloaded from http://www.cfilt.iitb.ac.in/cognitive-nlp/,"Our work has the following contributions. Firstly, we propose a novel way to predict read-ers’ rating of text by recording their eye move-ments as they read the texts. Secondly, we show that if a reader has understood the text com-pletely, their gaze behaviour is more reliable. Thirdly, we also release our dataset [Cite_Footnote_3] to help in further research in using gaze features in other tasks involving predicting the quality of texts.",Material,Dataset,True,Produce（引用目的）,True,P18-1219_1_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1255,11259," https://www.sr-research.com/products/eyelink-portable-duo/"," ['2 Motivation']","Recently, SR Research has come up with a portable eye-tracking system [Cite_Footnote_5] .",5 https://www.sr-research.com/products/eyelink-portable-duo/,"One of the major concerns is How are we go-ing to get the gaze data? This is because capa-bility to gather eye-tracking data is not available to the masses. However, top mobile device man-ufacturers, like Samsung, have started integrat-ing basic eye-tracking software into their smart-phones (Samsung Smart Scroll) that are able to detect where the eye is fixated, and can be used in applications like scrolling through a web page. Start-ups, like Cogisen , have started using gaze features in their applications, such as using gaze information to improve input to image processing systems. Recently, SR Research has come up with a portable eye-tracking system [Cite_Footnote_5] .",Method,Tool,True,Introduce（引用目的）,True,P18-1219_2_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1256,11260," https://writing.wisc.edu/Handbook/Transitions.html"," ['4 Features', '4.1 Text-based Features']","The first set of features that we use are length and count-based features, such as word length, word count, sentence length, count of transition phrases [Cite_Footnote_6] etc. (Persing and Ng, 2015; Zesch et al., 2015).",6 https://writing.wisc.edu/Handbook/Transitions.html,"The first set of features that we use are length and count-based features, such as word length, word count, sentence length, count of transition phrases [Cite_Footnote_6] etc. (Persing and Ng, 2015; Zesch et al., 2015).",補足資料,Document,True,Introduce（引用目的）,True,P18-1219_3_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1257,11261," http://www.nltk.org/"," ['4 Features', '4.1 Text-based Features']","These were extracted using NLTK [Cite_Footnote_7] (Loper and Bird, 2002).",7 http://www.nltk.org/,"The third set of features that we use are stylis-tic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK [Cite_Footnote_7] (Loper and Bird, 2002).",Method,Tool,True,Use（引用目的）,True,P18-1219_4_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1258,11262," https://pypi.python.org/pypi/pyenchant/"," ['4 Features', '4.1 Text-based Features']",We use the count of words that are absent in Google News word vec-tors and misspelled words using the PyEnchant [Cite_Footnote_9] library.,9 https://pypi.python.org/pypi/pyenchant/,"The fifth set of features that we use are lan-guage modeling features. We use the count of words that are absent in Google News word vec-tors and misspelled words using the PyEnchant [Cite_Footnote_9] library. In order to check the grammaticality of the text, we construct a 5-gram language model, using the Brown Corpus (Francis and Kucera, 1979).",Material,Knowledge,True,Use（引用目的）,True,P18-1219_5_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1259,11263," https://simple.wikipedia.org"," ['5 Experiment Details', 'Details of Texts']",[Cite_Footnote_10] .,"10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com","To the best of our knowledge there isn’t a pub-licly available dataset with gaze features for tex-tual quality. Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [Cite_Footnote_10] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each. The original arti-cles ranged from a couple hundred words (Simple English Wikipedia) to over a thousand words (En-glish Wikipedia). We first summarized the longer articles manually. Then, for the many articles over 200 words, we removed a few of the paragraphs and sentences. In this way, despite all the texts being published, we were able to introduce some poor quality texts into our dataset. The articles were sampled from a variety of genres, such as History, Science, Law, Entertainment, Education, Sports, etc.",Material,Dataset,True,Use（引用目的）,True,P18-1219_6_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1260,11264," https://en.wikipedia.org"," ['5 Experiment Details', 'Details of Texts']",[Cite_Footnote_10] .,"10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com","To the best of our knowledge there isn’t a pub-licly available dataset with gaze features for tex-tual quality. Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [Cite_Footnote_10] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each. The original arti-cles ranged from a couple hundred words (Simple English Wikipedia) to over a thousand words (En-glish Wikipedia). We first summarized the longer articles manually. Then, for the many articles over 200 words, we removed a few of the paragraphs and sentences. In this way, despite all the texts being published, we were able to introduce some poor quality texts into our dataset. The articles were sampled from a variety of genres, such as History, Science, Law, Entertainment, Education, Sports, etc.",Material,DataSource,True,Use（引用目的）,True,P18-1219_7_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1261,11265," https://newsela.com"," ['5 Experiment Details', 'Details of Texts']",[Cite_Footnote_10] .,"10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com","To the best of our knowledge there isn’t a pub-licly available dataset with gaze features for tex-tual quality. Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [Cite_Footnote_10] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each. The original arti-cles ranged from a couple hundred words (Simple English Wikipedia) to over a thousand words (En-glish Wikipedia). We first summarized the longer articles manually. Then, for the many articles over 200 words, we removed a few of the paragraphs and sentences. In this way, despite all the texts being published, we were able to introduce some poor quality texts into our dataset. The articles were sampled from a variety of genres, such as History, Science, Law, Entertainment, Education, Sports, etc.",Material,DataSource,True,Use（引用目的）,True,P18-1219_8_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Footnote
1262,11266," https://www.tensorflow.org/"," ['5 Experiment Details', '5.5 Classification Details']","The Feed Forward Neural Net-work was implemented using TensorFlow (Abadi et al., 2015) [Cite_Ref] in Python.","Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.","We split the data into a training - test split of sizes 70% and 30%. We used a Feed Forward Neural Network with 1 hidden layer containing 100 neurons (Bebis and Georgiopoulos, 1994) . The size of the input vector was 361 features. Out of these, there were 49 text features, plus 300 dimension word embeddings features, 11 gaze features, and 1 class label. The data was split using stratified sampling, to ensure that there is a similar distribution of classes in each of the train-ing and test splits. The Feed Forward Neural Net-work was implemented using TensorFlow (Abadi et al., 2015) [Cite_Ref] in Python. We ran the neural network over 10,000 epochs, with a learning rate of 0.001 in 10 batches. The loss function that we used was the mean square error.",Method,Tool,True,Use（引用目的）,True,P18-1219_9_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Reference
1263,11267," http://www.aclweb.org/anthology/P/P14/P14-5010"," ['4 Features', '4.1 Text-based Features']","These features were extracted using Stan-ford CoreNLP (Manning et al., 2014) [Cite_Ref] , and Mor-phAdorner (Burns, 2013).","Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations. pages 55–60. http://www.aclweb.org/anthology/P/P14/P14-5010.","The next set of features that we use are com-plexity features, namely the degree of polysemy, coreference distance, and the Flesch Reading Ease Score (FRES) (Flesch, 1948). These features help in normalizing the gaze features for text complex-ity. These features were extracted using Stan-ford CoreNLP (Manning et al., 2014) [Cite_Ref] , and Mor-phAdorner (Burns, 2013).",Method,Tool,True,Use（引用目的）,True,P18-1219_13_0,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,Reference
1264,11268," https://github.com/ziyi-yang/GEM"," ['1 Introduction']",Our implemen-tation is available online [Cite_Footnote_1] .,1 https://github.com/ziyi-yang/GEM,"The rest of this paper is organized as following. In Section 2, we describe our sentence embedding algorithm GEM. We evaluate our model on vari-ous tasks in Section 3 and Section 4. Finally, we summarize our work in Section 5. Our implemen-tation is available online [Cite_Footnote_1] .",Method,Tool,False,Produce（引用目的）,True,D19-1059_0_0,2019,Parameter-free Sentence Embedding via Orthogonal Basis,Footnote
1265,11269," https://lucene.apache.org/core/"," ['3 Crosslingual Vector-Based Writing Assistance (CroVeWA)']","To il-lustrate the differences to phrase vector-based sen-tence retrieval, we also offer a retrieval option based on direct word-to-text matching using the EDICT Japanese-English dictionary (Breen, 2004) and Apache Lucene [Cite_Footnote_1] for sentence retrieval.",1 https://lucene.apache.org/core/,"Our system encourages refining retrieved results and viewing relations in different contexts by sup-porting multiple queries. All queries and their cor-responding results are visualized together to aid a better understanding of their relationships. To il-lustrate the differences to phrase vector-based sen-tence retrieval, we also offer a retrieval option based on direct word-to-text matching using the EDICT Japanese-English dictionary (Breen, 2004) and Apache Lucene [Cite_Footnote_1] for sentence retrieval.",Method,Tool,True,Compare（引用目的）,True,N15-3019_0_0,2015,CroVeWA: Crosslingual Vector-Based Writing Assistance,Footnote
1266,11270," http://orchid.kuee.kyoto-u.ac.jp/ASPEC/"," ['3 Crosslingual Vector-Based Writing Assistance (CroVeWA)', '3.1 Inducing Crosslingually Constrained Word Representations']","The bilingual sentence-parallel resource used is the ASPEC corpus [Cite_Footnote_2] , which fea-tures sentence-aligned text from scientific paper ab-stracts.",2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/,"Currently we use Japanese and English resources to learn word embeddings, but plan to add more lan-guages in the future. The bilingual sentence-parallel resource used is the ASPEC corpus [Cite_Footnote_2] , which fea-tures sentence-aligned text from scientific paper ab-stracts. For monolingual data, we use subsets of the Japanese and English Wikipedia.",Material,DataSource,True,Use（引用目的）,True,N15-3019_1_0,2015,CroVeWA: Crosslingual Vector-Based Writing Assistance,Footnote
1267,11271," https://github.com/JetRunner/beyond-preserved-accuracy"," ['References']","By com-bining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness. [Cite_Footnote_1]",1 Our code is available at https://github.com/JetRunner/beyond-preserved-accuracy.,"Recent studies on compression of pretrained language models (e.g., BERT) usually use pre-served accuracy as the metric for evaluation. In this paper, we propose two new metrics, la-bel loyalty and probability loyalty that mea-sure how closely a compressed model (i.e., stu-dent) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowl-edge distillation and progressive module re-placing with loyalty and robustness. By com-bining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.832_0_0,2021,Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression,Footnote
1268,11272," https://github.com/jeknov/EMNLP_17_submission"," ['1 Introduction']","• Make all associated code and data publicly avail-able, including detailed analysis results. [Cite_Footnote_1]",1 Available for download at: https://github.com/jeknov/EMNLP_17_submission,"• Make all associated code and data publicly avail-able, including detailed analysis results. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D17-1238_0_0,2017,Why We Need New Evaluation Metrics for NLG,Footnote
1269,11273," https://github.com/shawnwun/RNNLG"," ['2 End-to-End NLG Systems']",[Cite_Footnote_2] The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface re-alisation.,2 https://github.com/shawnwun/RNNLG,"• RNNLG : [Cite_Footnote_2] The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface re-alisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far.",Method,Tool,True,Introduce（引用目的）,True,D17-1238_1_0,2017,Why We Need New Evaluation Metrics for NLG,Footnote
1270,11274," https://github.com/UFAL-DSG/tgen"," ['2 End-to-End NLG Systems']",• TG EN : [Cite_Footnote_3] The system by Dušek and Jurčı́ček (2015) learns to incrementally generate deep-syntax dependency trees of candidate sentence plans (i.e. which MR elements to mention and the overall sentence structure).,3 https://github.com/UFAL-DSG/tgen,"• TG EN : [Cite_Footnote_3] The system by Dušek and Jurčı́ček (2015) learns to incrementally generate deep-syntax dependency trees of candidate sentence plans (i.e. which MR elements to mention and the overall sentence structure). Surface realisation is performed using a separate, domain-independent rule-based module.",Method,Tool,True,Introduce（引用目的）,True,D17-1238_2_0,2017,Why We Need New Evaluation Metrics for NLG,Footnote
1271,11275," https://github.com/glampouras/JLOLS_NLG"," ['2 End-to-End NLG Systems']","• LOLS : [Cite_Footnote_4] The system by Lampouras and Vlachos (2016) learns sentence planning and surface reali-sation using Locally Optimal Learning to Search ( LOLS ), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions.",4 https://github.com/glampouras/JLOLS_ NLG,"• LOLS : [Cite_Footnote_4] The system by Lampouras and Vlachos (2016) learns sentence planning and surface reali-sation using Locally Optimal Learning to Search ( LOLS ), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions.",Method,Tool,True,Introduce（引用目的）,True,D17-1238_3_0,2017,Why We Need New Evaluation Metrics for NLG,Footnote
1272,11276," http://nlp.stanford.edu/software/parser-faq.shtml"," ['4 Metrics', '4.2 Grammar-based metrics (GBMs)']","The Stanford parser score is not designed to measure grammat-icality, however, it will generally prefer a gram-matical parse to a non-grammatical one. [Cite_Footnote_7]",7 http://nlp.stanford.edu/software/parser-faq.shtml,"Grammar-based measures have been explored in related fields, such as MT (Giménez and Màrquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. We focus on two important prop-erties of texts here – readability and grammatical-ity: data. As a first approximation of grammatical-ity, we measure the number of misspellings (msp) and the parsing score as returned by the Stanford parser (prs). The lower the msp, the more gram-matically correct an utterance is. The Stanford parser score is not designed to measure grammat-icality, however, it will generally prefer a gram-matical parse to a non-grammatical one. [Cite_Footnote_7] Thus, lower parser scores indicate less grammatically-correct utterances. In future work, we aim to use specifically designed grammar-scoring functions, e.g. (Napoles et al., 2016), once they become pub-licly available.",補足資料,Document,True,Introduce（引用目的）,True,D17-1238_4_0,2017,Why We Need New Evaluation Metrics for NLG,Footnote
1273,11277," https://doi.org/10.1007/978-3-540-30586-638"," ['1 Introduction']","This is rarely the case, as shown by various studies in NLG (Stent et al., 2005 [Cite_Ref] ; Belz and Reiter, 2006; Reiter and Belz, 2009), as well as in related fields, such as dialogue systems (Liu et al., 2016), machine translation (MT) (Callison-Burch et al., 2006), and image captioning (Elliott and Keller, 2014; Kilick-aya et al., 2017).","Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341–351. https://doi.org/10.1007/978-3-540-30586-638.","Automatic evaluation measures, such as BLEU (Pa-pineni et al., 2002), are used with increasing fre-quency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012–2015 relies on automatic metrics (Gkatzia and Mahamood, 2015). Auto-matic evaluation is popular because it is cheaper and faster to run than human evaluation, and it is needed for automatic benchmarking and tuning of algorithms. The use of such metrics is, however, only sensible if they are known to be sufficiently correlated with human preferences. This is rarely the case, as shown by various studies in NLG (Stent et al., 2005 [Cite_Ref] ; Belz and Reiter, 2006; Reiter and Belz, 2009), as well as in related fields, such as dialogue systems (Liu et al., 2016), machine translation (MT) (Callison-Burch et al., 2006), and image captioning (Elliott and Keller, 2014; Kilick-aya et al., 2017). This paper follows on from the above previous work and presents another evalu-ation study into automatic metrics with the aim to firmly establish the need for new metrics. We consider this paper to be the most complete study to date, across metrics, systems, datasets and do-mains, focusing on recent advances in data-driven NLG. In contrast to previous work, we are the first to:",補足資料,Paper,True,Introduce（引用目的）,True,D17-1238_43_0,2017,Why We Need New Evaluation Metrics for NLG,Reference
1274,11278," https://doi.org/10.1007/978-3-540-30586-638"," ['7 Relation of Human and Automatic Metrics', '8.3 Example-based Analysis']","NLG, weather forecast (Stent et al., 2005) [Cite_Ref] weak positive (ρ = 0.47, LSA )","Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341–351. https://doi.org/10.1007/978-3-540-30586-638.","Dimension of human ratings Study Sentence Planning this paper weak positive (ρ = 0.33, WPS) Surface Realisation Domain weak negative (ρ = 0. − 31, parser) NLG, restaurant/hotel search (Reiter and Belz, 2009) none strong positive (Pearson’s r = 0.96, NIST ) NLG, weather forecast (Stent et al., 2005) [Cite_Ref] weak positive (ρ = 0.47, LSA ) (Liu et al., 2016) weak positive (ρ = 0.35, BLEU -4 ) (Elliott and Keller, 2014) positive (ρ = 0.53, METEOR ) (Kilickaya et al., 2017) positive (ρ = 0.64, SPICE ) (Cahill, 2009) N/A (Espinosa et al., 2010) weak positive (ρ = 0.43, TER ) negative (ρ = −0.56, NIST ) paraphrasing of news N/A dialogue/Twitter pairs N/A image caption N/A image caption negative (ρ = −0.64, ROUGE ) NLG, German news texts positive (ρ = 0.62, BLEU -4 ) NLG, news texts",補足資料,Paper,False,Introduce（引用目的）,False,D17-1238_43_1,2017,Why We Need New Evaluation Metrics for NLG,Reference
1275,11279," https://doi.org/10.1007/978-3-540-30586-638"," ['9 Related Work']","The re-sults confirm that metrics can be reliable indica-tors at system-level (Reiter and Belz, 2009), while they perform less reliably at sentence-level (Stent et al., 2005) [Cite_Ref] .","Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341–351. https://doi.org/10.1007/978-3-540-30586-638.","Table 6 summarises results published by previous studies in related fields which investigate the re-lation between human scores and automatic met-rics. These studies mainly considered WBMs, while we are the first study to consider GBMs. Some studies ask users to provide separate ratings for surface realisation (e.g. asking about ‘clarity’ or ‘fluency’), whereas other studies focus only on sentence planning (e.g. ‘accuracy’, ‘adequacy’, or ‘correctness’). In general, correlations reported by previous work range from weak to strong. The re-sults confirm that metrics can be reliable indica-tors at system-level (Reiter and Belz, 2009), while they perform less reliably at sentence-level (Stent et al., 2005) [Cite_Ref] . Also, the results show that the met-rics capture realization better than sentence plan-ning. There is a general trend showing that best-performing metrics tend to be the more complex ones, combining word-overlap, semantic similar-ity and term frequency weighting. Note, however, that the majority of previous works do not report whether any of the metric correlations are signifi-cantly different from each other.",補足資料,Paper,True,Introduce（引用目的）,True,D17-1238_43_2,2017,Why We Need New Evaluation Metrics for NLG,Reference
1276,11280," http://www.lsi.upc.edu/~nlp/IQMT"," ['3 Metrics and Test Beds', '3.1 Metric Set']","Metric computation has been carried out using the IQ MT Framework for Auto-matic MT Evaluation (Giménez, 2007) [Cite_Footnote_1] .",1 http://www.lsi.upc.edu/˜nlp/IQMT,"Finally, we have also considered ULC, which is a very simple approach to metric combina-tion based on the unnormalized arithmetic mean of metric scores, as described by Giménez and Màrquez (2008a). ULC considers a subset of met-rics which operate at several linguistic levels. This approach has proven very effective in recent eval-uation campaigns. Metric computation has been carried out using the IQ MT Framework for Auto-matic MT Evaluation (Giménez, 2007) [Cite_Footnote_1] . The sim-plicity of this approach (with no training of the metric weighting scheme) ensures that the poten-tial advantages detected in our experiments are not due to overfitting effects.",補足資料,Website,True,Introduce（引用目的）,True,P09-1035_0_0,2009,The Contribution of Linguistic Features to Automatic Machine Translation Evaluation,Footnote
1277,11281," http://www.nist.gov/speech/tests/mt"," ['3 Metrics and Test Beds', '3.2 Test Beds']","We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005) [Cite_Footnote_2] .",2 http://www.nist.gov/speech/tests/mt,"We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005) [Cite_Footnote_2] . Both campaigns include two dif-ferent translations exercises: Arabic-to-English (‘AE’) and Chinese-to-English (‘CE’). Human as-sessments of adequacy and fluency, on a 1-5 scale, are available for a subset of sentences, each eval-uated by two different human judges. A brief nu-merical description of these test beds is available in Table 1. The corpus AE05 includes, apart from five automatic systems, one human-aided system that is only used in our last experiment.",Material,Knowledge,False,Use（引用目的）,True,P09-1035_1_0,2009,The Contribution of Linguistic Features to Automatic Machine Translation Evaluation,Footnote
1278,11282," http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09"," ['1 Introduction']","However, we identify several limitations in the schemas produced by their system. [Cite_Footnote_2]",2 Available at http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09,"However, we identify several limitations in the schemas produced by their system. [Cite_Footnote_2] Their schemas often lack coherence: mixing unrelated events and having actors whose entities do not play the same role in the schema. Table 2 shows an event schema from Chambers that mixes the events of fire spread-ing and disease spreading.",Material,Knowledge,False,Use（引用目的）,True,D13-1178_0_0,2013,Generating Coherent Event Schemas at Scale,Footnote
1279,11283," http://relgrams.cs.washington.edu"," ['1 Introduction', '1.1 Contributions']",We release our open domain event schemas and the Rel-grams database [Cite_Footnote_3] for further use by the NLP community.,3 Available at http://relgrams.cs.washington.edu,We release our open domain event schemas and the Rel-grams database [Cite_Footnote_3] for further use by the NLP community.,Material,Dataset,True,Produce（引用目的）,True,D13-1178_1_0,2013,Generating Coherent Event Schemas at Scale,Footnote
1280,11284," http://knowitall.github.io/ollie/"," ['3 Modeling Relational Co-occurrence', '3.1 Relations Extraction and Representation']","We extract relational triples from each sentence in a large corpus using the OLLIE Open IE system (Mausam et al., 2012). [Cite_Footnote_4]",4 Available at: http://knowitall.github.io/ollie/,"We extract relational triples from each sentence in a large corpus using the OLLIE Open IE system (Mausam et al., 2012). [Cite_Footnote_4] This provides relational tu-ples in the format (Arg1, Relation, Arg2) where each tuple element is a phrase from the sentence. The sentence “He cited a new study that was released by UCLA in 2008.” produces three tuples:",Method,Tool,True,Use（引用目的）,True,D13-1178_2_0,2013,Generating Coherent Event Schemas at Scale,Footnote
1281,11285," http://nlp.stanford.edu/software/CRF-NER.shtml"," ['3 Modeling Relational Co-occurrence', '3.1 Relations Extraction and Representation']","To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005) [Cite_Footnote_5] , and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target se-mantic types.",5 We used the system downloaded from: http://nlp.stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it.,"To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005) [Cite_Footnote_5] , and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target se-mantic types. We use regular expressions to recog-nize dates and numeric expressions, and map per-sonal pronouns to . We associate all types found by this mechanism with each argument. The tuples in the example above are normalized to the following:",Method,Tool,True,Use（引用目的）,True,D13-1178_3_0,2013,Generating Coherent Event Schemas at Scale,Footnote
1282,11286," http://nlp.stanford.edu/software/dcoref.shtml"," ['3 Modeling Relational Co-occurrence', '3.2 Co-occurrence Tabulation']","We use the Stan-ford Co-reference system (Lee et al., 2013) [Cite_Footnote_6] to de-tect co-referring mentions.",6 Available for download at: http://nlp.stanford.edu/software/dcoref.shtml,"Equality Constraints: Along with the co-occurrence counts, we record the equality of argu-ments in Rel-grams pairs. We assert an argument pair is equal if they are from the same token se-quence in the source sentence or one argument is a co-referent mention of the other. We use the Stan-ford Co-reference system (Lee et al., 2013) [Cite_Footnote_6] to de-tect co-referring mentions. There are four possible equalities depending on the specific pair of argu-ments in the tuples are the same, shown as E11, E12, E21 and E22 in Figure 1. For example, the E21 col-umn has counts for the number of times the Arg2 of T1 was determined to be the same as the Arg1 of T2. Implementation and Query Language: We pop-ulated the Rel-grams database using OLLIE extrac-tions from a set of 1.8 Million New York Times arti-cles drawn from the Gigaword corpus. The database consisted of approximately 320K tuples that have frequency ≥ 3 and 1.1M entries in the bigram table.",Method,Tool,True,Use（引用目的）,True,D13-1178_4_0,2013,Generating Coherent Event Schemas at Scale,Footnote
1283,11287," http://www.usna.edu/Users/cs/"," ['5 Evaluation', '5.2 Schemas Evaluation']","We compare Rel-grams schemas against the state-of-the-art narrative schemas released by Cham-bers (Chambers and Jurafsky, 2009). [Cite_Footnote_8]",8 Available at http://www.usna.edu/Users/cs/,"We compare Rel-grams schemas against the state-of-the-art narrative schemas released by Cham-bers (Chambers and Jurafsky, 2009). [Cite_Footnote_8] Chambers’ schemas are less expressive than ours – they do not associate types with actors and each schema has a constant pre-specified number of relations. For a fair comparison we use a similarly expressive ver-sion of our schemas that strips off argument types and has the same number of relations per schema (six) as their highest quality output set.",Material,Knowledge,False,Compare（引用目的）,True,D13-1178_5_0,2013,Generating Coherent Event Schemas at Scale,Footnote
1284,11288," http://relgrams.cs.washington.edu"," ['7 Conclusions']",Both are freely available to the research community [Cite_Footnote_9] and may prove useful for a wide range of NLP applications.,9 available at http://relgrams.cs.washington.edu,We have created a Rel-grams database with 1.1M entries and a set of over 2K event schemas from a corpus of 1.8M New York Times articles. Both are freely available to the research community [Cite_Footnote_9] and may prove useful for a wide range of NLP applications.,Mixed,Mixed,False,Produce（引用目的）,False,D13-1178_6_0,2013,Generating Coherent Event Schemas at Scale,Footnote
1285,11289," http://alt.qcri.org/semeval2014/task4/"," ['4 Experiments', '4.1 Datasets']","To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 [Cite_Footnote_4] (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010).",4 http://alt.qcri.org/semeval2014/task4/,"To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 [Cite_Footnote_4] (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017).",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.582_1_0,2020,Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction,Footnote
1286,11290," http://alt.qcri.org/semeval2015/task12/"," ['4 Experiments', '4.1 Datasets']","To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 [Cite_Footnote_5] (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010).",5 http://alt.qcri.org/semeval2015/task12/,"To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 [Cite_Footnote_5] (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017).",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.582_2_0,2020,Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction,Footnote
1287,11291," http://www.cs.pitt.edu/mpqa/"," ['4 Experiments', '4.1 Datasets']","To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus [Cite_Footnote_6] (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010).",6 http://www.cs.pitt.edu/mpqa/,"To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus [Cite_Footnote_6] (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017).",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.582_3_0,2020,Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction,Footnote
1288,11292," http://verbs.colorado.edu/jdpacorpus/"," ['4 Experiments', '4.1 Datasets']","To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora [Cite_Footnote_7] (JDPA) (Kessler et al., 2010).",7 http://verbs.colorado.edu/jdpacorpus/,"To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora [Cite_Footnote_7] (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1. For SemEval 2014 and 2015 datasets, we manually build rela-tions between aspects and opinion expressions be-cause the original datasets only contain the gold standard annotation for aspects. Note that we fol-low the annotations for opinion expressions pro-vided by Wang et al. (2016) and Wang et al. (2017).",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.582_4_0,2020,Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction,Footnote
1289,11293," https://github.com/wenhuchen/Variational-Vocabulary-Selection.git"," ['1 Introduction']",The code will be released in Github [Cite_Footnote_1] .,1 https://github.com/wenhuchen/Variational-Vocabulary-Selection.git,2. We propose a novel vocabulary selection algorithm based on variational dropout by re-formulating text classification under the Bayesian inference framework. The code will be released in Github [Cite_Footnote_1] .,Method,Code,True,Produce（引用目的）,True,N19-1352_0_0,2019,How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection,Footnote
1290,11294," https://github.com/dennybritz/cnn-text-classification-tf"," ['2 Vocabulary Selection', '2.1 Problem Definition']","Here we showcase the em-bedding matrix size of a popular text classification model [Cite_Footnote_2] on AG-news dataset (Zhang et al., 2015) in Table 1.",2 https://github.com/dennybritz/ cnn-text-classification-tf,"We now formally define the problem setting and introduce the notations for our problem. Conven-tionally, we assume the neural classification model vectorizes the discrete language input into a vec-tor representation via an embedding matrix W ∈ R V∗D , where V denotes the size of the vocabu-lary, and D denotes the vector dimension. The em-bedding is associated with a pre-defined word-to-index dictionary V = {w i : i|1 ≤ i ≤ V } where w i denotes a literal word corresponding to i th row in the embedding matrix. The embedding matrix W covers the subset of a vocabulary of interests for a particular NLP task, note that the value of V is known to be very large due to the rich variations in human languages. Here we showcase the em-bedding matrix size of a popular text classification model [Cite_Footnote_2] on AG-news dataset (Zhang et al., 2015) in Table 1. From which we can easily observe that the embedding matrix is commonly occupy-ing most of the parameter capacity, which could be the bottleneck in many real-world applications with limited computation resources.",Material,Knowledge,False,Produce（引用目的）,True,N19-1352_1_0,2019,How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection,Footnote
1291,11295," https://github.com/coetaur0/ESIM"," ['4 Experiments', '4.1 Datasets & Architectures']","In natural language in-ference, we follow the popular ESIM architec-ture (Williams et al., 2018; Chen et al., 2017) us-ing the Github implementation [Cite_Footnote_3] .",3 https://github.com/coetaur0/ESIM,"The main datasets we are using are listed in Ta-ble 2, which provides an overview of its descrip-tion and capacities. Specifically, we follow (Zhang et al., 2015; Goo et al., 2018; Williams et al., 2018) to pre-process the document classification datasets, natural language understanding dataset and natural language inference dataset. We ex-actly replicate their experiment settings to make our method comparable with theirs. Our mod-els is implemented with TensorFlow (Abadi et al., 2015). In order to evaluate the generalization abil-ity of VVD selection algorithm in deep learning architectures, we study its performance under dif-ferent established architectures (depicted in Fig-ure 3). In natural language understanding, we use the most recent attention-based model for in-tention tracking (Goo et al., 2018), this model first uses BiLSTM recurrent network to leverage left-to-right and right-to-left context information to form the hidden representation, then computes self-attention weights to aggregate the hidden rep-resentation and predicts user intention. In doc-ument classification, we mainly follow the CNN architecture (Kim, 2014) to extract n-gram fea-tures and then aggregate these features to pre-dict document category. In natural language in-ference, we follow the popular ESIM architec-ture (Williams et al., 2018; Chen et al., 2017) us-ing the Github implementation [Cite_Footnote_3] . In this structure, three main components input encoding, local in-ference modeling, and inference composition are used to perform sequential inference and composi-tion to simulate the interaction between premises and hypothesis. Note that, we do not apply the syntax-tree based LSTM proposed in (Chen et al., 2017) because we lost the parse tree (Klein and Manning, 2003) after the vocabulary compression, instead, we follow the simpler sequential LSTM framework without any syntax parse as input. Be-sides, the accuracy curve is obtained using the publicly available test split rather than the official online evaluation because we need to evaluate lots of times at different vocabulary capacity.",Method,Tool,False,Produce（引用目的）,False,N19-1352_2_0,2019,How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection,Footnote
1292,11296," https://github.com/adymaharana/StoryViz"," ['References']",We also present correlation experiments of our proposed automated metrics with hu-man evaluations. [Cite_Footnote_1],1 Code and data: https://github.com/adymaharana/StoryViz.,"Story visualization is an underexplored task that falls at the intersection of many impor-tant research directions in both computer vi-sion and natural language processing. In this task, given a series of natural language cap-tions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced re-current generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a num-ber of improvements to prior modeling ap-proaches, including (1) the addition of a dual learning framework that utilizes video caption-ing to reinforce the semantic alignment be-tween the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interac-tions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not ac-curately reflect performance. Therefore, we also provide an exploration of evaluation met-rics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to cap-tions, and the diversity of the generated im-ages. We also present correlation experiments of our proposed automated metrics with hu-man evaluations. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.194_0_0,2021,Improving Generation and Evaluation of Visual Stories via Semantic Consistency,Footnote
1293,11297," https://en.wikipedia.org/wiki/Languagefamily"," ['3 Multilingual NMT with Language Clustering', '3.1 Prior Knowledge Based Clustering']",Language family is a group of languages re-lated through descent from a common ancestral language or parental language [Cite_Footnote_2] .,2 https://en.wikipedia.org/wiki/Language family,"Language family is a group of languages re-lated through descent from a common ancestral language or parental language [Cite_Footnote_2] . There are dif-ferent kinds of taxonomies for language family in the world, among which, Ethnologue (Paul et al., 2009) is one of the most authoritative and com-monly accepted taxonomies. The 7,472 known languages in the world fall into 152 families ac-cording to Paul et al. (2009). We regard the lan-guages in the same family as similar languages and group them into one cluster.",補足資料,Document,True,Introduce（引用目的）,True,D19-1089_0_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1294,11298," https://www.ethnologue.com/browse/families"," ['3 Multilingual NMT with Language Clustering', '3.1 Prior Knowledge Based Clustering']","There are dif-ferent kinds of taxonomies for language family in the world, among which, Ethnologue (Paul et al., 2009) [Cite_Footnote_3] is one of the most authoritative and com-monly accepted taxonomies.",3 https://www.ethnologue.com/browse/families,"Language family is a group of languages re-lated through descent from a common ancestral language or parental language . There are dif-ferent kinds of taxonomies for language family in the world, among which, Ethnologue (Paul et al., 2009) [Cite_Footnote_3] is one of the most authoritative and com-monly accepted taxonomies. The 7,472 known languages in the world fall into 152 families ac-cording to Paul et al. (2009). We regard the lan-guages in the same family as similar languages and group them into one cluster.",補足資料,Document,True,Introduce（引用目的）,True,D19-1089_1_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1295,11299," https://en.wikipedia.org/wiki/Languageisolate"," ['3 Multilingual NMT with Language Clustering', '3.3 Discussions']","First, the tax-onomy built on language family does not cover all the languages in the world since some languages are isolate [Cite_Footnote_5] .",5 https://en.wikipedia.org/wiki/Language isolate,"We analyze and compare the two clustering meth-ods proposed in this section. Clustering based on prior knowledge (language family) is simple. The taxonomy based on language family is consistent with the human knowledge, easy to understand, and does not change with respect to data/time. This method also has drawbacks. First, the tax-onomy built on language family does not cover all the languages in the world since some languages are isolate [Cite_Footnote_5] . Second, there are many language families (152 according to Paul et al. (2009)), which means that we still need a large number of models to handle all the languages in the world. Third, language family cannot characterize all the features of a language to fully capture the similar-ity between languages.",補足資料,Document,True,Introduce（引用目的）,True,D19-1089_2_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1296,11300," https://wit3.fbk.eu/"," ['4 Experiment Setup']","We collect datasets from the IWSLT evaluation campaign [Cite_Footnote_6] from years 2011 to 2018, which consist of the translation pairs of 23 languages↔English.",6 https://wit3.fbk.eu/,"Datasets We evaluate our method on the IWSLT datasets which contain multiple languages from TED talks. We collect datasets from the IWSLT evaluation campaign [Cite_Footnote_6] from years 2011 to 2018, which consist of the translation pairs of 23 languages↔English. The detailed description of the training/validation/test set of the 23 transla-tion pairs can be found in Supplementary Mate-rials (Section 1). All the data has been tokenized and segmented into sub-word symbols using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We learn the BPE operations for all languages to-gether, which results in a shared vocabulary of 90K BPE tokens.",Material,DataSource,True,Use（引用目的）,True,D19-1089_3_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1297,11301," https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl"," ['4 Experiment Setup']","We evaluate the translation quality by tokenized case sensitive BLEU (Papineni et al., 2002) with multi-bleu.pl [Cite_Footnote_7] .",7 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl,"During inference, each source token is also added with the corresponding language embed-ding in order to give the model a sense of the lan-guage it is currently processing. We decode with beam search and set beam size to 6 and length penalty α = 1.1 for all the languages. We evaluate the translation quality by tokenized case sensitive BLEU (Papineni et al., 2002) with multi-bleu.pl [Cite_Footnote_7] .",Method,Tool,False,Use（引用目的）,True,D19-1089_4_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1298,11302," https://github.com/tensorflow/tensor2tensor"," ['5 Results', '5.1 Results of Language Clustering']","The language clustering based on language family is shown in Figure 2, which results in [Cite_Footnote_8] groups given our 23 languages.",8 https://github.com/tensorflow/tensor2tensor,"The language clustering based on language family is shown in Figure 2, which results in [Cite_Footnote_8] groups given our 23 languages. All the language names and their corresponding ISO-639-1 code can be found in Supplementary Materials (Section 2).",Material,Knowledge,True,Introduce（引用目的）,True,D19-1089_5_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1299,11303," https://www.iso.org/iso-639-language-codes.html"," ['5 Results', '5.1 Results of Language Clustering']",All the language names and their corresponding ISO-639-1 code [Cite_Footnote_9] can be found in Supplementary Materials (Section 2).,9 https://www.iso.org/iso-639-language-codes.html.,"The language clustering based on language family is shown in Figure 2, which results in groups given our 23 languages. All the language names and their corresponding ISO-639-1 code [Cite_Footnote_9] can be found in Supplementary Materials (Section 2).",Material,Knowledge,True,Introduce（引用目的）,True,D19-1089_6_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1300,11304," https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html"," ['5 Results', '5.1 Results of Language Clustering']","We use hierarchical clustering (Rokach and Maimon, 2005) [Cite_Footnote_10] method to group the languages based on language embeddings.",10 https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html,"We use hierarchical clustering (Rokach and Maimon, 2005) [Cite_Footnote_10] method to group the languages based on language embeddings. We use the el-bow method (Thorndike, 1953) to automatically decide the optimal number of clusters K. Note that we have tried to extract the language embed-dings from multiple model checkpoints randomly chosen in the later training process, and found that the clustering results based on these language em-beddings are stable. Figure 3 demonstrates the clustering results based on language embeddings. Each color represents a language cluster and there are 7 clusters according to the elbow method (the details of how this method determines the optimal number of clusters are shown in Figure 7). Fig-ure 3 clearly shows the agglomerative process of the languages and demonstrates the fine-grained relationship between languages.",補足資料,Document,True,Use（引用目的）,True,D19-1089_7_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1301,11305," https://en.wikipedia.org/wiki/Persian"," ['5 Results', '5.1 Results of Language Clustering']","Ar (Arabic) and He (Hebrew) share the same Semitic language branch in Afroasiatic language family, while Fa (Persian) has been influenced much by Arabic due to history and religion in West Asia [Cite_Footnote_12] .",12 https://en.wikipedia.org/wiki/Persian vocabulary#Arabic influence,"• Language embeddings capture the regional, cultural, and historical influences. The lan-guages in Cluster #5 (He, Ar, Fa) of Fig-ure 3 are close to each other in geographi-cal location (West Asia). Ar (Arabic) and He (Hebrew) share the same Semitic language branch in Afroasiatic language family, while Fa (Persian) has been influenced much by Arabic due to history and religion in West Asia [Cite_Footnote_12] . Language embedding can implicitly learn this relationship and cluster them to-gether.",補足資料,Document,True,Introduce（引用目的）,True,D19-1089_8_0,2019,Multilingual Neural Machine Translation with Language Clustering,Footnote
1302,11306," https://www.barna.com/research/hate-speech-increased/"," ['1 Introduction']","The occurrence of hatespeech has been increas-ing (Barna, 2019) [Cite_Ref] .",Barna. 2019. U.S. adults believe hate speech has increased — mainly online. research releases in culture & media. https://www.barna.com/research/hate-speech-increased/.,"The occurrence of hatespeech has been increas-ing (Barna, 2019) [Cite_Ref] . It has become easier than before to reach a large audience quickly via social media, causing an increase of the temptation for inappro-priate behaviors such as hatespeech, and potential damage to social systems. In particular, hatespeech interferes with civil discourse and turns good peo-ple away. Furthermore, hatespeech in the virtual world can lead to physical violence against cer-tain groups in the real world 12 , so it should not be ignored on the ground of freedom of speech.",補足資料,Document,False,Introduce（引用目的）,True,2020.emnlp-main.606_2_0,2020,HABERTOR: An Efficient and Effective Deep Hatespeech Detector,Reference
1303,11307," https://en.wikipedia.org/wiki/Quaternion"," ['References']","Then, pretraining with two language modeling tasks aims to minimize both loss functions L 1 and L 2 by: L LM = argmin θ L 1 + L 2 A.2 Quaternion In mathematics, Quaternions [Cite_Footnote_5] are a hypercomplex number system.",5 https://en.wikipedia.org/wiki/Quaternion,"Then, pretraining with two language modeling tasks aims to minimize both loss functions L 1 and L 2 by: L LM = argmin θ L 1 + L 2 A.2 Quaternion In mathematics, Quaternions [Cite_Footnote_5] are a hypercomplex number system. A Quaternion number P in a Quaternion space H is formed by a real component (r) and three imaginary components as follows:",補足資料,Document,True,Introduce（引用目的）,True,2020.emnlp-main.606_3_0,2020,HABERTOR: An Efficient and Effective Deep Hatespeech Detector,Footnote
1304,11308," http://www.tycho.iel.unicamp.br/~tycho/corpus/en/index.html"," ['2 Corpus description', '2.1 Part-of-speech tags']","1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010) [Cite_Ref] , totaling 4.5 million words.",Charlotte Galves and Pabol Faria. 2010. Tycho Brahe Parsed Corpus of Historical Portuguese. http://www.tycho.iel.unicamp.br/˜tycho/corpus/en/index.html.,"3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010) [Cite_Ref] , totaling 4.5 million words.",補足資料,Paper,True,Introduce（引用目的）,True,P14-2108_0_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1305,11309," http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html"," ['1 Introduction']","1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) [Cite_Ref] (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).","Anthony Kroch and Ann Taylor. 2000. Penn-Helsinki Parsed Corpus of Middle English, second edition. http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html.","1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) [Cite_Ref] (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).",Material,Knowledge,True,Introduce（引用目的）,True,P14-2108_1_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1306,11310," http://www.ling.upenn.edu/hist-corpora/PPCEME-RELEASE-2/index.html"," ['1 Introduction']","1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) [Cite_Ref] (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).","Anthony Kroch, Beatrice Santorini, and Ariel Diertani. 2004. Penn-Helsinki Parsed Cor-pus of Early Modern English. http: //www.ling.upenn.edu/hist-corpora/ PPCEME-RELEASE-2/index.html.","1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) [Cite_Ref] (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).",Material,Knowledge,True,Introduce（引用目的）,True,P14-2108_2_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1307,11311," http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html"," ['1 Introduction']","We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010) [Cite_Ref] , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999).","Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.","We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010) [Cite_Ref] , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct com-parison problematic. Some first steps at analysis of the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.",Material,Knowledge,True,Introduce（引用目的）,True,P14-2108_3_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1308,11312," https://code.google.com/p/berkeleyparser/"," ['5 Parsing Experiments']","The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) [Cite_Ref] and score using the standard evalb program (Sekine and Collins, 2008).","Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2008. The Berkeley Parser. https://code.google.com/p/berkeleyparser/.","The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) [Cite_Ref] and score using the standard evalb program (Sekine and Collins, 2008). We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006). Since the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags. Table 4 shows the results for both modes.",Method,Tool,True,Use（引用目的）,True,P14-2108_4_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1309,11313," http://nlp.cs.nyu.edu/evalb/"," ['5 Parsing Experiments']","The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) and score using the standard evalb program (Sekine and Collins, 2008) [Cite_Ref] .",Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.,"The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) and score using the standard evalb program (Sekine and Collins, 2008) [Cite_Ref] . We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006). Since the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags. Table 4 shows the results for both modes.",Method,Code,False,Use（引用目的）,True,P14-2108_5_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1310,11314," http://www-users.york.ac.uk/~lang22/YCOE/YcoeHome.htm"," ['2 Corpus description', '2.1 Part-of-speech tags']","1, there are also historical corpora of Old English (Taylor et al., 2003) [Cite_Ref] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.","Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/˜lang22/YCOE/YcoeHome.htm.","3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003) [Cite_Ref] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.",Material,Dataset,True,Introduce（引用目的）,True,P14-2108_6_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1311,11315," http://www-users.york.ac.uk/~lang22/PCEEC-manual/index.htm"," ['1 Introduction']","1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) [Cite_Ref] (2.2 million words).","Ann Taylor, Arja Nurmi, Anthony Warner, Susan Pintzuk, and Terttu Nevalainen. 2006. Parsed Corpus of Early English Correspondence. Com-piled by the CEEC Project Team. York: Uni-versity of York and Helsinki: University of Helsinki. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/˜lang22/PCEEC-manual/index.htm.","1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) [Cite_Ref] (2.2 million words).",Material,Knowledge,False,Introduce（引用目的）,True,P14-2108_7_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1312,11316," http://www.linguist.is/icelandic_treebank"," ['2 Corpus description', '2.1 Part-of-speech tags']","1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011) [Cite_Ref] , French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.","Joel Wallenberg, Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson. 2011. Icelandic Parsed Historical Corpus (IcePaHC) version 0.4. http://www.linguist.is/icelandic_treebank.","3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011) [Cite_Ref] , French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.",Material,Knowledge,True,Introduce（引用目的）,True,P14-2108_8_0,2014,The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis,Reference
1313,11317," https://github.com/IsakZhang/Generative-ABSA"," ['References']",This also validates the strong generality of the pro-posed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design. [Cite_Footnote_1],1 The data and code can be found at https://github.com/IsakZhang/Generative-ABSA,"Aspect-based sentiment analysis (ABSA) has received increasing attention recently. Most existing work tackles ABSA in a discrimina-tive manner, designing various task-specific classification networks for the prediction. De-spite their effectiveness, these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs. In this paper, we propose to tackle various ABSA tasks in a unified generative framework. Two types of paradigms, namely annotation-style and extraction-style modeling, are designed to enable the training process by formulating each ABSA task as a text generation problem. We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases. This also validates the strong generality of the pro-posed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-short.64_0_0,2021,Towards Generative Aspect-Based Sentiment Analysis ∗,Footnote
1314,11318," https://github.com/huggingface/transformers"," ['3 Experiments', '3.1 Experimental Setup']",Experiment Details We adopt the T5 base model from huggingface Transformer library [Cite_Footnote_2] for all experiments.,2 https://github.com/huggingface/ transformers,"Experiment Details We adopt the T5 base model from huggingface Transformer library [Cite_Footnote_2] for all experiments. T5 closely follows the original encoder-decoder architecture of the Transformer model, with some slight differences such as differ-ent position embedding schemes. Therefore, the encoder and decoder of it have similar parameter size as the B ERT -B ASE model. For all tasks, we use similar experimental settings for simplicity: we train the model with the batch size of 16 and accu-mulate gradients every two batches. The learning rate is set to be 3e-4. The model is trained up to 20 epochs for the AOPE, UABSA, and ASTE task and 30 epochs for the TASD task.",Method,Code,True,Use（引用目的）,True,2021.acl-short.64_1_0,2021,Towards Generative Aspect-Based Sentiment Analysis ∗,Footnote
1315,11319," https://github.com/ryankiros/skip-thoughts/"," ['3 Skip-Thought Generative Adversarial Network', '3.3 Model Architecture']",This work uses the 4800-dimensional vectors as they have been found to be the best performing in experiments [Cite_Footnote_1] .,1 https://github.com/ryankiros/skip-thoughts/,"The Skip-Thought encoder for the model en-codes sentences using 2400 GRU units (Chung et al., 2014) with a word vector dimensionality of 620. The encoder combines the sentence em-beddings to produce 4800-dimensional combine-skip vectors with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model. This work uses the 4800-dimensional vectors as they have been found to be the best performing in experiments [Cite_Footnote_1] . For training of the STGAN, the Skip-Thought encoder produces sentence embed-ding vectors which are labelled as real samples for GAN discriminator.",Method,Code,False,Use（引用目的）,True,N19-3008_0_0,2019,Generating Text through Adversarial Training using Skip-Thought Vectors,Footnote
1316,11320," https://github.com/clab/sp2016.11-731/tree/master/hw4/data"," ['4 Results and Discussion', '4.2 Language Generation.']","Language generation is performed on a dataset comprising simple English sentences referred to as CMU-SE [Cite_Footnote_2] (Rajeswar et al., 2017).",2 https://github.com/clab/sp2016.11-731/tree/master/hw4/data,"Language generation is performed on a dataset comprising simple English sentences referred to as CMU-SE [Cite_Footnote_2] (Rajeswar et al., 2017). The CMU-SE dataset consists of 44,016 sentences with a vocabulary of 3,122 words. The vectors are ex-tracted in batches of same-lengthed sentences for encoding. The samples represent how mode col-lapse is manifested when using least-squares dis-tance (Mao et al., 2016) f-measure without mini-batch discrimination. Table 3(a) contains sen-tences generated from a vanilla STGAN which mode collapse is observed, while 3(b) contains examples wherein it is not observed when using minibatch discrimination. Table 3(c) shows gen-erated samples from STGAN when using Wasser-stein distance f-measure as WGAN (Arjovsky et al., 2017)) and 3(d) contains samples when us-ing a gradient penalty regularizer term as WGAN-GP (Gulrajani et al., 2017). The two models gen-erate longer human-like sentences and over a more diverse vocabulary.",Material,Dataset,True,Use（引用目的）,True,N19-3008_1_0,2019,Generating Text through Adversarial Training using Skip-Thought Vectors,Footnote
1317,11321," https://github.com/ryankiros/neural-storyteller"," ['5 Conclusion']","In fu-ture, this work aims to be applied for synthesizing images from text, exploring complementary archi-tectures to projects like neural-storyteller [Cite_Footnote_3] where skip-thought embeddings are already used to per-form image captioning with story-style transfer.",3 https://github.com/ryankiros/ neural-storyteller,"This work presents a simple and effective model for text generation based on adversarial training using sentence embeddings. It shows how the use of sentence-level embeddings allows modelling the way of expression of an author in generated text in a better way than when using word-level embeddings. A performance comparison across several metrics is made between different GAN architectures with improved training stability and attention augmented LSTM models. Finally, it discusses how the automated corpus-based eval-uations correlate with human judgements. In fu-ture, this work aims to be applied for synthesizing images from text, exploring complementary archi-tectures to projects like neural-storyteller [Cite_Footnote_3] where skip-thought embeddings are already used to per-form image captioning with story-style transfer.",補足資料,Website,True,Compare（引用目的）,True,N19-3008_2_0,2019,Generating Text through Adversarial Training using Skip-Thought Vectors,Footnote
1318,11322," https://code.google.com/p/word2vec/"," ['4 Evaluation setting']",We use the word2vec soft-ware [Cite_Footnote_3] to build vectors of size 300 and using a con-text window of 5 words to either side of the target.,3 Available at https://code.google.com/p/word2vec/,"Construction of vector spaces We test two types of vector representations. The cbow model introduced in Mikolov et al. (2013a) learns vec-tor representations using a neural network archi-tecture by trying to predict a target word given the words surrounding it. We use the word2vec soft-ware [Cite_Footnote_3] to build vectors of size 300 and using a con-text window of 5 words to either side of the target. We set the sub-sampling option to 1e-05 and esti-mate the probability of a target word with the neg-ative sampling method, drawing 10 samples from the noise distribution (see Mikolov et al. (2013a) for details). We also implement a standard count-based bag-of-words distributional space (Turney and Pantel, 2010) which counts occurrences of a target word with other words within a symmetric window of size 5. We build a 300Kx300K sym-metric co-occurrence matrix using the top most frequent words in our source corpus, apply posi-tive PMI weighting and Singular Value Decompo-sition to reduce the space to 300 dimensions. For both spaces, the vectors are finally normalized to unit length.",Method,Tool,True,Use（引用目的）,True,P14-1059_0_0,2014,How to make words with vectors: Phrase generation in distributional semantics,Footnote
1319,11323," http://wacky.sslmit.unibo"," ['4 Evaluation setting']",The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [Cite_Footnote_5],"5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk","For both types of vectors we use 2.8 billion to-kens as input (ukWaC + Wikipedia + BNC). The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [Cite_Footnote_5] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures. We sanity-check the two spaces on MEN (Bruni et al., 2012), a 3,000 items word similarity data set. cbow sig-nificantly outperforms count (0.80 vs. 0.72 Spear-man correlations with human judgments). count performance is consistent with previously reported results. 6 (De)composition function training The train-ing data sets consist of the 50K most frequent hu, v, pi tuples for each phrase type, for example, hred, car, red.cari or hin, car, in.cari. 7 We con-catenate ~u and v~ vectors to obtain the [U; V ] ma-trix and we use the observed ~p vectors (e.g., the corpus vector of the red.car bigram) to obtain the phrase matrix P. We use these data sets to solve the least squares regression problems in eqs. (1) and (2), obtaining estimates of the composition and decomposition matrices, respectively. For the decomposition function in eq. (3), we replace the observed phrase vectors with those composed with f comp R (u~,~v), where f comp R is the previously esti-mated composition function for relation R.",Material,DataSource,True,Use（引用目的）,True,P14-1059_1_0,2014,How to make words with vectors: Phrase generation in distributional semantics,Footnote
1320,11324," http://www.natcorp.ox.ac.uk"," ['4 Evaluation setting']",The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [Cite_Footnote_5],"5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk","For both types of vectors we use 2.8 billion to-kens as input (ukWaC + Wikipedia + BNC). The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [Cite_Footnote_5] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures. We sanity-check the two spaces on MEN (Bruni et al., 2012), a 3,000 items word similarity data set. cbow sig-nificantly outperforms count (0.80 vs. 0.72 Spear-man correlations with human judgments). count performance is consistent with previously reported results. 6 (De)composition function training The train-ing data sets consist of the 50K most frequent hu, v, pi tuples for each phrase type, for example, hred, car, red.cari or hin, car, in.cari. 7 We con-catenate ~u and v~ vectors to obtain the [U; V ] ma-trix and we use the observed ~p vectors (e.g., the corpus vector of the red.car bigram) to obtain the phrase matrix P. We use these data sets to solve the least squares regression problems in eqs. (1) and (2), obtaining estimates of the composition and decomposition matrices, respectively. For the decomposition function in eq. (3), we replace the observed phrase vectors with those composed with f comp R (u~,~v), where f comp R is the previously esti-mated composition function for relation R.",Material,DataSource,True,Use（引用目的）,True,P14-1059_2_0,2014,How to make words with vectors: Phrase generation in distributional semantics,Footnote
1321,11325," http://clic.cimec.unitn.it/composes"," ['5 Noun phrase generation', '5.2 Recursive decomposition']","The data set contains the following 14 preposi-tions: after, against, at, before, between, by, for, from, in, on, per, under, with, without. [Cite_Footnote_8]",8 This dataset is available at http://clic.cimec.unitn.it/composes,"We continue by testing generation through recur-sive decomposition on the task of generating noun-preposition-noun (NPN) paraphrases of adjective-nouns (AN) phrases. We introduce a dataset con-taining 192 AN-NPN pairs (such as pre-election promises→ promises before election), which was created by the second author and additionally cor-rected by an English native speaker. The data set was created by analyzing a list of randomly se-lected frequent ANs. 49 further ANs (with adjec-tives such as amazing and great) were judged not NPN-paraphrasable and were used for the experi-ment reported in Section 7. The paraphrased sub-set focuses on preposition diversity and on includ-ing prepositions which are rich in semantic content and relevant to paraphrasing the AN. This has led to excluding of, which in most cases has the purely syntactic function of connecting the two nouns. The data set contains the following 14 preposi-tions: after, against, at, before, between, by, for, from, in, on, per, under, with, without. [Cite_Footnote_8]",Material,Dataset,True,Produce（引用目的）,True,P14-1059_3_0,2014,How to make words with vectors: Phrase generation in distributional semantics,Footnote
1322,11326," http://opus.lingfil.uu.se/"," ['6 Noun phrase translation']","Adjective-noun translation dataset We ran-domly extract 1,000 AN-AN En-It phrase pairs from a phrase table built from parallel movie sub-titles, available at [Cite] http://opus.lingfil.",,"Adjective-noun translation dataset We ran-domly extract 1,000 AN-AN En-It phrase pairs from a phrase table built from parallel movie sub-titles, available at [Cite] http://opus.lingfil.uu.se/ (OpenSubtitles2012, en-it) (Tiedemann, 2012). translation: phrases are composed in source lan-guage and decomposed in target language. Train-ing on composed phrase representations (eq. (3)) (with observed phrase training (eq. 2) results are ≈50% lower).",Material,DataSource,True,Use（引用目的）,True,P14-1059_4_0,2014,How to make words with vectors: Phrase generation in distributional semantics,Body
1323,11327," https://osf.io/rtzv4"," ['3 The DAIS dataset']","On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were “about the same” (see Appendix A for details) [Cite_Footnote_1] .",1 Our procedure and behavioral analysis plan were pre-registered at https://osf.io/rtzv4 and we have re-leased all data and analysis code at https://github.com/taka-yamakoshi/neural_constructions.,"We collected judgments from 1011 participants on Amazon Mechanical Turk. Each participant was shown 50 dative alternation pairs (DO vs. PO) using unique verbs, balanced across the possible recipient and theme conditions. On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were “about the same” (see Appendix A for details) [Cite_Footnote_1] .",補足資料,Paper,True,Produce（引用目的）,True,2020.emnlp-main.376_0_0,2020,Investigating representations of verb bias in neural language models,Footnote
1324,11328," https://github.com/taka-yamakoshi/neural_constructions"," ['3 The DAIS dataset']","On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were “about the same” (see Appendix A for details) [Cite_Footnote_1] .",1 Our procedure and behavioral analysis plan were pre-registered at https://osf.io/rtzv4 and we have re-leased all data and analysis code at https://github.com/taka-yamakoshi/neural_constructions.,"We collected judgments from 1011 participants on Amazon Mechanical Turk. Each participant was shown 50 dative alternation pairs (DO vs. PO) using unique verbs, balanced across the possible recipient and theme conditions. On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were “about the same” (see Appendix A for details) [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.376_1_0,2020,Investigating representations of verb bias in neural language models,Footnote
1325,11329," http://news.bbc.co.uk/"," ['4 Experimental Setup']","It contains 3,361 documents that have been down-loaded from the BBC News website. [Cite_Footnote_2]",2 http://news.bbc.co.uk/,"Data We trained the multimodal topic model on the corpus created in Feng and Lapata (2008). It contains 3,361 documents that have been down-loaded from the BBC News website. [Cite_Footnote_2] Each doc-ument comes with an image that depicts some of its content. The images are usually 203 pixels wide and 152 pixels high. The average document length is 133.85 words. The corpus has 542,414 words in total. Our experiments used a vocabulary of 6,253 textual words. These were words that occurred at least five times in the whole corpus, excluding stopwords. The accompanying images were prepro-cessed as follows. We first extracted SIFT features from each image (150 on average) which we subse-quently quantized into a discrete set of visual terms using K-means. As we explain below, we deter-mined an optimal value for K experimentally.",補足資料,Website,True,Use（引用目的）,True,N10-1011_0_0,2010,Visual Information in Semantic Representation,Footnote
1326,11330," http://www.usf.edu/Freeassociation"," ['4 Experimental Setup']","In order to simulate word association, we used the human norms collected by Nelson et al. (1999). [Cite_Footnote_3]",3 http://www.usf.edu/Freeassociation.,"In order to simulate word association, we used the human norms collected by Nelson et al. (1999). [Cite_Footnote_3] These were established by presenting a large num-ber of participants with a cue word (e.g., rice) and asking them to name an associate word in response (e.g., Chinese, wedding, food, white). For each word, the norms provide a set of associates and the fre-quencies with which they were named. We can thus compute the probability distribution over associates for each cue. Analogously, we can estimate the de-gree of similarity between a cue and its associates using our model (and any of the measures in Sec-tion 3.3). And consequently examine (using corre-lation analysis) the degree of linear relationship be-tween the human cue-associate probabilities and the automatically derived similarity values. We also re-port how many times the word with the highest prob-ability under the model was the first associate in the norms. The norms contain 10,127 unique words in total. Of these, we created semantic representations for the 3,895 words that appeared in our corpus.",Material,Knowledge,False,Use（引用目的）,True,N10-1011_1_0,2010,Visual Information in Semantic Representation,Footnote
1327,11331," http://www-nlpir.nist.gov/MINDS/FINAL/MT.web.pdf"," ['1 Introduction']","As the machine translation (MT) working groups stated on page 3 of their final report (Lavie et al., 2006) [Cite_Ref] , “These approaches have resulted in small im-provements in MT quality, but have not fundamen-tally solved the problem.",A. Lavie et al. 2006. MINDS Workshops Machine Translation Working Group Final Report. http://www-nlpir.nist.gov/MINDS/FINAL/MT.web.pdf,"The Markov chain (n-gram) source models, which predict each word on the basis of previous n-1 words, have been the workhorses of state-of-the-art speech recognizers and machine translators that help to resolve acoustic or foreign language ambiguities by placing higher probability on more likely original underlying word strings. Research groups (Brants et al., 2007; Zhang, 2008) have shown that using an immense distributed computing paradigm, up to 6-grams can be trained on up to billions and trillions of words, yielding consistent system improvements, but Zhang (2008) did not observe much improve-ment beyond 6-grams. Although the Markov chains are efficient at encoding local word interactions, the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages. As the machine translation (MT) working groups stated on page 3 of their final report (Lavie et al., 2006) [Cite_Ref] , “These approaches have resulted in small im-provements in MT quality, but have not fundamen-tally solved the problem. There is a dire need for de-veloping novel approaches to language modeling.”",補足資料,Paper,True,Introduce（引用目的）,True,P11-1021_0_0,2011,"A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation",Reference
1328,11332," http://www.codeproject.com/KB/recipes/englishparsing.aspx"," ['4 Experimental results']","We use the “openNLP” software (Northedge, 2005) [Cite_Ref] to parse a large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000).",R. Northedge. 2005. OpenNLP software http://www.codeproject.com/KB/recipes/englishparsing.aspx,"Similar to SLM (Chelba and Jelinek, 2000), af-ter the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the “openNLP” software (Northedge, 2005) [Cite_Ref] to parse a large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that contain 230 million tokens, then use them to initial-ize model parameters. The parser at ”openNLP” is trained by Upenn treebank with 1 million tokens and there is a mismatch between Upenn treebank and LDC English Gigaword corpus. Nevertheless, ex-perimental results show that this approach is effec-tive to provide initial values of model parameters.",Method,Tool,True,Use（引用目的）,True,P11-1021_1_0,2011,"A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation",Reference
1329,11333," https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py"," ['3 Experimental Setup']","Significance testing is performed using a randomization test, with the script from the CoNLL 2017 Shared Task. [Cite_Footnote_1]",1 https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py,"For evaluation we use labeled attachment score (LAS). Significance testing is performed using a randomization test, with the script from the CoNLL 2017 Shared Task. [Cite_Footnote_1]",Method,Code,False,Use（引用目的）,True,P18-2098_0_0,2018,Parser Training with Heterogeneous Treebanks,Footnote
1330,11334," https://github.com/UppsalaNLP/uuparser"," ['3 Experimental Setup', '3.1 The Parser']","We use UUParser [Cite_Footnote_2] (de Lhoneux et al., 2017a), which is based on the transition-based parser of Kiperwasser and Goldberg (2016), and adapted to UD.",2 https://github.com/UppsalaNLP/ uuparser,"We use UUParser [Cite_Footnote_2] (de Lhoneux et al., 2017a), which is based on the transition-based parser of Kiperwasser and Goldberg (2016), and adapted to UD. It uses the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). This model allows the construction of non-projective dependency trees (Nivre, 2009).",Method,Tool,True,Use（引用目的）,True,P18-2098_1_0,2018,Parser Training with Heterogeneous Treebanks,Footnote
1331,11335," http://nlp.cs.lth.se/lth_srl"," ['4 Conclusion']",We have described a dependency-based system [Cite_Footnote_1] for semantic role labeling of English in the PropBank framework.,1 Our system is freely available for download at http://nlp.cs.lth.se/lth_srl.,"We have described a dependency-based system [Cite_Footnote_1] for semantic role labeling of English in the PropBank framework. Our evaluations show that the perfor-mance of our system is close to the state of the art. This holds regardless of whether a segment-based or a dependency-based metric is used. In-terestingly, our system has a complete proposition accuracy that surpasses other systems by nearly 3 percentage points. Our system is the first semantic role labeler based only on syntactic dependency that achieves a competitive performance.",Method,Tool,True,Produce（引用目的）,True,D08-1008_0_0,2008,Dependency-based Semantic Role Labeling of PropBank,Footnote
1332,11336," https://github.com/jacobeisenstein/"," ['1 Description', '1.1 Format']",These notebooks will be shared along with publicly available data in a github repository for the tuto-rial. [Cite_Footnote_1],"1 https://github.com/jacobeisenstein/ language-change-tutorial Tutorial Abstracts, pages 9–14 Association for Computational Linguistics","The format of this three-hour tutorial will com-bine lecture-style surveys of various research ar-eas with interactive coding demonstrations. The coding demonstrations will use Jupyter notebook and the numpy, scipy, and pandas libraries. These notebooks will be shared along with publicly available data in a github repository for the tuto-rial. [Cite_Footnote_1]",補足資料,Document,True,Produce（引用目的）,True,N19-5003_0_0,2019,Measuring and Modeling Language Change,Footnote
1333,11337," https://github.com/jacobeisenstein/gt-css-class"," ['3 Presenter']","Jacob’s Georgia Tech course on Computational Social Science covers some of the same themes as this tutorial, and includes some additional material. [Cite_Footnote_2]",2 https://github.com/jacobeisenstein/ gt-css-class,"Jacob Eisenstein is Associate Professor in the School of Interactive Computing at the Georgia In-stitute of Technology, which he joined in 2012. He is on sabbatical at Facebook Artificial Intelligence Research in Seattle. His research on computa-tional sociolinguistics is supported by an NSF CA-REER award and by a young investigator award from the Air Force Office of Scientific Research (AFOSR). Results from this research have been published in traditional natural language process-ing venues, in sociolinguistics journals, and in more general venues. Jacob’s Georgia Tech course on Computational Social Science covers some of the same themes as this tutorial, and includes some additional material. [Cite_Footnote_2] He recently completed an in-troductory textbook on natural language process-ing.",Mixed,Mixed,True,Produce（引用目的）,True,N19-5003_1_0,2019,Measuring and Modeling Language Change,Footnote
1334,11338," http://www.natcorp.ox.ac.uk"," ['2 Pro3Gres and its Design Policy']",We have used it to parse the entire 100 million words British National Corpus ( [Cite] http://www.natcorp.ox.ac.uk) and similar amounts of biomedical texts.,,"The parser is fast enough for large-scale appli-cation to unrestricted texts, and it delivers depen-dency relations which are a suitable base for a range of applications. We have used it to parse the entire 100 million words British National Corpus ( [Cite] http://www.natcorp.ox.ac.uk) and similar amounts of biomedical texts. Its parsing speed is about 500,000 words per hour. The flowchart of the parser can be seen in figure 1.",Material,Dataset,True,Extend（引用目的）,True,D07-1128_0_0,2007,Pro3Gres Parser in the CoNLL Domain Adaptation Shared Task,Body
1335,11339," http://en.wikipedia.org/wiki/PepsiChallenge"," ['1 Introduction']","Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge [Cite_Footnote_1] , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola’s branding has made it more popular.",1 http://en.wikipedia.org/wiki/Pepsi Challenge,"In marketing science, branding is a modern market-ing strategy of creating a unique image for a prod-uct in the customers’ mind. Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965; Lederer and Hill, 2001; Kim et al., 2013). In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge [Cite_Footnote_1] , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola’s branding has made it more popular. Even now, Microsoft uses similar blind taste tests to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing’s results, Google’s brand might have strengthened over the years. These studies all suggest that brand and its associations play impor-tant roles in the customers’ perceptions and deci-sions.",補足資料,Website,True,Introduce（引用目的）,True,D13-1131_0_0,2013,This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics,Footnote
1336,11340," http://www.bingiton.com/"," ['1 Introduction']","Even now, Microsoft uses similar blind taste tests [Cite_Footnote_2] to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing’s results, Google’s brand might have strengthened over the years.",2 http://www.bingiton.com/,"In marketing science, branding is a modern market-ing strategy of creating a unique image for a prod-uct in the customers’ mind. Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965; Lederer and Hill, 2001; Kim et al., 2013). In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola’s branding has made it more popular. Even now, Microsoft uses similar blind taste tests [Cite_Footnote_2] to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing’s results, Google’s brand might have strengthened over the years. These studies all suggest that brand and its associations play impor-tant roles in the customers’ perceptions and deci-sions.",補足資料,Website,True,Compare（引用目的）,True,D13-1131_1_0,2013,This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics,Footnote
1337,11341," http://www.cs.cmu.edu/~yww/data/emnlp2013.zip"," ['Coffee Shops Reviews']",The coffee shop dataset is freely available [Cite_Footnote_6] for re-search purposes.,6 http://www.cs.cmu.edu/˜yww/data/emnlp2013.zip,The coffee shop dataset is freely available [Cite_Footnote_6] for re-search purposes.,補足資料,Website,True,Produce（引用目的）,True,D13-1131_2_0,2013,This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics,Footnote
1338,11342," https://github.com/RishabhMaheshwary/query-attack"," ['1 Introduction']","In this paper [Cite_Footnote_1] , we address the above discussed drawbacks through following contributions:",1 Code and adversarial examples available at: https://github.com/RishabhMaheshwary/query-attack,"To compare a new search method with previous methods, the new search method must be bench-marked on the same search space used in the previ-ous search methods. However, a study conducted in (Yoo et al., 2020) have shown that prior attacks often modify the search space while evaluating their search method. This does not ensure a fair comparison between the search methods because it is hard to distinguish whether the increase in attack success rate is due to the improved search method or modified search space. For example, (Jin et al., 2019) compares their search method with (Alzan-tot et al., 2018) where the former uses Universal Sentence Encoder (USE) (Cer et al., 2018) and the latter use language model as a constraint. Also, all the past works evaluate their search methods only on a single search space. In this paper [Cite_Footnote_1] , we address the above discussed drawbacks through following contributions:",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.661_0_0,2021,A Strong Baseline for Query Efficient Attacks in a Black Box Setting,Footnote
1339,11343," https://languagetool.org/"," ['4 Experiments', '4.4 Evaluation Metrics']","We use (1) attack success rate – the ratio of the suc-cessful attacks to total number of attacks, (2) query count – the number of queries, (3) perturbation rate – the percentage of words substituted in an in-put and (4) grammatical correctness – the average grammatical error increase rate (calculated using Language-Tool [Cite_Footnote_2] ) to verify the quality of generated adversarial examples.",2 https://languagetool.org/,"We use (1) attack success rate – the ratio of the suc-cessful attacks to total number of attacks, (2) query count – the number of queries, (3) perturbation rate – the percentage of words substituted in an in-put and (4) grammatical correctness – the average grammatical error increase rate (calculated using Language-Tool [Cite_Footnote_2] ) to verify the quality of generated adversarial examples. For all the metrics except attack success rate, lower the value better the re-sult. Also, for all metrics, we report the average score across all the generated adversarial examples on each dataset. Further, we also conducted hu-man evaluation to assess the quality of generated adversarial examples.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.661_1_0,2021,A Strong Baseline for Query Efficient Attacks in a Black Box Setting,Footnote
1340,11344," http://badaa.mngl.net/docs.php?p=trans_table"," ['3 Methodology', '3.6 N-gram retrieval']","We romanize all the Katakana words in the dictionary and index them using consecutive N 1 [Cite] http://badaa.mngl.net/docs.php?p=trans_table (May, 2006) characters.",,"We romanize all the Katakana words in the dictionary and index them using consecutive N 1 [Cite] http://badaa.mngl.net/docs.php?p=trans_table (May, 2006) characters. We also romanize each candidate loanword when use as a query. We experimentally set N = 2, and use the Okapi BM25 (Robertson et al., 1995) for the retrieval model.",補足資料,Document,True,Use（引用目的）,True,P06-1083_0_0,2006,Extracting loanwords from Mongolian corpora and producing a,Body
1341,11345," http://www.itpark.mn/"," ['4 Experiments', '4.1 Method']","We collected 1,118 technical reports published in Mongolian from the “Mongolian IT Park” [Cite_Footnote_2] and used them as a Mongolian corpus.","2 http://www.itpark.mn/ (May, 2006)","We collected 1,118 technical reports published in Mongolian from the “Mongolian IT Park” [Cite_Footnote_2] and used them as a Mongolian corpus. The number of phrase types and phrase tokens in our corpus were 110,458 and 263,512, respectively.",Material,DataSource,True,Use（引用目的）,True,P06-1083_1_0,2006,Extracting loanwords from Mongolian corpora and producing a,Footnote
1342,11346," http://nlp.stanford.edu/software/corenlp.shtml"," ['2 Framework Overview']","The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [Cite_Footnote_1] , OpenNLP or Lucene .",1 http://nlp.stanford.edu/software/corenlp.shtml,"K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [Cite_Footnote_1] , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github . Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group of the University of Roma, Tor Vergata.",補足資料,Website,True,Introduce（引用目的）,True,P15-4004_0_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1343,11347," https://opennlp.apache.org/"," ['2 Framework Overview']","The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP [Cite_Footnote_2] or Lucene .",2 https://opennlp.apache.org/,"K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP [Cite_Footnote_2] or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github . Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group of the University of Roma, Tor Vergata.",補足資料,Website,True,Introduce（引用目的）,True,P15-4004_1_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1344,11348," http://lucene.apache.org/"," ['2 Framework Overview']","The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene [Cite_Footnote_3] .",3 http://lucene.apache.org/,"K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene [Cite_Footnote_3] . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github . Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group of the University of Roma, Tor Vergata.",補足資料,Website,True,Introduce（引用目的）,True,P15-4004_2_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1345,11349," https://github.com/SAG-KeLP"," ['2 Framework Overview']",K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [Cite_Footnote_4] .,4 https://github.com/SAG-KeLP,"K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [Cite_Footnote_4] . Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group of the University of Roma, Tor Vergata.",Method,Code,True,Produce（引用目的）,True,P15-4004_3_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1346,11350," http://sag.art.uniroma2.it/demo-software/kelp/"," ['2 Framework Overview']","A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [Cite_Footnote_5] of the University of Roma, Tor Vergata.",5 http://sag.art.uniroma2.it/demo-software/kelp/,"K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github . Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [Cite_Footnote_5] of the University of Roma, Tor Vergata.",Method,Tool,True,Produce（引用目的）,True,P15-4004_4_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1347,11351," http://sag.art.uniroma2.it/demo-software/kelp/"," ['3 Case Studies in NLP']","Further examples are available on the K E LP website [Cite_Footnote_7] where it is shown how to instantiate each algorithm or kernel via JSON and how to add new algorithms, represen-tations and kernels.",7 http://sag.art.uniroma2.it/demo-software/kelp/,"In this Section, the functionalities and use of the learning platform are shown. We apply K E LP to very different NLP tasks, i.e. Sentiment Analysis in Twitter, Text Categorization and Question Clas-sification, providing examples of kernel-based and linear learning algorithms. Further examples are available on the K E LP website [Cite_Footnote_7] where it is shown how to instantiate each algorithm or kernel via JSON and how to add new algorithms, represen-tations and kernels.",Method,Tool,True,Produce（引用目的）,True,P15-4004_5_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1348,11352," https://code.google.com/p/word2vec/"," ['3 Case Studies in NLP', '3.1 Sentiment Analysis in Twitter']","The last is obtained by linearly combining the distri-butional vectors corresponding to the words of a message; these vectors are obtained by applying a Skip-gram model (Mikolov et al., 2013) with the word2vec tool [Cite_Footnote_8] over 20 million of tweets.",8 https://code.google.com/p/word2vec/,"The task of Sentiment Analysis in Twitter has been proposed in 2013 during the SemEval competi-tion (Nakov et al., 2013). We built a classifier for the subtask B, i.e. the classification of a tweet with respect to the positive, negative and neutral classes. The contribution of different kernel func-tions is evaluated using the Support Vector Ma-chine learning algorithm. As shown in Table 1, we apply linear (Lin), polynomial (Poly) and Gaus-sian (Rbf) kernels on two different data represen-tations: a Bag-Of-Words model of tweets (BoW ) and a distributional representation (WS). The last is obtained by linearly combining the distri-butional vectors corresponding to the words of a message; these vectors are obtained by applying a Skip-gram model (Mikolov et al., 2013) with the word2vec tool [Cite_Footnote_8] over 20 million of tweets. The lin-ear combination of the proposed kernel functions is also applied, e.g. Poly Bow +Rbf WS . The mean F1-measure of the positive and negative classes (pn) as well as of all the classes (pnn) is shown in Table 1.",Method,Tool,True,Use（引用目的）,True,P15-4004_6_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1349,11353," http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/"," ['3 Case Studies in NLP', '3.2 Text Categorization']","We selected the Text Categorization task on the RCV1 dataset (Lewis et al., 2004) with the setting that can be found on the LibLinear website [Cite_Footnote_10] .",10 http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/,"We selected the Text Categorization task on the RCV1 dataset (Lewis et al., 2004) with the setting that can be found on the LibLinear website [Cite_Footnote_10] . In this version of the dataset, CCAT and ECAT are collapsed into a positive class, while GCAT and MCAT are the negative class, resulting in a dataset composed by 20, 242 examples. As shown in Ta-ble 2, we applied the LibLinear, Pegasos and Lin-ear Passive-Aggressive implementations, comput-ing the accuracy and the standard deviation with respect to a 5-fold cross validation strategy.",Material,Dataset,True,Use（引用目的）,True,P15-4004_7_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1350,11354," http://cogcomp.cs.illinois.edu/Data/QA/QC/"," ['3 Case Studies in NLP', '3.3 Question Classification']","The reference cor-pus is the UIUC dataset (Li and Roth, 2002), in-cluding 5,452 questions for training and 500 ques-tions for test [Cite_Footnote_11] , organized in six coarse-grained classes, such as HUMAN or LOCATION.",11 http://cogcomp.cs.illinois.edu/Data/QA/QC/,"The third case study explores the application of Tree Kernels to Question Classification (QC), an inference task required in many Question Answer-ing processes. In this problem, questions writ-ten in natural language are assigned to different classes. A QC system should select the correct class given an instance question. In this setting, Tree Kernels allow to directly model the examples in terms of their parse trees. The reference cor-pus is the UIUC dataset (Li and Roth, 2002), in-cluding 5,452 questions for training and 500 ques-tions for test [Cite_Footnote_11] , organized in six coarse-grained classes, such as HUMAN or LOCATION. Again, Kernel-based SVM has been evaluated adopting the same setup of (Croce et al., 2011). A pure lex-ical model based on a linear kernel over a Bag-of- Words (BoW) is considered a baseline. The con-tribution of the syntactic information is demon-strated by the results achieved by the Partial Tree Kernel (PTK), the Smoothed Partial Tree Kernels (SPTK) and the Compositionally Smoothed Par-tial Tree Kernel (CSPTK), as shown in Table 3.",Material,Dataset,True,Introduce（引用目的）,True,P15-4004_8_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Footnote
1351,11355," http://mallet.cs.umass.edu"," ['4 Related Work']","Mallet (McCallum, 2002) [Cite_Ref] is more oriented to NLP applications.",Andrew Kachites McCallum. 2002. Mal-let: A machine learning for language toolkit. http://mallet.cs.umass.edu.,"Mallet (McCallum, 2002) [Cite_Ref] is more oriented to NLP applications. It is entirely in Java and in-cludes feature extraction tools for converting text into vectors and statistical analysis tools for docu-ment classification, clustering, topic modeling, in-formation extraction, and other machine learning applications to text. Regarding the kernel-based learning both Weka and Mallet leverage on Lib-SVM, and obviously inherit its limits.",補足資料,Paper,True,Introduce（引用目的）,True,P15-4004_9_0,2015,KeLP: a Kernel-based Learning Platform for Natural Language Processing,Reference
1352,11356," https://github.com/"," ['1 Introduction']","The code is available at [Cite] https://github.com/ the results using the pre-trained model only, and leave more briemadu/inc-bidirectional.",,"ATIS and SNIPS, respectively), named entity recog- The results in Table 1 (above) support the observa-nition, part-of-speech tagging and semantic role tion that, in general, bidirectional models do have labeling (Weischedel et al., 2013); and four for a better non-incremental performance than LSTMs sentence classification: intent (Hemphill et al., (except for IntentATIS and ProsCons) and that there marginal difference in the evaluation metrics. We thus present 4 The code is available at [Cite] https://github.com/ the results using the pre-trained model only, and leave more briemadu/inc-bidirectional. For more details on exploration of fine-tuning for future work. implementation and data for reproducibility, see Appendix. is an overall considerable improvement in the use of BERT model for all tasks. Truncated training re-duces overall performance but even so BERT with truncated training outperforms all models, even with usual training, in most tasks (except for slot filling and IntentATIS).",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.26_1_0,2020,Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU,Body
1353,11357," https://www.comet.ml/docs/python-sdk/introduction-optimizer/"," ['References']","[Cite_Footnote_6] , which bal- Learning rate",6 https://www.comet.ml/docs/python-sdk/introduction-optimizer/,"• We use Comet’s Bayes algorithm [Cite_Footnote_6] , which bal- Learning rate",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.26_2_0,2020,Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU,Footnote
1354,11358," https://github.com/megagonlabs/opiniondigest"," ['References']",Human studies on two cor-pora verify that O PINION D IGEST produces informative summaries and shows promising customization capabilities [Cite_Footnote_1] .,1 Our code is available at https://github.com/megagonlabs/opiniondigest.,"We present O PINION D IGEST , an abstrac-tive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained Trans-former model, which verbalizes them into an opinion summary. O PINION D IGEST can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sen-timent. Automatic evaluation on Y ELP data shows that our framework outperforms com-petitive baselines. Human studies on two cor-pora verify that O PINION D IGEST produces informative summaries and shows promising customization capabilities [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2020.acl-main.513_0_0,2020,O PINION D IGEST : A Simple Framework for Opinion Summarization,Footnote
1355,11359," http://extremereader.megagon.info/"," ['3 Evaluation', '3.4 Results']","Based on O PINION D IGEST , we have built an online demo (Wang et al., 2020) [Cite_Footnote_5] that allows users to customize the generated summary by specifying search terms.",5 http://extremereader.megagon.info/,"Finally, we provide qualitative analysis of the controllable summarization abilities of O PINION D IGEST , which are enabled by input opinion filtering. As discussed in Section 2.2, we filtered input opinions based on predicted aspect categories and sentiment polarity. The examples of controlled summaries (last 4 rows of Table 5) show that O PINION D IGEST can generate aspect/sentiment-specific summaries. These examples have redundant opinions and incorrect extractions in the input, but O PINION D IGEST is able to convert the input opinions into natural summaries. Based on O PINION D IGEST , we have built an online demo (Wang et al., 2020) [Cite_Footnote_5] that allows users to customize the generated summary by specifying search terms.",Method,Tool,False,Produce（引用目的）,True,2020.acl-main.513_1_0,2020,O PINION D IGEST : A Simple Framework for Opinion Summarization,Footnote
1356,11360," https://www.figure-eight.com/"," ['B Human Evaluation Setup']",We conducted user study via crowdsourcing using the FigureEight [Cite_Footnote_6] platform.,6 https://www.figure-eight.com/,"We conducted user study via crowdsourcing using the FigureEight [Cite_Footnote_6] platform. To ensure the quality of annotators, we used a dedicated expert-worker pool provided by FigureEight. We present the detailed setup of our user studies as follows.",補足資料,Website,True,Use（引用目的）,True,2020.acl-main.513_2_0,2020,O PINION D IGEST : A Simple Framework for Opinion Summarization,Footnote
1357,11361," https://abci.ai/"," ['A Appendix', 'A.2 Environment']",All experiments were executed on a shared rt G.small instance on the ABCI compute cluster [Cite_Footnote_1] .,1 https://abci.ai/,"All experiments were executed on a shared rt G.small instance on the ABCI compute cluster [Cite_Footnote_1] . An rt G.small node has 6 segregated CPU cores from a Xeon Gold 6148, a Tesla V100 GPU with 16GB VRAM, and 60GBs of memory. The training data and experimental code was streamed from a shared GPFS mount. Each experiment requires a different amount of compute budget. The longest running experiment finished in 10 hours of wall clock time and the shortest finished in 2 hours of wall clock time. The average runtime for each ex-periment was approximately 5.5 hours.",補足資料,Website,True,Use（引用目的）,True,2020.emnlp-main.631_0_0,2020,"PatchBERT: Just-in-Time, Out-of-Vocabulary Patching",Footnote
1358,11362," http://www-personal.umich.edu/~benking/resources/mixed-language-annotations-release-v1.0.tgz"," ['3 Task Definition', '3.1 Evaluation Data']","For researchers who wish to make use this data, the set of annotations used in this paper is available from the first author’s website [Cite_Footnote_1] .",1 http://www-personal.umich.edu/˜benking/resources/mixed-language-annotations-release-v1.0.tgz,"For researchers who wish to make use this data, the set of annotations used in this paper is available from the first author’s website [Cite_Footnote_1] .",Material,Dataset,True,Produce（引用目的）,True,N13-1131_0_0,2013,Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods,Footnote
1359,11363," http://www.unicode.org/udhr/"," ['3 Task Definition', '3.4 Training Data']","Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights [Cite_Footnote_2] , non-English Wikipedias , the Jehovah’s Witnesses website , and the Rosetta project (Lands-bergen, 1989).",2 The Universal Declaration of Human Rights is a document created by the United Nations and translated into many lan-guages. As of February 2011 there were 365 versions available from http://www.unicode.org/udhr/,"Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights [Cite_Footnote_2] , non-English Wikipedias , the Jehovah’s Witnesses website , and the Rosetta project (Lands-bergen, 1989).",Material,DataSource,True,Use（引用目的）,True,N13-1131_2_0,2013,Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods,Footnote
1360,11364," http://meta.wikimedia.org/wiki/List"," ['3 Task Definition', '3.4 Training Data']","Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights , non-English Wikipedias [Cite_Footnote_3] , the Jehovah’s Witnesses website , and the Rosetta project (Lands-bergen, 1989).","3 As of February 2011, there were 113 Wikipedias in differ-ent languages. Current versions of Wikipedia can be accessed from http://meta.wikimedia.org/wiki/List of Wikipedias","Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights , non-English Wikipedias [Cite_Footnote_3] , the Jehovah’s Witnesses website , and the Rosetta project (Lands-bergen, 1989).",Material,DataSource,True,Use（引用目的）,True,N13-1131_3_0,2013,Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods,Footnote
1361,11365," http://www.watchtower.org"," ['3 Task Definition', '3.4 Training Data']","Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights , non-English Wikipedias , the Jehovah’s Witnesses website [Cite_Footnote_4] , and the Rosetta project (Lands-bergen, 1989).","4 As of February 2011, there were 310 versions of the site available at http://www.watchtower.org","Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights , non-English Wikipedias , the Jehovah’s Witnesses website [Cite_Footnote_4] , and the Rosetta project (Lands-bergen, 1989).",Material,DataSource,True,Use（引用目的）,True,N13-1131_4_0,2013,Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods,Footnote
1362,11366," http://mallet.cs.umass.edu"," ['4 Word-level Language Classification', '4.2 Classifiers']","Using all available features, we compare four MAL-LET (McCallum, 2002) [Cite_Ref] classifiers: logistic regres-sion, naı̈ve Bayes, decision tree, and Winnow2.",Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.,"Using all available features, we compare four MAL-LET (McCallum, 2002) [Cite_Ref] classifiers: logistic regres-sion, naı̈ve Bayes, decision tree, and Winnow2. Fig-ure 1 shows the learning curves for each classifier as the number of sampled words comprising each train-ing example is varied from 10 to 1000.",補足資料,Paper,True,Introduce（引用目的）,True,N13-1131_5_0,2013,Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods,Reference
1363,11367," http://mallet.cs.umass.edu"," ['5 Methods', '5.1 Conditional Random Field Model trained with Generalized Expectation']","We use the implementation of CRF with GE cri-teria from MALLET (McCallum, 2002) [Cite_Ref] , which uses a gradient descent algorithm to optimize the objec-tive function.",Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.,"We use the implementation of CRF with GE cri-teria from MALLET (McCallum, 2002) [Cite_Ref] , which uses a gradient descent algorithm to optimize the objec-tive function. (Mann and McCallum, 2008; Druck, 2011)",補足資料,Paper,True,Introduce（引用目的）,True,N13-1131_5_1,2013,Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods,Reference
1364,11368," http://www.bbc.com/news/technology-24299742"," ['1 Introduction']",It has been reported that up to 25% of the reviews on Yelp.com could be fraudu-lent [Cite_Footnote_1] .,1 http://www.bbc.com/news/technology-24299742,"With the development of E-commerce, more and more customers share their experiences about prod-ucts and services by posting reviews on the we-b. These reviews could heavily guide the purchas-ing behaviors of customers. The products which receive more positive reviews tend to attract more consumers and result in more profits. Studies on Yelp.com have shown that an extra half-star rating could cause a restaurant to sell out 19% more prod-ucts (Anderson and Magruder, 2012), and a one-star increase leads to a 5-9% profit increase (Luca, 2011). Therefore, more and more sellers and manu-facturers have begun to place emphasis on analyzing reviews. However, the question remains: is every online review trustful? It has been reported that up to 25% of the reviews on Yelp.com could be fraudu-lent [Cite_Footnote_1] . Due to the great profit or reputation, impostors or spammers energetically post fake reviews on the web to promote or defame targeted products (Jindal and Liu, 2008). Such fake reviews could mislead consumers and damage the online review websites’ reputations. Therefore, it is necessary and urgent to detect fake reviews (review spam).",補足資料,Document,False,Introduce（引用目的）,True,D16-1083_0_0,2016,Learning to Represent Review with Tensor Decomposition for Spam Detection,Footnote
1365,11369," http://www.dianping.com"," ['1 Introduction']","For exam-ple, based on the datasets from Dianping site [Cite_Footnote_4] , Li et al. (2015) find that the real users tend to review the restaurants nearby, but the spammers are not re-stricted to the geographical location, they may come from anywhere.",4 http://www.dianping.com,"Although, the existing work has made signifi-cant progress in combating review spamming, they also have several limitations as follows. (1) The representations of reviews rely heavily on experts’ prior knowledge or developers’ ingenuity. To dis-cover more discriminative features for representing reviews, previous work (Mukherjee et al., 2013b; Rayana and Akoglu, 2015) have spent lots of man-power and time on the statistics of the review datasets. Besides, experts’ prior knowledge or de-velopers’ ingenuity is not always reliable with the variations of domains and languages. For exam-ple, based on the datasets from Dianping site [Cite_Footnote_4] , Li et al. (2015) find that the real users tend to review the restaurants nearby, but the spammers are not re-stricted to the geographical location, they may come from anywhere. However, it is not true in the Yelp datasets (Mukherjee et al., 2013b). We found that 72% of the Yelp’s review spam is posted from the areas near the restaurants, but only 64% of the au-thentic reviews are near the restaurants. Therefore, how to learn the representations of reviews direct-ly from data instead of heavily relying on the ex-perts’ prior knowledge or developers’ ingenuity be-comes crucial and urgent. (2) Furthermore, limited by the experts’ knowledge, previous work only uses partial information of the review system. For exam-ple, traditional behavioral features (Lim et al., 2010; Mukherjee et al., 2013c) only utilize the information of individual reviewer. Although the work (Wang et al., 2011; Rayana and Akoglu, 2015) have tried to employ graph structure to consider the interac-tions among the reviewers and products, it is a kind of local interaction defined within the same produc-t review page. However, the interaction among the reviewers and products from different review pages also provides much useful and global information, which is ignored by the previous work.",補足資料,Website,True,Introduce（引用目的）,True,D16-1083_1_0,2016,Learning to Represent Review with Tensor Decomposition for Spam Detection,Footnote
1366,11370," https://www.gsk.or.jp/catalog/gsk2014-a/"," ['3 Experiments', '3.1 Settings']",Dataset We use the Extended Named Entity Cor-pus for English and Japanese. [Cite_Footnote_3],3 https://www.gsk.or.jp/catalog/gsk2014-a/,"Dataset We use the Extended Named Entity Cor-pus for English and Japanese. [Cite_Footnote_3] fine-grained NER (Mai et al., 2018) In this dataset, each NE is assigned one of 200 entity labels defined in the Extended Named Entity Hierarchy (Sekine et al., 2002). For the English dataset, we follow the train-ing/development/test split defined by Mai et al. (2018). For the Japanese dataset, we follow the training/development/test split of Universal Depen-dencies (UD) Japanese-BCCWJ. (Asahara et al., 2018) Table 1 shows the statistics of the dataset.",Material,Dataset,True,Use（引用目的）,True,2020.acl-srw.30_0_0,2020,Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition,Footnote
1367,11371," https://github.com/UniversalDependencies/UD_Japanese-BCCWJ"," ['3 Experiments', '3.1 Settings']","(Asahara et al., 2018) [Cite_Footnote_4] Table 1 shows the statistics of the dataset.",4 https://github.com/UniversalDependencies/UD_Japanese-BCCWJ,"Dataset We use the Extended Named Entity Cor-pus for English and Japanese. fine-grained NER (Mai et al., 2018) In this dataset, each NE is assigned one of 200 entity labels defined in the Extended Named Entity Hierarchy (Sekine et al., 2002). For the English dataset, we follow the train-ing/development/test split defined by Mai et al. (2018). For the Japanese dataset, we follow the training/development/test split of Universal Depen-dencies (UD) Japanese-BCCWJ. (Asahara et al., 2018) [Cite_Footnote_4] Table 1 shows the statistics of the dataset.",Material,Dataset,True,Use（引用目的）,True,2020.acl-srw.30_1_0,2020,Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition,Footnote
1368,11372," https://github.com/kamalkraj/BERT-NER"," ['3 Experiments', '3.1 Settings']","Model setup As the encoder f(x,X) in Equa-tion 2 in Section 2.1, we use BERT [Cite_Footnote_5] (Devlin et al., 2019), which is a state-of-the-art language model.",5 We use the open-source NER model utilizing BERT: https://github.com/kamalkraj/BERT-NER.,"Model setup As the encoder f(x,X) in Equa-tion 2 in Section 2.1, we use BERT [Cite_Footnote_5] (Devlin et al., 2019), which is a state-of-the-art language model. As the baseline model, we use the general label embedding matrix without considering label com-ponents, i.e., each label embedding W[y] in Equa-tion 2 is randomly initialized and independently learned. In contrast, our proposed model calculates the label embedding matrix from label components (Equations 3 and 4). The only difference between these models is the label embedding matrix, so if a performance gap between them is observed, it stems from this point.",Material,Knowledge,True,Use（引用目的）,True,2020.acl-srw.30_2_0,2020,Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition,Footnote
1369,11373," http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/"," ['1 Introduction']",The code will be available at [Cite] http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.,,We are currently making preparations for the project to be released with an open-source license. The code will be available at [Cite] http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.,Method,Code,True,Produce（引用目的）,True,P14-5014_0_0,2014,KyotoEBMT: An Example-Based Dependency-to-Dependency Translation Framework,Body
1370,11374," http://kheafield.com/code/kenlm/"," ['4 Decoding']","We use KenLM [Cite_Footnote_3] (Heafield, 2011) for computing the target language model score.",3 http://kheafield.com/code/kenlm/,"The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a varia-tion of cube-pruning (Chiang, 2007). We use KenLM [Cite_Footnote_3] (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estima-tions(Heafield et al., 2012).",Method,Tool,True,Use（引用目的）,True,P14-5014_1_0,2014,KyotoEBMT: An Example-Based Dependency-to-Dependency Translation Framework,Footnote
1371,11375," http://www.umiacs.umd.edu/~hal/megam/"," ['5 Features and Tuning']","The optimal weights for each feature are estimated using the Pairwise Ranking Op-timization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM [Cite_Footnote_4] .",4 http://www.umiacs.umd.edu/~hal/megam/,"The optimal weights for each feature are estimated using the Pairwise Ranking Op-timization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM [Cite_Footnote_4] . We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM.",Material,Knowledge,True,Use（引用目的）,True,P14-5014_2_0,2014,KyotoEBMT: An Example-Based Dependency-to-Dependency Translation Framework,Footnote
1372,11376," http://orchid.kuee.kyoto-u.ac.jp/ASPEC/"," ['6 Experiments']","For Japanese-Chinese, we used parallel scientific paper ex-cerpts from the ASPEC [Cite_Footnote_5] corpus and com-pared against the same baseline system as for Japanese-English.",5 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/,"For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the official baseline scores. For Japanese-Chinese, we used parallel scientific paper ex-cerpts from the ASPEC [Cite_Footnote_5] corpus and com-pared against the same baseline system as for Japanese-English. The corpora contain 3M parallel sentences for Japanese-English and 670K for Japanese-Chinese.",Material,DataSource,True,Use（引用目的）,True,P14-5014_3_0,2014,KyotoEBMT: An Example-Based Dependency-to-Dependency Translation Framework,Footnote
1373,11377," http://www.lpl.univ-"," ['3 The Methodology', '3.5 Handling Unknown Words']",[Cite] http://www.lpl.univ-,,Jean Vdronis. 1996a. Multext-East References (Copernicus 106). [Cite] http://www.lpl.univ-,補足資料,Paper,False,Introduce（引用目的）,False,A00-2013_0_0,2000,Morphological Tagging: Data vs. Dictionaries,Body
1374,11378," http://nlp.stanford.edu/software/lex-parser.shtml"," ['4 EDU Model']","Parse trees are obtained using the Stanford Parser [Cite_Footnote_1] , and each clause is treated as an EDU.",1 http://nlp.stanford.edu/software/lex-parser.shtml,"Parse trees are obtained using the Stanford Parser [Cite_Footnote_1] , and each clause is treated as an EDU. For a given parent p in the tree and its two children c 1 (associated with vector representation h c 1 ) and c 2 (associated with vector representation h c 2 ), stan-dard recursive networks calculate the vector for parent p as follows: where [h c 1 , h c 2 ] denotes the concatenating vector for children representations h c 1 and h c 2 ; W is a K × 2K matrix and b is the 1 × K bias vector; and f(·) is the function tanh. Recursive neural models compute parent vectors iteratively until the root node’s representation is obtained, and use the root embedding to represent the whole sentence.",Method,Tool,True,Use（引用目的）,True,D14-1220_0_0,2014,Recursive Deep Models for Discourse Parsing,Footnote
1375,11379," http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/"," ['4 Experiments', '4.1 Baselines']","This is the standard 1-vs-Rest multiclass SVM with Platt Probability Estimation (Platt, 2000), and it is implemented based on LIBSVM [Cite_Footnote_1] (version 3.20) (Chang and Lin, 2011).",1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/,"1-vs-Rest multiclass SVM (1-vs-rest-SVM). This is the standard 1-vs-Rest multiclass SVM with Platt Probability Estimation (Platt, 2000), and it is implemented based on LIBSVM [Cite_Footnote_1] (version 3.20) (Chang and Lin, 2011). It works in the same way as the proposed cbsSVM (Section 3.4) except that it uses the document space classification. Linear ker-nel is used as it is shown by many researchers that linear SVM performs the best for text classification (Joachims, 1998; Colas and Brazdil, 2006). 1-vs-Set Machine (1-vs-set-linear). For this base-line (Scheirer et al., 2013), we use all the default parameter settings in the original paper. That is, the near and far plane pressures are set at 𝑝 ! = 1.6 and 𝑝 ! = 4 respectively; regularization constant 𝜆 ! = 1 and no explicit hard constraints are used on the training error (𝛼 = 0, 𝛽 = 1).",Method,Tool,True,Use（引用目的）,True,N16-1061_0_0,2016,Breaking the Closed World Assumption in Text Classification,Footnote
1376,11380," https://github.com/Vastlab/liblinear.git"," ['4 Experiments', '4.1 Baselines']","Source code for different baselines (1-vs-Set Machine [Cite_Footnote_2] , W-SVM and P I - SVM , and Exploratory learning ) was provided by the authors of their original papers.",2 https://github.com/Vastlab/liblinear.git,"All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine [Cite_Footnote_2] , W-SVM and P I - SVM , and Exploratory learning ) was provided by the authors of their original papers.",Material,Knowledge,False,Introduce（引用目的）,False,N16-1061_1_0,2016,Breaking the Closed World Assumption in Text Classification,Footnote
1377,11381," https://github.com/ljain2/libsvm-openset"," ['4 Experiments', '4.1 Baselines']","Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM [Cite_Footnote_3] , and Exploratory learning ) was provided by the authors of their original papers.",3 https://github.com/ljain2/libsvm-openset,"All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM [Cite_Footnote_3] , and Exploratory learning ) was provided by the authors of their original papers.",Material,Knowledge,False,Introduce（引用目的）,False,N16-1061_2_0,2016,Breaking the Closed World Assumption in Text Classification,Footnote
1378,11382," http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip"," ['4 Experiments', '4.1 Baselines']","Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [Cite_Footnote_4] ) was provided by the authors of their original papers.",4 http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip,"All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [Cite_Footnote_4] ) was provided by the authors of their original papers.",Material,Knowledge,False,Introduce（引用目的）,False,N16-1061_3_0,2016,Breaking the Closed World Assumption in Text Classification,Footnote
1379,11383," http://www.csie.ntu.edu.tw/~cjlin/libsvm"," ['3 Proposed Method', '3.4 Multiclass Open Classification']","The SVM scores for each classifier are first converted to probabilities based on a variation of Platt’s (2000) algorithm, which is supported in LIBSVM (Chang and Lin, 2011) [Cite_Ref] .","Chang, C-C. and Lin, C-J. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intel-ligent Systems and Technology, 2:27:1--27:27, http://www.csie.ntu.edu.tw/~cjlin/libsvm","The preceding discussion is based on binary open classification. We follow the standard technique of combining a set of 1-vs-rest binary classifiers to perform multiclass classification with a rejection option for unknown. The SVM scores for each classifier are first converted to probabilities based on a variation of Platt’s (2000) algorithm, which is supported in LIBSVM (Chang and Lin, 2011) [Cite_Ref] . Let 𝑃 𝑦|𝐱 be a probably estimate, where 𝑦 ∈ 𝑌 is a class label and 𝐱 is a feature vector, and let 𝜆 be the decision threshold (usually 0.5). Let 𝑌 be the set of known classes, 𝐶 !!! be the unknown class, and 𝑦 ∗ is the final predicted class for x. The final classifier (called cbsSVM) uses this following for classification.",補足資料,Paper,True,Introduce（引用目的）,True,N16-1061_4_0,2016,Breaking the Closed World Assumption in Text Classification,Reference
1380,11384," http://www.csie.ntu.edu.tw/~cjlin/libsvm"," ['4 Experiments', '4.1 Baselines']","(Chang and Lin, 2011) [Cite_Ref] .","Chang, C-C. and Lin, C-J. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intel-ligent Systems and Technology, 2:27:1--27:27, http://www.csie.ntu.edu.tw/~cjlin/libsvm","1-vs-Rest multiclass SVM (1-vs-rest-SVM). This is the standard 1-vs-Rest multiclass SVM with Platt Probability Estimation (Platt, 2000), and it is implemented based on LIBSVM (version 3.20) (Chang and Lin, 2011) [Cite_Ref] . It works in the same way as the proposed cbsSVM (Section 3.4) except that it uses the document space classification. Linear ker-nel is used as it is shown by many researchers that linear SVM performs the best for text classification (Joachims, 1998; Colas and Brazdil, 2006). 1-vs-Set Machine (1-vs-set-linear). For this base-line (Scheirer et al., 2013), we use all the default parameter settings in the original paper. That is, the near and far plane pressures are set at 𝑝 ! = 1.6 and 𝑝 ! = 4 respectively; regularization constant 𝜆 ! = 1 and no explicit hard constraints are used on the training error (𝛼 = 0, 𝛽 = 1).",補足資料,Paper,True,Introduce（引用目的）,True,N16-1061_4_1,2016,Breaking the Closed World Assumption in Text Classification,Reference
1381,11385," https://github.com/microsoft/Multilingual-Model-Transfer"," ['References']","Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target lan-guage and each individual source language [Cite_Footnote_1] .",1 The code is available at https://github.com/microsoft/Multilingual-Model-Transfer.,"Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applica-ble to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by lever-aging labeled data from other (source) lan-guages. In this work, we focus on the multi-lingual transfer setting where training data in multiple source languages is leveraged to fur-ther boost target language performance. Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at in-stance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target lan-guage and each individual source language [Cite_Footnote_1] . This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource set-ting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classifi-cation and sequence tagging tasks including a large-scale industry dataset.",Method,Code,True,Produce（引用目的）,True,P19-1299_0_0,2019,Multi-Source Cross-Lingual Model Transfer: Learning What to Share,Footnote
1382,11386," https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/"," ['4 Experiments', '4.1 Cross-Lingual Semantic Slot Filling', '4.1.1 Results']","In this work, we adopt the Mi-crosoft Translator [Cite_Footnote_4] , a strong commercial MT sys-tem.",4 https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/,"MT baselines employ machine translation (MT) for cross-lingual transfer. In particular, the train-on-trans(lation) method translates the entire En-glish training set into each target language which are in turn used to train a supervised system on the target language. On the other hand, the test-on-trans(lation) method trains an English sequence tagger, and utilizes MT to translate the test set of each target language into English in order to make predictions. In this work, we adopt the Mi-crosoft Translator [Cite_Footnote_4] , a strong commercial MT sys-tem. Note that for a MT system to work for se-quence tagging tasks, word alignment informa-tion must be available, in order to project word-level annotations across languages. This rules out many MT systems such as Google Translate since they do not provide word alignment information through their APIs.",Method,Tool,True,Use（引用目的）,True,P19-1299_1_0,2019,Multi-Source Cross-Lingual Model Transfer: Learning What to Share,Footnote
1383,11387," http://colah.github.io/posts/2015-08-Understanding-LSTMs/"," ['2 Related Work']",Figure 1: RNN (a) and LSTM (b) [Cite_Footnote_1],1 This figure referred to http://colah.github.io/posts/2015-08-Understanding-LSTMs/,Figure 1: RNN (a) and LSTM (b) [Cite_Footnote_1],補足資料,Document,False,Introduce（引用目的）,False,P16-1053_0_0,2016,A Sentence Interaction Network for Modeling Dependence between Sentences,Footnote
1384,11388," http://aka.ms/WikiQA"," ['3 Method', '3.3 SIN with Convolution (SIN-CONV)']",The aver-age of all hidden states are treated as sentence vec-tors v scnn 1 and v scnn [Cite_Footnote_2] .,2 http://aka.ms/WikiQA,"In SIN-CONV, we first use a convolution layer to obtain phrase representations for the two sen-tences s 1 and s 2 , and the SIN interaction proce-dure is then applied to these phrase representations as before to model phrase interactions. The aver-age of all hidden states are treated as sentence vec-tors v scnn 1 and v scnn [Cite_Footnote_2] . Thus, SIN-CONV is SIN with word vectors substituted by phrase vectors. The two phrase-based sentence vectors are then fed to a classifier along with the two word-based sentence vectors together for classification.",Material,DataSource,False,Use（引用目的）,True,P16-1053_1_0,2016,A Sentence Interaction Network for Modeling Dependence between Sentences,Footnote
1385,11389," http://nlp.stanford.edu/projects/glove/"," ['4 Experiments', '4.1 Answer Selection', '4.1.2 Setup']","We use the 100-dimensional GloVe vectors [Cite_Footnote_3] (Pen-nington et al., 2014) to initialize our word embed-dings, and those words that do not appear in Glove vectors are treated as unknown.",3 http://nlp.stanford.edu/projects/glove/,"We use the 100-dimensional GloVe vectors [Cite_Footnote_3] (Pen-nington et al., 2014) to initialize our word embed-dings, and those words that do not appear in Glove vectors are treated as unknown. The dimension of all hidden states is set to 100 as well. The window size of the convolution layer is 2. To avoid overfit-ting, dropout is introduced to the sentence vectors, namely setting some dimensions of the sentence vectors to 0 with a probability p (0.5 in our experi-ment) randomly. No handcrafted features are used in our methods and the baselines.",Material,Knowledge,False,Use（引用目的）,True,P16-1053_2_0,2016,A Sentence Interaction Network for Modeling Dependence between Sentences,Footnote
1386,11390," http://compprag.christopherpotts.net/swda.html"," ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']","We use the Switch-board Dialogue Act (SwDA) corpus (Calhoun et al., 2010) in our experiments [Cite_Footnote_5] .",5 http://compprag.christopherpotts.net/swda.html.,"We use the Switch-board Dialogue Act (SwDA) corpus (Calhoun et al., 2010) in our experiments [Cite_Footnote_5] . SwDA contains the transcripts of several people discussing a given topic on the telephone. There are 42 dialogue act tags in SwDA, and we list the 10 most frequent tags in Table 3.",Material,Dataset,True,Use（引用目的）,True,P16-1053_3_0,2016,A Sentence Interaction Network for Modeling Dependence between Sentences,Footnote
1387,11391," http://web.stanford.edu/%7ejurafsky/ws97/"," ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']","There are 1,115 dia-logues in the training set and 19 dialogues in the test set [Cite_Footnote_7] .",7 http://web.stanford.edu/%7ejurafsky/ws97/,"The same data split as in Stolcke et al. (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [Cite_Footnote_7] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).",Material,Dataset,True,Use（引用目的）,True,P16-1053_4_0,2016,A Sentence Interaction Network for Modeling Dependence between Sentences,Footnote
1388,11392," http://nlp.lsi.upc.edu/asiya/"," ['1 Introduction']","Two open source feature extraction toolkits are available for that: A SIYA [Cite_Footnote_1] and Q U E ST (Specia et al., 2013).",1 http://nlp.lsi.upc.edu/asiya/,"Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to in-duce models from examples of sentence transla-tions annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statis-tical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA [Cite_Footnote_1] and Q U E ST (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).",Method,Tool,True,Introduce（引用目的）,True,P15-4020_0_0,2015,Multi-level Translation Quality Prediction with Q U E ST ++,Footnote
1389,11393," http://www.quest.dcs.shef.ac.uk/"," ['1 Introduction']","Two open source feature extraction toolkits are available for that: A SIYA and Q U E ST [Cite_Footnote_2] (Specia et al., 2013).",2 http://www.quest.dcs.shef.ac.uk/,"Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to in-duce models from examples of sentence transla-tions annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statis-tical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA and Q U E ST [Cite_Footnote_2] (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).",Method,Tool,True,Introduce（引用目的）,True,P15-4020_1_0,2015,Multi-level Translation Quality Prediction with Q U E ST ++,Footnote
1390,11394," http://scikit-learn.org/"," ['2 Architecture']","Machine learning Q U E ST ++ provides scripts to interface the Python toolkit scikit-learn [Cite_Footnote_3] (Pedregosa et al., ).",3 http://scikit-learn.org/,"Figure 1 depicts the architecture of Q U E ST ++ . Document and Paragraph classes are used for document-level feature extraction. A Document is a group of Paragraphs, which in turn is a group of Sentences. Sentence is used for both word- and sentence-level feature extraction. A Feature Pro-cessing Module was created for each level. Each processing level is independent and can deal with the peculiarities of its type of feature. Machine learning Q U E ST ++ provides scripts to interface the Python toolkit scikit-learn [Cite_Footnote_3] (Pedregosa et al., ). This module is indepen-dent from the feature extraction code and uses the extracted feature sets to build and test QE models. The module can be configured to run different regression and classification algorithms, feature selection methods and grid search for hyper-parameter optimisation. Algorithms from scikit-learn can be easily integrated by modifying existing scripts.",Method,Tool,True,Introduce（引用目的）,True,P15-4020_2_0,2015,Multi-level Translation Quality Prediction with Q U E ST ++,Footnote
1391,11395," http://nlp.stanford.edu/software/lex-parser.shtml"," ['3 Features', '3.1 Word level']",The POS tags of target sentence are produced by the Stanford Parser [Cite_Footnote_4] (integrated in Q U E ST ++ ).,4 http://nlp.stanford.edu/software/lex-parser.shtml,The syntactic backoff behavior is calculated in an analogous fashion: it verifies for the existence of n-grams of POS tags in a POS-tagged LM. The POS tags of target sentence are produced by the Stanford Parser [Cite_Footnote_4] (integrated in Q U E ST ++ ).,Method,Tool,True,Use（引用目的）,True,P15-4020_3_0,2015,Multi-level Translation Quality Prediction with Q U E ST ++,Footnote
1392,11396," http://www.lexvo.org/uwn/"," ['3 Features', '3.1 Word level']","We employ the Universal WordNet, [Cite_Footnote_5] which provides access to WordNets of various languages.",5 http://www.lexvo.org/uwn/,"Semantic These features explore the polysemy of target and source words, i.e. the number of senses existing as entries in a WordNet for a given target word t i or a source word s i . We employ the Universal WordNet, [Cite_Footnote_5] which provides access to WordNets of various languages. Pseudo-reference This binary feature explores the similarity between the target sentence and a translation for the source sentence produced by an-other MT system. The feature is 1 if the given word t i in position i of a target sentence S is also present in a pseudo-reference translation R. In our experiments, the pseudo-reference is produced by Moses systems trained over parallel corpora.",補足資料,Website,True,Use（引用目的）,True,P15-4020_4_0,2015,Multi-level Translation Quality Prediction with Q U E ST ++,Footnote
1393,11397," https://github.com/ghpaetzold/questplusplus"," ['5 Remarks']","The source code for the framework, the datasets and extra resources can be downloaded from [Cite] https://github.com/ghpaetzold/questplusplus.",,"The source code for the framework, the datasets and extra resources can be downloaded from [Cite] https://github.com/ghpaetzold/questplusplus.",Mixed,Mixed,True,Produce（引用目的）,True,P15-4020_5_0,2015,Multi-level Translation Quality Prediction with Q U E ST ++,Body
1394,11398," http://www.chokkan.org/software/crfsuite/"," ['2 Architecture']","++ provides an interface for CRFSuite (Okazaki, 2007) [Cite_Ref] , a se-quence labelling C++ library for Conditional Ran-dom Fields (CRF).",N. Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields. http://www.chokkan.org/software/crfsuite/.,"For word-level prediction, Q U E ST ++ provides an interface for CRFSuite (Okazaki, 2007) [Cite_Ref] , a se-quence labelling C++ library for Conditional Ran-dom Fields (CRF). One can configure CRFSuite training settings, produce models and test them.",Method,Tool,True,Introduce（引用目的）,True,P15-4020_6_0,2015,Multi-level Translation Quality Prediction with Q U E ST ++,Reference
1395,11399," http://news.bbc.co.uk/"," ['3 BBC News Database']","We downloaded 3,361 news articles from the BBC News website. [Cite_Footnote_2]",2 http://news.bbc.co.uk/,"We downloaded 3,361 news articles from the BBC News website. [Cite_Footnote_2] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples. The documents cover a wide range of topics including national and international politics, advanced tech-nology, sports, education, etc. An example of an en-try in our database is illustrated in Figure 2. Here, the image caption is Marcin and Florent face intense competition from outside Europe and the accompa-nying article discusses EU subsidies to farmers. The images are usually 203 pixels wide and 152 pix-els high. The average caption length is 5.35 tokens, and the average document length 133.85 tokens. Our captions have a vocabulary of 2,167 words and our documents 6,253. The vocabulary shared between captions and documents is 2,056 words.",補足資料,Website,True,Use（引用目的）,True,P08-1032_0_0,2008,Automatic Image Annotation Using Auxiliary Text Information,Footnote
1396,11400," http://www.cs.princeton.edu/~blei/lda-c/index.html"," ['5 Experimental Setup']",We trained an LDA model with 20 top-ics on our document collection using David Blei’s implementation. [Cite_Footnote_3],3 Available from http://www.cs.princeton.edu/˜blei/lda-c/index.html.,"The model presented in Section 4 has a few pa-rameters that must be selected empirically on the development set. These include the vocabulary size, which is dependent on the n words with the high-est tf ∗idf scores in each document, and the num-ber of topics for the LDA-based re-ranker. We ob-tained best performance with n set to 100 (no cutoff was applied in cases where the vocabulary was less than 100). We trained an LDA model with 20 top-ics on our document collection using David Blei’s implementation. [Cite_Footnote_3] We used this model to re-rank the output of our annotation model according to the three most likely topics in each document.",Method,Tool,False,Use（引用目的）,True,P08-1032_1_0,2008,Automatic Image Annotation Using Auxiliary Text Information,Footnote
1397,11401," https://bit.ly/fsdg_emnlp2019"," ['4 Dialogue Knowledge Transfer Network', '4.2 Stage 2. Transfer']","The model (as well as its variants listed above) is imple-mented in PyTorch (Paszke et al., 2017), and the code is openly available [Cite_Footnote_1] .",1 https://bit.ly/fsdg_emnlp2019,"DiKTNet is visualized in Figure 2. The model (as well as its variants listed above) is imple-mented in PyTorch (Paszke et al., 2017), and the code is openly available [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,False,D19-1183_0_0,2019,Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks,Footnote
1398,11402," https://github.com/Elbria/utdsm_naacl2018"," ['1 Introduction']",The code of the present work is publicly avail-able [Cite_Footnote_1] .,1 https://github.com/Elbria/utdsm_ naacl2018,"To our knowledge, this is the first time that map-pings between semantic spaces are applied to the problem of learning multiple embeddings for pol-ysemous words. Our multi-topic word representa-tions are evaluated on the contextual semantic sim-ilarity task and yield state-of-the-art performance compared to other unsupervised multi-prototype word embedding approaches. We further per-form experiments on two NLP downstream tasks: text classification and paraphrase identification and demonstrate that our learned word represen-tations consistently provide higher performance than single-prototype word embedding models. The code of the present work is publicly avail-able [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,N19-1110_0_0,2019,Cross-Topic Distributional Semantic Representations Via Unsupervised Mappings,Footnote
1399,11403," http://qwone.com/jason/20Newsgroups/"," ['4 Experimental Setup', '4.5 Downstream NLP Tasks']","We used the 20NewsGroup [Cite_Footnote_6] dataset, which consists of about 20000 docu-ments.",6 http://qwone.com/jason/20Newsgroups/,"Text classification. We used the 20NewsGroup [Cite_Footnote_6] dataset, which consists of about 20000 docu-ments. Our goal is to classify each document into one of the 20 different newsgroups based on its content.",Material,Dataset,True,Use（引用目的）,True,N19-1110_1_0,2019,Cross-Topic Distributional Semantic Representations Via Unsupervised Mappings,Footnote
1400,11404," https://goo.gl/yHWeJp"," ['2 Datasets']","In practice, exact values of percent-upvoted are not directly available; the site adds “vote fuzzing” to fight vote manipulation. [Cite_Footnote_6]","6 Prior to Dec. 2016, vote information was fuzzed accord-ing to a different algorithm; however, vote statistics for all posts were recomputed according to a new algorithm that, according to a reddit moderator, can “actually be trusted;” https://goo.gl/yHWeJp","Percent Upvoted on Reddit. We quantify the rel-ative proportion of upvotes and downvotes on a post using percent-upvoted, a measure provided by Reddit that gives an estimate of the percent of all votes on a post that are upvotes. In practice, exact values of percent-upvoted are not directly available; the site adds “vote fuzzing” to fight vote manipulation. [Cite_Footnote_6] To begin with, we first dis-card posts with fewer than 30 comments. Then, we query for the noisy percent-upvoted from each post ten times using the Reddit API, and take a mean to produce a final estimate.",補足資料,Document,False,Introduce（引用目的）,False,N19-1166_0_0,2019,Something’s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features,Footnote
1401,11405," http://contrib.scikit-learn.org/lightning/"," ['4 Early Prediction of Controversy', '4.1 Comparing Text Models']",These are trained using lightning ( [Cite] http: //contrib.scikit-learn.org/lightning/).,,"16 We cross-validate regularization strength 10ˆ(-100,-5,- 4,-3,-2,-1,0,1), model type (SVM vs. Logistic L1 vs. Logistic L2 vs. Logistic L1/L2), and whether or not to apply feature standardization for each feature set and cross-validation split separately. These are trained using lightning ( [Cite] http: //contrib.scikit-learn.org/lightning/). W2V. We consider a mean, 300D word2vec (Mikolov et al., 2013) embedding representation, computed from a GoogleNews corpus.",Method,Code,True,Use（引用目的）,True,N19-1166_1_0,2019,Something’s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features,Body
1402,11406," http://www.cs.cornell.edu/~jhessel/cats/cats.html"," ['4 Early Prediction of Controversy', '4.2 Post-time Metadata']","Many non-content factors can influence commu-nity reception of posts, e.g., Hessel et al. (2017) [Cite_Ref] find that when a post is made on Reddit can signif-icantly influence its eventual popularity.","Jack Hessel, Lillian Lee, and David Mimno. 2017. Cats and captions vs. creators and the clock: Comparing multimodal content to context in pre-dicting relative popularity. In WWW. Project homepage at http://www.cs.cornell.edu/˜jhessel/cats/cats.html.","Many non-content factors can influence commu-nity reception of posts, e.g., Hessel et al. (2017) [Cite_Ref] find that when a post is made on Reddit can signif-icantly influence its eventual popularity.",補足資料,Paper,True,Introduce（引用目的）,True,N19-1166_3_0,2019,Something’s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features,Reference
1403,11407," https://chenhaot.com/papers/changemyview.html"," ['4 Early Prediction of Controversy', '4.1 Comparing Text Models']",We consider a number of hand-designed features related to the textual content of posts in-spired by Tan et al. (2016) [Cite_Ref] .,"Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, and Lillian Lee. 2016. Win-ning arguments: Interaction dynamics and per-suasion strategies in good-faith online discus-sions. In WWW, pages 613–624. Paper home-page at https://chenhaot.com/papers/changemyview.html.",HAND. We consider a number of hand-designed features related to the textual content of posts in-spired by Tan et al. (2016) [Cite_Ref] . TFIDF. We encode posts according to tfidf fea-ture vectors. Words are included in the vocabulary if they appear more than 5 times in the correspond-ing cross-validation split.,補足資料,Paper,True,Introduce（引用目的）,True,N19-1166_4_0,2019,Something’s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features,Reference
1404,11408," https://github.com/trangvu/alil-dream"," ['1 Introduction']","Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [Cite_Footnote_1] .",1 Source code is available at https://github.com/trangvu/alil-dream,"Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [Cite_Footnote_1] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task.",Method,Code,True,Produce（引用目的）,True,P19-1401_0_0,2019,Learning How to Active Learn by Dreaming,Footnote
1405,11409," http://www.nactem.ac.uk/tsujii/GENIA/ERtask/report.html"," ['4 Experiments', '4.3 Biomedical Named Entity Recognition']",We use Genia4ER named entity corpus of MEDLINE abstracts from JNLPBA 2004 shared task. [Cite_Footnote_3],3 http://www.nactem.ac.uk/tsujii/GENIA/ERtask/report.html,"In Section 4.2, we evaluated our approach on transferring the AL policy to a target task which shares the same labelling scheme as the source task. We further evaluate our methods in the sce-nario where the source and target tasks have dif-ferent characteristic. Specifically, we conduct ex-periment on cross-domain cross-annotation NER. Data and setup. We transfer the AL policy trained on the CoNLL2003 English NER task in the previous experiment, which is in the news do-main, to the biomedical NER (BioNER) task. We use Genia4ER named entity corpus of MEDLINE abstracts from JNLPBA 2004 shared task. [Cite_Footnote_3] The Genia4ER corpus is annotated in IBO2 scheme and contains five classes protein, DNA, RNA, cell-line and cell-type. The dataset has two subsets: training set of 18,758 sentences and test set of 3,918 sentences. We take out 1,758 sentences from the training set as validation set.",補足資料,Website,True,Introduce（引用目的）,False,P19-1401_2_0,2019,Learning How to Active Learn by Dreaming,Footnote
1406,11410," https://github.com/cambridgeltl/BioNLP-2016"," ['4 Experiments', '4.3 Biomedical Named Entity Recognition']","For the word embed-ding, we use the pre-trained English BioNLP em-bedding [Cite_Footnote_4] (Chiu et al., 2016) with 200 dimension.",4 https://github.com/cambridgeltl/ BioNLP-2016,"The experiment setup for policy transfer and un-derlying model is kept the same as in the NER ex-periments in Section 4.2. For the word embed-ding, we use the pre-trained English BioNLP em-bedding [Cite_Footnote_4] (Chiu et al., 2016) with 200 dimension. Vocabulary size is set to 20,000.",Material,Knowledge,True,Use（引用目的）,True,P19-1401_3_0,2019,Learning How to Active Learn by Dreaming,Footnote
1407,11411," https://nlp.jhu.edu/rams/"," ['4 Experiments', '4.1 Experiment Setup']","We conduct experiments on the RAMS [Cite_Footnote_1] dataset, which is annotated with 139 event types and 65 corresponding argument roles.",1 https://nlp.jhu.edu/rams/,"Dataset. We conduct experiments on the RAMS [Cite_Footnote_1] dataset, which is annotated with 139 event types and 65 corresponding argument roles. Each in-stance consists of a 5-sentences context around the typed event trigger, and there are several typed ar-guments to be extracted. RAMS dataset consists of 7329, 924, and 871 instances in the training, devel-opment, and test set, respectively.",Material,Dataset,True,Use（引用目的）,True,2021.acl-long.360_0_0,2021,Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction,Footnote
1408,11412," https://github.com/GChrysostomou/tasc.git"," ['References']","Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques. [Cite_Footnote_1]",1 Code is available at: https://github.com/GChrysostomou/tasc.git,"Neural network architectures in natural lan-guage processing often use attention mech-anisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to im-prove performance in various tasks, while its weights have been extensively used as explanations for model predictions. Re-cent studies (Jain and Wallace, 2019; Ser-rano and Smith, 2019; Wiegreffe and Pin-ter, 2019) have showed that it cannot gener-ally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to im-prove the faithfulness of attention-based expla-nations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attention-based explanations across two attention mech-anisms, five encoders and five text classifica-tion datasets without sacrificing predictive per-formance. Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques. [Cite_Footnote_1]",補足資料,Document,True,Compare（引用目的）,True,2021.acl-long.40_0_0,2021,Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification,Footnote
1409,11413," https://di.unipi.it/~gulli/AG_corpus_of_news_articles.html"," ['6 Experiments and Results', '6.1 Data']","We use five datasets for text classification follow-ing Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al., 2011); (iii) ADR Tweets (Sarker et al., 2015); (iv) AG News; [Cite_Footnote_9] and (v) MIMIC Anemia (Johnson et al., 2016).",9 https://di.unipi.it/˜gulli/AG_corpus_of_news_articles.html,"We use five datasets for text classification follow-ing Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al., 2011); (iii) ADR Tweets (Sarker et al., 2015); (iv) AG News; [Cite_Footnote_9] and (v) MIMIC Anemia (Johnson et al., 2016). See Table 1 for detailed data statistics.",Material,Dataset,True,Use（引用目的）,True,2021.acl-long.40_1_0,2021,Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification,Footnote
1410,11414," https://archive.org/details/twitterstream"," ['3 Experimental Setup']",Social Media Corpora Our English Twitter corpus is obtained from Archive Team’s Twitter stream grab [Cite_Footnote_4] .,4 https://archive.org/details/twitterstream,"Social Media Corpora Our English Twitter corpus is obtained from Archive Team’s Twitter stream grab [Cite_Footnote_4] . The Chinese Weibo corpus comes from Open Weiboscope Data Access 5 (Fu et al., 2013). Both corpora cover the whole year of 2012. We then randomly down-sample each corpus to 100 million messages where each message con-tains at least 10 characters, normalize the text (Han et al., 2012), lemmatize the text (Manning et al., 2014) and use LTP (Che et al., 2010) to perform word segmentation for the Chinese corpus.",Material,DataSource,True,Use（引用目的）,True,P18-1066_0_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1411,11415," http://weiboscope.jmsc.hku.hk/datazip/"," ['3 Experimental Setup']","We train En-glish and Chinese monolingual word embedding respectively using word2vec’s skip-gram method with a window size of [Cite_Footnote_5] (Mikolov et al., 2013b).",5 http://weiboscope.jmsc.hku.hk/datazip/,"Entity Linking and Word Embedding Entity linking is a preprocessing step which links vari-ous entity mentions (surface forms) to the identity of corresponding entities. For the Twitter corpus, we use Wikifier (Ratinov et al., 2011; Cheng and Roth, 2013), a widely used entity linker in En-glish. Because no sophisticated tool for Chinese short text is available, we implement our own tool that is greedy for high precision. We train En-glish and Chinese monolingual word embedding respectively using word2vec’s skip-gram method with a window size of [Cite_Footnote_5] (Mikolov et al., 2013b).",Material,Dataset,True,Use（引用目的）,True,P18-1066_1_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1412,11416," http://www.bing.com/translator/api/Dictionary/Lookup?from=en&to=zh-CHS&text="," ['3 Experimental Setup']","Our bilingual lexicon is collected from Microsoft Translator [Cite_Footnote_6] , which trans-lates English words to multiple Chinese words with confidence scores.",6 http://www.bing.com/translator/api/Dictionary/ Lookup?from=en&to=zh-CHS&text=,"Bilingual Lexicon Our bilingual lexicon is collected from Microsoft Translator [Cite_Footnote_6] , which trans-lates English words to multiple Chinese words with confidence scores. Note that all named en-tities and slang terms used in the following exper-iments are excluded from this bilingual lexicon.",Method,Tool,True,Use（引用目的）,True,P18-1066_2_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1413,11417," https://www.chinasmack.com/glossary"," ['5 Task 2: Finding most similar words for slang across languages', '5.1 Ground Truth']",We collect the Chinese slang terms from an online Chinese slang glossary [Cite_Footnote_8] consisting of 200 popular slang terms with English expla-nations.,8 https://www.chinasmack.com/glossary,"Slang Terms. We collect the Chinese slang terms from an online Chinese slang glossary [Cite_Footnote_8] consisting of 200 popular slang terms with English expla-nations. For English, we resort to a slang word Truth Sets. For each Chinese slang term, its truth set is a set of words extracted from its English ex-planation. For example, we construct the truth set of the Chinese slang term “二百五” by manually extracting significant words about its slang mean-ings (bold) in the glossary:",Material,DataSource,True,Use（引用目的）,True,P18-1066_3_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1414,11418," http://onlineslangdictionary.com/word-list/"," ['5 Task 2: Finding most similar words for slang across languages', '5.1 Ground Truth']",Table 5: ACS Sum Results of Slang Translation list from OnlineSlangDictionary [Cite_Footnote_9] with explana-tions and downsample the list to 200 terms.,9 http://onlineslangdictionary.com/word-list/,Table 5: ACS Sum Results of Slang Translation list from OnlineSlangDictionary [Cite_Footnote_9] with explana-tions and downsample the list to 200 terms.,Material,DataSource,True,Use（引用目的）,True,P18-1066_4_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1415,11419," https://cc-cedict.org/wiki/"," ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']","Another baseline method for Chinese is CC-CEDICT [Cite_Footnote_10] (CC), an on-line public Chinese-English dictionary, which is constantly updated for popular slang terms.",10 https://cc-cedict.org/wiki/,"We propose two types of baseline methods for this task. The first is based on well-known on-line translators, namely Google (Gg), Bing (Bi) and Baidu (Bd). Note that experiments using them are done in August, 2017. Another baseline method for Chinese is CC-CEDICT [Cite_Footnote_10] (CC), an on-line public Chinese-English dictionary, which is constantly updated for popular slang terms.",Material,DataSource,True,Use（引用目的）,True,P18-1066_5_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1416,11420," http://www.urbandictionary.com/"," ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']","Thus, we utilize example sentences of their slang meanings from some websites (mainly from Urban Dictionary [Cite_Footnote_11] ).",11 http://www.urbandictionary.com/,"Considering situations where many slang terms have literal meanings, it may be unfair to re-trieve target terms from such machine translators by solely inputing slang terms without specific contexts. Thus, we utilize example sentences of their slang meanings from some websites (mainly from Urban Dictionary [Cite_Footnote_11] ). The following example shows how we obtain the target translation terms for the slang word “fruitcake” (an insane person):",Material,DataSource,True,Use（引用目的）,True,P18-1066_6_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1417,11421," http://www.englishbaby.com/lessons/4349/slang/fruitcake"," ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']",She is a total fruitcake. [Cite_Footnote_12],12 http://www.englishbaby.com/lessons/4349/slang/ fruitcake,"Input sentence: Oh man, you don’t want to date that girl. She’s always drunk and yelling. She is a total fruitcake. [Cite_Footnote_12]",Material,Knowledge,True,Use（引用目的）,True,P18-1066_7_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1418,11422," https://nlp.stanford.edu/projects/glove/"," ['5 Task 2: Finding most similar words for slang across languages', '5.3 Experimental Results']",The embeddings used in ACS computations are pre-trained GloVe word vectors [Cite_Footnote_13] and thus the computation is fair among different methods.,13 https://nlp.stanford.edu/projects/glove/,"The above equation illustrates such computation, where A and B are the two word sets: A is the truth set and B is a similar list produced by each method. In the previous case of “二百五” (Sec-tion 5.1), A is {foolish, stubborn, rude, impetu-ous} while B can be {imbecile, brainless, scum-bag, imposter}. A i and B j denote the word vector of the i th word in A and j th word in B respec-tively. The embeddings used in ACS computations are pre-trained GloVe word vectors [Cite_Footnote_13] and thus the computation is fair among different methods.",Material,Knowledge,False,Use（引用目的）,False,P18-1066_8_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1419,11423," https://github.com/adapt-sjtu/socvec"," ['7 Conclusion']","Future di-rections include: 1) mining cross-cultural differ-ences in general concepts other than names and slang, 2) merging the mined knowledge into exist-ing knowledge bases, and 3) applying the SocVec in downstream tasks like machine translation. [Cite_Footnote_14]",14 We will make our code and data available at https: //github.com/adapt-sjtu/socvec.,"We present the SocVec method to compute cross-cultural differences and similarities, and evaluate it on two novel tasks about mining cross-cultural differences in named entities and computing cross-cultural similarities in slang terms. Through ex-tensive experiments, we demonstrate that the pro-posed lightweight yet effective method outper-forms a number of baselines, and can be useful in translation applications and cross-cultural stud-ies in computational social science. Future di-rections include: 1) mining cross-cultural differ-ences in general concepts other than names and slang, 2) merging the mined knowledge into exist-ing knowledge bases, and 3) applying the SocVec in downstream tasks like machine translation. [Cite_Footnote_14]",Mixed,Mixed,True,Produce（引用目的）,True,P18-1066_9_0,2018,Mining Cross-Cultural Differences and Similarities in Social Media,Footnote
1420,11424," http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm"," ['4 Evaluation', '4.2 Results']",We calculated statistical significance for the main results on the development section using bootstrap random sampling. [Cite_Footnote_6],6 Scripts for running these tests are available at http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm,"We calculated statistical significance for the main results on the development section using bootstrap random sampling. [Cite_Footnote_6] After re-sampling 1000 times, significance was calculated using a paired t-test (999 d.f.). The results indicated that lp-only exceeded the baseline, lp-ngram and lp-syn exceeded lp-only, and the full model exceeded lp-syn, with p < 0.0001 in each case.",Material,Knowledge,False,Use（引用目的）,False,D09-1043_0_0,2009,Perceptron Reranking for CCG Realization,Footnote
1421,11425," http://www.icst.pku.edu.cn/lcwm/grass"," ['References']",Our implementa-tion is available at [Cite] http://www.icst.,,"This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of fac-torization together with different higher-order features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrat-ing heterogeneous factorizations results in intractability in decoding. We pro-pose a principled method to obtain opti-mal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a signifi-cant improvement over the best published result in the literature. Our implementa-tion is available at [Cite] http://www.icst.pku.edu.cn/lcwm/grass.",Method,Tool,True,Produce（引用目的）,True,P15-1149_0_0,2015,"A Data-Driven, Factorization Parser for CCG Dependency Structures",Body
1422,11426," http://github.com/noutenki/vex"," ['1 Introduction']","VEX is available as free, open-source soft-ware for download at [Cite] http://github.com/noutenki/vex and as a web service at http://cosyne.h-its.org/vex.",,"VEX is available as free, open-source soft-ware for download at [Cite] http://github.com/noutenki/vex and as a web service at http://cosyne.h-its.org/vex.",Method,Tool,True,Produce（引用目的）,False,P15-4007_0_0,2015,Visual Error Analysis for Entity Linking,Body
1423,11427," http://cosyne.h-its.org/vex"," ['1 Introduction']","VEX is available as free, open-source soft-ware for download at http://github.com/noutenki/vex and as a web service at [Cite] http://cosyne.h-its.org/vex.",,"VEX is available as free, open-source soft-ware for download at http://github.com/noutenki/vex and as a web service at [Cite] http://cosyne.h-its.org/vex.",Method,Tool,True,Produce（引用目的）,False,P15-4007_1_0,2015,Visual Error Analysis for Entity Linking,Body
1424,11428," http://github.com/wikilinks/neleval"," ['4 Implementation']","Currently, the annotation for-mat read by the official TAC 2014 scorer [Cite_Footnote_5] , as well as a simple JSON input format are supported.",5 http://github.com/wikilinks/neleval,"VEX consists of three main components. The input component, implemented in Java 8, reads gold and system annotations files, as well as the original documents. Currently, the annotation for-mat read by the official TAC 2014 scorer [Cite_Footnote_5] , as well as a simple JSON input format are supported. All system and gold character offset ranges contained in the input files are converted into HTML spans and inserted into the document text. Since HTML elements are required to conform to a tree struc-ture, any overlap or nesting of spans is handled by breaking up such spans into non-overlapping sub-spans.",Method,Tool,True,Introduce（引用目的）,True,P15-4007_2_0,2015,Visual Error Analysis for Entity Linking,Footnote
1425,11429," https://github.com/jknack/handlebars.java"," ['4 Implementation']",The output component employs a tem-plate engine [Cite_Footnote_7] to convert the data collected by the processing component into HTML and JavaScript for handling display and user interac-tion in the web browser.,7 https://github.com/jknack/handlebars. java,The output component employs a tem-plate engine [Cite_Footnote_7] to convert the data collected by the processing component into HTML and JavaScript for handling display and user interac-tion in the web browser.,Method,Tool,True,Use（引用目的）,True,P15-4007_3_0,2015,Visual Error Analysis for Entity Linking,Footnote
1426,11430," https://github.com/abetusk/euclideanmst.js"," ['4 Implementation', '4.1 Design Decisions']","Since the actual positions of mention span el-ements on the user’s screen depend on various user environment factors such as font size and browser window dimensions, the MSTs of dis-played entities are computed using a client-side JavaScript library [Cite_Footnote_9] and are automatically redrawn if the browser window is resized.","9 https://github.com/abetusk/euclideanmst.js. This library employs Kruskal’s algorithm (Kruskal, 1956) for finding MSTs.","Since the actual positions of mention span el-ements on the user’s screen depend on various user environment factors such as font size and browser window dimensions, the MSTs of dis-played entities are computed using a client-side JavaScript library [Cite_Footnote_9] and are automatically redrawn if the browser window is resized. Drawing of edges is performed via jsPlumb , a highly cus-tomizable library for line drawing in HTML doc-uments.",Method,Code,True,Use（引用目的）,True,P15-4007_4_0,2015,Visual Error Analysis for Entity Linking,Footnote
1427,11431," http://www.jsplumb.org"," ['4 Implementation', '4.1 Design Decisions']","Drawing of edges is performed via jsPlumb [Cite_Footnote_10] , a highly cus-tomizable library for line drawing in HTML doc-uments.",10 http://www.jsplumb.org,"Since the actual positions of mention span el-ements on the user’s screen depend on various user environment factors such as font size and browser window dimensions, the MSTs of dis-played entities are computed using a client-side JavaScript library and are automatically redrawn if the browser window is resized. Drawing of edges is performed via jsPlumb [Cite_Footnote_10] , a highly cus-tomizable library for line drawing in HTML doc-uments.",Method,Code,True,Use（引用目的）,True,P15-4007_5_0,2015,Visual Error Analysis for Entity Linking,Footnote
1428,11432," https://code.google.com/p/disfluency-detection/downloads"," ['4 Experiments', '4.2 Experimental results', '4.2.1 Performance of disfluency detection on English Swtichboard corpus']",The re-sults of M 3 N † come from our experiments with the toolkit [Cite_Footnote_2] released by Qian and Liu (2013) which uses our data set with the same pre-processing.,2 The toolkit is available at https://code.google.com/p/disfluency-detection/downloads.,"The evaluation results of both disfluency detec-tion and parsing accuracy are presented in Table 2. The accuracy of M 3 N directly refers to the re-sults reported in (Qian and Liu, 2013). The re-sults of M 3 N † come from our experiments with the toolkit [Cite_Footnote_2] released by Qian and Liu (2013) which uses our data set with the same pre-processing. It is comparable between our models and the L2R parsing based joint model presented by Honni-bal and Johnson (2014), as we all conduct experi-ments on the same pre-processed data set. In order to compare parsing accuracy, we use the CRF and M 3 N † model to pre-process the test set by remov-ing all the detected disfluencies, then evaluate the parsing performance on the processed set. From the table, our BCT model with new disfluency fea-tures achieves the best performance on disfluency detection as well as dependency parsing.",Method,Tool,True,Use（引用目的）,True,P15-1048_0_0,2015,Efficient Disfluency Detection with Transition-based Parsing,Footnote
1429,11433," http://homepages.inf.ed.ac.uk/lzhang10/maxent.html"," ['5 Experiments']","For our system, we used the Le Zhang’s maximum entropy modeling toolkit to train the shift-reduce parsing model after extracting 32.6M events from the training data. [Cite_Footnote_4]",4 http://homepages.inf.ed.ac.uk/lzhang10/maxent.html,"All the three systems share with the same target-side parsed, word-aligned training data. The his-togram pruning parameter b is set to 100 and phrase table limit is set to 20 for all the three sys-tems. Moses shares the same feature set with our system except for the dependency features. For the bottom-up string-to-dependency system, we in-cluded both well-formed and ill-formed structures in chart parsing. To control the grammar size, we only extracted “tight” initial phrase pairs (i.e., the boundary words of a phrase must be aligned) as suggested by (Chiang, 2007). For our system, we used the Le Zhang’s maximum entropy modeling toolkit to train the shift-reduce parsing model after extracting 32.6M events from the training data. [Cite_Footnote_4] We set the iteration limit to 100. The accuracy on the training data is 90.18%.",Material,Dataset,True,Use（引用目的）,True,P13-1001_0_0,2013,A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation,Footnote
1430,11434," http://www.cis.upenn.edu/dbikel/software.html"," ['5 Experiments']",We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel’s randomized parsing evaluation com-parator [Cite_Footnote_6] and found that both the precision and recall gains were significant at p ≤ 0.01.,6 Available at http://www.cis.upenn.edu/dbikel/software.html,"While the main benefit of our joint model is the ability to get a consistent output over both types of annotations, we also found that modeling the parse and named entities jointly resulted in improved per-formance on both. When looking at these numbers, it is important to keep in mind that the sizes of the training and test sets are significantly smaller than the Penn Treebank. The largest of the six datasets, CNN, has about one seventh the amount of training data as the Penn Treebank, and the smallest, MNB, has around 500 sentences from which to train. Parse performance was improved by the joint model for five of the six datasets, by up to 1.36%. Looking at the parsing improvements on a per-label basis, the largest gains came from improved identication of NML consituents, from an F-score of 45.9% to 57.0% (on all the data combined, for a total of 420 NML constituents). This label was added in the new treebank annotation conventions, so as to identify in-ternal left-branching structure inside previously flat NPs. To our surprise, performance on NPs only in-creased by 1%, though over 12,949 constituents, for the largest improvement in absolute terms. The sec-ond largest gain was on PPs, where we improved by 1.7% over 3,775 constituents. We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel’s randomized parsing evaluation com-parator [Cite_Footnote_6] and found that both the precision and recall gains were significant at p ≤ 0.01.",Method,Tool,True,Use（引用目的）,True,N09-1037_0_0,2009,Joint Parsing and Named Entity Recognition,Footnote
1431,11435," http://www.ntu.edu.au/education/langs/ielex/IE-DATA1"," ['1 Introduction', '1.1 Previous work']",This information is obtained from manually curated cognate lists such as the data of Dyen et al. (1997) [Cite_Ref] .,"I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.","The task of reconstructing phylogenetic trees for languages has been studied by several authors. These approaches descend from glottochronology (Swadesh, 1955), which views a language as a col-lection of shared cognates but ignores the structure of those cognates. This information is obtained from manually curated cognate lists such as the data of Dyen et al. (1997) [Cite_Ref] .",Material,Knowledge,True,Use（引用目的）,True,D07-1093_0_0,2007,A Probabilistic Approach to Diachronic Phonology,Reference
1432,11436," http://www.ntu.edu.au/education/langs/ielex/IE-DATA1"," ['4 Experiments', '4.1 Corpus']","Since we used automatic tools for preparing our corpus rather than careful linguistic analysis, our cognate list is much noiser in terms of the pres-ence of borrowed words and phonemeic transcrip-tion errors compared to the ones used by previous approaches (Swadesh, 1955; Dyen et al., 1997 [Cite_Ref] ).","I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.","Since we used automatic tools for preparing our corpus rather than careful linguistic analysis, our cognate list is much noiser in terms of the pres-ence of borrowed words and phonemeic transcrip-tion errors compared to the ones used by previous approaches (Swadesh, 1955; Dyen et al., 1997 [Cite_Ref] ). The benefit of our mechanical preprocessing is that more cognate data can easily be made available, allowing us to effectively train richer models. We show in the rest of this section that our phonological model can indeed overcome this noise and recover meaningful patterns from the data. length |w i |+|w j | and d is the Levenshtein distance.",Material,Knowledge,True,Use（引用目的）,True,D07-1093_0_1,2007,A Probabilistic Approach to Diachronic Phonology,Reference
1433,11437," http://stream.twitter.com"," ['Corpora']",The document stream consists of 52 million tweets gathered through Twitter’s streaming API [Cite_Footnote_1] .,1 http://stream.twitter.com,"Traditional tracking datasets are unsuitable to approach tracking at scale as they consist of only a few thousand documents and several hundred topics (Allan, 2002). We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter’s streaming API [Cite_Footnote_1] . The tweets are order by their time-stamps. Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.",Method,Code,True,Use（引用目的）,True,P15-1170_0_0,2015,Tracking unbounded Topic Streams,Footnote
1434,11438," http://en.wikipedia.org/wiki/Wikipediadatabase"," ['Corpora']","To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [Cite_Footnote_2] as a proxy for a collection of topics and turn it into a stream.",2 http://en.wikipedia.org/wiki/Wikipedia database,"Traditional tracking datasets are unsuitable to approach tracking at scale as they consist of only a few thousand documents and several hundred topics (Allan, 2002). We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter’s streaming API . The tweets are order by their time-stamps. Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [Cite_Footnote_2] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.",Material,Dataset,True,Use（引用目的）,True,P15-1170_1_0,2015,Tracking unbounded Topic Streams,Footnote
1435,11439," http://newsblaster.cs.columbia.edu"," ['5 Validating the results on current news']","For these pur-poses, we downloaded 45 clusters from one day’s output from Newsblaster [Cite_Footnote_4] .",4 http://newsblaster.cs.columbia.edu,"We tested the classifiers on data different from that provided by DUC, and also tested human consen-sus on the hearer-new/old distinction. For these pur-poses, we downloaded 45 clusters from one day’s output from Newsblaster [Cite_Footnote_4] . We then automatically compiled the list of people mentioned in the ma-chine summaries for these clusters. There were 107 unique people that appeared in the machine sum-maries, out of 1075 people in the input clusters.",Material,DataSource,True,Use（引用目的）,True,H05-1031_0_0,2005,Automatically Learning Cognitive Status for Multi-Document Summarization of Newswire,Footnote
1436,11440," http://www.icsi.berkeley.edu/~framenet/"," ['2 FrameNet']","FrameNet [Cite_Footnote_1] is a lexical resource based on Fillmore’s Frame Semantics (Fillmore, 1985).",1 http://www.icsi.berkeley.edu/~framenet/,"This section presents the semantic role paradigm and the role-annotated corpus on which the present study is based. FrameNet [Cite_Footnote_1] is a lexical resource based on Fillmore’s Frame Semantics (Fillmore, 1985). It de-scribes frames, representations of prototypical situa-tions. Each frame provides its set of semantic roles, the entities or concepts pertaining to the prototypi-cal situation. Each frame is further associated with a set of target predicates (nouns, verbs or adjectives), occurrences of which can introduce the frame.",Material,DataSource,True,Use（引用目的）,True,H05-1084_0_0,2005,Analyzing models for semantic role assignment using confusability,Footnote
1437,11441," http://ilk.uvt.nl/downloads/pub/papers/ilk0310.ps.gz"," ['3 Experiment 1: Variance in role assignment']","The first learner, TiMBL (Daelemans et al., 2003) [Cite_Ref] is an implementa-tion of nearest-neighbor classification algorithms in the memory-based learning paradigm .","W. Daelemans, J. Zavrel, K. van der Sloot, A. van den Bosch. 2003. Timbl: Tilburg memory based learner, version 5.0, reference guide. Technical Re-port ILK 03-10, Tilburg University, 2003. Available from http://ilk.uvt.nl/downloads/pub/papers/ilk0310.ps.gz.","Modeling. We model role assignment as a clas-sification task, with parse tree constituents as in-stances to be classified. We repeated the classifica-tion with two different learners: The first learner, TiMBL (Daelemans et al., 2003) [Cite_Ref] is an implementa-tion of nearest-neighbor classification algorithms in the memory-based learning paradigm . The second learner, Malouf’s probabilistic maximum entropy (Maxent) system (Malouf, 2002), uses the LMVM algorithm to estimate log-linear models. We did not perform smoothing.",補足資料,Paper,True,Introduce（引用目的）,True,H05-1084_1_0,2005,Analyzing models for semantic role assignment using confusability,Reference
1438,11442," https://datasets.maluuba.com/Frames"," ['4 Experiments and Results', '4.3 User Simulator']","Furthermore, at each turn, the agent receives a reward of [Cite_Footnote_1] so that shorter dialogue sessions are encouraged.",1 https://datasets.maluuba.com/Frames,"Training reinforcement learners is challenging be-cause they need an environment to interact with. In the dialogue research community, it is common to use simulated users as shown in Figure 3 for this purpose (Schatzmann et al., 2007; Asri et al., 2016). In this work, we adapted the publicly-available user simulator, developed by Li et al. (2016), to the composite task-completion dialogue setting using the human-human conversation data described in Section 4.1. During training, the simulator provides the agent with an (extrinsic) re-ward signal at the end of the dialogue. A dialogue is considered to be successful only when a travel plan is made successfully, and the information provided by the agent satisfies user’s constraints. At the end of each dialogue, the agent receives a positive reward of 2⇤max turn (max turn = 60 in our experiments) for success, or a negative re-ward of max turn for failure. Furthermore, at each turn, the agent receives a reward of [Cite_Footnote_1] so that shorter dialogue sessions are encouraged.",Material,Knowledge,False,Use（引用目的）,True,D17-1237_0_0,2017,Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning,Footnote
1439,11443," https://fever.ai/"," ['1 Introduction']","To this end, the Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018a) [Cite_Footnote_1] introduced a dataset for evidence-based fact verification.",1 https://fever.ai/,"A rapid increase in the spread of misinformation on the Internet has necessitated automated solu-tions to determine the validity of a given piece of information. To this end, the Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018a) [Cite_Footnote_1] introduced a dataset for evidence-based fact verification. Given a claim, the task in-volves extracting relevant evidence sentences from a given Wikipedia dump and assigning a label to the claim by reasoning over the extracted evidence (S UPPORTS / R EFUTES / N OT E NOUGH I NFO ).",補足資料,Website,False,Introduce（引用目的）,True,2020.emnlp-main.629_0_0,2020,Constrained Fact Verification for FEVER,Footnote
1440,11444," https://github.com/adithya7/constrained-fever"," ['1 Introduction']",Our datasets and code are publicly available. [Cite_Footnote_2],2 https://github.com/adithya7/ constrained-fever,• We create an anonymized version of the FEVER dataset to facilitate investigation into the factual knowledge through named entities. Our datasets and code are publicly available. [Cite_Footnote_2],Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.629_1_0,2020,Constrained Fact Verification for FEVER,Footnote
1441,11445," https://huggingface.co/transformers/"," ['2 Constrained Verification', '2.3 Methodology', 'Taker datasets following Clark et al. (2020).']","We use huggingface transform-ers (Wolf et al., 2019) in all of our experiments. [Cite_Footnote_7]",7 https://huggingface.co/transformers/,"We compare the above-proposed curricula (CWA, Skip–fact) against a baseline curriculum (Original) where we initialize the verification models with standard pretrained BERT weights (bert-base-cased). We use huggingface transform-ers (Wolf et al., 2019) in all of our experiments. [Cite_Footnote_7]",Method,Tool,False,Produce（引用目的）,True,2020.emnlp-main.629_2_0,2020,Constrained Fact Verification for FEVER,Footnote
1442,11446," http://jmcauley.ucsd.edu/data/amazon"," ['3 Approach', '3.3 Aspect Conditional Masked Language Model']",Table [Cite_Footnote_4] shows an example of the generation pro-cess.,4 http://jmcauley.ucsd.edu/data/amazon,"Table [Cite_Footnote_4] shows an example of the generation pro-cess. We initialize the template sequence X 0 as (universe, [MASK], . . . , ##ble) with length T . At each iteration i, a position t i is sampled uniformly at random from {1, . . . , T} and the token at t i (i.e. x it i ) of the current sequence X i is replaced by [MASK]. After that, we obtain the conditional probability of x t i as",補足資料,Document,True,Introduce（引用目的）,True,D19-1018_1_0,2019,Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects,Footnote
1443,11447," http://pytorch.org/docs/master/index.html"," ['4 Experiments', '4.3 Implementation Detail']",We use PyTorch [Cite_Footnote_6] to implement our models.,6 http://pytorch.org/docs/master/index.html,We use PyTorch [Cite_Footnote_6] to implement our models.,Method,Tool,True,Use（引用目的）,True,D19-1018_2_0,2019,Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects,Footnote
1444,11448," https://github.com/huggingface/pytorch-pretrained-BERT"," ['4 Experiments', '4.3 Implementation Detail']","For ACMLM, we build our model based on the BERT implementation from HuggingFace. [Cite_Footnote_7]",7 https://github.com/huggingface/pytorch-pretrained- BERT,"For Req2Seq and AP-Ref2Seq, we set the hid-den size and word embedding size as 256. We ap-ply a dropout rate of 0.5 for the encoder and 0.2 for the decoder. The size of the justification refer-ence l r is set to 5 and the number of fine-grained aspects K in the user persona and item profile is set to 30. We train the model using Adam with learning rate 2e −4 and stop training either when it reaches 20 epochs or the perplexity does not im-prove (on the Dev set). For ACMLM, we build our model based on the BERT implementation from HuggingFace. [Cite_Footnote_7] We initialize our decoder using the pre-trained ‘Bert-base’ model and set the max sequence length to 30. We train the model for 5 epochs using Adam with learning rate 2e −5 . For models using beam search, we set the beam size as 10. For models using ‘top-k’ sampling, we set k to 5. For ACMLM, we use a burn-in step equal to the length of the initial sequence. Our data and code are available online. 8",Method,Tool,True,Use（引用目的）,True,D19-1018_3_0,2019,Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects,Footnote
1445,11449," https://github.com/nijianmo/recsysjustification.git"," ['4 Experiments', '4.3 Implementation Detail']",Our data and code are available online. [Cite_Footnote_8],8 https://github.com/nijianmo/recsysjustification.git,"For Req2Seq and AP-Ref2Seq, we set the hid-den size and word embedding size as 256. We ap-ply a dropout rate of 0.5 for the encoder and 0.2 for the decoder. The size of the justification refer-ence l r is set to 5 and the number of fine-grained aspects K in the user persona and item profile is set to 30. We train the model using Adam with learning rate 2e −4 and stop training either when it reaches 20 epochs or the perplexity does not im-prove (on the Dev set). For ACMLM, we build our model based on the BERT implementation from HuggingFace. 7 We initialize our decoder using the pre-trained ‘Bert-base’ model and set the max sequence length to 30. We train the model for 5 epochs using Adam with learning rate 2e −5 . For models using beam search, we set the beam size as 10. For models using ‘top-k’ sampling, we set k to 5. For ACMLM, we use a burn-in step equal to the length of the initial sequence. Our data and code are available online. [Cite_Footnote_8]",Mixed,Mixed,True,Produce（引用目的）,True,D19-1018_4_0,2019,Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects,Footnote
1446,11450," https://github.com/YatinChaudhary/"," ['1 Introduction']",The code is avail-able at [Cite] https://github.com/YatinChaudhary/ Multi-view-Multi-source-Topic-Modeling .,,"We evaluate the effectiveness of multi-source neural topic modeling in multi-view embedding spaces using 7 (5 low-resource and 2 high-resource) target and 5 (high-resource) source corpora from news and medical domains, consisting of short-text, long-text, small and large document col-lections. We have shown state-of-the-art re-sults with significant gains quantified by gener-alization (perplexity), interpretability (topic co-herence) and text retrieval. The code is avail-able at [Cite] https://github.com/YatinChaudhary/ Multi-view-Multi-source-Topic-Modeling . pretrained word embeddings from a WordPool at each autoregressive step i. Double circle → multinomial (soft-max) unit (Larochelle and Lauly, 2012). (Right) DocNADE (GVT+MST): Multi-source transfer learning in NTM by introducing pretrained (latent) topic embeddings from a TopicPool, illustrating topic alignments between source and target corpora. Each outgoing row from Z k ∈R H×K signify a topic embedding of corresponding kth source corpus, DC k . Here, NTM refers to a DocNADE (Larochelle and Lauly, 2012) based Neural Topic Model.",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.332_0_0,2021,Multi-source Neural Topic Modeling in Multi-view Embedding Spaces,Body
1447,11451," https://github.com/Franck-Dernoncourt/pubmed-rct"," ['A Data Description']",Source: [Cite] https://github.com/Franck-Dernoncourt/pubmed-rct.,,"To prepare knowledge base of word embed-ings (local semantics) and latent topics (global semantics) features, we use the following six datasets in the source S: (1) 20NS: 20News-Groups corpus, a collection of news stories from nltk.corpus . (2) TMN: The Tag My News (TMN) news dataset. (3) R21578: Reuters corpus, a collection of new stories from nltk. corpus . (4) AGnews: AGnews data sellection. PubMed: Medical abstracts of randomized con-trolled trials. Source: [Cite] https://github.com/Franck-Dernoncourt/pubmed-rct.",Material,DataSource,False,Use（引用目的）,True,2021.naacl-main.332_1_0,2021,Multi-source Neural Topic Modeling in Multi-view Embedding Spaces,Body
1448,11452," https://github.com/clarkkev/deep-coref"," ['1 Introduction']","Our fi-nal system achieves CoNLL F 1 scores of 65.29 for English and 63.66 for Chinese, substantially out-performing other state-of-the-art systems. [Cite_Footnote_1]",1 Code and trained models are available at https://github.com/clarkkev/deep-coref.,"Our system uses little manual feature engineer-ing, which means it is easily extended to multiple languages. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset. The cluster-ranking model signifi-cantly outperforms a mention-ranking model that does not use entity-level information. We also show that using an easy-first strategy improves the performance of the cluster-ranking model. Our fi-nal system achieves CoNLL F 1 scores of 65.29 for English and 63.66 for Chinese, substantially out-performing other state-of-the-art systems. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,P16-1061_0_0,2016,Improving Coreference Resolution by Learning Entity-Level Distributed Representations,Footnote
1449,11453," http://www.statmt.org/moses/giza/GIZA++.html"," ['1 Introduction']","IBM models (Brown et al., 1993), which are based on word sequences, have been widely used for obtaining word align-ments because they are fast and their implementa-tion is available as GIZA++. [Cite_Footnote_1]",1 http://www.statmt.org/moses/giza/GIZA++.html,"In statistical machine translation (SMT), word alignment plays an essential role in obtaining phrase tables (Och and Ney, 2004; Koehn et al., 2003) or syntactic transformation rules (Chiang, 2007; Shen et al., 2008). IBM models (Brown et al., 1993), which are based on word sequences, have been widely used for obtaining word align-ments because they are fast and their implementa-tion is available as GIZA++. [Cite_Footnote_1]",Method,Tool,True,Introduce（引用目的）,True,P16-3002_0_0,2016,Dependency Forest based Word Alignment,Footnote
1450,11454," http://jasonriesa.github.io/nile/"," ['1 Introduction']","Recently, a hierarchical alignment model (whose implementation is known as Nile [Cite_Footnote_2] ) (Riesa et al., 2011), which performs better than IBM models, has been proposed.",2 http://jasonriesa.github.io/nile/,"Recently, a hierarchical alignment model (whose implementation is known as Nile [Cite_Footnote_2] ) (Riesa et al., 2011), which performs better than IBM models, has been proposed. In the hierarchi-cal alignment model, both source and target con-stituency trees are used for incorporating syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees. It achieved significantly better results than the IBM Model4 in Arabic-English and Chinese-English word alignment tasks, even though the model was trained on only 2,280 and 1,102 par-allel sentences as gold standard alignments. How-ever, their models rely only on 1-best source and target side parse trees, which are not necessarily good for word alignment tasks.",Method,Tool,True,Introduce（引用目的）,True,P16-3002_1_0,2016,Dependency Forest based Word Alignment,Footnote
1451,11455," https://github.com/hitochan777/mt-tools/releases/tag/1.0.1"," ['3 Experiments', '3.1 Experimental Settings']",We converted con-stituent parse trees obtained by Berkeley Parser to dependency parse trees using rules. [Cite_Footnote_3],3 The conversion program is available at https://github.com/hitochan777/mt-tools/releases/tag/1.0.1,"We conducted alignment experiments on the Japanese-English language pair. For dependency parsers, we used KNP (Kawahara and Kurohashi, 2006) for Japanese and Berkeley Parser (Petrov and Klein, 2007) for English. We converted con-stituent parse trees obtained by Berkeley Parser to dependency parse trees using rules. [Cite_Footnote_3] We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200. The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile , the Bayesian subtree alignment model (Nakazawa model) (Nakazawa and Kurohashi, 2011) and IBM Model4. We used Nile without automatically ex-tracted rule features and constellation features to make a fair comparison with our model.",Method,Code,True,Produce（引用目的）,True,P16-3002_2_0,2016,Dependency Forest based Word Alignment,Footnote
1452,11456," http://lotus.kuee.kyoto-u.ac.jp/ASPEC/"," ['3 Experiments', '3.1 Experimental Settings']","We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. [Cite_Footnote_4]",4 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/,"We conducted alignment experiments on the Japanese-English language pair. For dependency parsers, we used KNP (Kawahara and Kurohashi, 2006) for Japanese and Berkeley Parser (Petrov and Klein, 2007) for English. We converted con-stituent parse trees obtained by Berkeley Parser to dependency parse trees using rules. We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. [Cite_Footnote_4] Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200. The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile , the Bayesian subtree alignment model (Nakazawa model) (Nakazawa and Kurohashi, 2011) and IBM Model4. We used Nile without automatically ex-tracted rule features and constellation features to make a fair comparison with our model.",Material,DataSource,True,Use（引用目的）,True,P16-3002_3_0,2016,Dependency Forest based Word Alignment,Footnote
1453,11457," https://github.com/xuuuluuu/SynLSTM-for-NER"," ['References']",Our fur-ther analysis demonstrates that our model can capture longer dependencies compared with strong baselines. [Cite_Footnote_1],1 We make our code publicly available at https://github.com/xuuuluuu/SynLSTM-for-NER.,"It has been shown that named entity recogni-tion (NER) could benefit from incorporating the long-distance structured information cap-tured by dependency trees. We believe this is because both types of features – the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural net-works such as graph convolutional networks (GCNs) for building improved NER models, where the exact interaction mechanism be-tween the two different types of features is not very clear, and the performance gain does not appear to be significant. In this work, we pro-pose a simple and robust solution to incorpo-rate both types of features with our Synergized-LSTM (Syn-LSTM), which clearly captures how the two types of features interact. We con-duct extensive experiments on several standard datasets across four languages. The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. Our fur-ther analysis demonstrates that our model can capture longer dependencies compared with strong baselines. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.271_0_0,2021,Better Feature Integration for Named Entity Recognition,Footnote
1454,11458," https://github.com/hanxiao/bert-as-service"," ['4 Experiments']","Specifically, we use bert-as-service (Xiao, 2018) [Cite_Ref] to generate the contextualized word representation without fine-tuning.",Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.,"Experimental Setup For Catalan, Spanish, and Chinese, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to initialize the word embeddings. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [Cite_Ref] to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data. We use the cased version of BERT base model for the experiments on the other three datasets. For the character embedding, we ran-domly initialize the character embeddings and set the dimension as 30, and set the hidden size of character-level BiLSTM as 50. The hidden size of GCN and Syn-LSTM is set as 200, the number of GCN layer is 2. We adopt stochastic gradient de-scent (SGD) to optimize our model with batch size 100, L2 regularization 10 −8 , initial learning rate lr 0.2 and the learning rate is decayed with respect to the number of epoch. We select the best model based on the performance on the dev set and apply it to the test set. We use the bootstrapping t-test to compare the results.",Method,Tool,False,Use（引用目的）,True,2021.naacl-main.271_1_0,2021,Better Feature Integration for Named Entity Recognition,Reference
1455,11459," https://github.com/hanxiao/bert-as-service"," ['References']","Specifically, we use bert-as-service (Xiao, 2018) [Cite_Ref] to generate the contextualized word representation without fine-tuning.",Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.,"For hyper-parameter, we use the FastText (Grave et al., 2018) 300 dimensional embeddings to ini-tialize the word embeddings for Catalan, Spanish, and Chinese. For OntoNotes 5.0 English, we adopt the publicly available GloVE (Pennington et al., 2014) 100 dimensional embeddings to initialize the word embeddings. For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [Cite_Ref] to generate the contextualized word representation without fine-tuning. Following Luo et al. (2020), we select the 18 th layer of the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data. We use the the 9 th layer of cased version of BERT base model for the experiments on the rest three datasets. For the character embedding, we randomly initialize the character embeddings and set the dimension as 30, and set the hidden size of character-level BiLSTM as 50. The hidden size of GCN and Syn-LSTM is set as 200. Note that we only use one layer of bi-directional Syn-LSTM for our experi-ments. Dropout is set to 0.5 for input embeddings and hidden states. We adopt stochastic gradient descent (SGD) to optimize our model with batch size 100, L2 regularization 10 −8 , learning rate 0.2 and the learning rate is decayed with respect to the number of epoch 9 . indicates the value of gate m t , the y-axis denotes the number of cells. sentence length. y-axis:F 1 score (%). Note that DGLSTM-CRF +ELMO have better performance com-pared to DGLSTM-CRF +BERT based on the results in the main paper.",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.271_1_1,2021,Better Feature Integration for Named Entity Recognition,Reference
1456,11460," https://github.com/h-shahidi/"," ['References']",Code is avail-able at [Cite] https://github.com/h-shahidi/ 2birds-gen .,,"A number of researchers have recently ques-tioned the necessity of increasingly complex neural network (NN) architectures. In partic-ular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and un-structured data. We consider neural table-to-text generation and neural question gener-ation (NQG) tasks for text generation from structured and unstructured data, respectively. Table-to-text generation aims to generate a de-scription based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models. Experimen-tal results demonstrate that a basic attention-based seq2seq model trained with the expo-nential moving average technique achieves the state of the art in both tasks. Code is avail-able at [Cite] https://github.com/h-shahidi/ 2birds-gen .",Method,Code,True,Produce（引用目的）,True,2020.acl-main.355_0_0,2020,"Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data",Body
1457,11461," http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html"," ['3 Experiments', '3.1 Data and Tools']","The phrase structures in the treebank are converted into dependencies using Penn2Malt tool [Cite_Footnote_3] with the stan-dard head rules (Yamada and Matsumoto, 2003).",3 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html,"In the first two experiments, we used the Wal-l Street Journal (WSJ) and Brown (B) portion-s of the English Penn TreeBank (Marcus et al., 1993). In the first experiment denoted by “WSJ-to-B”, WSJ corpus is used as the source domain and Brown corpus as the target domain. In the second experiment, we use the reverse order of the two corpora and denote it by “B-to-WSJ”. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool [Cite_Footnote_3] with the stan-dard head rules (Yamada and Matsumoto, 2003).",Method,Tool,True,Use（引用目的）,True,P13-2104_0_0,2013,Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data,Footnote
1458,11462," http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz"," ['3 Experiments', '3.1 Data and Tools']","In the third experiment denoted by ’“WSJ-to- G”, we used WSJ corpus as the source domain and Genia corpus (G) [Cite_Footnote_4] as the target domain.",4 Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz,"In the third experiment denoted by ’“WSJ-to- G”, we used WSJ corpus as the source domain and Genia corpus (G) [Cite_Footnote_4] as the target domain. Following Plank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007). The Genia corpus is converted to CoNLL format with LTH converter, too. We randomly selected about 1000 sentences from the training portion of Genia data and use them as the labeled data of the target domain, and the rest of training data of Ge-nia as the unlabeled data of the target domain. Ta-ble 1 shows the number of sentences of each data set used in the experiments.",Material,Dataset,True,Use（引用目的）,True,P13-2104_1_0,2013,Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data,Footnote
1459,11463," http://sourceforge.net/projects/maxparser/"," ['3 Experiments', '3.2 Comparison Systems']","Self-Training: Following Reichart and Rap-poport (2007), we train a parser with the union of the source and target labeled data, parse the unlabeled data in the target domain, 5 [Cite] http://sourceforge.net/projects/maxparser/ add the entire auto-parsed trees to the man-ually labeled data in a single step without checking their parsing quality, and retrain the parser.",,"Self-Training: Following Reichart and Rap-poport (2007), we train a parser with the union of the source and target labeled data, parse the unlabeled data in the target domain, 5 [Cite] http://sourceforge.net/projects/maxparser/ add the entire auto-parsed trees to the man-ually labeled data in a single step without checking their parsing quality, and retrain the parser.",Method,Tool,True,Use（引用目的）,True,P13-2104_2_0,2013,Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data,Body
1460,11464," https://github.com/IBM/regen"," ['References']",More details in [Cite] https://github.com/IBM/regen.,,"Automatic construction of relevant Knowl-edge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirec-tional generation of text and graph leveraging Reinforcement Learning (RL) to improve per-formance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the genera-tive direction, which in turn allows the use of Reinforcement Learning for sequence train-ing where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive in-vestigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and T EK G EN datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improv-ing upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details in [Cite] https://github.com/IBM/regen.",補足資料,Document,True,Produce（引用目的）,True,2021.emnlp-main.83_0_0,2021,ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models,Body
1461,11465," https://www.davidsilver.uk/teaching/"," ['3 Models', '3.1 Reinforcement Learning']","This interpretation enables the reformulation of Seq2Seq generation within the Reinforcement Learning (RL) framework (Sutton and Barto, 2018; Silver, 2015 [Cite_Ref] ).",David Silver. 2015. Lectures on reinforcement learn-ing. URL : https://www.davidsilver.uk/teaching/.,"Sequence generation can be seen as an agent mak-ing sequential decisions of picking words from a given vocabulary. The agent reacts to its envi-ronment by accounting for past predictions and getting rewarded along the way, while its state is defined by the partial sequence generated so far. This interpretation enables the reformulation of Seq2Seq generation within the Reinforcement Learning (RL) framework (Sutton and Barto, 2018; Silver, 2015 [Cite_Ref] ). More precisely, a sequence gen-eration task can be recast as a Markov Decision Process (MDP) where the agent behavior follows a policy π(a t |s t ). Action a t corresponds to picking a particular word w t at time t from a vocabulary V, conditioned on state s t expressed as the par-tial sequence generation s t = x̂ 1:t = [ŵ 1 , . . . , ŵ t ], that is sequence of words/tokens already picked. π(a t |s t ) is a stochastic policy that defines a proba-bility distribution of a t . Once the action a t is taken, the agent receives a reward r t = r(s t , a t ) before it transitions to the next state s t+1 . A sequence of actions a 1:T = [a 1 , . . . , a T ] is selected until the end of generation is reached. The agent aims at maximizing the expectation of cumulative reward where γ is a discounting factor used to control the horizon of the cumulative reward, γ ∈ [0, 1]. The expectation is taken over trajectories τ, sequences made of {s 1 , a 1 , r 1 , . . . , s T , a T , r T }, where a t was chosen from policy π(a t |s t ). RL provides both on-policy and off-policy approaches to maximize J(π) in Eq. (3). We are particularly interested in on-policy techniques that rely on data samples gen-erated from the model to train, especially since our models start from large fine-tuned PLMs that can already generate good samples. This helps avoid the common drawback of on-policy techniques of generating poor samples at first when trained from scratch. These policy-based (Williams, 1992; Zaremba and Sutskever, 2016) and actor-critic based techniques (Bahdanau et al., 2017; Rennie et al., 2017) have been studied for text generation and often update the underlying model with policy gradient (Ranzato et al., 2016; Li et al., 2016; Tan et al., 2019; Paulus et al., 2017). Policy-based meth-ods focus on a parameterized policy π θ where θ is optimized to maximize J(π θ ). The policy π θ (a t |s t ) is the PLM generative model p θ , CE fine-tuned as described at the beginning of Section 3.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.83_1_0,2021,ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models,Reference
1462,11466," https://github.com/nlpsoc/reliability_bias"," ['References']","More-over, we urge researchers to be more critical about the application of such measures. [Cite_Footnote_1]",1 Our code is available at https://github.com/nlpsoc/reliability_bias.,"Various measures have been proposed to quan-tify human-like social biases in word embed-dings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reli-ability, concerning the extent to which a mea-sure produces consistent results. In this pa-per, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and in-ternal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures’ reliability scores. Our findings inform better design of word embedding gender bias measures. More-over, we urge researchers to be more critical about the application of such measures. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.785_0_0,2021,Assessing the Reliability of Word Embedding Gender Bias Measures,Footnote
1463,11467," https://archive.org/download/2015_reddit_comments_corpus/reddit_data/2014/"," ['5 Experiments', '5.1 Experimental Setup']",[Cite_Footnote_5] .,"5 We use all posts and replies from 2014, retrieved from https://archive.org/download/2015_reddit_comments_corpus/reddit_data/2014/.","Training Embeddings We select three corpora with different characteristics to train word embed-dings. Two are from subReddits: r/AskScience (∼ 158 million tokens) and r/AskHistorians (∼ 137 million tokens, also used by Antoniak and Mimno 2018) [Cite_Footnote_5] . The third is the training set of WikiText-103 (∼ 527 million tokens, Merity et al., 2016), consisting of high-quality Wikipedia articles.",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.785_1_0,2021,Assessing the Reliability of Word Embedding Gender Bias Measures,Footnote
1464,11468," https://github.com/stanfordnlp/GloVe"," ['5 Experiments', '5.1 Experimental Setup']","For all other hyper-parameters, we use the default values of previous implementations. [Cite_Footnote_6]","6 For SGNS, we use Gensim 3.8.3 (Řehůřek and Sojka, 2010) with a window size of 5, a minimum word count of 5 and 5 iterations. For GloVe, we use the official implemen-tation https://github.com/stanfordnlp/GloVe, with a window size of 15, a minimum word count of 5 and 15 iterations. Because we do not fine-tune hyper-parameters, our results do not necessarily indicate which algorithm itself is of better reliability. To investigate the potential impact of this decision, we also include an explorative study on the influence of hyper-parameters in Appendix E.","We use two popular word embedding algo-rithms: Skip-Gram with Negative Sampling (SGNS; Mikolov et al. 2013) and GloVe (Penning-ton et al., 2014). For both algorithms, we set the number of embedding dimensions to 300. For all other hyper-parameters, we use the default values of previous implementations. [Cite_Footnote_6] For each corpus-algorithm pair, we train k = 32 word embedding models using different random seeds.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.785_2_0,2021,Assessing the Reliability of Word Embedding Gender Bias Measures,Footnote
1465,11469," https://github.com/first20hours/google-10000-english"," ['5 Experiments', '5.1 Experimental Setup']",[Cite_Footnote_7] and 5) the full vocabulary of each corpus (Full).,7 https://github.com/first20hours/google-10000-english.,"Target Word Lists For the assessment of test-retest reliability and inter-rater consistency, we in-clude three word lists used in previous word embed-ding bias studies: 1) 320 occupation words from Bolukbasi et al. (2016) (OCC16), 2) 76 occupation words from Garg et al. (2018) (OCC18) and 3) 230 adjectives from Garg et al. (2018) (ADJ). How-ever, these three lists are very specific (i.e. only concerning occupation words and adjectives) and thus unlikely applicable to other (future) research where different biases are of interest and different target words might be used (e.g. measuring gen-der biases of a whole corpus). Therefore, we also consider two additional, larger target word lists: 4) the top 10,000 most frequent words of Google’s trillion word corpus (Google10K) [Cite_Footnote_7] and 5) the full vocabulary of each corpus (Full).",Material,Knowledge,True,Introduce（引用目的）,True,2021.emnlp-main.785_3_0,2021,Assessing the Reliability of Word Embedding Gender Bias Measures,Footnote
1466,11470," https://github.com/explosion/spaCy"," ['B Environmental Setup']","Data Preprocessing For Reddit data (i.e. r/AskScience and r/AskHistorians), we lower-cased, removed redundant spaces/urls, and used the Spacy [Cite_Footnote_11] library to tokenize each sentence.",11 https://github.com/explosion/spaCy The word embeddings are trained with GloVe on r/AskScience. The word embeddings are trained with GloVe on WikiText-103. The word embeddings are trained with SGNS on r/AskHistorians. SGNS word embeddings trained with three iterations or 100 dimensions on r/AskHistorians. dings trained with three iterations or 100 dimensions on r/AskHistorians.,"Data Preprocessing For Reddit data (i.e. r/AskScience and r/AskHistorians), we lower-cased, removed redundant spaces/urls, and used the Spacy [Cite_Footnote_11] library to tokenize each sentence. For training GloVe embeddings, we substi-tuted “” symbols in WikiText-103 with “” symbols.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.785_4_0,2021,Assessing the Reliability of Word Embedding Gender Bias Measures,Footnote
1467,11471," https://code.google.com/p/universal-tagger/"," ['6 Conclusion']",We made our code are available for download. [Cite_Footnote_4],4 https://code.google.com/p/universal-tagger/,"We have proposed a method for unsupervised POS tagging that performs on par with the current state-of-the-art (Das and Petrov, 2011), but is substan-tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM). The complexity of our algorithm is O(nlogn) compared to O(n 2 ) for that of Das and Petrov (2011) where n is the size of training data. We made our code are available for download. [Cite_Footnote_4]",Method,Code,True,Produce（引用目的）,True,P13-2112_0_0,2013,Simpler unsupervised POS tagging with bilingual projections,Footnote
1468,11472," http://www.delph-in.net/matrix/customize/matrix.cgi"," ['2 Background', '2.1 The LinGO Grammar Matrix']",The Matrix is accessed through a web-based configuration system [Cite_Footnote_1] which elicits ty-pological information from the user-linguist through a questionnaire and then outputs a grammar consist-ing of the Matrix core plus selected types and con-straints from the libraries according to the specifica-tions in the questionnaire.,1 http://www.delph-in.net/matrix/customize/matrix.cgi,"The Grammar Matrix consists of a cross-linguistic core type hierarchy and a collection of phenomenon-specific libraries. The core type hierar-chy defines the basic feature geometry, the ways that heads combine with arguments and adjuncts, linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Min-imal Recursion Semantics (Copestake et al., 2005; Flickinger and Bender, 2003). The libraries provide collections of analyses for cross-linguistically vari-able phenomena. The current libraries include anal-yses of major constituent word order (SOV, SVO, etc), sentential negation, coordination, and yes-no question formation. The Matrix is accessed through a web-based configuration system [Cite_Footnote_1] which elicits ty-pological information from the user-linguist through a questionnaire and then outputs a grammar consist-ing of the Matrix core plus selected types and con-straints from the libraries according to the specifica-tions in the questionnaire.",Method,Tool,False,Introduce（引用目的）,True,P08-1111_0_0,2008,Evaluating a Crosslinguistic Grammar Resource: A Case Study of Wambaya,Footnote
1469,11473," http://homepage.fudan.edu.cn/zhengxq/deeplearning/"," ['1 Introduction and Motivation']",We call this proposed model an incremental neural de-pendency parsing (INDP) [Cite_Footnote_1] .,1 The source code is available at http://homepage.fudan.edu.cn/zhengxq/deeplearning/,"The proposed parser first encodes each word in a sentence by distributed embeddings using a con-volutional neural network and constructs an initial parse graph by head-modifier predictions with a maximum directed spanning tree algorithm based on the first-order features (i.e. the score is fac-tored over the arcs in a graph). Once an initial parse graph is built, the high-order features (such as grandparent, sibling, and uncle) can be defined, and used to refine the structure of the parse tree in an iterative way. Theoretically, the refinement will continue until no change is made in the iteration. But experimental results demonstrated that pretty good performance can be achieved with no more than twice updates because many dependencies are determined by independent arc prediction and a few head-modifier pairs need to be re-estimated after one update (i.e. only a few changes above and beyond the dominant first-order scores). We call this proposed model an incremental neural de-pendency parsing (INDP) [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,D17-1173_0_0,2017,Incremental Graph-based Neural Dependency Parsing,Footnote
1470,11474," http://code.google.com/p/word2vec/"," ['3 Experiments', '3.2 Training Strategy']","English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [Cite_Footnote_3] proposed in (Mikolov et al., 2013).",3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep,"Previous work demonstrated that the performance can be improved by using word embeddings learned from large-scale unlabeled data in many NLP tasks both in English (Collobert et al., 2011; Socher et al., 2011) and Chinese (Zheng et al., 2013). Unsupervised pretraining guides the learn-ing towards basins of attraction of minima that support better generalization (Erhan et al., 2010). We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [Cite_Footnote_3] proposed in (Mikolov et al., 2013).",Method,Tool,True,Use（引用目的）,True,D17-1173_1_0,2017,Incremental Graph-based Neural Dependency Parsing,Footnote
1471,11475," http://www-nlp.stanford.edu/software/bioprocess"," ['b Transfer of ions']",The data and code for this paper are avail-able at [Cite] http://www-nlp.stanford.edu/software/bioprocess.,,The data and code for this paper are avail-able at [Cite] http://www-nlp.stanford.edu/software/bioprocess.,Mixed,Mixed,True,Produce（引用目的）,True,D14-1159_0_0,2014,Modeling Biological Processes for Reading Comprehension,Body
1472,11476," http://www.gurobi.com/"," ['6 Empirical Evaluation', '6.1 Experimental setup']",We used the Gurobi optimization package [Cite_Footnote_3] for infer-ence.,3 http://www.gurobi.com/,"We used 150 processes (435 questions) for train-ing and 50 processes (150 questions) as the test set. For development, we randomly split the train-ing set 10 times (80%/20%), and tuned hyper-parameters by maximizing average accuracy on question answering. We preprocessed the para-graphs with the Stanford CoreNLP pipeline ver-sion 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package [Cite_Footnote_3] for infer-ence.",Method,Tool,True,Use（引用目的）,True,D14-1159_1_0,2014,Modeling Biological Processes for Reading Comprehension,Footnote
1473,11477," https://github.com/e-bug/nmt-difficulty"," ['References']",Code for replicating our experiments is available online at [Cite] https://github.com/e-bug/nmt-difficulty.,,"The performance of neural machine transla-tion systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more dif-ficult to model. In this paper, we propose cross-mutual information (XMI): an asymmet-ric information-theoretic metric of machine translation difficulty that exploits the proba-bilistic nature of most neural machine trans-lation models. XMI allows us to better eval-uate the difficulty of translating text into the target language while controlling for the dif-ficulty of the target-side generation compo-nent independent of the translation task. We then present the first systematic and con-trolled study of cross-lingual translation dif-ficulties using modern neural translation sys-tems. Code for replicating our experiments is available online at [Cite] https://github.com/e-bug/nmt-difficulty.",Method,Code,True,Produce（引用目的）,True,2020.acl-main.149_0_0,2020,It’s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information,Body
1474,11478," https://w3techs.com/technologies/overview/content_language"," ['5 Results and Analysis']","We train 40 systems, translating each language into and from English. [Cite_Footnote_4]","4 Due to resource limitations, we chose these tasks be-cause most of the information available in the web is in English (https://w3techs.com/technologies/overview/content_language) and effectively trans-lating it into any other language would reduce the digital language divide (http://labs.theguardian.com/digital-language-divide/). Besides, translating into English gives most people access to any local information.","We train 40 systems, translating each language into and from English. [Cite_Footnote_4] The models’ performance in terms of BLEU scores, and the cross-mutual infor-mation (XMI) and cross-entropy values over the test sets are reported in Table 1 with significant values marked in App. B. Translating into English When translating into the same target language (in this case, English), BLEU scores are, in fact, comparable, and can be used as a proxy for difficulty. We can then conclude, for instance, that Lithuanian (lt) is the hardest language to translate from, while Spanish (es) is the easiest. In this scenario, given the good correlation of BLEU scores with human evalua-tions, it is desirable that XMI correlates well with BLEU. This behavior is indeed apparent in the blue points in the left part of Figure 2, confirm-ing the efficacy of XMI in evaluating the difficulty of translation while still being independent of the target language generation component.",補足資料,Document,True,Introduce（引用目的）,True,2020.acl-main.149_1_0,2020,It’s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information,Footnote
1475,11479," http://labs.theguardian.com/digital-language-divide/"," ['5 Results and Analysis']","We train 40 systems, translating each language into and from English. [Cite_Footnote_4]","4 Due to resource limitations, we chose these tasks be-cause most of the information available in the web is in English (https://w3techs.com/technologies/overview/content_language) and effectively trans-lating it into any other language would reduce the digital language divide (http://labs.theguardian.com/digital-language-divide/). Besides, translating into English gives most people access to any local information.","We train 40 systems, translating each language into and from English. [Cite_Footnote_4] The models’ performance in terms of BLEU scores, and the cross-mutual infor-mation (XMI) and cross-entropy values over the test sets are reported in Table 1 with significant values marked in App. B. Translating into English When translating into the same target language (in this case, English), BLEU scores are, in fact, comparable, and can be used as a proxy for difficulty. We can then conclude, for instance, that Lithuanian (lt) is the hardest language to translate from, while Spanish (es) is the easiest. In this scenario, given the good correlation of BLEU scores with human evalua-tions, it is desirable that XMI correlates well with BLEU. This behavior is indeed apparent in the blue points in the left part of Figure 2, confirm-ing the efficacy of XMI in evaluating the difficulty of translation while still being independent of the target language generation component.",補足資料,Document,True,Introduce（引用目的）,True,2020.acl-main.149_2_0,2020,It’s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information,Footnote
1476,11480," http://bioai.lab.asu.edu/logicgridpuzzles"," ['References']",The code and the data will be made pub-licly available at [Cite] http://bioai.lab.,,"Logic grid puzzle is a genre of logic puz-zles in which we are given (in a natural language) a scenario, the object to be de-duced and certain clues. The reader has to figure out the solution using the clues provided and some generic domain con-straints. In this paper, we present a sys-tem, L OGICIA , that takes a logic grid puz-zle and the set of elements in the puz-zle and tries to solve it by translating it to the knowledge representation and rea-soning language of Answer Set Program-ming (ASP) and then using an ASP solver. The translation to ASP involves extrac-tion of entities and their relations from the clues. For that we use a novel learn-ing based approach which uses varied su-pervision, including the entities present in a clue and the expected representa-tion of a clue in ASP. Our system, L O - GICIA , learns to automatically translate a clue with 81.11% accuracy and is able to solve 71% of the problems of a corpus. This is the first learning system that can solve logic grid puzzles described in natu-ral language in a fully automated manner. The code and the data will be made pub-licly available at [Cite] http://bioai.lab.asu.edu/logicgridpuzzles.",Mixed,Mixed,True,Produce（引用目的）,True,D15-1118_0_0,2015,Learning to Automatically Solve Logic Grid Puzzles,Body
1477,11481," https://github.com/google-research-datasets/timedial"," ['References']",The dataset is pub-licly available at: [Cite] https://github.com/google-research-datasets/timedial.,,"Everyday conversations require understanding everyday events, which in turn, requires un-derstanding temporal commonsense concepts interwoven with those events. Despite re-cent progress with massive pre-trained lan-guage models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in di-alogs remains largely under-explored. In this paper, we present the first study to investi-gate pre-trained LMs for their temporal rea-soning capabilities in dialogs by introducing a new task and a crowd-sourced English chal-lenge set, T IME D IAL . We formulate T IME - D IAL as a multiple choice cloze task with over 1.1K carefully curated dialogs. Empirical re-sults demonstrate that even the best perform-ing models struggle on this task compared to humans, with 23 absolute points of gap in ac-curacy. Furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling tem-poral concepts in text and robust contextual reasoning about them. The dataset is pub-licly available at: [Cite] https://github.com/google-research-datasets/timedial.",Material,Dataset,True,Produce（引用目的）,True,2021.acl-long.549_0_0,2021,T IME D IAL : Temporal Commonsense Reasoning in Dialog,Body
1478,11482," https://nlp.stanford.edu/software/sutime.shtml"," ['3 Dataset: T IME D IAL', '3.1 Data Collection']","Temporal expressions are automatically identified with SU-Time (Chang and Manning, 2012), an off-the-shelf temporal expression detector. [Cite_Footnote_1]",1 https://nlp.stanford.edu/software/sutime.shtml,"Temporal expression identification. Here, we select dialogs that are rich with temporal informa-tion, in order to focus on complex temporal rea-soning that arises in natural dialogs. Temporal expressions are automatically identified with SU-Time (Chang and Manning, 2012), an off-the-shelf temporal expression detector. [Cite_Footnote_1] We keep only the dialogs with more than 3 temporal expressions and at least one expression that contains numerals like “two weeks” (as opposed to non-numeric spans, like “summer”, “right now”, and “later”). In our initial experiment, we observe that language models can often correctly predict these non-numerical tempo-ral phrases.",Method,Tool,True,Use（引用目的）,True,2021.acl-long.549_1_0,2021,T IME D IAL : Temporal Commonsense Reasoning in Dialog,Footnote
1479,11483," https://cloud.google.com/tpu"," ['5 Experiments and Analyses']",We use either 4x4 or 8x8 Cloud TPUs V3 pod slices [Cite_Footnote_5] for fine-tuning and one V100 GPU for infer-ence.,5 https://cloud.google.com/tpu,"Using the proposed T IME D IAL challenge set, we next conduct extensive experiments and analyses on the different model variants and context settings. We use either 4x4 or 8x8 Cloud TPUs V3 pod slices [Cite_Footnote_5] for fine-tuning and one V100 GPU for infer-ence. We provide more details of the experiment configurations in the appendix.",Method,Tool,True,Use（引用目的）,True,2021.acl-long.549_2_0,2021,T IME D IAL : Temporal Commonsense Reasoning in Dialog,Footnote
1480,11484," https://github.com/sunlab-osu/covid-faq"," ['Appendix B Aggregation Schemes']",Our COUGH dataset is available at [Cite] https://github.com/sunlab-osu/covid-faq.,,"We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval. Simi-lar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ∼16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we in-troduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains ∼32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ re-trieval models built on top of BM25 and BERT, among which the best model achieves 48.8 under P@5, indicating a great challenge pre-sented by COUGH and encouraging future re-search for further improvement. Our COUGH dataset is available at [Cite] https://github.com/sunlab-osu/covid-faq.",Material,Dataset,True,Produce（引用目的）,True,2021.emnlp-main.305_0_0,2021,"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval Xinliang Frederick Zhang 1,2, , ∗ Heming Sun 1,3,* , Xiang Yue 1 , Simon Lin 4 , and Huan Sun 1",Body
1481,11485," https://www.copyright.gov/title17/92chap1.html#107"," ['8 Ethical Considerations', '8.1 Dataset']",We also consulted Section 107 [Cite_Footnote_9] of U.S. Copyright Act and ensured that our collection action fell under fair use category.,9 https://www.copyright.gov/title17/92chap1.html#107,"IRB approval. All FAQ items were collected in a manner which is consistent with the terms of use of original sources and the intellectual property and privacy rights of the original authors of the texts (i.e., source owners). This project is approved by IRB (institutional review board) at our institution as Exempt Research, which is a human subject study that presents no greater than minimal risk to participants. We consulted data officers at our institution about copyrights. They informed us that “Website content is generally copyrighted. How-ever, you could claim the concept of fair use which allows the use of copyrighted material without per-mission from the copyright holder when it is used for research, scholarship, and teaching”. We also consulted Section 107 [Cite_Footnote_9] of U.S. Copyright Act and ensured that our collection action fell under fair use category. We release our dataset under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.305_1_0,2021,"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval Xinliang Frederick Zhang 1,2, , ∗ Heming Sun 1,3,* , Xiang Yue 1 , Simon Lin 4 , and Huan Sun 1",Footnote
1482,11486," https://creativecommons.org/licenses/by-nc-sa/4.0/"," ['8 Ethical Considerations', '8.1 Dataset']",We release our dataset under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [Cite_Footnote_10] .,10 https://creativecommons.org/licenses/by-nc-sa/4.0/,"IRB approval. All FAQ items were collected in a manner which is consistent with the terms of use of original sources and the intellectual property and privacy rights of the original authors of the texts (i.e., source owners). This project is approved by IRB (institutional review board) at our institution as Exempt Research, which is a human subject study that presents no greater than minimal risk to participants. We consulted data officers at our institution about copyrights. They informed us that “Website content is generally copyrighted. How-ever, you could claim the concept of fair use which allows the use of copyrighted material without per-mission from the copyright holder when it is used for research, scholarship, and teaching”. We also consulted Section 107 of U.S. Copyright Act and ensured that our collection action fell under fair use category. We release our dataset under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [Cite_Footnote_10] .",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.305_2_0,2021,"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval Xinliang Frederick Zhang 1,2, , ∗ Heming Sun 1,3,* , Xiang Yue 1 , Simon Lin 4 , and Huan Sun 1",Footnote
1483,11487," https://www.nltk.org/"," ['Appendix C Implementation Details']",We first preprocess user query and FAQ items with nltk porter stemmer 5 [Cite_Footnote_14] .,14 https://www.nltk.org/,"We first preprocess user query and FAQ items with nltk porter stemmer 5 [Cite_Footnote_14] . For baselines including BM25 and Sentence-BERT , we take the stan-dard off-the-shelf version. More specifically, we keep the default k1 as 2 and b as 0.75 for BM25 over Q-q, Q-a and Q-q+a settings. When deploy-ing synthetic query generation model (i.e., GPT2), hyper-parameters are set as instructed by Mass et al. (2020) (see their Section 3.4). We adopt the in-batch negatives training strategy to fine-tune both Sentence-BERT and cross-encoder BERT. For both BERT models, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5 and fine-tune up to 10 epochs. We set the batch sizes as 24 and 4 for Sentence-BERT and cross-encoder BERT, respectively. All experiments are conducted using one single GeForce GTX 2080 Ti 12 GB GPU (with significant CPU resources).",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.305_3_0,2021,"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval Xinliang Frederick Zhang 1,2, , ∗ Heming Sun 1,3,* , Xiang Yue 1 , Simon Lin 4 , and Huan Sun 1",Footnote
1484,11488," https://pypi.org/project/rank-bm25/"," ['Appendix C Implementation Details']","For baselines including BM25 [Cite_Footnote_15] and Sentence-BERT , we take the stan-dard off-the-shelf version.",15 https://pypi.org/project/rank-bm25/,"We first preprocess user query and FAQ items with nltk porter stemmer 5 . For baselines including BM25 [Cite_Footnote_15] and Sentence-BERT , we take the stan-dard off-the-shelf version. More specifically, we keep the default k1 as 2 and b as 0.75 for BM25 over Q-q, Q-a and Q-q+a settings. When deploy-ing synthetic query generation model (i.e., GPT2), hyper-parameters are set as instructed by Mass et al. (2020) (see their Section 3.4). We adopt the in-batch negatives training strategy to fine-tune both Sentence-BERT and cross-encoder BERT. For both BERT models, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5 and fine-tune up to 10 epochs. We set the batch sizes as 24 and 4 for Sentence-BERT and cross-encoder BERT, respectively. All experiments are conducted using one single GeForce GTX 2080 Ti 12 GB GPU (with significant CPU resources).",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.305_4_0,2021,"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval Xinliang Frederick Zhang 1,2, , ∗ Heming Sun 1,3,* , Xiang Yue 1 , Simon Lin 4 , and Huan Sun 1",Footnote
1485,11489," https://github.com/UKPLab/sentence-transformers"," ['Appendix C Implementation Details']","For baselines including BM25 and Sentence-BERT [Cite_Footnote_16] , we take the stan-dard off-the-shelf version.",16 https://github.com/UKPLab/sentence-transformers and we use distilbert-base-nli-stsb-quora-ranking model card.,"We first preprocess user query and FAQ items with nltk porter stemmer 5 . For baselines including BM25 and Sentence-BERT [Cite_Footnote_16] , we take the stan-dard off-the-shelf version. More specifically, we keep the default k1 as 2 and b as 0.75 for BM25 over Q-q, Q-a and Q-q+a settings. When deploy-ing synthetic query generation model (i.e., GPT2), hyper-parameters are set as instructed by Mass et al. (2020) (see their Section 3.4). We adopt the in-batch negatives training strategy to fine-tune both Sentence-BERT and cross-encoder BERT. For both BERT models, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5 and fine-tune up to 10 epochs. We set the batch sizes as 24 and 4 for Sentence-BERT and cross-encoder BERT, respectively. All experiments are conducted using one single GeForce GTX 2080 Ti 12 GB GPU (with significant CPU resources).",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.305_5_0,2021,"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval Xinliang Frederick Zhang 1,2, , ∗ Heming Sun 1,3,* , Xiang Yue 1 , Simon Lin 4 , and Huan Sun 1",Footnote
1486,11490," https://twitter.com"," ['4 Empirical Evaluation', '4.1 Datasets']",• Tweets - Twitter [Cite_Footnote_2] is a microblogging plat-form which allows users to post statuses of less than 140 characters.,2 https://twitter.com,"• Tweets - Twitter [Cite_Footnote_2] is a microblogging plat-form which allows users to post statuses of less than 140 characters. We use two collections for sarcasm detection on tweets. More specifically, we use the dataset ob-tained from (1) (Ptáček et al., 2014) in which tweets are trained via hashtag based semi-supervised learning, i.e., hashtags such as #not, #sarcasm and #irony are marked as sar-castic tweets and (2) (Riloff et al., 2013) in which Tweets are hand annotated and manu-ally checked for sarcasm. For both datasets, we retrieve. Tweets using the Twitter API us-ing the provided tweet IDs.",Material,DataSource,True,Use（引用目的）,True,P18-1093_0_0,2018,Reasoning with Sarcasm by Reading In-between,Footnote
1487,11491," https://reddit.com"," ['4 Empirical Evaluation', '4.1 Datasets']",[Cite_Footnote_3] is a highly popular social forum and community.,3 https://reddit.com,"• Reddit - Reddit [Cite_Footnote_3] is a highly popular social forum and community. Similar to Tweets, sarcastic posts are obtained via the tag ‘/s’ which are marked by the authors themselves. We use two Reddit datasets which are ob-tained from the subreddits /r/movies and /r/technology respectively. Datasets are sub-sets from (Khodak et al., 2017).",補足資料,Website,True,Use（引用目的）,True,P18-1093_1_0,2018,Reasoning with Sarcasm by Reading In-between,Footnote
1488,11492," https://nlds.soe.ucsc.edu/sarcasm1"," ['4 Empirical Evaluation', '4.1 Datasets']","• Debates - We use two datasets [Cite_Footnote_4] from the In-ternet Argument Corpus (IAC) (Lukin and Walker, 2017) which have been hand anno-tated for sarcasm.",4 https://nlds.soe.ucsc.edu/sarcasm1,"• Debates - We use two datasets [Cite_Footnote_4] from the In-ternet Argument Corpus (IAC) (Lukin and Walker, 2017) which have been hand anno-tated for sarcasm. This dataset, unlike the first two, is mainly concerned with long text and provides a diverse comparison from the other datasets. The IAC corpus was designed for research on political debates on online fo-rums. We use the V1 and V2 versions of the sarcasm corpus which are denoted as IAC-V1 and IAC-V2 respectively.",Material,Dataset,True,Use（引用目的）,True,P18-1093_2_0,2018,Reasoning with Sarcasm by Reading In-between,Footnote
1489,11493," https://github.com/chrisjbryant/errant"," ['3 Erroneous Span Detection']","In order to train the tagging model, we align [Cite_Footnote_1] tokens across the source and target sentence in training data.",1 Alignment can be solved by dynamic programming like Levenshtein distance. We here use ERRANT (https://github.com/chrisjbryant/errant) for alignment.,"To identify incorrect spans, we use a binary se-quence tagging model in which tag 0 means the token is in a correct span; while tag 1 means the to-ken is in a grammatically incorrect span that needs to be edited, as shown in Figure 1(a). In order to train the tagging model, we align [Cite_Footnote_1] tokens across the source and target sentence in training data. With token alignment, we can identify the text spans that are edited and thus can annotate the edited text spans in the original sentences as erroneous spans.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.581_0_0,2020,Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,Footnote
1490,11494," https://github.com/nusnlp/m2scorer"," ['5 Experiments', '5.1 Experimental Setting']","We use CoNLL-2013 test set as the dev set to choose the best-performing models, and evaluate on the well-known GEC benchmark datasets: CoNLL-2014 (Ng et al., 2014) and BEA-2019 test set with the official evaluation scripts (m2scorer [Cite_Footnote_2] for CoNLL-14, ERRANT for BEA-19).",2 https://github.com/nusnlp/m2scorer,"Following recent work in English GEC, we conduct experiments in the same setting with the restricted track of the BEA-2019 GEC shared task (Bryant et al., 2019), using FCE (Yannakoudakis et al., 2011), Lang-8 Corpus of Learner English (Mizu-moto et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Granger, 1998; Bryant et al., 2019) as training data. We use CoNLL-2013 test set as the dev set to choose the best-performing models, and evaluate on the well-known GEC benchmark datasets: CoNLL-2014 (Ng et al., 2014) and BEA-2019 test set with the official evaluation scripts (m2scorer [Cite_Footnote_2] for CoNLL-14, ERRANT for BEA-19). As previous work (Grundkiewicz et al., 2019) trained with synthetic data, we synthesize",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.581_1_0,2020,Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,Footnote
1491,11495," https://www.cl.cam.ac.uk/research/nl/bea2019st/"," ['References']","Except the sythetic data, all the data can be found at the website [Cite_Footnote_4] of the BEA-19 shared task.",4 https://www.cl.cam.ac.uk/research/nl/bea2019st/,"We thank Xin Sun and Xiangxin Zhou for their help with experiments. We thank the anonymous reviewers for their valuable comments. The corre-sponding author of this paper is Tao Ge. A Experiment Details Table 5 describes the details of datasets used for English GEC. Except the sythetic data, all the data can be found at the website [Cite_Footnote_4] of the BEA-19 shared task. The synthetic data is generated from English Wikipedia , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task. For sentence corruption, we follow Edunov et al. (2018) to randomly insert, delete, replace and swap adjacent tokens in a sentence.",補足資料,Website,True,Use（引用目的）,True,2020.emnlp-main.581_2_0,2020,Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,Footnote
1492,11496," https://en.wikipedia.org/"," ['References']","The synthetic data is generated from English Wikipedia [Cite_Footnote_5] , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption.",5 https://en.wikipedia.org/,"We thank Xin Sun and Xiangxin Zhou for their help with experiments. We thank the anonymous reviewers for their valuable comments. The corre-sponding author of this paper is Tao Ge. A Experiment Details Table 5 describes the details of datasets used for English GEC. Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia [Cite_Footnote_5] , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task. For sentence corruption, we follow Edunov et al. (2018) to randomly insert, delete, replace and swap adjacent tokens in a sentence.",Material,DataSource,True,Extend（引用目的）,True,2020.emnlp-main.581_3_0,2020,Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,Footnote
1493,11497," http://data.statmt.org/news-crawl/en/"," ['References']","The synthetic data is generated from English Wikipedia , English Gigaword (Parker et al., 2011) and Newscrawl [Cite_Footnote_6] as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption.",6 http://data.statmt.org/news-crawl/en/,"We thank Xin Sun and Xiangxin Zhou for their help with experiments. We thank the anonymous reviewers for their valuable comments. The corre-sponding author of this paper is Tao Ge. A Experiment Details Table 5 describes the details of datasets used for English GEC. Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia , English Gigaword (Parker et al., 2011) and Newscrawl [Cite_Footnote_6] as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task. For sentence corruption, we follow Edunov et al. (2018) to randomly insert, delete, replace and swap adjacent tokens in a sentence.",Material,DataSource,True,Extend（引用目的）,True,2020.emnlp-main.581_4_0,2020,Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,Footnote
1494,11498," https://github.com/nju-websoft/GLRE"," ['4 Experiments and Results']",The source code and datasets are available online. [Cite_Footnote_1],1 https://github.com/nju-websoft/GLRE,"We implemented our GLRE with PyTorch 1.5. The source code and datasets are available online. [Cite_Footnote_1] In this section, we report our experimental results.",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.303_0_0,2020,Global-to-Local Neural Networks for Document-Level Relation Extraction,Footnote
1495,11499," http://mpqa.cs.pitt.edu/corpora/"," ['3 Task Definition', '3.1 Gold Standard Corpus: MPQA 3.0']","MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015). [Cite_Footnote_4]",4 Available at http://mpqa.cs.pitt.edu/corpora/,"MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015). [Cite_Footnote_4] It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agen-cies, and covers a wide range of topics.",Material,Dataset,True,Introduce（引用目的）,False,D15-1018_0_0,2015,Joint Prediction for Entity/Event-Level Sentiment Analysis using Probabilistic Soft Logic Models,Footnote
1496,11500," http://mpqa.cs.pitt.edu/lexicons/effectlexicon/"," ['4 PSL for Sentiment Analysis', '4.4 PSL Augmented with +/-Effect Events (PSL3)']","+E FFECT (x) and -E FFECT (x): We use the +/-effect sense-level lexicon (Choi and Wiebe, 2014) [Cite_Footnote_7] to extract the +/-effect events in each sen-tence.",7 Available at: http://mpqa.cs.pitt.edu/lexicons/effectlexicon/,"+E FFECT (x) and -E FFECT (x): We use the +/-effect sense-level lexicon (Choi and Wiebe, 2014) [Cite_Footnote_7] to extract the +/-effect events in each sen-tence. The score of +E FFECT (x) is the fraction of that word’s senses that are +effect senses accord-ing to the lexicon, and the score of -E FFECT (x) is the fraction of that word’s senses that are -effect senses according to the lexicon. If a word does not appear in the lexicon, we do not treat it as a +/-effect event, and thus assign 0 to both +E FFECT (x) and -E FFECT (x).",Material,Knowledge,True,Use（引用目的）,True,D15-1018_1_0,2015,Joint Prediction for Entity/Event-Level Sentiment Analysis using Probabilistic Soft Logic Models,Footnote
1497,11501," https://github.com/Zikangli/SOM-NCSCM"," ['References']","In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [Cite_Footnote_1]",1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.,"Sentence Compression (SC), which aims to shorten sentences while retaining important words that express the essential meanings, has been studied for many years in many lan-guages, especially in English. However, im-provements on Chinese SC task are still quite few due to several difficulties: scarce of par-allel corpora, different segmentation granular-ity of Chinese sentences, and imperfect per-formance of syntactic analyses. Furthermore, entire neural Chinese SC models have been under-investigated so far. In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [Cite_Footnote_1] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.33_0_0,2021,SOM-NCSCM: An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map,Footnote
1498,11502," https://github.com/Embedding/Chinese-Word-Vectors"," ['5 Experiments', '5.1 Dataset and Experiment Settings']","In the experiment, we use the pre-trained Chinese word vectors with 300 dimensions to initialize the Chinese word embed-dings (Li et al., 2018). [Cite_Footnote_6]",6 We use the merge_sgns_bigram_char300.txt from https://github.com/Embedding/Chinese-Word-Vectors.,"Implementation Details. In the experiment, we use the pre-trained Chinese word vectors with 300 dimensions to initialize the Chinese word embed-dings (Li et al., 2018). [Cite_Footnote_6] We use the Stanford CoreNLP to extract POS and NE features. The MiniSom is employed for constructing the neural clustering model, and we choose an 11 × 11 square map with a sigma of 4, an initial learning rate of 0.5, the Euclidean distance function to activate the map and the Gaussian function to weigh the neigh-borhood of nodes in the map. The representations of POS, NE and cluster index features are all ran-domly initialized as 32-dimensional vectors in the training stage. The depth of the LSTM layer is set to 2, while the hidden size of Bi-LSTM in both the baseline model and the extra attention-based model is 128, and the size of the dense layer is set to 64. Besides, to avoid overfitting, we use dropout before the Bi-LSTM layer and the dense layer with a dropout rate of 0.5. We set the batch size to 64 and use the optimization algorithm Adam (Kingma and Ba, 2015) with default parameters as an initial learning rate of 0.001. Our models are all trained on a single GPU and the results are reported on the test set.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.33_1_0,2021,SOM-NCSCM: An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map,Footnote
1499,11503," https://github.com/stanfordnlp/CoreNLP"," ['5 Experiments', '5.1 Dataset and Experiment Settings']",We use the Stanford CoreNLP to extract POS and NE features. [Cite_Footnote_7],7 https://github.com/stanfordnlp/CoreNLP,"Implementation Details. In the experiment, we use the pre-trained Chinese word vectors with 300 dimensions to initialize the Chinese word embed-dings (Li et al., 2018). We use the Stanford CoreNLP to extract POS and NE features. [Cite_Footnote_7] The MiniSom is employed for constructing the neural clustering model, and we choose an 11 × 11 square map with a sigma of 4, an initial learning rate of 0.5, the Euclidean distance function to activate the map and the Gaussian function to weigh the neigh-borhood of nodes in the map. The representations of POS, NE and cluster index features are all ran-domly initialized as 32-dimensional vectors in the training stage. The depth of the LSTM layer is set to 2, while the hidden size of Bi-LSTM in both the baseline model and the extra attention-based model is 128, and the size of the dense layer is set to 64. Besides, to avoid overfitting, we use dropout before the Bi-LSTM layer and the dense layer with a dropout rate of 0.5. We set the batch size to 64 and use the optimization algorithm Adam (Kingma and Ba, 2015) with default parameters as an initial learning rate of 0.001. Our models are all trained on a single GPU and the results are reported on the test set.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.33_2_0,2021,SOM-NCSCM: An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map,Footnote
1500,11504," https://github.com/JustGlowing/minisom"," ['5 Experiments', '5.1 Dataset and Experiment Settings']","The MiniSom [Cite_Footnote_8] is employed for constructing the neural clustering model, and we choose an 11 × 11 square map with a sigma of 4, an initial learning rate of 0.5, the Euclidean distance function to activate the map and the Gaussian function to weigh the neigh-borhood of nodes in the map.",8 https://github.com/JustGlowing/minisom,"Implementation Details. In the experiment, we use the pre-trained Chinese word vectors with 300 dimensions to initialize the Chinese word embed-dings (Li et al., 2018). We use the Stanford CoreNLP to extract POS and NE features. The MiniSom [Cite_Footnote_8] is employed for constructing the neural clustering model, and we choose an 11 × 11 square map with a sigma of 4, an initial learning rate of 0.5, the Euclidean distance function to activate the map and the Gaussian function to weigh the neigh-borhood of nodes in the map. The representations of POS, NE and cluster index features are all ran-domly initialized as 32-dimensional vectors in the training stage. The depth of the LSTM layer is set to 2, while the hidden size of Bi-LSTM in both the baseline model and the extra attention-based model is 128, and the size of the dense layer is set to 64. Besides, to avoid overfitting, we use dropout before the Bi-LSTM layer and the dense layer with a dropout rate of 0.5. We set the batch size to 64 and use the optimization algorithm Adam (Kingma and Ba, 2015) with default parameters as an initial learning rate of 0.001. Our models are all trained on a single GPU and the results are reported on the test set.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.33_3_0,2021,SOM-NCSCM: An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map,Footnote
1501,11505," http://www.wikipedia.com"," ['3 Methodology', '3.3 Pre-trained Knowledge Encoder']","We adopt this direc-tion in our model and build an encoder, pre-trained with Roberta (Liu et al., 2019c), which has been pre-trained on the huge language corpora (e.g., BooksCorpus (Zhu et al., 2015), Wikipedia (Remy, 2002) [Cite_Ref] ) to capture implicit knowledge.",M. Remy. 2002. Wikipedia: The free encyclope-dia200214wikipedia: The free encyclopedia. 2001 – updated daily. gratis http://www.wikipedia.com.,"We want to incorporate implicit external knowl-edge as well as math-aware knowledge which can be learned from the training set in our model. Lan-guage models, and especially transformer-based language models, have shown to contain com-monsense and factual knowledge (Petroni et al., 2019; Jiang et al., 2019). We adopt this direc-tion in our model and build an encoder, pre-trained with Roberta (Liu et al., 2019c), which has been pre-trained on the huge language corpora (e.g., BooksCorpus (Zhu et al., 2015), Wikipedia (Remy, 2002) [Cite_Ref] ) to capture implicit knowledge. We tokenize a description Q using WordPiece (Wu et al., 2016) as in BERT (Devlin et al., 2019a), giving us a se-quence of |Q| tokens and embed them with the pre-trained Roberta embeddings and append Roberta’s positional encoding, giving us a sequence of d-dimensional token representation x Q1 , ..., x Q|Q| . We feed these into the transformer-based pre-trained knowledge encoder, fine-tuning the representation during training. We mean-pool the output of all transformer steps to get our combined implicit knowledge representation Y p .",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.272_0_0,2021,Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning,Reference
1502,11506," https://github.com/viking-sudo-rm/rr-experiments"," ['5 Experiments']",We now test empirically whether these predictions carry over to the learnable capac-ity of unsaturated RNNs. [Cite_Footnote_11],11 https://github.com/viking-sudo-rm/ rr-experiments,"In Subsection 4.3, we showed that different satu-rated RNNs vary in their ability to recognize a n b n and a n b n Σ ∗ . We now test empirically whether these predictions carry over to the learnable capac-ity of unsaturated RNNs. [Cite_Footnote_11] We compare the QRNN and LSTM when coupled with a linear decoder D 1 . We also train a 2-layer QRNN (“QRNN2”) and a 1-layer QRNN with a D 2 decoder (“QRNN+”).",Method,Tool,False,Compare（引用目的）,True,2020.acl-main.43_0_0,2020,A Formal Hierarchy of RNN Architectures,Footnote
1503,11507," https://github.com/viking-sudo-rm/"," ['0 < [h t ] 4 ∨ [h t ] 1 + [h t ] 2 + [h t ] 3 ≤ 0 . (76) E Experimental Details']",The code is avail-able at [Cite] https://github.com/viking-sudo-rm/ rr-experiments .,,"Models were trained on strings up to length 64, and, at each index t, were asked to classify whether or not the prefix up to t was a valid string in the language. Models were then tested on indepen-dent datasets of lengths 64, 128, 256, 512, 1024, and 2048. The training dataset contained 100000 strings, and the validation and test datasets con-tained 10000. We discuss task-specific schemes for sampling strings in the next paragraph. All models were trained for a maximum of 100 epochs, with early stopping after 10 epochs based on the valida-tion cross entropy loss. We used default hyperpa-rameters provided by the open-source AllenNLP framework (Gardner et al., 2018). The code is avail-able at [Cite] https://github.com/viking-sudo-rm/ rr-experiments .",Method,Code,True,Produce（引用目的）,True,2020.acl-main.43_1_0,2020,A Formal Hierarchy of RNN Architectures,Body
1504,11508," https://github.com/WadeYin9712/SentiBERT"," ['1 Introduction']",The source code is available at [Cite] https://github.com/WadeYin9712/SentiBERT.,,"Results on phrase-level sentiment classification on Stanford Sentiment Treebank (SST) (Socher et al., 2013) indicate that SentiBERT improves significantly over recursive networks and the base-composition module based on an attention mechanism; Module III is a predictor for phrase-level sentiment. The semantic composition module is a two layer attention-based network (see Section 3.1) The first layer (Attention to Tokens) generates representation for each phrase based on the token it covers and the second layer (Attention to Children) refines the phrase representation obtained from the first layer based on its children. line BERT model. As phrase-level sentiment labels are expensive to obtain, we further explore if the compositional sentiment semantics learned from one task can be transferred to others. In particular, we find that SentiBERT trained on SST can be transferred well to other related tasks such as twit-ter sentiment analysis (Rosenthal et al., 2017) and emotion intensity classification (Mohammad et al., 2018) and contextual emotion detection (Chatter-jee et al., 2019). Furthermore, we conduct com-prehensive quantitative and qualitative analyses to evaluate the effectiveness of SentiBERT under various situations and to demonstrate the seman-tic compositionality captured by the model. The source code is available at [Cite] https://github.com/WadeYin9712/SentiBERT.",Method,Code,True,Produce（引用目的）,True,2020.acl-main.341_0_0,2020,SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics,Body
1505,11509," https://github.com/huggingface"," ['4 Experiments', '4.1 Experimental Settings']","We build SentiBERT on the HuggingFace li-brary [Cite_Footnote_1] and initialize the model parameters using pre-trained BERT-base and RoBERTa-base models whose maximum length is 128, layer number is 12, and embedding dimension is 768.",1 https://github.com/huggingface,"We build SentiBERT on the HuggingFace li-brary [Cite_Footnote_1] and initialize the model parameters using pre-trained BERT-base and RoBERTa-base models whose maximum length is 128, layer number is 12, and embedding dimension is 768. For the train-ing on SST-phrase, the learning rate is 2 × 10 −5 , batch size is 32 and the number of training epochs is 3. For masking mechanism, to put emphasis on modeling sentiments, the probability of masking opinion words which can be retrieved from Senti-WordNet (Baccianella et al., 2010) is set to 20%, and for the other words, the probability is 15%. For fine-tuning on downstream tasks, the learning rate is {1×10 −5 −1×10 −4 }, batch size is {16, 32} and the number of training epochs is 1−5. We use Stan-ford CoreNLP API (Manning et al., 2014) to obtain binary constituency trees for the sentences of these tasks to keep consistent with the settings on SST-phrase. Note that when fine-tuning on sentence-level sentiment and emotion classification tasks, the objective is to correctly label the root of tree, instead of targeting at the [CLS] token representa-tion as in the original BERT.",Method,Code,True,Use（引用目的）,True,2020.acl-main.341_1_0,2020,SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics,Footnote
1506,11510," http://www.mdbg.net/chindict/chindict.php?page=cedict"," ['4 Emotion Detection via Bilingual and Sentiment Information', '4.1 Bilingual Information']","For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon [Cite_Footnote_1] , and is the language model, and p θ LM (c) is the bigram language model which is trained from a large s-cale Weibo data set .",1 MDBG CC-CEDICT is adopted as the bilingual lexicon: http://www.mdbg.net/chindict/chindict.php?page=cedict,"For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon [Cite_Footnote_1] , and is the language model, and p θ LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary and PMI based word correlation are used to enhance the language mod-el for machine translation. p θ SYN (c) denotes the synonym similarity between translated words and the contexts. This is necessary since the sense of translated words and the contexts are expected to be similar; and p θ PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine (Tur-ney, 2002). This is to ensure that the translated words are highly associated with the contexts.",Material,Knowledge,True,Use（引用目的）,True,P15-2125_0_0,2015,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,Footnote
1507,11511," http://www.ltp-cloud.com/"," ['4 Emotion Detection via Bilingual and Sentiment Information', '4.1 Bilingual Information']","As text in micro-blogs is in-formal, synonym dictionary [Cite_Footnote_3] and PMI based word correlation are used to enhance the language mod-el for machine translation.",3 TongYiCiLin is adopted as the Chinese synonym dictio-nary: http://www.ltp-cloud.com/,"For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon , and is the language model, and p θ LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary [Cite_Footnote_3] and PMI based word correlation are used to enhance the language mod-el for machine translation. p θ SYN (c) denotes the synonym similarity between translated words and the contexts. This is necessary since the sense of translated words and the contexts are expected to be similar; and p θ PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine (Tur-ney, 2002). This is to ensure that the translated words are highly associated with the contexts.",Material,Knowledge,True,Use（引用目的）,True,P15-2125_1_0,2015,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,Footnote
1508,11512," http://www.bing.com/"," ['4 Emotion Detection via Bilingual and Sentiment Information', '4.1 Bilingual Information']","This is necessary since the sense of translated words and the contexts are expected to be similar; and p θ PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine [Cite_Footnote_4] (Tur-ney, 2002).",4 We use BING.com as the search engine for PMI: http://www.bing.com/,"For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon , and is the language model, and p θ LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary and PMI based word correlation are used to enhance the language mod-el for machine translation. p θ SYN (c) denotes the synonym similarity between translated words and the contexts. This is necessary since the sense of translated words and the contexts are expected to be similar; and p θ PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine [Cite_Footnote_4] (Tur-ney, 2002). This is to ensure that the translated words are highly associated with the contexts.",Method,Tool,True,Use（引用目的）,True,P15-2125_2_0,2015,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,Footnote
1509,11513," http://ir.dlut.edu.cn/EmotionOntologyDownload.aspx"," ['4 Emotion Detection via Bilingual and Sentiment Information', '4.2 Sentimental Information']","In this paper, both Chinese [Cite_Footnote_5] and English sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002).",5 DUTIR Sentiment Lexicon is adopt-ed as the Chinese sentiment lexicon: http://ir.dlut.edu.cn/EmotionOntologyDownload.aspx,"In this paper, both Chinese [Cite_Footnote_5] and English sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002).",Material,Knowledge,True,Use（引用目的）,True,P15-2125_3_0,2015,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,Footnote
1510,11514," http://mpqa.cs.pitt.edu/lexicons/subjlexicon/"," ['4 Emotion Detection via Bilingual and Sentiment Information', '4.2 Sentimental Information']","In this paper, both Chinese and English [Cite_Footnote_6] sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002).",6 English sentiment lexicon is uti-lized from MPQA Subjectivity Lexicon: http://mpqa.cs.pitt.edu/lexicons/subjlexicon/,"In this paper, both Chinese and English [Cite_Footnote_6] sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002).",Material,Knowledge,True,Use（引用目的）,True,P15-2125_4_0,2015,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,Footnote
1511,11515," https://github.com/xpqiu/fnlp/"," ['5 Experiments', '5.1 Experimental Settings']",FNLP [Cite_Footnote_7] is used for Chinese word segmentation.,"7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/","As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [Cite_Footnote_7] is used for Chinese word segmentation.",Method,Tool,True,Use（引用目的）,True,P15-2125_5_0,2015,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,Footnote
1512,11516," http://mallet.cs.umass.edu"," ['5 Experiments', '5.2 Experimental Results']","Figure 3 shows the experimental results of different models, where ME is the basic Maximum Entropy (ME) classification model [Cite_Footnote_8] in which all Chinese and English words of each post function as a feature, ME-CN and ME-EN in which only the Chinese or English text of each post function as features, and BLP-BS, our proposed LP-based ap-proach which incorporates both bilingual and sen-timental information.","8 ME algorithm is implemented with the MALLET Toolkit, http://mallet.cs.umass.edu","Our first group of experiments is to investigate whether our proposed label propagation model with both bilingual and sentimental information can improve emotion detection in code-switching texts. Figure 3 shows the experimental results of different models, where ME is the basic Maximum Entropy (ME) classification model [Cite_Footnote_8] in which all Chinese and English words of each post function as a feature, ME-CN and ME-EN in which only the Chinese or English text of each post function as features, and BLP-BS, our proposed LP-based ap-proach which incorporates both bilingual and sen-timental information. We adopt F1-Measure (F1.) to measure the performance of each model in the respective emotions.",Method,Tool,True,Use（引用目的）,True,P15-2125_6_0,2015,Emotion Detection in Code-switching Texts via Bilingual and Sentimental Information,Footnote
1513,11517," https://competitions.codalab.org/competitions/12731"," ['6 Summary']","To facilitate GEC evaluation, we have set up an online platform [Cite_Footnote_8] for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available.",8 https://competitions.codalab.org/competitions/12731,"To facilitate GEC evaluation, we have set up an online platform [Cite_Footnote_8] for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available.",補足資料,Website,True,Produce（引用目的）,True,D16-1228_0_0,2016,There’s No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction,Footnote
1514,11518," https://github.com/cnap/grammaticality-metrics"," ['6 Summary']","To facilitate GEC evaluation, we have set up an online platform for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available. [Cite_Footnote_9]",9 https://github.com/cnap/ grammaticality-metrics,"To facilitate GEC evaluation, we have set up an online platform for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available. [Cite_Footnote_9]",Method,Code,True,Produce（引用目的）,True,D16-1228_1_0,2016,There’s No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction,Footnote
1515,11519," https://github.com/iceboal/word-representations-bptf"," ['3 Experiments', '3.1 Evaluation settings', '3.1.2 Resource for training']",These pairs were provided by Zhang et al. (2014) [Cite_Footnote_1] .,1 https://github.com/iceboal/ word-representations-bptf,"For supervised dataset, we used synonym and antonym pairs in two thesauri: WordNet (Miller, 1995) and Roget (Kipfer, 2009). These pairs were provided by Zhang et al. (2014) [Cite_Footnote_1] . There were 52,760 entries (words), each of which had 11.7 synonyms on average, and 21,319 entries, each of which had 6.5 antonyms on average.",Material,Knowledge,True,Use（引用目的）,True,N15-1100_0_0,2015,Word Embedding-based Antonym Detection using Thesauri and Distributional Information,Footnote
1516,11520," http://www.iptc.org"," ['5 Dynamic Hierarchies']","The pre-defined categories would form the static hierarchy – the IPTC Subject Reference System [Cite_Footnote_2] , for example – on top of all event-based information organization, and the mod-els for the categories could be built on the basis of the test set.","2 International Press Telecommunications Council, http://www.iptc.org","We suggest the we adopt text categorization on top of topic detection and tracking, similar to Figure 3. There has been good results in text categorization (see, e.g., (Yang and Liu, 1999; Sebastiani, 2002)) The pre-defined categories would form the static hierarchy – the IPTC Subject Reference System [Cite_Footnote_2] , for example – on top of all event-based information organization, and the mod-els for the categories could be built on the basis of the test set.",Material,Knowledge,True,Introduce（引用目的）,True,N03-3008_0_0,2003,Investigations on Event Evolution Proceedings in TDT of HLT-NAACL 2003,Footnote
1517,11521," https://keras.io/"," ['3 Models', '3.2 Neural model architecture']","Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras [Cite_Footnote_1] .",1 https://keras.io/,"Additionally, we also use word embeddings from pre-trained word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models, represent-ing each utterance summing all of the vectors for the component words. Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras [Cite_Footnote_1] . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 2 . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014). With the word2vec and GloVe embeddings, we use a convolutional neural network (CNN) model with global max pooling, trained for 20 epochs with a batch size of 128.",Material,DataSource,True,Introduce（引用目的）,True,2021.acl-srw.29_0_0,2021,Predicting pragmatic discourse features in the language of adults with autism spectrum disorder,Footnote
1518,11522," https://code.google.com/archive/p/word2vec/"," ['3 Models', '3.2 Neural model architecture']","For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [Cite_Footnote_2] .",2 https://code.google.com/archive/p/word2vec/,"Additionally, we also use word embeddings from pre-trained word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models, represent-ing each utterance summing all of the vectors for the component words. Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [Cite_Footnote_2] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014). With the word2vec and GloVe embeddings, we use a convolutional neural network (CNN) model with global max pooling, trained for 20 epochs with a batch size of 128.",Method,Tool,False,Use（引用目的）,True,2021.acl-srw.29_1_0,2021,Predicting pragmatic discourse features in the language of adults with autism spectrum disorder,Footnote
1519,11523," https://catalog.ldc.upenn.edu/LDC2002T07"," ['1 Introduction']","In particu-lar, the availability of RST annotations on a se-lection of 385 Wall Street Journal articles from the Penn Treebank [Cite_Footnote_1] (Carlson et al., 2001) has facilitated RST-based discourse analysis of writ-ten texts, since it provides a standard benchmark for comparing the performance of different tech-niques for document-level discourse parsing (Joty et al., 2013; Feng and Hirst, 2014).",1 https://catalog.ldc.upenn.edu/ LDC2002T07,"RST is a descriptive framework that has been widely used in the analysis of discourse organiza-tion of written texts (Taboada and Mann, 2006b), and has been applied to various natural lan-guage processing tasks, including language gen-eration, text summarization, and machine trans-lation (Taboada and Mann, 2006a). In particu-lar, the availability of RST annotations on a se-lection of 385 Wall Street Journal articles from the Penn Treebank [Cite_Footnote_1] (Carlson et al., 2001) has facilitated RST-based discourse analysis of writ-ten texts, since it provides a standard benchmark for comparing the performance of different tech-niques for document-level discourse parsing (Joty et al., 2013; Feng and Hirst, 2014).",Material,DataSource,True,Use（引用目的）,True,P17-2041_0_0,2017,Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework,Footnote
1520,11524," http://www.isi.edu/licensed-sw/RSTTool/index.html"," ['3 Annotation', '3.1 Guidelines']","In addition, the re-sponse also includes an awkward Comment-Topic relation between EDU 2 and the node combin-ing EDUs 3-11, indicated by awkward-Comment-Topic-2; in this multinuclear relation, the annota-tor judged that the second branch of the relation was awkward, which is indicated by the [Cite_Footnote_2] that was appended to the relation label.",2 Downloaded from http://www.isi.edu/licensed-sw/RSTTool/index.html,"The discourse annotation tool used in the RST Discourse Treebank 2 was also adopted for this study. Using this tool, annotators incrementally build hierarchical discourse trees in which the leaves are the EDUs and the internal nodes cor-respond to contiguous spans of text. When the an-notators assign the rhetorical relation for a node of the tree, they provide the relation’s label (drawn from the pre-defined set of relations in the anno-tation guidelines) and also indicate whether the spans that comprise the relation are nuclei or satel-lites. Figure 1 shows an example of an annotated RST tree for a response with a proficiency score of 1. This response includes three disfluencies (EDUs 3, 6, and 9), which are satellites of the corresponding repair nuclei. In addition, the re-sponse also includes an awkward Comment-Topic relation between EDU 2 and the node combin-ing EDUs 3-11, indicated by awkward-Comment-Topic-2; in this multinuclear relation, the annota-tor judged that the second branch of the relation was awkward, which is indicated by the [Cite_Footnote_2] that was appended to the relation label.",Material,Knowledge,False,Use（引用目的）,False,P17-2041_1_0,2017,Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework,Footnote
1521,11525," https://toeflpractice.ets.org/"," ['3 Annotation', '3.2 Pilot Annotation']","First, a pi-lot annotation was conducted to train and cali-brate the annotators based on 48 training samples drawn from the TOEFL R Practice Online (TPO) product [Cite_Footnote_3] , which offers practice tests simulating the TOEFL iBT testing experience with authentic test questions.",3 https://toeflpractice.ets.org/,"The manual annotations were provided by two ex-perts with prior experience in various types of data annotation on both text and speech. First, a pi-lot annotation was conducted to train and cali-brate the annotators based on 48 training samples drawn from the TOEFL R Practice Online (TPO) product [Cite_Footnote_3] , which offers practice tests simulating the TOEFL iBT testing experience with authentic test questions. The training samples were selected from a TPO test form with 6 test questions and were balanced according to test question and pro-ficiency score, i.e., 2 responses from each score level for each question.",Material,DataSource,True,Use（引用目的）,True,P17-2041_2_0,2017,Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework,Footnote
1522,11526," https://github.com/dbamman/litbank"," ['1 Introduction']",The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at [Cite] https://github.com/dbamman/litbank.,,"We present in this work a new dataset of en-tity annotations for a wide sample of 210,532 to-kens from 100 literary texts to help address these issues and help advance computational work on literature. These annotations follow the guide-lines set forth by the ACE 2005 entity tagging task (LDC, 2005) in labeling all nominal entities (named and common alike), including those with nested structure. In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at [Cite] https://github.com/dbamman/litbank.",Material,Dataset,True,Produce（引用目的）,False,N19-1220_0_0,2019,An Annotated Dataset of Literary Entities,Body
1523,11527," https://github.com/dbamman/"," ['5 Conclusion']",All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: [Cite] https://github.com/dbamman/ litbank; code to support this work can be found at: https://github.com/dbamman/NAACL2019-literary-entities.,,All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: [Cite] https://github.com/dbamman/ litbank; code to support this work can be found at: https://github.com/dbamman/NAACL2019-literary-entities.,Material,Dataset,True,Produce（引用目的）,False,N19-1220_1_0,2019,An Annotated Dataset of Literary Entities,Body
1524,11528," https://github.com/dbamman/NAACL2019-literary-entities"," ['5 Conclusion']",All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: https://github.com/dbamman/ litbank; code to support this work can be found at: [Cite] https://github.com/dbamman/NAACL2019-literary-entities.,,All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: https://github.com/dbamman/ litbank; code to support this work can be found at: [Cite] https://github.com/dbamman/NAACL2019-literary-entities.,Method,Code,True,Produce（引用目的）,True,N19-1220_2_0,2019,An Annotated Dataset of Literary Entities,Body
1525,11529," https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf"," ['1 Introduction']","These annotations follow the guide-lines set forth by the ACE 2005 entity tagging task (LDC, 2005) [Cite_Ref] in labeling all nominal entities (named and common alike), including those with nested structure.",LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.,"We present in this work a new dataset of en-tity annotations for a wide sample of 210,532 to-kens from 100 literary texts to help address these issues and help advance computational work on literature. These annotations follow the guide-lines set forth by the ACE 2005 entity tagging task (LDC, 2005) [Cite_Ref] in labeling all nominal entities (named and common alike), including those with nested structure. In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at https://github.com/dbamman/litbank.",補足資料,Document,True,Use（引用目的）,True,N19-1220_3_0,2019,An Annotated Dataset of Literary Entities,Reference
1526,11530," https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf"," ['3 Annotation', '3.1 Entity types']","ACE guidelines define a facility as a “functional, primarily man-made structure” de-signed for human habitation (buildings, muse-ums), storage (barns, parking garages), transporta-tion infrastructure (streets, highways), and main-tained outdoor spaces (gardens) (LDC, 2005) [Cite_Ref] .",LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.,"FAC. ACE guidelines define a facility as a “functional, primarily man-made structure” de-signed for human habitation (buildings, muse-ums), storage (barns, parking garages), transporta-tion infrastructure (streets, highways), and main-tained outdoor spaces (gardens) (LDC, 2005) [Cite_Ref] . We adopt the ACE threshold for taggability here as well, and rooms and closets within a house as the smallest possible facility.",補足資料,Document,True,Use（引用目的）,True,N19-1220_3_1,2019,An Annotated Dataset of Literary Entities,Reference
1527,11531," https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf"," ['3 Annotation', '3.1 Entity types']","Geo-political entities are single units that contain a population, government, physical loca-tion, and political boundaries (LDC, 2005) [Cite_Ref] .",LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.,"GPE. Geo-political entities are single units that contain a population, government, physical loca-tion, and political boundaries (LDC, 2005) [Cite_Ref] . In literary data, this includes not only cities that have known geographical locations within the real world (London, New York), or nations (England, the United States), but also both named and com-mon imagined entities as well (the town, the vil-lage).",補足資料,Document,True,Use（引用目的）,True,N19-1220_3_2,2019,An Annotated Dataset of Literary Entities,Reference
1528,11532," https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf"," ['3 Annotation', '3.1 Entity types']","Literary texts include a number of vehi-cles defined as “a physical device primarily de-signed to move an object from one location to an-other” (LDC, 2005) [Cite_Ref] ; ships, trains, and carriages dominate since nearly all texts were written before the rise of automobiles.",LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.,"VEH. Literary texts include a number of vehi-cles defined as “a physical device primarily de-signed to move an object from one location to an-other” (LDC, 2005) [Cite_Ref] ; ships, trains, and carriages dominate since nearly all texts were written before the rise of automobiles.",補足資料,Document,True,Use（引用目的）,True,N19-1220_3_3,2019,An Annotated Dataset of Literary Entities,Reference
1529,11533," https://catalog.ldc.upenn.edu/LDC2011T07"," ['4 Experimental Setup', '4.4 Baselines and Models']","• Caciularu et al. (2021) – graph-based Bh2019 model using representations from a Long-2 [Cite] https://catalog.ldc.upenn.edu/LDC2011T07 former (Beltagy et al., 2020) with cross-document attention during pre-training",,"• Caciularu et al. (2021) – graph-based Bh2019 model using representations from a Long-2 [Cite] https://catalog.ldc.upenn.edu/LDC2011T07 former (Beltagy et al., 2020) with cross-document attention during pre-training",補足資料,Document,False,Use（引用目的）,False,2021.emnlp-main.382_0_0,2021,Sequential Cross-Document Coreference Resolution,Body
1530,11534," http://www.newsreader-project.eu/results/data/the-ecb-corpus/"," ['References']",A Implementation details The dataset is available here: [Cite] http: //www.newsreader-project.eu/ results/data/the-ecb-corpus/.,,"In this paper, we propose a new model for cross-document coreference resolution that extends the efficient sequential prediction paradigm to multiple documents. The sequential prediction is combined with incremental candidate composition that allows the model to use the history of past coreference de-cisions at every step. Our model achieves compet-itive results for both entity and event coreference and our analysis provides strong evidence of the efficacy of both sequential models and higher-order inference in cross-document settings. In future, we intend to adapt this model to coreference across document streams and investigate alternatives to greedy prediction (e.g., beam search). A Implementation details The dataset is available here: [Cite] http: //www.newsreader-project.eu/ results/data/the-ecb-corpus/.",Material,Dataset,True,Produce（引用目的）,True,2021.emnlp-main.382_1_0,2021,Sequential Cross-Document Coreference Resolution,Body
1531,11535," http://www.seas.upenn.edu/∼pdtb/tools.shtml#annotator"," ['4 Annotation experiment', '4.1 Set-up']",The annotation is carried out on the PDTB annotation tool [Cite_Footnote_3] .,3 http://www.seas.upenn.edu/∼pdtb/tools.shtml#annotator,"The agreement statistics come from annotation con-ducted by two annotators in training so far. The data set consists of 98 files taken from the Chinese Tree-bank (Xue et al., 2005). The source of these files is Xinhua newswire. The annotation is carried out on the PDTB annotation tool [Cite_Footnote_3] .",Method,Tool,True,Use（引用目的）,True,P12-1008_0_0,2012,PDTB-style Discourse Annotation of Chinese Text,Footnote
1532,11536," https://github.com/DeepLearnXMU/NSEG"," ['3 Experiments', '3.1 Datasets']","NIPS abstract [Cite_Footnote_1] is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018).",1 https://github.com/DeepLearnXMU/NSEG,"The experiments are conducted on six public datasets in different domains: NIPS abstract, AAN abstract, NSF abstract, arXiv abstract: These datasets contain ab-stracts of research papers. NIPS abstract [Cite_Footnote_1] is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018). AAN abstract (Logeswaran et al., 2018) is col-lected from ACL Anthology Network corpus. ACL papers published up to year 2010 for train-ing, year 2011 for validation and 2012-2013 for testing. NSF abstract (Logeswaran et al., 2018) is from NSF Research Award abstract dataset, where abstracts in years 1990-1999/2000/2001-2003 for training/validation/testing. ArXiv abstract (Gong et al., 2016; Chen et al., 2016) is from arXiv web-site 2 . The validation and test sets of this dataset are the first and last 10% abstracts from the shuf-fled data, and the remaining data are for training.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.511_0_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1533,11537," https://github.com/FudanNLP/NeuralSentenceOrdering"," ['3 Experiments', '3.1 Datasets']","ArXiv abstract (Gong et al., 2016; Chen et al., 2016) is from arXiv web-site [Cite_Footnote_2] .",2 https://github.com/FudanNLP/NeuralSentenceOrdering,"The experiments are conducted on six public datasets in different domains: NIPS abstract, AAN abstract, NSF abstract, arXiv abstract: These datasets contain ab-stracts of research papers. NIPS abstract 1 is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018). AAN abstract (Logeswaran et al., 2018) is col-lected from ACL Anthology Network corpus. ACL papers published up to year 2010 for train-ing, year 2011 for validation and 2012-2013 for testing. NSF abstract (Logeswaran et al., 2018) is from NSF Research Award abstract dataset, where abstracts in years 1990-1999/2000/2001-2003 for training/validation/testing. ArXiv abstract (Gong et al., 2016; Chen et al., 2016) is from arXiv web-site [Cite_Footnote_2] . The validation and test sets of this dataset are the first and last 10% abstracts from the shuf-fled data, and the remaining data are for training.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.511_1_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1534,11538," http://visionandlanguage.net/VIST/dataset.html"," ['3 Experiments', '3.1 Datasets']","SIND, ROCStory: SIND is a visual storytelling dataset [Cite_Footnote_3] (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split.",3 http://visionandlanguage.net/VIST/dataset.html,"SIND, ROCStory: SIND is a visual storytelling dataset [Cite_Footnote_3] (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset (Wang and Wan, 2019; Mostafazadeh et al., 2016). It is randomly split by 8:1:1 for the training/validation/test sets. Both of two datasets consist of 5 sentences in each story text.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.511_2_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1535,11539," https://github.com/sodawater/SentenceOrdering"," ['3 Experiments', '3.1 Datasets']","ROCStory is a commonsense story dataset [Cite_Footnote_4] (Wang and Wan, 2019; Mostafazadeh et al., 2016).",4 https://github.com/sodawater/SentenceOrdering,"SIND, ROCStory: SIND is a visual storytelling dataset (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset [Cite_Footnote_4] (Wang and Wan, 2019; Mostafazadeh et al., 2016). It is randomly split by 8:1:1 for the training/validation/test sets. Both of two datasets consist of 5 sentences in each story text.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.511_3_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1536,11540," https://github.com/DeepLearnXMU/NSEG"," ['3 Experiments', '3.3 Experimental Setup']","To diminish the effects of randomness in training, the results of our model are averaged with [Cite_Footnote_5] random initializa-tions.",5 Code for metrics: https://github.com/DeepLearnXMU/NSEG,"We adopt the BERT BASE in the experiment and fine-tune it on each dataset. The paragraph en-coder has 2 self-attention layers with 8 heads. The hidden size is 768 and beam size is 16. Adam is employed as the optimizer. To search for the op-timal hyper-parameters, we adopt the grid search strategy for learning rate from {2e-5, 5e-5}, batch size from {8, 16, 32}, the number of epochs from {5, 10, 20}, and the coefficient α in the loss func-tion from {0.2, 0.4, 0.6, 0.8, 1.0}. The model with the best performance on the validation set is se-lected for each setting. The recommended hyper-parameter configuration of the model on each dataset are presented in Table 3. To diminish the effects of randomness in training, the results of our model are averaged with [Cite_Footnote_5] random initializa-tions. For data preprocessing, we use the tok-enizer 6 from BERT to preprocess the sentences. The experiments are conducted on GeForce GTX 1080Ti GPU with PyTorch framework .",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.511_4_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1537,11541," https://github.com/google-research/bert"," ['3 Experiments', '3.3 Experimental Setup']","For data preprocessing, we use the tok-enizer [Cite_Footnote_6] from BERT to preprocess the sentences.",6 https://github.com/google-research/bert,"We adopt the BERT BASE in the experiment and fine-tune it on each dataset. The paragraph en-coder has 2 self-attention layers with 8 heads. The hidden size is 768 and beam size is 16. Adam is employed as the optimizer. To search for the op-timal hyper-parameters, we adopt the grid search strategy for learning rate from {2e-5, 5e-5}, batch size from {8, 16, 32}, the number of epochs from {5, 10, 20}, and the coefficient α in the loss func-tion from {0.2, 0.4, 0.6, 0.8, 1.0}. The model with the best performance on the validation set is se-lected for each setting. The recommended hyper-parameter configuration of the model on each dataset are presented in Table 3. To diminish the effects of randomness in training, the results of our model are averaged with 5 random initializa-tions. For data preprocessing, we use the tok-enizer [Cite_Footnote_6] from BERT to preprocess the sentences. The experiments are conducted on GeForce GTX 1080Ti GPU with PyTorch framework .",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.511_5_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1538,11542," https://github.com/shrimai/Topological-Sort-for-Sentence-Ordering"," ['A Appendix', 'A.2 Two other metrics used in the Analysis']",The consecutiveness is not necessary for it [Cite_Footnote_10] .,10 Codes for metric: https://github.com/shrimai/Topological-Sort-for-Sentence-Ordering,"Longest Common Subsequence (LCS): It cal-culates the percentage of longest correct sub-sequence between the predicted order and the gold order (Gong et al., 2016). The consecutiveness is not necessary for it [Cite_Footnote_10] .",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.511_6_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1539,11543," https://github.com/DeepLearnXMU/NSEG"," ['A Appendix', 'A.2 Two other metrics used in the Analysis']","This metric (Chen et al., 2016; Gong et al., 2016) measures the fraction of pairs of sen-tences whose predicted relative order is the same as the ground truth order [Cite_Footnote_11] .",11 Code for metric: https://github.com/DeepLearnXMU/NSEG,"Rouge-S: This metric (Chen et al., 2016; Gong et al., 2016) measures the fraction of pairs of sen-tences whose predicted relative order is the same as the ground truth order [Cite_Footnote_11] . It allows for any arbi-trary gaps between two sentences as long as their relative order is correctly identified.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.511_7_0,2020,BERT-enhanced Relational Sentence Ordering Network,Footnote
1540,11544," https://github.com/fenchri/dsre-vae"," ['References']","Additional exploration of em-ploying Knowledge Base priors into the VAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results [Cite_Footnote_1] .",1 Source code is available at https://github.com/fenchri/dsre-vae,"We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extrac-tion by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the la-tent space of sentences via a Variational Au-toencoder ( VAE ) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indi-cate that multi-task learning results in perfor-mance benefits. Additional exploration of em-ploying Knowledge Base priors into the VAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.2_0_0,2021,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,Footnote
1541,11545," https://developers.google.com/freebase"," ['3 Experimental Settings', '3.1 Datasets']","For the choice of the Knowledge Base, we use a subset of Freebase [Cite_Footnote_2] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019).",2 https://developers.google.com/freebase 3 https://www.wikidata.org/,"For the choice of the Knowledge Base, we use a subset of Freebase [Cite_Footnote_2] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.",Material,Knowledge,True,Use（引用目的）,True,2021.naacl-main.2_1_0,2021,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,Footnote
1542,11546," https://www.wikidata.org/"," ['3 Experimental Settings', '3.1 Datasets']","For the choice of the Knowledge Base, we use a subset of Freebase [Cite_Footnote_2] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019).",2 https://developers.google.com/freebase 3 https://www.wikidata.org/,"For the choice of the Knowledge Base, we use a subset of Freebase [Cite_Footnote_2] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.",Material,Knowledge,True,Use（引用目的）,True,2021.naacl-main.2_2_0,2021,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,Footnote
1543,11547," https://deepgraphlearning.github.io/project/wikidata5m"," ['3 Experimental Settings', '3.1 Datasets']","For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [Cite_Footnote_4] ), containing approxi-mately 5 million entities.",4 https://deepgraphlearning.github.io/project/wikidata5m,"For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [Cite_Footnote_4] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning.",Material,Knowledge,True,Use（引用目的）,True,2021.naacl-main.2_3_0,2021,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,Footnote
1544,11548," https://www.wikidata.org/wiki/Wikidata:List_of_properties"," ['A Appendix', 'A.3 Hyper-parameter Settings']","A.4 W IKI D ISTANT Relation Categories Since W IKI D ISTANT contains 454 relations, their labels are used directly from the WikiData proper-ties [Cite_Footnote_9] .",9 https://www.wikidata.org/wiki/Wikidata: List_of_properties,"Knowledge Base Embeddings. In order to train KB entity embeddings we used the DGL-KE toolkit (Zheng et al., 2020). We use the same set of hyper-parameters for both Freebase and Wikidata as shown in Table 10. For Freebase we select 5, 000 triples as the validation set, while for Wikidata we use the validation set provided in the transductive setting (5, 136 triples). A.4 W IKI D ISTANT Relation Categories Since W IKI D ISTANT contains 454 relations, their labels are used directly from the WikiData proper-ties [Cite_Footnote_9] . Here, we add explanations about the top 10 most frequent categories used in Figures 4c, 4d. A.5 Additional Plots Figure 5 illustrates the t-SNE plot of the latent space for the NYT 10 validation set. We observe similar clusters to that of the KB (Figure 4a).",補足資料,Document,True,Use（引用目的）,True,2021.naacl-main.2_4_0,2021,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,Footnote
1545,11549," https://github.com/saffsd/langid.py"," ['6 Data and Evaluation']","A language identification tool langid.py [Cite_Footnote_1] (Lui and Baldwin, 2011) is then used to obtain purely English sentences.",1 https://github.com/saffsd/langid.py,"The training data consist of two corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the Lang-8 Learner Corpora v2 (Mizumoto et al., 2011). We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py [Cite_Footnote_1] (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source-target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2. A sentence pair where the source or target sentence has more than 80 words is also removed from both NUCLE and Lang-8. The statistics of the data after pre-processing are shown in Table 1.",Method,Tool,True,Use（引用目的）,True,D16-1195_0_0,2016,Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models,Footnote
1546,11550," http://deeplearning.net/software/theano"," ['7 Experiments and Results', '7.2 NNJM Adaptation']","We implement NNJM in Python using the deep learning library Theano [Cite_Footnote_2] (Bergstra et al., 2010) in order to use the massively parallel processing power of GPUs for training.",2 http://deeplearning.net/software/theano,"We implement NNJM in Python using the deep learning library Theano [Cite_Footnote_2] (Bergstra et al., 2010) in order to use the massively parallel processing power of GPUs for training. We first train an NNJM (NNJM B ASELINE ) with complete training data for 10 epochs. The source context window size is set to 5 and the target context window size is set to 4, mak-ing it a (5+5)-gram joint model. Training is done using stochastic gradient descent with a mini-batch of GEC systems. All results are averaged over 5 runs of tuning size of 128 and learning rate of 0.1. To speed up training and decoding, a single hidden layer neural network is used with an input embedding dimen-sion of 192 and 512 hidden units. We use a self-normalization coefficient of 0.1. We pick 16,000 and 32,000 most frequent words on the source and tar-get sides as our source context vocabulary and target context vocabulary, respectively. The output vocab-ulary is set to be the same as the target vocabulary. The vocabulary is selected from the complete train-ing data, and not based on the L1-specific in-domain data. We add the self-normalized NNJM as a fea-ture to our baseline GEC system, S C ONCAT to build a stronger baseline. This is referred to as S C ONCAT + NNJM B ASELINE in Table 4.",Method,Code,True,Use（引用目的）,True,D16-1195_1_0,2016,Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models,Footnote
1547,11551," http://personalitycafe.com/forum"," ['4 Experiments', '4.1 Datasets']","The Kaggle dataset is col-lected from PersonalityCafe, [Cite_Footnote_5] where people share their personality types and discussions about health, behavior, care, etc.",5 http://personalitycafe.com/forum,"We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, [Cite_Footnote_5] where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora is another dataset collected from Reddit, where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.",補足資料,Website,True,Use（引用目的）,True,2021.acl-long.326_0_0,2021,Psycholinguistic Tripartite Graph Network for Personality Detection,Footnote
1548,11552," https://psy.takelab.fer.hr/datasets/"," ['4 Experiments', '4.1 Datasets']","Pan-dora [Cite_Footnote_6] is another dataset collected from Reddit, where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves.",6 https://psy.takelab.fer.hr/datasets/,"We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora [Cite_Footnote_6] is another dataset collected from Reddit, where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.",Material,Dataset,True,Produce（引用目的）,False,2021.acl-long.326_1_0,2021,Psycholinguistic Tripartite Graph Network for Personality Detection,Footnote
1549,11553," https://www.reddit.com/"," ['4 Experiments', '4.1 Datasets']","Pan-dora is another dataset collected from Reddit, [Cite_Footnote_7] where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves.",7 https://www.reddit.com/,"We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurković et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora is another dataset collected from Reddit, [Cite_Footnote_7] where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.",Material,DataSource,True,Use（引用目的）,True,2021.acl-long.326_2_0,2021,Psycholinguistic Tripartite Graph Network for Personality Detection,Footnote
1550,11554," https://pytorch.org/"," ['4 Experiments', '4.3 Training Details']",We implement our TrigNet in Pytorch [Cite_Footnote_8] and train it on four NVIDIA RTX 2080Ti GPUs.,8 https://pytorch.org/,"We implement our TrigNet in Pytorch [Cite_Footnote_8] and train it on four NVIDIA RTX 2080Ti GPUs. Adam (Kingma and Ba, 2014) is utilized as the optimizer, with the learning rate of BERT set to 2e-5 and of other components set to 1e-3. We set the maxi-mum number of posts, r, to 50 and the maximum length of each post, s, to 70, considering the limit of available computational resources. After tuning on the validation dataset, we set the dropout rate to 0.2 and the mini-batch size to 32. The maximum number of nodes, r + m + n, is set to 500 for Kag-gle and 970 for Pandora, which cover 98.95% and 97.07% of the samples, respectively. Moreover, the two hyperparameters, the numbers of flow GAT layers L and heads K, are searched in {1, 2, 3} and {1, 2, 4, 6, 8, 12, 16, 24}, respectively, and the best choices are L = 1 and K = 12. The reasons for L = 1 are likely twofold. First, our flow GAT can already realize the interactions between nodes when L = 1, whereas the vanilla GAT needs to stack 4 layers. Second, after trying L = 2 and L = 3, we find that they lead to slight performance drops compared to that of L = 1.",Method,Tool,True,Use（引用目的）,True,2021.acl-long.326_3_0,2021,Psycholinguistic Tripartite Graph Network for Personality Detection,Footnote
1551,11555," http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf"," ['2 Related Work', '2.1 Personality Detection']","As an emerging research problem, text-based per-sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017 [Cite_Ref] ; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).",Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).,"As an emerging research problem, text-based per-sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017 [Cite_Ref] ; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).",補足資料,Paper,True,Introduce（引用目的）,True,2021.acl-long.326_4_0,2021,Psycholinguistic Tripartite Graph Network for Personality Detection,Reference
1552,11556," http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf"," ['2 Related Work', '2.1 Personality Detection']","These features are then fed into a classi-fier such as SVM (Cui and Qi, 2017) [Cite_Ref] and XGBoost (Tadesse et al., 2018) to predict the personality traits.",Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).,"Traditional studies on this problem generally re-sort to feature-engineering methods, which first ex-tracts various psychological categories via LIWC (Tausczik and Pennebaker, 2010) or statistical fea-tures by the bag-of-words model (Zhang et al., 2010). These features are then fed into a classi-fier such as SVM (Cui and Qi, 2017) [Cite_Ref] and XGBoost (Tadesse et al., 2018) to predict the personality traits. Despite interpretable features that can be expected, feature engineering has such limitations as it relies heavily on manually designed features.",Method,Tool,True,Use（引用目的）,True,2021.acl-long.326_4_1,2021,Psycholinguistic Tripartite Graph Network for Personality Detection,Reference
1553,11557," http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf"," ['4 Experiments', '4.2 Baselines']","The following mainstream models are adopted as baselines to evaluate our model: SVM (Cui and Qi, 2017) [Cite_Ref] and XGBoost (Tadesse et al., 2018): Support vector machine (SVM) or XGBoost is utilized as the classifier with features extracted by TF-IDF and LIWC from all posts.",Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).,"The following mainstream models are adopted as baselines to evaluate our model: SVM (Cui and Qi, 2017) [Cite_Ref] and XGBoost (Tadesse et al., 2018): Support vector machine (SVM) or XGBoost is utilized as the classifier with features extracted by TF-IDF and LIWC from all posts.",補足資料,Paper,True,Introduce（引用目的）,True,2021.acl-long.326_4_2,2021,Psycholinguistic Tripartite Graph Network for Personality Detection,Reference
1554,11558," http://www.h-its.org/nlp/download"," ['3 Corpus Creation', '3.3 Gold Standard']",The corpus will be made publically available as OntoNotes annotation layer via [Cite] http://www.,,Our final gold standard corpus consists of 50 texts from the WSJ portion of the OntoNotes corpus- The corpus will be made publically available as OntoNotes annotation layer via [Cite] http://www.h-its.org/nlp/download.,Material,Dataset,True,Produce（引用目的）,True,P12-1084_0_0,2012,Collective Classification for Fine-grained Information Status,Body
1555,11559," http://statnlp.org/research/sp"," ['References']",Analysis also justifies the effectiveness of us-ing our new dependency-based representa-tion. [Cite_Footnote_1],1 We make our system and code available at http://statnlp.org/research/sp.,"We propose a novel dependency-based hybrid tree model for semantic parsing, which con-verts natural language utterance into machine interpretable meaning representations. Un-like previous state-of-the-art models, the se-mantic information is interpreted as the la-tent dependency between the natural language words in our joint representation. Such depen-dency information can capture the interactions between the semantics and natural language words. We integrate a neural component into our model and propose an efficient dynamic-programming algorithm to perform tractable inference. Through extensive experiments on the standard multilingual GeoQuery dataset with eight languages, we demonstrate that our proposed approach is able to achieve state-of-the-art performance across several languages. Analysis also justifies the effectiveness of us-ing our new dependency-based representa-tion. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D18-1265_0_0,2018,Dependency-based Hybrid Trees for Semantic Parsing,Footnote
1556,11560," https://gitlab.com/sutdnlp/statnlp-core"," ['4 Experiments']","Our com-plete implementation is based on the StatNLP [Cite_Footnote_12] structured prediction framework (Lu, 2017).",12 https://gitlab.com/sutdnlp/statnlp-core,"Hyperparameters We set the maximum depth c of the semantic tree to 20, following Lu (2015). The L 2 regularization coefficient is tuned from 0.01 to 0.05 using 5-fold cross-validation on the training set. The Polyglot (Al-Rfou et al., 2013) multilingual word embeddings (with 64 dimen-sions) are used for all languages. We use L-BFGS (Liu and Nocedal, 1989) to optimize the D EP HT model until convergence and stochas-tic gradient descent (SGD) with a learning rate of 0.05 to optimize the neural D EP HT model. We implemented our neural component with the Torch7 library (Collobert et al., 2011). Our com-plete implementation is based on the StatNLP [Cite_Footnote_12] structured prediction framework (Lu, 2017).",Method,Tool,False,Extend（引用目的）,False,D18-1265_1_0,2018,Dependency-based Hybrid Trees for Semantic Parsing,Footnote
1557,11561," https://github.com/facebookresearch/EGG"," ['1 Introduction']",EGG can be installed from [Cite] https://github.com/facebookresearch/EGG.,,"EGG is implemented in PyTorch (Paszke et al., 2017) and it is licensed under the MIT license. EGG can be installed from [Cite] https://github.com/facebookresearch/EGG.",Method,Tool,True,Introduce（引用目的）,False,D19-3010_1_0,2019,EGG: a toolkit for research on Emergence of lanGuage in Games,Body
1558,11562," https://www.tensorflow.org/tensorboard"," ['1 Introduction']","Notable features of EGG include: (a) Prim-itives for implementing single-symbol or variable-length communication (with vanilla RNNs (Elman, 1990), GRUs (Cho et al., 2014), LSTMs (Hochreiter and Schmidhuber, 1997)); (b) Training with optimization of the com-munication channel through REINFORCE or Gumbel-Softmax relaxation via a common interface; (c) Simplified configuration of the general components, such as check-pointing, optimization, Tensorboard support, [Cite_Footnote_2] etc.; (d) A simple CUDA-aware command-line tool for hyperparameter grid-search.",2 https://www.tensorflow.org/ tensorboard,"Notable features of EGG include: (a) Prim-itives for implementing single-symbol or variable-length communication (with vanilla RNNs (Elman, 1990), GRUs (Cho et al., 2014), LSTMs (Hochreiter and Schmidhuber, 1997)); (b) Training with optimization of the com-munication channel through REINFORCE or Gumbel-Softmax relaxation via a common interface; (c) Simplified configuration of the general components, such as check-pointing, optimization, Tensorboard support, [Cite_Footnote_2] etc.; (d) A simple CUDA-aware command-line tool for hyperparameter grid-search.",Method,Tool,False,Introduce（引用目的）,True,D19-3010_2_0,2019,EGG: a toolkit for research on Emergence of lanGuage in Games,Footnote
1559,11563," https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb"," ['4 Implementing a game']",The full implementation can be found in an online tuto-rial. [Cite_Footnote_4],4 https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb,"In this Section we walk through the main steps to build a communication game in EGG. We illus-trate them through a MNIST (LeCun et al., 1998) communication-based autoencoding task: Sender observes an image and sends a message to Re-ceiver. In turn, Receiver tries to reconstruct the image. We only cover here the core aspects of the implementation, ignoring standard pre- and post-processing steps, such as data loading. The full implementation can be found in an online tuto-rial. [Cite_Footnote_4]",補足資料,Document,True,Produce（引用目的）,True,D19-3010_3_0,2019,EGG: a toolkit for research on Emergence of lanGuage in Games,Footnote
1560,11564," https://colab.research.google.com/github/facebookresearch/EGG/blob/master/egg/zoo/language_bottleneck/"," ['5 Some pre-implemented games']","It contains code that was re-cently used to study the communicative efficiency of artificial LSTM-based agents (Chaabouni et al., 2019a) and the information-minimization proper-ties of emergent discrete codes (Kharitonov et al., 2019). [Cite_Footnote_5]",5 A small illustration can be run in Google Colab-oratory: https://colab.research.google.com/github/facebookresearch/EGG/blob/master/egg/zoo/language_bottleneck/mnist-style-transfer-via-bottleneck.ipynb.,"EGG contains implementations of several games. They (a) illustrate how EGG can be used to ex-plore interesting research questions, (b) provide reference usage patterns and building blocks, (c) serve as means to ensure reproducibility of stud-ies reported in the literature. For example, EGG incorporates an implementation of the signaling game of Lazaridou et al. (2016) and Bouchacourt and Baroni (2018). It contains code that was re-cently used to study the communicative efficiency of artificial LSTM-based agents (Chaabouni et al., 2019a) and the information-minimization proper-ties of emergent discrete codes (Kharitonov et al., 2019). [Cite_Footnote_5] Finally, EGG provides a pre-implemented game that allows to train agents entirely via the command line and external input/output files, without having to write a single line of Python code. We hope this will lower the learning curve for those who want to experiment with language emergence without previous coding experience.",Method,Code,False,Produce（引用目的）,False,D19-3010_4_0,2019,EGG: a toolkit for research on Emergence of lanGuage in Games,Footnote
1561,11565," https://github.com/HuthLab/IGF"," ['5 Results', '5.1 Language Model Fine-tuning']","Standard fine-tuning on Books achieves a median perplexity of 57.3, compared to 56.9 for IGF with a constant threshold and 54.0 for IGF with the shifting threshold schedule. [Cite_Footnote_2]","2 Demo code and data can be found at https://github.com/HuthLab/IGF. dit comments (25% of contexts) and a corpus of books (75% of contexts) were mixed into a single training dataset. Using the predicted q-value generated from our convolutional secondary learner, we can achieve good separation of the corpora using the information gain metric despite computing the true q-value using a small objective set. The percentage of examples from the Books corpus that are higher than several frequently the WikiText-103 dataset. We plot the model perplexity over many batches as in Figure 4 of the paper. This fig-ure can be replicated by following the Jupyter tutorial provided along with the supplementary material. ity of standard at each step. This serves as a barome-ter for comparing the relative efficiency of fine-tuning. In the early stages of fine-tuning, we can see that IGF requires 30%-40% fewer backpropagation steps over standard fine-tuning. This suggests that IGF could be used as a more energy efficient alternative to standard language model fine-tuning. Note that since IGF con-verges to a lower final value than standard fine-tuning, these values asymptote to a fixed value.","We first compare IGF directly to standard fine-tuning, which we define as basic batched stochas-tic gradient descent with Adam (Kingma and Ba, 2015) using random samples from the tar-get corpus. For initial tests, we chose the pre-trained GPT-2 Small Transformer model, a com-monly used unidirectional language model with roughly 124 million parameters. We used the pub-licly available GPT-2 Small implementation of the transformers package (Wolf et al., 2020). We performed 50 runs each of standard fine-tuning on (1) training examples sampled from the Mixed corpus, and (2) from the easier Books corpus. We then performed 50 runs of IGF using two thresh-olding schedules, one with a fixed T S KIP and one with shifting T S KIP . For both methods, batches of size 16 were used to train the language model with a learning rate of 5 × 10 −5 and β 1 = 0.9,β 2 = 0.999. The convolutional network that we used for our secondary learner was trained using SGD with Adam with a learning rate of 10 −5 and β 1 = 0.9,β 2 = 0.999. Both types of IGF runs were performed on the strictly more challenging Mixed corpus only. In all cases model perplexity was tested on a set drawn solely from the Books cor-pus. Figure 1 plots the averaged fine-tuning curves of these 4 different approaches over 60 batches. We see that IGF significantly improves final test perplexity when compared to standard fine-tuning on both the Mixed corpus and the Books corpus. Standard fine-tuning on Books achieves a median perplexity of 57.3, compared to 56.9 for IGF with a constant threshold and 54.0 for IGF with the shifting threshold schedule. [Cite_Footnote_2] All 50 runs of IGF with a shifting schedule outperformed all 50 stan-dard fine-tuning runs. This means that the over-all improvements to data order that IGF achieves through selective sampling of informative contexts are far in excess of what might be reasonably achieved through random sampling of contexts. by IGF persist across several choices of dataset, fine-tuning specifications, and model architecture. Figure 2 shows the final converged values for fine-tuning GPT-2 Small on a different dataset from Figure 1 (WikiText-103), a different archi-tecture (GPT2-Medium), a different embedding space with different directionality (BERT) (De-vlin et al., 2019), and a different overall fine-tuning task (SST-2) (Socher et al., 2013). In ev-ery case, IGF exceeds the performance of standard fine-tuning. This suggests that IGF is a resilient method that is broadly applicable to a variety of fine-tuning modalities and domains. dataset. Box plots show results from 50 runs with each method. Top left: IGF outperforms standard fine-tuning with an average test perplexity of 67.8 compared to 69.8 when fine-tuning on GPT2-Small. Top right: When using the GPT2-Medium pretrained model, IGF converges to 27.1 as opposed to 27.4 for standard fine-tuning. Bottom left: When fine-tuning BERT (a bi-directional language model trained to minimize masked perplexity rather than next-word perplexity), masked perplexity declines from 4.33 to 4.29. Bottom right: When fine-tuning instead to the Stanford Sentiment Treebank, a sentiment analysis task, IGF improves accuracy from an average of 94.06 to 94.27. The WikiText-103 dataset was use for all comparisons ex-cept for SST-2. All other model parameters are as in Figure 1 and use a shifting thresholding schedule. When fine-tuning on BERT and SST-2, the plotted met-rics (masked perplexity and accuracy) were used in-stead of next-word perplexity to compute IG(X). All differences are statistically significant to p < 10 −3 .",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.87_0_0,2021,Selecting Informative Contexts Improves Language Model Fine-tuning,Footnote
1562,11566," https://seekingalpha.com/"," ['3 Data and pre-processing']","Our data [Cite_Footnote_2] consists of transcripts of 12,285 earn-ings calls held between January 1, 2010 and De-cember 31, 2017.","2 In Appendix A in supplemental material we provide the stock tickers for the calls in our data; the corpus can be re-assembled from multiple sources, such as https: //seekingalpha.com/.","Our data [Cite_Footnote_2] consists of transcripts of 12,285 earn-ings calls held between January 1, 2010 and De-cember 31, 2017. In order to control for analyst coverage effects (larger companies with a greater market share will typically be covered by more an-alysts), we include only calls from S&P 500 com-panies. We split the data by year into training, val-idation and testing sets (see Table 1).",Material,DataSource,True,Introduce（引用目的）,True,P19-1047_0_0,2019,Modeling financial analysts’ decision making via the pragmatics and semantics of earnings calls,Footnote
1563,11567," https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0"," ['4 Pragmatic correlations with analysts’ pre-call judgments', '4.1 Pragmatic lexical features']","For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [Cite_Footnote_6] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts.","6 Version 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0. pdf Section 2.6.","Named entity counts and concreteness ratio. For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [Cite_Footnote_6] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn.",Material,DataSource,True,Use（引用目的）,True,P19-1047_1_0,2019,Modeling financial analysts’ decision making via the pragmatics and semantics of earnings calls,Footnote
1564,11568," https://pytorch.org/"," ['5 Predicting changes in analysts’ post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']","The networks are written in Pytorch [Cite_Footnote_20] and optimized with Adam (Kingma and Ba, 2014) .",20 https://pytorch.org/,"Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [Cite_Footnote_20] and optimized with Adam (Kingma and Ba, 2014) .",Method,Tool,True,Use（引用目的）,True,P19-1047_2_0,2019,Modeling financial analysts’ decision making via the pragmatics and semantics of earnings calls,Footnote
1565,11569," https://www.msci.com/gics"," ['5 Predicting changes in analysts’ post-call forecasts', '5.4 Results.']",We ana-lyze errors on the validation data by segmenting earnings calls by each company’s Global Industry Classification Standard (GICS) sector [Cite_Footnote_21] .,21 See https://www.msci.com/gics. There are 11 broad industry sectors.,"Breakdown of results by industry. We ana-lyze errors on the validation data by segmenting earnings calls by each company’s Global Industry Classification Standard (GICS) sector [Cite_Footnote_21] . See Fig-ure 2 for the breakdown results. Notably, the bag-of-words model performs almost 2.5 times worse on earnings calls from the Materials sector versus the Utilities and Telecommunication Services sec-tors. This suggests industry-specific models may be important in future work.",Material,Knowledge,True,Use（引用目的）,True,P19-1047_3_0,2019,Modeling financial analysts’ decision making via the pragmatics and semantics of earnings calls,Footnote
1566,11570," https://cloud.google.com/natural-language/docs/analyzing-entities"," ['2 Mapping Questions to Entities']","We analyze four QA tasks: NQ , [Cite_Footnote_1] SQuAD (Rajpurkar et al., 2016), QB (Boyd-Graber et al., 2012) and Trivia QA (Joshi et al., 2017).","1 For NQ , we only consider questions with short answers. 2https://cloud.google.com/natural-language/docs/ analyzing-entities","We analyze four QA tasks: NQ , [Cite_Footnote_1] SQuAD (Rajpurkar et al., 2016), QB (Boyd-Graber et al., 2012) and Trivia QA (Joshi et al., 2017). Google CLOUD - NL 2 finds and links entity mentions in QA examples.",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.444_0_0,2021,Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy,Footnote
1567,11571," http://code.google.com/p/mate-tools/"," ['4 Experiments']","In this dataset (J&L), entities are annotated as entity [Cite_Footnote_1] or entity depend-ing on their position before or after the predicate.",1 http://code.google.com/p/mate-tools/,"We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity [Cite_Footnote_1] or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.",Material,Knowledge,False,Use（引用目的）,False,D13-1194_0_0,2013,Detection of Product Comparisons – How Far Does an Out-of-the-box Semantic Role Labeling System Take You?,Footnote
1568,11572," http://verbs.colorado.edu/jdpacorpus/"," ['4 Experiments']","In this dataset (J&L), entities are annotated as entity or entity [Cite_Footnote_2] depend-ing on their position before or after the predicate.",2 Available from http://verbs.colorado.edu/jdpacorpus/ – we ignore cars batch 009 where no arguments of comparative predicates are annotated.,"We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity or entity [Cite_Footnote_2] depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.",Material,Dataset,True,Produce（引用目的）,True,D13-1194_1_0,2013,Detection of Product Comparisons – How Far Does an Out-of-the-box Semantic Role Labeling System Take You?,Footnote
1569,11573," http://www.cs.uic.edu/~liub/FBS/data.tar.gz"," ['4 Experiments']","We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) [Cite_Footnote_3] .","3 Available from http://www.cs.uic.edu/˜liub/FBS/data.tar.gz – although the original paper works on some unknown subset of this data, so our results are not directly","We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) [Cite_Footnote_3] . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.",Material,Dataset,True,Use（引用目的）,True,D13-1194_2_0,2013,Detection of Product Comparisons – How Far Does an Out-of-the-box Semantic Role Labeling System Take You?,Footnote
1570,11574," http://nlp.stanford.edu/software/corenlp.shtml"," ['4 Experiments']",We do sentence segmentation and tokenization with the Stanford Core NLP [Cite_Footnote_4] .,4 http://nlp.stanford.edu/software/corenlp.shtml,"We do sentence segmentation and tokenization with the Stanford Core NLP [Cite_Footnote_4] . Annotations are mapped to the extracted tokens. We ignore anno-tations that do not correspond to complete tokens. In the JDPA corpus, if an annotated argument is out-side the current sentence, we follow the coreference chain to find a coreferent annotation in the same sen-tence. If this is not successful, the argument is ig-nored. We extract all sentences where we found at least one comparative predicate as our dataset.",Method,Tool,True,Use（引用目的）,True,D13-1194_3_0,2013,Detection of Product Comparisons – How Far Does an Out-of-the-box Semantic Role Labeling System Take You?,Footnote
1571,11575," http://www.dmoz.org/"," ['4 Building Noun Similarity Lists', '4.1 Web Corpus']","Initially, we use the links from Open Directory project [Cite_Footnote_3] as seed links for our spider.",3 http://www.dmoz.org/,"We set up a spider to download roughly 70 million web pages from the Internet. Initially, we use the links from Open Directory project [Cite_Footnote_3] as seed links for our spider. Each webpage is stripped of HTML tags, tokenized, and sentence segmented. Each docu-ment is language identified by the software TextCat which implements the paper by Cavnar and Trenkle (1994). We retain only English documents. The web contains a lot of duplicate or near-duplicate docu-ments. Eliminating them is critical for obtaining bet-ter representation statistics from our collection. The problem of identifying near duplicate documents in linear time is not trivial. We eliminate duplicate and near duplicate documents by using the algorithm de-scribed by Kolcz et al. (2004). This process of dupli-cate elimination is carried out in linear time and in-volves the creation of signatures for each document. Signatures are designed so that duplicate and near duplicate documents have the same signature. This algorithm is remarkably fast and has high accuracy. This entire process of removing non English docu-ments and duplicate (and near-duplicate) documents reduces our document set from 70 million web pages to roughly 31 million web pages. This represents roughly 138GB of uncompressed text.",補足資料,Website,True,Use（引用目的）,True,P05-1077_0_0,2005,Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High Speed Noun Clustering,Footnote
1572,11576," http://odur.let.rug.nl/∼vannoord/TextCat/"," ['4 Building Noun Similarity Lists', '4.1 Web Corpus']",Each docu-ment is language identified by the software TextCat [Cite_Footnote_4] which implements the paper by Cavnar and Trenkle (1994).,4 http://odur.let.rug.nl/∼vannoord/TextCat/,"We set up a spider to download roughly 70 million web pages from the Internet. Initially, we use the links from Open Directory project as seed links for our spider. Each webpage is stripped of HTML tags, tokenized, and sentence segmented. Each docu-ment is language identified by the software TextCat [Cite_Footnote_4] which implements the paper by Cavnar and Trenkle (1994). We retain only English documents. The web contains a lot of duplicate or near-duplicate docu-ments. Eliminating them is critical for obtaining bet-ter representation statistics from our collection. The problem of identifying near duplicate documents in linear time is not trivial. We eliminate duplicate and near duplicate documents by using the algorithm de-scribed by Kolcz et al. (2004). This process of dupli-cate elimination is carried out in linear time and in-volves the creation of signatures for each document. Signatures are designed so that duplicate and near duplicate documents have the same signature. This algorithm is remarkably fast and has high accuracy. This entire process of removing non English docu-ments and duplicate (and near-duplicate) documents reduces our document set from 70 million web pages to roughly 31 million web pages. This represents roughly 138GB of uncompressed text.",Method,Tool,True,Use（引用目的）,True,P05-1077_1_0,2005,Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High Speed Noun Clustering,Footnote
1573,11577," http://w3.usf.edu/FreeAssociation/"," ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']","To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set [Cite_Footnote_3] (Nelson et al., 2004), and (2) the VerbNet verb lexicon (Kipper et al., 2004; Kipper et al., 2008).",3 http://w3.usf.edu/FreeAssociation/,"To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set [Cite_Footnote_3] (Nelson et al., 2004), and (2) the VerbNet verb lexicon (Kipper et al., 2004; Kipper et al., 2008).",Material,Knowledge,True,Use（引用目的）,True,D16-1235_0_0,2016,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,Footnote
1574,11578," http://verbs.colorado.edu/verb-index/"," ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']","To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set (Nelson et al., 2004), and (2) the VerbNet verb lexicon [Cite_Footnote_4] (Kipper et al., 2004; Kipper et al., 2008).",4 http://verbs.colorado.edu/verb-index/,"To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set (Nelson et al., 2004), and (2) the VerbNet verb lexicon [Cite_Footnote_4] (Kipper et al., 2004; Kipper et al., 2008).",Material,Knowledge,True,Use（引用目的）,True,D16-1235_1_0,2016,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,Footnote
1575,11579," http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf"," ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']","According to the official VerbNet guidelines, [Cite_Footnote_5] “Verb Classes are numbered according to shared se-mantics and syntax, and classes which share a top-level number (9-109) have corresponding semantic relationships.”",5 http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf,"VerbNet (VN) is the largest online verb lexicon currently available for English. It is hierarchical, domain-independent, and broad-coverage. VN is or-ganised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class mem-bers. According to the official VerbNet guidelines, [Cite_Footnote_5] “Verb Classes are numbered according to shared se-mantics and syntax, and classes which share a top-level number (9-109) have corresponding semantic relationships.” For instance, all verbs from the top-level Class 9 are labelled “Verbs of Putting”, all verbs from Class 30 are labelled “Verbs of Perception”, while Class 39 contains “Verbs of Ingesting”.",補足資料,Document,True,Introduce（引用目的）,True,D16-1235_2_0,2016,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,Footnote
1576,11580," https://verbs.colorado.edu/verb-index/vn/break-45.1.php"," ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']","Among others, three basic types of information are covered in VN: (1) verb subcategorization frames (SCFs), which describe the syntactic realization of the predicate-argument structure (e.g. The window broke), (2) selectional preferences (SPs), which cap-ture the semantic preferences verbs have for their arguments (e.g. a breakable physical object broke) and (3) lexical-semantic verb classes (VCs) which provide a shared level of abstraction for verbs similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45). [Cite_Footnote_6]",6 https://verbs.colorado.edu/verb-index/vn/break-45.1.php,"Among others, three basic types of information are covered in VN: (1) verb subcategorization frames (SCFs), which describe the syntactic realization of the predicate-argument structure (e.g. The window broke), (2) selectional preferences (SPs), which cap-ture the semantic preferences verbs have for their arguments (e.g. a breakable physical object broke) and (3) lexical-semantic verb classes (VCs) which provide a shared level of abstraction for verbs similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45). [Cite_Footnote_6] The basic overview of the VerbNet structure already suggests that measur-ing verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.",補足資料,Document,True,Introduce（引用目的）,True,D16-1235_3_0,2016,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,Footnote
1577,11581," https://verbs.colorado.edu/semlink/"," ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']","Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007). [Cite_Footnote_7]",7 https://verbs.colorado.edu/semlink/,"The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenom-ena. Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007). [Cite_Footnote_7]",補足資料,Website,True,Introduce（引用目的）,True,D16-1235_4_0,2016,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,Footnote
1578,11582," https://www.kilgarriff.co.uk/bnc-readme.html"," ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']","We performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997). [Cite_Footnote_9]",9 https://www.kilgarriff.co.uk/bnc-readme.html,"We performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997). [Cite_Footnote_9] After ranking all BNC verbs according to their frequency, we divided the list into quartiles: Q1 (most frequent verbs in BNC) - Q4 (least frequent verbs in BNC). Out of the 827 SimVerb-3500 verb types, 677 are contained in Q1, 122 in Q2, 18 in Q3, 4 in Q4 (to enroll, to hitchhike, to implode, to whelp), while 6 verbs are not covered in the BNC list. 2,818 verb pairs contain Q1 verbs, while there are 43 verb pairs with both verbs not in Q1. Further empirical analyses are provided in § 6.",補足資料,Document,True,Use（引用目的）,True,D16-1235_5_0,2016,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,Footnote
1579,11583," https://prolific.ac/"," ['4 Word Pair Scoring']","We employ the Prolific Academic (PA) crowdsourc-ing platform, [Cite_Footnote_11] an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower.",11 https://prolific.ac/ (We chose PA for logistic reasons.),"We employ the Prolific Academic (PA) crowdsourc-ing platform, [Cite_Footnote_11] an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower.",Method,Tool,True,Use（引用目的）,True,D16-1235_6_0,2016,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,Footnote
1580,11584," http://www.umiacs.umd.edu/~jbg/static/downloads_and_media.html"," ['4 Experiments', '4.3 Sentiment Prediction']","We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. [Cite_Footnote_6]",6 We followed Pang and Lee’s method for creating a nu-merical score between 0 and 1 from a star rating. We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not as-sume integer values). The German movie review corpus is available at http://www.umiacs.umd.edu/˜jbg/static/downloads_and_media.html,"We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. [Cite_Footnote_6]",Material,DataSource,True,Use（引用目的）,True,D10-1005_0_0,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Footnote
1581,11585," http://www.mdbg.net/chindict/"," ['3 Bridges Across Languages']","Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997) [Cite_Ref] , and a Chinese-German dictionary (Hefti, 2005).",Paul Denisowski. 1997. CEDICT. http://www.mdbg.net/chindict/.,"Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997) [Cite_Ref] , and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b).",Material,DataSource,True,Use（引用目的）,True,D10-1005_1_0,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Reference
1582,11586," http://chdw.de"," ['3 Bridges Across Languages']","Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005) [Cite_Ref] .",Jan Hefti. 2005. HanDeDict. http://chdw.de.,"Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005) [Cite_Ref] . As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b).",Material,DataSource,True,Use（引用目的）,True,D10-1005_2_0,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Reference
1583,11587," http://www.statmt.org/europarl/"," ['4 Experiments', '4.1 Matching on Multilingual Topics']","We took the 1996 documents from the Europarl cor-pus (Koehn, 2005) [Cite_Ref] using three bridges: GermaNet, dictionary, and the uninformative flat matching.",Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit. http://www.statmt.org/europarl/.,"We took the 1996 documents from the Europarl cor-pus (Koehn, 2005) [Cite_Ref] using three bridges: GermaNet, dictionary, and the uninformative flat matching. The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework (as there is no associated response variable for Eu-roparl documents); this experiment is to demonstrate the effectiveness of the multilingual aspect of the model. To test whether the topics learned by the model are consistent across languages, we represent each document using the probability distribution θ d over topic assignments. Each θ d is a vector of length K and is a language-independent representation of the document.",Material,DataSource,True,Use（引用目的）,True,D10-1005_3_0,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Reference
1584,11588," http://snowball.tartarus.org/credits.php"," ['3 Bridges Across Languages']","We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970) [Cite_Ref] .",Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php.,"WordNet We took the alignment of GermaNet to WordNet 1.6 (Kunze and Lemnitzer, 2002) and re-moved all synsets that were had no mapped German words. Any German synsets that did not have English translations had their words mapped to the lowest extant English hypernym (e.g. “beinbruch,” a bro-ken leg, was mapped to “fracture”). We stemmed all words to account for inflected forms not being present (Porter and Boulton, 1970) [Cite_Ref] . An example of the paths for the German word “wunsch” (wish, request) is shown in Figure 2(a).",Method,Tool,True,Use（引用目的）,True,D10-1005_4_0,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Reference
1585,11589," http://snowball.tartarus.org/credits.php"," ['4 Experiments', '4.1 Matching on Multilingual Topics']","4 For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970) [Cite_Ref] , and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size).",Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php.,"4 For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970) [Cite_Ref] , and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content words were excluded.",Method,Tool,False,Use（引用目的）,True,D10-1005_4_1,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Reference
1586,11590," http://www-user.tu-chemnitz.de/fri/ding/"," ['3 Bridges Across Languages']","Entries were taken from an English-German dictionary (Richter, 2008) [Cite_Ref] a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005).",Frank Richter. 2008. Dictionary nice grep. http://www-user.tu-chemnitz.de/fri/ding/.,"Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) [Cite_Ref] a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage. An example for German is shown in Figure 2(b).",Material,DataSource,True,Use（引用目的）,True,D10-1005_5_0,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Reference
1587,11591," http://www.filmrezension.de"," ['4 Experiments', '4.3 Sentiment Prediction']","We gathered 330 film reviews from a German film review site (Vetter et al., 2000) [Cite_Ref] and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. 6","Tobias Vetter, Manfred Sauer, and Philipp Wallutat. 2000. Filmrezension.de: Online-magazin für filmkritik. http://www.filmrezension.de.","We gathered 330 film reviews from a German film review site (Vetter et al., 2000) [Cite_Ref] and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. 6",Material,DataSource,True,Use（引用目的）,True,D10-1005_8_0,2010,Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation,Reference
1588,11592," https://github.com/tttthomasssss/acl2017"," ['1 Introduction']",Our method is unsupervised and maintains the intrinsic interpretability of A PT s [Cite_Footnote_1] .,1 We release our code and data at https://github.com/tttthomasssss/acl2017,"In this paper, we generalise distributional in-ference (DI) in A PT s and show how precisely the same mechanism that was introduced to sup-port distributional composition, namely “offset-ting” A PT representations, gives rise to a novel form of distributional inference, allowing us to in-fer co-occurrences from neighbours of these rep-resentations. For example, by transforming a rep-resentation of white to a representation of “things that can be white”, inference of unobserved, but plausible, co-occurrences can be based on find-ing near neighbours (which will be nouns) of the “things that can be white” structure. This further-more exposes an interesting connection between distributional inference and distributional compo-sition. Our method is unsupervised and maintains the intrinsic interpretability of A PT s [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,P17-2069_0_0,2017,Improving Semantic Composition with Offset Inference,Footnote
1589,11593," http://www.nlm.nih.gov/research/umls/"," ['1 Introduction']","Knowledge in this domain can be thought of as belonging to two categories: (1) Background Knowledge captured in medical ontolo-gies like UMLS (Url1, 2013) [Cite_Ref] , MeSH and SNOMED CT and (2) Discourse Knowledge driven by the fact that the narratives adhere to a specific writing style.","Url1. 2013. Umls: Unified medical language system (http://www.nlm.nih.gov/research/umls/) (ac-cessed july 1, 2013).","Clinical narratives, unlike newswire data, provide a domain with significant knowledge that can be ex-ploited systematically to improve the accuracy of the prediction task. Knowledge in this domain can be thought of as belonging to two categories: (1) Background Knowledge captured in medical ontolo-gies like UMLS (Url1, 2013) [Cite_Ref] , MeSH and SNOMED CT and (2) Discourse Knowledge driven by the fact that the narratives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the lat-ter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not. We exploit this structural knowledge in our global inference for-mulation.",Material,Knowledge,False,Introduce（引用目的）,True,D13-1186_1_0,2013,Using Soft Constraints in Joint Inference for Clinical Concept Recognition,Reference
1590,11594," http://metamap.nlm.nih.gov/"," ['2 Identifying Medical Concepts']","Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013 [Cite_Ref] ), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) of words in a window of size 3.","Url2. 2013. Metamap (http://metamap.nlm.nih.gov/) (accessed july 1, 2013).","Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013 [Cite_Ref] ), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) of words in a window of size 3. We also use conjunctions of the features.",Material,Knowledge,True,Use（引用目的）,True,D13-1186_2_0,2013,Using Soft Constraints in Joint Inference for Clinical Concept Recognition,Reference
1591,11595," http://cogcomp.cs.illinois.edu/page/softwareview/"," ['2 Identifying Medical Concepts']","Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) [Cite_Ref] of words in a window of size 3.","Url3. 2013. Illinois part-of-speech tagger (http://cogcomp.cs.illinois.edu/page/softwareview/ pos) (accessed july 1, 2013).","Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) [Cite_Ref] of words in a window of size 3. We also use conjunctions of the features.",Method,Tool,True,Use（引用目的）,True,D13-1186_3_0,2013,Using Soft Constraints in Joint Inference for Clinical Concept Recognition,Reference
1592,11596," http://www.nlm.nih.gov/mesh/meshhome.html"," ['2 Identifying Medical Concepts']","Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) [Cite_Ref] and SNOMED CT (Url5, 2013), MeSH and SNOMED CT descriptors.","Url4. 2013. Mesh: Medical subject headings (http://www.nlm.nih.gov/mesh/meshhome.html) (ac-cessed july 1, 2013).","After finding concept boundaries, we determine the probability distribution for each concept over 4 possible types (TEST, TRE, PROB or NULL). These probability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011). Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) [Cite_Ref] and SNOMED CT (Url5, 2013), MeSH and SNOMED CT descriptors.",Material,Knowledge,True,Use（引用目的）,True,D13-1186_4_0,2013,Using Soft Constraints in Joint Inference for Clinical Concept Recognition,Reference
1593,11597," http://www.ihtsdo.org/snomed-ct/"," ['2 Identifying Medical Concepts']","Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) and SNOMED CT (Url5, 2013) [Cite_Ref] , MeSH and SNOMED CT descriptors.","Url5. 2013. Snomed ct: Snomed clinical terms (http://www.ihtsdo.org/snomed-ct/) (accessed july 1, 2013).","After finding concept boundaries, we determine the probability distribution for each concept over 4 possible types (TEST, TRE, PROB or NULL). These probability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011). Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) and SNOMED CT (Url5, 2013) [Cite_Ref] , MeSH and SNOMED CT descriptors.",Material,Knowledge,True,Use（引用目的）,True,D13-1186_5_0,2013,Using Soft Constraints in Joint Inference for Clinical Concept Recognition,Reference
1594,11598," http://www.gurobi.com/"," ['3 Modeling Global Inference', '3.2 Final Optimization Problem - An IQP']","We used Gurobi toolkit (Url6, 2013) [Cite_Ref] to solve such IQPs.","Url6. 2013. Gurobi optimization toolkit (http://www.gurobi.com/) (accessed july 1, 2013).","After incorporating all the constraints mentioned above, the final optimization problem (an IQP) is shown in Figure 2. We used Gurobi toolkit (Url6, 2013) [Cite_Ref] to solve such IQPs. In our case, it solves 76 IQPs per second on a quad-core server with In-tel Xeon X5650 @ 2.67 GHz processors and 50 GB RAM.",Method,Tool,True,Use（引用目的）,True,D13-1186_6_0,2013,Using Soft Constraints in Joint Inference for Clinical Concept Recognition,Reference
1595,11599," https://www.ldc.upenn.edu/collaborations/past-projects/ace"," ['4 Research Plan']","In ad-dition, ACE [Cite_Footnote_1] and NNE (Ringland, 2016) are newswire corpora with nested entity mentions.",1 https://www.ldc.upenn.edu/collaborations/past-projects/ace,"Besides employing active learning to create spe-cific data set with nested, overlapping, discontinu-ous entity mentions, we notice that there are some off-the-shelf corpora in biomedical domain that we can use to evaluate our proposed methods, al-though, to our knowledge, none of these data sets contains all three kinds of complex mentions, e.g., GENIA (Kim et al., 2003) only contains nested en-tity mentions, and CADEC (Karimi et al., 2015) and SemEval2014 (Pradhan et al., 2014) contain overlapping and discontinuous mentions. In ad-dition, ACE [Cite_Footnote_1] and NNE (Ringland, 2016) are newswire corpora with nested entity mentions.",Material,Dataset,True,Compare（引用目的）,True,P18-3006_0_0,2018,Recognizing Complex Entity Mentions: A Review and Future Directions,Footnote
1596,11600," http://ai.stanford.edu/"," ['References']","We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper-forms competitive baselines and other neural language models. [Cite_Footnote_1]",1 The dataset and word vectors can be downloaded at http://ai.stanford.edu/ ∼ ehhuang/.,"Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one represen-tation per word. This is problematic because words are often polysemous and global con-text can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the se-mantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning mul-tiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper-forms competitive baselines and other neural language models. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P12-1092_0_0,2012,Improving Word Representations via Global Context and Multiple Word Prototypes,Footnote
1597,11601," http://tiny.cc/ancient-text-restoration"," ['1 Introduction']","To aid and encourage future research, P YTHIA and PHI-ML have been open-sourced at [Cite] http://tiny.cc/ancient-text-restoration.",,"Restoring text is a complex and time-consuming task (Woodhead, 1967; Mattingly, 1996). Epigraphists rely on accessing vast repositories of information to find textual and contextual “parallels” (recurring expressions in similar documents). These repositories primarily consist in a researcher’s mnemonic repertoire of such parallels, and in digital corpora for performing “string matching” searches (The Packard Human-ities Institute, 2005; Clauss, 2012). However, minor differences in the search query can exclude or obfuscate relevant results, making it hard to estimate the true probability distribution of pos-sible restorations. To the best of our knowledge, this is the first work to bypass the constraints of current epigraphic methods by means of a fully automated deep learning model, P YTHIA , which aids the task of ancient text restoration. It is supplemented by PHI-ML, an epigraphic dataset of a machine actionable text. P YTHIA takes as input a sequence of damaged text, and is Figure 1: Damaged inscription: a decree concerning the Acropolis of Athens (485/4 BCE). IG I 3 4B. (CC trained to predict character sequences comprising the hypothesised restorations. It works both at a character- and a word-level, thereby effectively handling incomplete or missing words. P YTHIA can furthermore be used by all disciplines dealing with ancient texts (philology, papyrology, codi-cology) and applies to any language (ancient or modern). To aid and encourage future research, P YTHIA and PHI-ML have been open-sourced at [Cite] http://tiny.cc/ancient-text-restoration.",Mixed,Mixed,True,Produce（引用目的）,False,D19-1668_0_0,2019,Restoring ancient text using deep learning: a case study on Greek epigraphy,Body
1598,11602,http://www.dh.uni-leipzig.de/wo/projects/open-greek-and-latin-project/,[\'3 Generating PHI-ML\'],"We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) [Cite_Ref] for ancient literary texts.","Gregory Ralph Crane, Monica Berti, Annette Geßner, Matthew Munson, and Tabea Selle. 2014. Open greek and latin project. http://www.dh.uni-leipzig.de/wo/projects/open-greek-and-latin-project/. Ac-cessed on 2019-04-24.","Due to availability of digitised epigraphic corpora, P YTHIA has been trained on ancient Greek (hence-forth, ""AG"") inscriptions, written in the ancient Greek language between 7 th century BCE and 5 th century CE. We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) [Cite_Ref] for ancient literary texts.",Material,Dataset,False,Introduce（引用目的）,True,D19-1668_1_0,2019,Restoring ancient text using deep learning: a case study on Greek epigraphy,Reference
1599,11603,https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions,[\'3 Generating PHI-ML\'],"We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017 [Cite_Ref] ) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) for ancient literary texts.",Laura Gawlinski. 2017. Review: Packard hu-manities institute’s searchable greek inscrip-tions. https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions. Accessed on 2019-08-26.,"Due to availability of digitised epigraphic corpora, P YTHIA has been trained on ancient Greek (hence-forth, ""AG"") inscriptions, written in the ancient Greek language between 7 th century BCE and 5 th century CE. We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017 [Cite_Ref] ) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) for ancient literary texts.",Material,Dataset,False,Introduce（引用目的）,True,D19-1668_2_0,2019,Restoring ancient text using deep learning: a case study on Greek epigraphy,Reference
1600,11604," http://people.cs.kuleuven.be/∼ivan.vulic/software/"," ['4 Experimental Setup']","In total, we have designed 360 sentences for each language pair (ES/IT/NL-EN), 1080 sentences in total. [Cite_Footnote_1] .",1 Available at http://people.cs.kuleuven.be/∼ivan.vulic/software/,"Test Data. We have constructed test datasets in Spanish (ES), Italian (IT) and Dutch (NL), where the aim is to find their correct translation in En-glish (EN) given the sentential context. We have selected 15 polysemous nouns (see tab. 2 for the list of nouns along with their possible transla-tions) in each of the 3 languages, and have man-ually extracted 24 sentences (not present in the training data) for each noun that capture different meanings of the noun from Wikipedia. In order to construct datasets that are balanced across dif-ferent possible translations of a noun, in case of q different translation candidates in T for some word w S1 , the dataset contains exactly 24/q sen-tences for each translation from T . In total, we have designed 360 sentences for each language pair (ES/IT/NL-EN), 1080 sentences in total. [Cite_Footnote_1] . We have used 5 extra nouns with 20 sentences each as a development set to tune the parameters of our models. As a by-product, we have built an initial repository of ES/IT/NL ambiguous words. Tab. 1 presents a small sample from the IT evaluation dataset, and illustrates the task of suggesting word translations in context.",Method,Tool,False,Produce（引用目的）,False,D14-1040_0_0,2014,Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data,Footnote
1601,11605," http://chasen.naist.jp/hiki/ChaSen/"," ['4 Experiments', '4.1 Setup']",We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen [Cite_Footnote_1] 2.3.3 and then labeled the NEs.,1 http://chasen.naist.jp/hiki/ChaSen/ (in Japanese),"We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen [Cite_Footnote_1] 2.3.3 and then labeled the NEs. Unreadable to-kens such as parentheses were removed in to-kenization. After tokenization, the text cor-pus had 264,388 words of 60 part-of-speech types. Since three different kinds of charac-ters are used in Japanese, the character types used as features included: single-kanji (words written in a single Chinese charac-ter), all-kanji (longer words written in Chi-nese characters), hiragana (words written in hiragana Japanese phonograms), katakana (words written in katakana Japanese phono-grams), number, single-capital (words with a single capitalized letter), all-capital, capitalized (only the first letter is capital-ized), roman (other roman character words), and others (all other words). We used all the fea-tures that appeared in each training set (no feature selection was performed). The chunking states in-cluded in the NE classes were: BEGIN (beginning of a NE), MIDDLE (middle of a NE), END (ending of a NE), and SINGLE (a single-word NE). There were 33 NE classes (eight categories * four chunk-ing states + OTHER), and therefore we trained 33 SVMs to distinguish words of a class from words of other classes. For NER, we used an SVM-based chunk annotator YamCha 2 0.33 with a quadratic kernel (1 + x~ · y~) and a soft margin parameter of SVMs C=0.1 for training and applied sigmoid function s n (x) with β n =1.0 and Viterbi search to the SVMs’ outputs. These parameters were exper-imentally chosen using the test set.",Method,Tool,True,Use（引用目的）,True,P06-1078_0_0,2006,Incorporating speech recognition confidence into discriminative named entity recognition of speech data,Footnote
1602,11606," http://www.chasen.org/~taku/software/yamcha/"," ['4 Experiments', '4.1 Setup']","For NER, we used an SVM-based chunk annotator YamCha 2 0.33 with a quadratic kernel (1 + x~ · y~) [Cite_Footnote_2] and a soft margin parameter of SVMs C=0.1 for training and applied sigmoid function s n (x) with β n =1.0 and Viterbi search to the SVMs’ outputs.",2 http://www.chasen.org/˜taku/software/yamcha/,"We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen 2.3.3 and then labeled the NEs. Unreadable to-kens such as parentheses were removed in to-kenization. After tokenization, the text cor-pus had 264,388 words of 60 part-of-speech types. Since three different kinds of charac-ters are used in Japanese, the character types used as features included: single-kanji (words written in a single Chinese charac-ter), all-kanji (longer words written in Chi-nese characters), hiragana (words written in hiragana Japanese phonograms), katakana (words written in katakana Japanese phono-grams), number, single-capital (words with a single capitalized letter), all-capital, capitalized (only the first letter is capital-ized), roman (other roman character words), and others (all other words). We used all the fea-tures that appeared in each training set (no feature selection was performed). The chunking states in-cluded in the NE classes were: BEGIN (beginning of a NE), MIDDLE (middle of a NE), END (ending of a NE), and SINGLE (a single-word NE). There were 33 NE classes (eight categories * four chunk-ing states + OTHER), and therefore we trained 33 SVMs to distinguish words of a class from words of other classes. For NER, we used an SVM-based chunk annotator YamCha 2 0.33 with a quadratic kernel (1 + x~ · y~) [Cite_Footnote_2] and a soft margin parameter of SVMs C=0.1 for training and applied sigmoid function s n (x) with β n =1.0 and Viterbi search to the SVMs’ outputs. These parameters were exper-imentally chosen using the test set.",Method,Tool,True,Use（引用目的）,True,P06-1078_1_0,2006,Incorporating speech recognition confidence into discriminative named entity recognition of speech data,Footnote
1603,11607," http://archive.ics.uci.edu/ml"," ['3 Learning Classifiers from Language', '3.1 Mapping language to constraints', '3.1.1 Semantic Parser components']","In this work, we trained this classifier based on a manually annotated set of 80 sentences describing classes in the small UCI Zoo dataset (Lichman, 2013) [Cite_Ref] , and used this model for all experiments.",M. Lichman. 2013. UCI machine learning repository. http://archive.ics.uci.edu/ml.,"Since the constraint type is determined by syntactic and dependency parse features, this component does not need to be retrained for new domains. In this work, we trained this classifier based on a manually annotated set of 80 sentences describing classes in the small UCI Zoo dataset (Lichman, 2013) [Cite_Ref] , and used this model for all experiments.",Material,Dataset,False,Use（引用目的）,True,P18-1029_2_0,2018,Zero-shot Learning of Classifiers from Natural Language Quantification,Reference
1604,11608," http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN"," ['5 Experimental Settings']","We used JUMAN (Kurohashi and Kawahara, 2009a) [Cite_Ref] for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005.","Sadao Kurohashi and Daisuke Kawahara, 2009a. Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.","We used JUMAN (Kurohashi and Kawahara, 2009a) [Cite_Ref] for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence com-pression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the re-source for detecting obligatory or adjacent cases.",Method,Tool,True,Use（引用目的）,True,P13-1101_0_0,2013,Subtree Extractive Summarization via Submodular Maximization,Reference
1605,11609," http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP"," ['5 Experimental Settings']","For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b) [Cite_Ref] .","Sadao Kurohashi and Daisuke Kawahara, 2009b. KN parser (Kurohashi-Nagao parser) 3.0 Users Man-ual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.","We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b) [Cite_Ref] . Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence com-pression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the re-source for detecting obligatory or adjacent cases.",Method,Tool,True,Use（引用目的）,True,P13-1101_1_0,2013,Subtree Extractive Summarization via Submodular Maximization,Reference
1606,11610," http://www.cnts.ua.ac.be/conll2000/chunking/output.html"," ['6 Experiments', '6.2 Baselines']","In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script [Cite_Footnote_3]",3 http://www.cnts.ua.ac.be/conll2000/chunking/output.html,"To apply these methods except for Target, we treat each of the eight domains in turn as the test domain, with one of remaining seven domain as the source domain. As in general domain adap-tation setting, we assume that the source domain has a sufficient amount of labeled data but the tar-get domain has an insufficient amount of labeled data. Specifically, For each test or target domain, we only use 10% of the training examples to sim-ulate data scarcity. In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script [Cite_Footnote_3]",Method,Code,False,Use（引用目的）,True,P15-1046_0_0,2015,New Transfer Learning Techniques for Disparate Label Sets,Footnote
1607,11611," https://github.com/deansong/contextLSTMCNN"," ['angus.roberts@kcl.ac.uk']","We therefore introduce a new method, Context-LSTM-CNN [Cite_Footnote_1] , which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method (Zhang et al., 2015), and an architecture that combines an LSTM and CNN for the focus sentence.",1 The code is publicly available at https://github.com/deansong/contextLSTMCNN,"In many cases, however, the context available is of significant size. We therefore introduce a new method, Context-LSTM-CNN [Cite_Footnote_1] , which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method (Zhang et al., 2015), and an architecture that combines an LSTM and CNN for the focus sentence. The method consis-tently improves over results obtained from either LSTM alone, CNN alone, or these two combined, with little increase in training time.",Method,Code,True,Use（引用目的）,True,D18-1107_0_0,2018,A Deep Neural Network Sentence Level Classification Method with Context Information,Footnote
1608,11612," http://sail.usc.edu/iemocap/iemocap_release.htm"," ['4 Experiments']","Interactive Emotional Dyadic Motion Cap-ture Database (Busso et al., 2008) [Cite_Footnote_2] (IEMO-CAP).",2 http://sail.usc.edu/iemocap/iemocap_release.htm,"Interactive Emotional Dyadic Motion Cap-ture Database (Busso et al., 2008) [Cite_Footnote_2] (IEMO-CAP). Originally created for the analysis of hu-man emotions based on speech and video, a tran-script of the speech component is available for NLP research. Each sentence in the dialogue is annotated with one of 10 types of emotion. There is a class imbalance in the labelled data, and so we follow the approach of (Chernykh et al., 2017), and only use sentences classified with one of four labels (‘Anger’, ‘Excitement’, ‘Neutral’ and ‘Sad-ness’). For this dataset, instead of using left and right contexts, we assign all sentences from one person to one context and all sentences from the other person to the other context. While only the sentences with the four classes of interest are used for classification, all sentences of the dialog are used as the context. This results in a set of 4936 la-belled sentences with average sentence length 14, and average document length is 986.",Material,Dataset,True,Produce（引用目的）,False,D18-1107_1_0,2018,A Deep Neural Network Sentence Level Classification Method with Context Information,Footnote
1609,11613," https://sites.google.com/site/adecorpus/home/document"," ['4 Experiments']","Drug-related Adverse Effects (Gurulingappa et al., 2012) [Cite_Footnote_3] (ADE).",3 https://sites.google.com/site/adecorpus/home/document,"Drug-related Adverse Effects (Gurulingappa et al., 2012) [Cite_Footnote_3] (ADE). This dataset contains sen-tences sampled from the abstracts of medical case reports. For each sentence, the annotation indi-cates whether adverse effects of a drug are be-ing described (‘Positive’) or not (‘Negative’). The original release of the data does not contain the document context, which we reconstructed from PubMed . Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129.",Material,Dataset,True,Introduce（引用目的）,True,D18-1107_2_0,2018,A Deep Neural Network Sentence Level Classification Method with Context Information,Footnote
1610,11614," https://www.ncbi.nlm.nih.gov/pubmed/"," ['4 Experiments']","The original release of the data does not contain the document context, which we reconstructed from PubMed [Cite_Footnote_4] .",4 https://www.ncbi.nlm.nih.gov/pubmed/,"Drug-related Adverse Effects (Gurulingappa et al., 2012) (ADE). This dataset contains sen-tences sampled from the abstracts of medical case reports. For each sentence, the annotation indi-cates whether adverse effects of a drug are be-ing described (‘Positive’) or not (‘Negative’). The original release of the data does not contain the document context, which we reconstructed from PubMed [Cite_Footnote_4] . Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129.",Material,DataSource,True,Use（引用目的）,True,D18-1107_3_0,2018,A Deep Neural Network Sentence Level Classification Method with Context Information,Footnote
1611,11615," https://github.com/attardi/wikiextractor"," ['3 Experimental settings']","More concretely, we use WikiExtractor [Cite_Footnote_4] to ex-tract plain text from Wikipedia dumps, and pre-process the resulting corpus using standard Moses tools (Koehn et al., 2007) by applying sentence splitting, punctuation normalization, tokenization with aggressive hyphen splitting, and lowercasing.",4 https://github.com/attardi/ wikiextractor,"More concretely, we use WikiExtractor [Cite_Footnote_4] to ex-tract plain text from Wikipedia dumps, and pre-process the resulting corpus using standard Moses tools (Koehn et al., 2007) by applying sentence splitting, punctuation normalization, tokenization with aggressive hyphen splitting, and lowercasing. We then train word embeddings for each language using the skip-gram implementation of fastText (Bojanowski et al., 2017) with default hyperpa-rameters, restricting the vocabulary to the 200,000 most frequent tokens. The official embeddings in the MUSE dataset were trained using these exact same settings, so our embeddings only differ in the Wikipedia dump used to extract the training cor-pus and the pre-processing applied to it, which is not documented in the original dataset.",Method,Tool,True,Use（引用目的）,True,P19-1494_0_0,2019,Bilingual Lexicon Induction through Unsupervised Machine Translation,Footnote
1612,11616," https://github.com/artetxem/monoses"," ['6 Conclusions and future work']",Our code is available at [Cite] https://github.com/artetxem/monoses.,,"We propose a new approach to BLI which, instead of directly inducing bilingual dictionaries from cross-lingual embedding mappings, uses them to build an unsupervised machine translation system, which is then used to generate a synthetic paral-lel corpus from which to extract bilingual lexica. Our approach does not require any additional re-source besides the monolingual corpora used to train the embeddings, and outperforms traditional retrieval techniques by a substantial margin. We thus conclude that, contrary to recent trend, future work in BLI should not focus exclusively in direct retrieval approaches, nor should BLI be the only evaluation task for cross-lingual embeddings. Our code is available at [Cite] https://github.com/artetxem/monoses.",Method,Code,True,Produce（引用目的）,True,P19-1494_1_0,2019,Bilingual Lexicon Induction through Unsupervised Machine Translation,Body
1613,11617," http://reporterslab.org/fact-checking/"," ['1 Introduction']","Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. [Cite_Footnote_2]",2 http://reporterslab.org/fact-checking/,"Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. [Cite_Footnote_2] While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.",補足資料,Document,False,Introduce（引用目的）,False,2020.acl-main.332_0_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1614,11618," http://www.politifact.com/"," ['1 Introduction']","While some organizations debunked just a couple of hundred claims, others such as Politifact, [Cite_Footnote_3] FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.",3 http://www.politifact.com/ 4 http://www.factcheck.org/,"Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, [Cite_Footnote_3] FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.",補足資料,Website,True,Compare（引用目的）,True,2020.acl-main.332_1_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1615,11619," http://www.factcheck.org/"," ['1 Introduction']","While some organizations debunked just a couple of hundred claims, others such as Politifact, [Cite_Footnote_3] FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.",3 http://www.politifact.com/ 4 http://www.factcheck.org/,"Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, [Cite_Footnote_3] FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.",補足資料,Website,True,Compare（引用目的）,True,2020.acl-main.332_2_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1616,11620," http://www.snopes.com/"," ['1 Introduction']","While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, [Cite_Footnote_5] and Full Fact have fact-checked thou-sands or even tens of thousands of claims.",5 http://www.snopes.com/,"Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, [Cite_Footnote_5] and Full Fact have fact-checked thou-sands or even tens of thousands of claims.",補足資料,Website,True,Compare（引用目的）,True,2020.acl-main.332_3_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1617,11621," http://fullfact.org/"," ['1 Introduction']","While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, and Full Fact [Cite_Footnote_6] have fact-checked thou-sands or even tens of thousands of claims.",6 http://fullfact.org/,"Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, and Full Fact [Cite_Footnote_6] have fact-checked thou-sands or even tens of thousands of claims.",補足資料,Website,True,Compare（引用目的）,True,2020.acl-main.332_4_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1618,11622," http://tinyurl.com/yblcb5q5"," ['1 Introduction']","Indeed, viral claims of-ten come back after a while in social media, and politicians are known to repeat the same claims over and over again. [Cite_Footnote_7]",7 President Trump has repeated one claim over 80 times: http://tinyurl.com/yblcb5q5.,"From a fact-checkers’ point of view, the abun-dance of previously fact-checked claims increases the likelihood that the next claim that needs to be checked would have been fact-checked already by some trusted organization. Indeed, viral claims of-ten come back after a while in social media, and politicians are known to repeat the same claims over and over again. [Cite_Footnote_7] Thus, before spending hours fact-checking a claim manually, it is worth first making sure that nobody has done it already.",補足資料,Document,True,Introduce（引用目的）,True,2020.acl-main.332_5_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1619,11623," https://github.com/sshaar/That-is-a-Known-Lie"," ['1 Introduction']","• We create a specialized dataset, which we release to the research community. [Cite_Footnote_8]",8 Data and code are available at the following URL: https://github.com/sshaar/ That-is-a-Known-Lie,"• We create a specialized dataset, which we release to the research community. [Cite_Footnote_8] Un-like previous work in fact-checking, which used normalized claims from fact-checking datasets, we work with naturally occurring claims, e.g., in debates or in social media.",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.332_6_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1620,11624," http://toolbox.google.com/factcheck/explorer"," ['2 Related Work']","In an industrial setting, Google has developed Fact Check Explorer, [Cite_Footnote_9] which is an exploration tool that allows users to search a number of fact-checking websites (those that use ClaimReview from schema.org ) for the mentions of a topic, a person, etc.",9 http://toolbox.google.com/factcheck/ explorer,"In an industrial setting, Google has developed Fact Check Explorer, [Cite_Footnote_9] which is an exploration tool that allows users to search a number of fact-checking websites (those that use ClaimReview from schema.org ) for the mentions of a topic, a person, etc. However, the tool cannot handle a complex claim, as it runs Google search, which is not optimized for semantic matching of long claims. While this might change in the future, as there have been reports that Google has started using BERT in its search, at the time of writing, the tool could not handle a long claim as an input.",Method,Tool,True,Introduce（引用目的）,True,2020.acl-main.332_7_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1621,11625," http://schema.org/ClaimReview"," ['2 Related Work']","In an industrial setting, Google has developed Fact Check Explorer, which is an exploration tool that allows users to search a number of fact-checking websites (those that use ClaimReview from schema.org [Cite_Footnote_10] ) for the mentions of a topic, a person, etc.",10 http://schema.org/ClaimReview,"In an industrial setting, Google has developed Fact Check Explorer, which is an exploration tool that allows users to search a number of fact-checking websites (those that use ClaimReview from schema.org [Cite_Footnote_10] ) for the mentions of a topic, a person, etc. However, the tool cannot handle a complex claim, as it runs Google search, which is not optimized for semantic matching of long claims. While this might change in the future, as there have been reports that Google has started using BERT in its search, at the time of writing, the tool could not handle a long claim as an input.",Method,Tool,True,Introduce（引用目的）,True,2020.acl-main.332_8_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1622,11626," http://www.snopes.com/fact-check-ratings/"," ['4 Datasets', '4.2 Snopes Dataset']","We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [Cite_Footnote_13] ).",13 http://www.snopes.com/fact-check-ratings/,"We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [Cite_Footnote_13] ).",Material,Knowledge,False,Use（引用目的）,True,2020.acl-main.332_12_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1623,11627," http://tanbih.qcri.org/"," ['Acknowledgments']","This research is part of the Tanbih project, [Cite_Footnote_14] which aims to limit the effect of ‘fake news,” disinforma-tion, propaganda, and media bias by making users aware of what they are reading.",14 http://tanbih.qcri.org/,"This research is part of the Tanbih project, [Cite_Footnote_14] which aims to limit the effect of ‘fake news,” disinforma-tion, propaganda, and media bias by making users aware of what they are reading.",補足資料,Website,True,Introduce（引用目的）,True,2020.acl-main.332_13_0,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,Footnote
1624,11628," https://github.com/thu-coai/OpenMEVA"," ['∗ Corresponding author']","In addition, the toolkit provides data perturbation techniques for generating customized test cases beyond AU - TOS , which can facilitate fast development of new automatic metrics [Cite_Footnote_1] .","1 All the tools, data, and evaluation scripts are available at https://github.com/thu-coai/OpenMEVA","We also provide an open-source toolkit which implements various metrics, and therefore supports the comparison and analysis of metrics. In addition, the toolkit provides data perturbation techniques for generating customized test cases beyond AU - TOS , which can facilitate fast development of new automatic metrics [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.500_0_0,2021,OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics,Footnote
1625,11629," http://svmlight.joachims.org/svmstruct.html"," ['3 Problem Formulation', '3.3 The Large Margin Model']","(2005) framework for learning Support Vector Machines (SVMs) with structured output spaces, us-ing the SVM struct implementation. [Cite_Footnote_1]",1 http://svmlight.joachims.org/svmstruct.html,"Figure 3: Induced synchronous grammar from a sen-tence pair using a strategy that extracts general rules. grammatical or are poor compressions. The train-ing procedure learns weights such that the model can discriminate between these trees and predict a good target tree. For this we develop a discriminative training process which learns a weighted tree-to-tree transducer. Our model is based on Tsochantaridis et al.’s (2005) framework for learning Support Vector Machines (SVMs) with structured output spaces, us-ing the SVM struct implementation. [Cite_Footnote_1] We briefly sum-marise the approach below; for a more detailed de-scription we refer the interested reader to Tsochan-taridis et al. (2005).",Method,Tool,False,Produce（引用目的）,True,D07-1008_0_0,2007,Large Margin Synchronous Generation and its Application to Sentence Compression,Footnote
1626,11630," http://homepages.inf.ed.ac.uk/s0460084/data/"," ['4 Evaluation Set-up']",We also present results on Clarke and Lapata’s (2006a) Broadcast News corpus. [Cite_Footnote_4],4 The corpus can be downloaded from http://homepages.inf.ed.ac.uk/s0460084/data/.,"Corpora We evaluated our system on two dif-ferent corpora. The first is the compression cor-pus of Knight and Marcu (2002) derived automati-cally from the document-abstract pairs of the Ziff-Davis corpus. Previous compression work has al-most exclusively used this corpus. Our experiments follow Knight and Marcu’s partition of training, test, and development sets (1,002/36/12 instances). We also present results on Clarke and Lapata’s (2006a) Broadcast News corpus. [Cite_Footnote_4] This corpus was created manually (annotators were asked to produce com-pressions for 50 Broadcast news stories) and poses more of a challenge than Ziff-Davis. Being a speech corpus, it often contains incomplete and ungram-matical utterances and speech artefacts such as dis-fluencies, false starts and hesitations. Furthermore, spoken utterances have varying lengths, some are very wordy whereas others cannot be reduced any further. Thus a hypothetical compression system trained on this domain should be able to leave some sentences uncompressed. Again we used Clarke and Lapata’s training, test, and development set split (882/410/78 instances).",Material,Dataset,True,Use（引用目的）,True,D07-1008_1_0,2007,Large Margin Synchronous Generation and its Application to Sentence Compression,Footnote
1627,11631," https://www.research.ibm.com/haifa/dept/vst/mlta_data.shtml"," ['1 Introduction']","We believe that the new TR9856 benchmark, which is freely available for research purposes, [Cite_Footnote_1] along with the reported results, will contribute to the development of novel term relat-edness methods.",1 https://www.research.ibm.com/haifa/dept/vst/mlta_data.shtml,"We report various consistency measures that indicate the validity of TR9856. In addition, we report baseline results over TR9856 for sev-eral methods, commonly used to assess term– relatedness. Furthermore, we demonstrate how the new data can be exploited to train an ensemble– based method, that relies on these methods as un-derlying features. We believe that the new TR9856 benchmark, which is freely available for research purposes, [Cite_Footnote_1] along with the reported results, will contribute to the development of novel term relat-edness methods.",Material,Dataset,True,Produce（引用目的）,True,P15-2069_0_0,2015,TR9856: A Multi-word Term Relatedness Benchmark,Footnote
1628,11632," http://idebate.org/debatabase"," ['3 Dataset generation methodology', '3.1 Defining topics and articles of interest']","Correspondingly, we focus on 47 topics selected from Debatabase [Cite_Footnote_2] .",2 http://idebate.org/debatabase,"We start by observing that framing the related-ness question within a pre-specified context may simplify the task for humans and machines alike, in particular since the correct sense of ambigu-ous terms can be identified. Correspondingly, we focus on 47 topics selected from Debatabase [Cite_Footnote_2] . For each topic, 5 human annotators searched Wikipedia for relevant articles as done in (Aharoni et al., 2014). All articles returned by the annota-tors – an average of 21 articles per topic – were considered in the following steps. The expectation was that articles associated with a particular topic will be enriched with terms related to that topic, hence with terms related to one another.",Material,DataSource,True,Use（引用目的）,True,P15-2069_1_0,2015,TR9856: A Multi-word Term Relatedness Benchmark,Footnote
1629,11633," https://github.com/madaan/thinkaboutit"," ['References']","This result is significant as it il-lustrates that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reflexively. [Cite_Footnote_1]",1 Data and code located at https://github.com/madaan/thinkaboutit,"Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning sug-gests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural mod-els can similarly benefit from envisioning the question scenario before answering a defeasi-ble query. Our approach is, given a question, to have a model first create a graph of relevant influences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS , achieves a new state-of-the-art on three different defeasible reason-ing datasets. This result is significant as it il-lustrates that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reflexively. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.508_0_0,2021,Think about it! Improving defeasible reasoning by first modeling the question scenario,Footnote
1630,11634," https://github.com/PyTorchLightning/pytorch-lightning,3"," ['C Hyperparameters']","We base our implementation on PyTorch (Paszke et al., 2017) and also use PyTorch Lightning (Fal-con, 2019) [Cite_Ref] and Huggingface (Wolf et al., 2019).","et al. Falcon, WA. 2019. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning,3.","Training details All of our experiments were done on a single Nvidia GeForceRTX 2080 Ti. We base our implementation on PyTorch (Paszke et al., 2017) and also use PyTorch Lightning (Fal-con, 2019) [Cite_Ref] and Huggingface (Wolf et al., 2019). The gates and the experts in our MoE model were a single layer MLP. For the experts, we set the in-put size set to be the same as output size. Table 7 shows the parameters shared by all the methods, and 8 shows the hyperparameters applicable to GCN encoder.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.508_1_0,2021,Think about it! Improving defeasible reasoning by first modeling the question scenario,Reference
1631,11635," https://github.com/yym6472/OVSlotTagging"," ['1 Introduction']","(3) Experi-ments show that our proposed method consistently outperforms various SOTA baselines, especially in open-vocabulary slot f1. [Cite_Footnote_1]",1 Our code is available at https://github.com/yym6472/OVSlotTagging,"In this paper, we propose a robust adversarial slot filling approach that explicitly decouples local se-mantic representation inherent in open-vocabulary slot words from the global context. Our approach aims to focus more on the holistic semantics at the level of the whole sentence, not only the vicin-ity of the local context within open-vocabulary slots. Specifically, our approach generates model-agnostic adversarial worst-case perturbations to the inputs in the direction that significantly increases the model’s loss. Our main contributions are three-fold: (1) We dive into the issues of open-vocabulary slots in slot filling task and propose a novel adver-sarial semantic decoupling method which distin-guishes local semantics from the global context. (2) Our method can be easily applied to all the pre-vious slot filling neural-based models. (3) Experi-ments show that our proposed method consistently outperforms various SOTA baselines, especially in open-vocabulary slot f1. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.490_0_0,2020,Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots,Footnote
1632,11636," https://groups.csail.mit.edu/sls/downloads/restaurant/"," ['3 Experiment', '3.1 Setup']","Datasets To evaluate our approach, we conduct experiments on two public benchmark datasets, Snips (Coucke et al., 2018) and MIT-restaurant (MR) [Cite_Footnote_4] .",4 https://groups.csail.mit.edu/sls/downloads/restaurant/,"Datasets To evaluate our approach, we conduct experiments on two public benchmark datasets, Snips (Coucke et al., 2018) and MIT-restaurant (MR) [Cite_Footnote_4] . Snips contains user utterances from vari-ous domains resulting in relatively extensive open-vocabulary slots, such as album and movie name. MR is a single-domain dataset associated with restaurant reservations, which contains open-vocabulary slots, such as restaurant name and amenity. Table 1 shows the full statistics and Ta-ble 2 shows all the open-vocabulary slots of Snips and MR datasets. Note that we identify the open-vocabulary slots according to the diversity of dif-ferent slot values as well as the average length of slot values.",Material,Dataset,True,Compare（引用目的）,True,2020.emnlp-main.490_1_0,2020,Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots,Footnote
1633,11637," http://glaros.dtc.umn.edu/gkhome/views/cluto"," ['2 Related Work', '3.2 NE-based Clustering for LM Adaptation']",We use the CLUTO [Cite_Footnote_1] toolkit to perform clustering.,1 Available at http://glaros.dtc.umn.edu/gkhome/views/cluto,"Clustering is a simple unsupervised topic analysis method. We use NEs to construct feature vectors for the documents, rather than considering all the words as in most previous work. We use the CLUTO [Cite_Footnote_1] toolkit to perform clustering. It finds a predefined number of clusters based on a specific criterion, for which we chose the following func-tion: where K is the desired number of clusters, S i is the set of documents belonging to the i th cluster, v and u represent two documents, and sim(v, u) is the similarity between them. We use the cosine dis-tance to measure the similarity between two docu-ments: where vr and ur are the feature vectors represent-ing the two documents respectively, in our experi-ments composed of NEs. For clustering, the elements in every feature vector are scaled based on their term frequency and inverse document fre-quency, a concept widely used in information re-trieval.",Method,Tool,True,Use（引用目的）,True,P07-1085_0_0,2007,Unsupervised Language Model Adaptation Incorporating Named Entity Information,Footnote
1634,11638," http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf"," ['2 Related Work', '4.3 Clustering vs. LDA Based LM Adaptation']","We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [Cite_Footnote_2] .",2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf,"We suspect that using only named entities may not provide enough information about the ‘topics’ of the documents, therefore we investigate expanding the feature vectors with other words. Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [Cite_Footnote_2] . This is similar to the removal of stop words widely used in information retrieval.",Material,Knowledge,True,Use（引用目的）,True,P07-1085_1_0,2007,Unsupervised Language Model Adaptation Incorporating Named Entity Information,Footnote
1635,11639," https://github.com/TemporalKGTeam/TANGO"," ['4 Experiments', '4.1 Experimental Setup']",We evaluate our model by performing future link prediction on five tKG datasets [Cite_Footnote_1] .,1 Code and datasets are available at https://github.com/TemporalKGTeam/TANGO. filtered MRR (%) on ICEWS05-15 with regard to dif-ferent ∆t. preprint arXiv:2012.08492.,"We evaluate our model by performing future link prediction on five tKG datasets [Cite_Footnote_1] . We compare TANGO’s performance with several existing meth-ods and evaluate its potential with inductive link prediction and long horizontal link forecasting. Be-sides, an ablation study is conducted to show the effectiveness of our graph transition layer. 4.1.1 Datasets We use five benchmark datasets to evaluate TANGO: 1) ICEWS14 (Trivedi et al., 2017) 2) ICEWS18 (Boschee et al., 2015) 3) ICEWS05-15 (García-Durán et al., 2018) 4) YAGO (Mahdisoltani et al., 2013) 5) WIKI (Leblay and Chekol, 2018). Integrated Crisis Early Warning System (ICEWS) (Boschee et al., 2015) is a dataset consisting of timestamped political events, e.g., (Barack Obama, visit, India, 2015-01-25). Specifically, ICEWS14 contains events occurring in 2014, while ICEWS18 contains events from January 1, 2018, to Octo-ber 31, 2018. ICEWS05-15 is a long-term dataset that contains the events between 2005 and 2015. WIKI and YAGO are two subsets extracted from Wikipedia and YAGO3 (Mahdisoltani et al., 2013), respectively. The details of each dataset and the dataset split strategy are provided in Appendix D.",Material,Dataset,True,Compare（引用目的）,True,2021.emnlp-main.658_0_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Footnote
1636,11640," https://openreview.net/pdf?id=rke2P1BFwS"," ['† Corresponding author.']","While lots of work (García-Durán et al., 2018; Goel et al., 2020; Lacroix et al., 2020 [Cite_Ref] ) focus on the tem-poral KG completion task and predict missing links at observed timestamps, recent work (Jin et al., 2019; Trivedi et al., 2017) paid attention to forecast future links of temporal KGs.","Timothee Lacroix, Guillaume Obozinski, and Nico-las Usunier. 2020. Tensor decompositions for tem-poral knowledge base completion. ICLR preprint https://openreview.net/pdf?id=rke2P1BFwS.","A tKG represents a temporal fact as a quadruple (s, r, o, t) by extending a static triple with time t, describing that this fact is valid at time t. In recent years, several sizable temporal knowledge graphs, such as ICEWS (Boschee et al., 2015), have been developed that provide widespread availability of such data and enable reasoning on temporal KGs. While lots of work (García-Durán et al., 2018; Goel et al., 2020; Lacroix et al., 2020 [Cite_Ref] ) focus on the tem-poral KG completion task and predict missing links at observed timestamps, recent work (Jin et al., 2019; Trivedi et al., 2017) paid attention to forecast future links of temporal KGs. In this work, we focus on the temporal KG forecasting task, which is more challenging than the completion task.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.658_1_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Reference
1637,11641," https://openreview.net/pdf?id=rke2P1BFwS"," ['4 Experiments', '4.1.3 Baseline Methods']","For tKG baselines, we report the performance of TTransE (Leblay and Chekol, 2018), TA-Distmult (García-Durán et al., 2018), CyGNet (Zhu et al., 2020), DE-SimplE (Goel et al., 2020), TNTComplEx (Lacroix et al., 2020) [Cite_Ref] , and RE-Net (Jin et al., 2019).","Timothee Lacroix, Guillaume Obozinski, and Nico-las Usunier. 2020. Tensor decompositions for tem-poral knowledge base completion. ICLR preprint https://openreview.net/pdf?id=rke2P1BFwS.","We compare our model performance with nine base-lines. We take three static KG models as the static baselines, including Distmult (Yang et al., 2014), TuckER (Balazevic et al., 2019), and COMPGCN (Vashishth et al., 2019). For tKG baselines, we report the performance of TTransE (Leblay and Chekol, 2018), TA-Distmult (García-Durán et al., 2018), CyGNet (Zhu et al., 2020), DE-SimplE (Goel et al., 2020), TNTComplEx (Lacroix et al., 2020) [Cite_Ref] , and RE-Net (Jin et al., 2019). We provide implementation details of baselines and TANGO in Appendix C.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.658_1_1,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Reference
1638,11642," https://github.com/ibalazevic/TuckER"," ['C Implementation Details']","We use the official implementation of TuckER [Cite_Footnote_2] , COMPGCN , and RE-Net .",2 https://github.com/ibalazevic/TuckER,"We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER [Cite_Footnote_2] , COMPGCN , and RE-Net . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE 5 , TNTComplEx 6 , and CyGNet 7 . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.658_2_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Footnote
1639,11643," https://github.com/malllabiisc/CompGCN"," ['C Implementation Details']","We use the official implementation of TuckER , COMPGCN [Cite_Footnote_3] , and RE-Net .",3 https://github.com/malllabiisc/CompGCN,"We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER , COMPGCN [Cite_Footnote_3] , and RE-Net . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE 5 , TNTComplEx 6 , and CyGNet 7 . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.658_3_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Footnote
1640,11644," https://github.com/INK-USC/RE-Net"," ['C Implementation Details']","We use the official implementation of TuckER , COMPGCN , and RE-Net [Cite_Footnote_4] .",4 https://github.com/INK-USC/RE-Net,"We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER , COMPGCN , and RE-Net [Cite_Footnote_4] . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE 5 , TNTComplEx 6 , and CyGNet 7 . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.658_4_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Footnote
1641,11645," https://github.com/BorealisAI/de-simple"," ['C Implementation Details']","[Cite_Footnote_5] , TNTComplEx , and CyGNet .",5 https://github.com/BorealisAI/de-simple,"We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER 2 , COMPGCN 3 , and RE-Net 4 . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE [Cite_Footnote_5] , TNTComplEx , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.",Method,Tool,True,Extend（引用目的）,True,2021.emnlp-main.658_5_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Footnote
1642,11646," https://github.com/facebookresearch/tkbc"," ['C Implementation Details']","We use the released code to imple-ment DE-SimplE , TNTComplEx [Cite_Footnote_6] , and CyGNet .",6 https://github.com/facebookresearch/tkbc,"We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER 2 , COMPGCN 3 , and RE-Net 4 . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx [Cite_Footnote_6] , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.",Method,Tool,True,Extend（引用目的）,True,2021.emnlp-main.658_6_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Footnote
1643,11647," https://github.com/CunchaoZ/CyGNet"," ['C Implementation Details']","We use the released code to imple-ment DE-SimplE , TNTComplEx , and CyGNet [Cite_Footnote_7] .",7 https://github.com/CunchaoZ/CyGNet,"We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER 2 , COMPGCN 3 , and RE-Net 4 . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing. Besides, we set the history length of RE-Net to 10 and use the max-pooling in the global model. Additionally, we use the implementation of TTransE and TA-Distmult provided in (Jin et al., 2019). For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx , and CyGNet [Cite_Footnote_7] . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.",Method,Tool,True,Extend（引用目的）,True,2021.emnlp-main.658_7_0,2021,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,Footnote
1644,11648," http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/"," ['2 The Humor Dataset', '2.1 Task Description']","We allow for replacement of only those entities that are well-known, according to the Microsoft Knowledge Base [Cite_Footnote_1] .",1 http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/,"To identify the replaceable entities, we apply named entity recognition (NER) and POS tag-ging using the Stanford CoreNLP toolkit (Man-ning et al., 2014). We allow for replacement of only those entities that are well-known, according to the Microsoft Knowledge Base [Cite_Footnote_1] . This improves the likelihood that the terms are familiar to both headline editors and humor judges. We allow a noun (or verb) to be replaced if it is an unambigu-ous noun (or verb) in WordNet (Fellbaum, 1998) (i.e., has a single WordNet POS). Editors are only allowed to replace one of the selected replaceable words/entities in the headline.",補足資料,Document,True,Use（引用目的）,True,N19-1012_0_0,2019,“President Vows to Cut Taxes Hair”: Dataset and Analysis of Creative Text Editing for Humorous Headlines,Footnote
1645,11649," https://cloud.google.com/bigquery"," ['2 The Humor Dataset', '2.2 Collecting Headlines']",We obtain all Reddit posts from the popular sub-reddits r/worldnews and r/politics from January 2017 to May 2018 using Google Big-Query [Cite_Footnote_2] .,2 https://cloud.google.com/bigquery,"We obtain all Reddit posts from the popular sub-reddits r/worldnews and r/politics from January 2017 to May 2018 using Google Big-Query [Cite_Footnote_2] . Each of these posts is a headline from a news source. We remove duplicate headlines and headlines that have fewer than 4 words or more than 20 words. Finally, we keep only the headlines from the 25 English news sources that contribute the most headlines in the Reddit data, resulting in a total of 287,076 news headlines.",Method,Tool,True,Use（引用目的）,True,N19-1012_1_0,2019,“President Vows to Cut Taxes Hair”: Dataset and Analysis of Creative Text Editing for Humorous Headlines,Footnote
1646,11650," http://en.wikipedia.org/wiki/Summaryjudgment"," ['2 Docket Entry Classification']",Summary Judgment is a le-gal term which means that a court has made a deter-mination (a judgment) without a full trial. [Cite_Footnote_1],"1 See e.g., Wikipedia for more information: http://en.wikipedia.org/wiki/Summary judgment","In this work, we are primarily interested in iden-tifying one such complex event called “Order re: Summary Judgment”. Summary Judgment is a le-gal term which means that a court has made a deter-mination (a judgment) without a full trial. [Cite_Footnote_1] Such a judgment may be issued as to the merits of an entire case, or of specific issues in that case. Typically, one of the parties (plaintiff or defendant) involved in the case moves a motion for summary judgment, (usu-ally) in an attempt to eliminate the risk of losing a trial. In an “Order re: Summary Judgment” event, the court may grant or deny a motion for summary judgment upon inspecting all the evidence and facts in the case. The task then, is to identify all docket entries in a set of cases that list occurrences of “Or-der re: Summary Judgment” events. We will call them OSJ events in short.",補足資料,Document,True,Introduce（引用目的）,True,D08-1046_0_0,2008,Legal Docket-Entry Classification: Where Machine Learning stumbles,Footnote
1647,11651," http://www.csie.ntu.edu.tw/∼cjlin/libsvm/"," ['3 Experiments and results', '3.1 Basic SVM']",First we implemented the standard linear SVM [Cite_Footnote_6] on this problem with only word-based features (uni-grams and bigrams) as the input.,6 All our SVM experiments were performed us-ing the libsvm implementation downloadable from http://www.csie.ntu.edu.tw/∼cjlin/libsvm/,"First we implemented the standard linear SVM [Cite_Footnote_6] on this problem with only word-based features (uni-grams and bigrams) as the input. Quite surprisingly, the model achieves an F1 score of only 79.44% as shown in entry 1 of table 5. On inspection, we no-ticed that the SVM assigns high weights to many spurious features owing to their strong correlation with the class.",Method,Tool,True,Use（引用目的）,True,D08-1046_1_0,2008,Legal Docket-Entry Classification: Where Machine Learning stumbles,Footnote
1648,11652," https://github.com/csong27/collision-bert"," ['References']",Our code is available at [Cite] https://github.com/csong27/collision-bert.,,"We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts—including paraphrase identification, document retrieval, response suggestion, and extractive summa-rization—are vulnerable to semantic colli-sions. For example, given a target query, insert-ing a crafted collision into an irrelevant doc-ument can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at [Cite] https://github.com/csong27/collision-bert.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.344_0_0,2020,Adversarial Semantic Collisions Congzheng Song Alexander M. Rush Vitaly Shmatikov,Body
1649,11653," https://github.com/castorini/birch"," ['5 Experiments', '5.1 Tasks and Models']",We use the published mod-els [Cite_Footnote_3] and coefficient values for evaluation.,3 https://github.com/castorini/birch,"Our target model is Birch (Yilmaz et al., 2019a,b). Birch retrieves 1,000 candidate docu-ments using the BM25 and RM3 baseline (Abdul-jaleel et al., 2004) and re-ranks them using the similarity scores from a fine-tuned BERT model. Given a query x q and a document x d , the BERT model assigns similarity scores S(x q , x i ) for each sentence x i in x d . The final score used by Birch for re-reranking is: γ·S BM25 +(1−γ)·P i κ i ·S(x q , x i ) where S BM25 is the baseline BM25 score and γ, κ i are weight coefficients. We use the published mod-els [Cite_Footnote_3] and coefficient values for evaluation.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.344_1_0,2020,Adversarial Semantic Collisions Congzheng Song Alexander M. Rush Vitaly Shmatikov,Footnote
1650,11654," https://parl.ai/docs/zoo.html"," ['5 Experiments', '5.1 Tasks and Models']",We use the published models [Cite_Footnote_4] for evaluation.,4 https://parl.ai/docs/zoo.html,"We use transformer-based Bi- and Poly-encoders that achieved state-of-the-art results on this dataset (Humeau et al., 2020). Bi-encoders com-pute a similarity score for the dialogue context x a and each possible next utterance x b as S(x a , x b ) = f pool (x a ) > f pool (x b ) where f pool (x) ∈ R h is the pooling-over-time representation from transform-ers. Poly-encoders extend Bi-encoders compute S(x a , x b ) = P Ti=1 α i · f(x a ) i> f pool (x b ) where α i is the weight from attention and f(x a ) i is the ith token’s contextualized representation. We use the published models [Cite_Footnote_4] for evaluation.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.344_2_0,2020,Adversarial Semantic Collisions Congzheng Song Alexander M. Rush Vitaly Shmatikov,Footnote
1651,11655," https://github.com/nlpyang/PreSumm"," ['5 Experiments', '5.1 Tasks and Models']",We use the published models [Cite_Footnote_5] for evaluation.,5 https://github.com/nlpyang/PreSumm,"Our target model is PreSumm (Liu and Lapata, 2019). Given a text x d , PreSumm first obtains a vector representation φ i ∈ R h for each sentence x i using BERT, and scores each sentence x i in the text as S(x d , x i ) = sigmoid(u > f(φ 1 , . . . , φ T ) i ) where u is a weight vector, f is a sentence-level transformer, and f(·) i is the ith sentence’s contex-tualized representation. Our objective is to insert a collision c into x d such that the rank of S(x d , c) among all sentences is high. We use the published models [Cite_Footnote_5] for evaluation.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.344_3_0,2020,Adversarial Semantic Collisions Congzheng Song Alexander M. Rush Vitaly Shmatikov,Footnote
1652,11656," http://opennlp.sourceforge.net/"," ['3 Description of SRES', '3.2 Instance Extractor']","In order to be able to match the slots of the patterns, the Instance Extractor utilizes an external shallow parser from the OpenNLP package ( [Cite] http://opennlp.sourceforge.net/), which is able to find all proper and common noun phrases in a sen-tence.",,"The Instance Extractor applies the patterns gener-ated by the Pattern Learner to the text corpus. In order to be able to match the slots of the patterns, the Instance Extractor utilizes an external shallow parser from the OpenNLP package ( [Cite] http://opennlp.sourceforge.net/), which is able to find all proper and common noun phrases in a sen-tence. These phrases are matched to the slots of the patterns. In other respects, the pattern matching and extraction process is straightforward.",Method,Tool,True,Use（引用目的）,False,P07-1076_0_0,2007,Using Corpus Statistics on Entities to Improve Semi-supervised Relation Extraction from the Web,Body
1653,11657," http://haonanyu.com/research/acl2013/"," ['6 Experiment']","Our corpus of 159 training samples pairs some videos with more than one sentence and some sen-tences with more than one video, with an average of 2.6 sentences per video [Cite_Footnote_1] .","1 Our code, videos, and sentential annotations are available at http://haonanyu.com/research/acl2013/.","We filmed 61 video clips (each 3–5 seconds at 640×480 resolution and 40 fps) that depict a va-riety of different compound events. Each clip de-picts multiple simultaneous events between some subset of four objects: a person, a backpack, a chair, and a trash-can. These clips were filmed in three different outdoor environments which we use for cross validation. We manually annotated each video with several sentences that describe what occurs in that video. The sentences were constrained to conform to the grammar in Table 1. Our corpus of 159 training samples pairs some videos with more than one sentence and some sen-tences with more than one video, with an average of 2.6 sentences per video [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,P13-1006_0_0,2013,Grounded Language Learning from Video Described with Sentences,Footnote
1654,11658," https://github.com/UKPLab/emnlp2021-hypercoref-cdcr"," ['-']","With our work, we free CDCR research from depending on costly human-annotated training data and open up possibili-ties for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages. [Cite_Footnote_1]",1 Data and model available at https://github.com/UKPLab/emnlp2021-hypercoref-cdcr,"(CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, ex-plaining why existing corpora are small and lack domain coverage. To overcome this bot-tleneck, we automatically extract event coref-erence data from hyperlinks in online news: When referring to a significant real-world event, writers often add a hyperlink to another article covering this event. We demonstrate that collecting hyperlinks which point to the same article(s) produces extensive and high-quality CDCR data and create a corpus of 2M documents and 2.7M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that models trained on small subsets of HyperCoref are highly competitive, with performance similar to models trained on gold-standard data. With our work, we free CDCR research from depending on costly human-annotated training data and open up possibili-ties for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.38_0_0,2021,Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News,Footnote
1655,11659," https://commoncrawl.org/"," ['1 Introduction']",This holds back research on open-domain CDCR systems which [Cite_Footnote_2] Fundamentals could increase the (currently limited) applicability,2 https://commoncrawl.org/ to expert-annotated CDCR corpora.,"This data bottleneck is problematic for three rea-sons. Firstly, recent state-of-the-art CDCR systems are based on pretrained language models (Devlin et al., 2019; Liu et al., 2019) fine-tuned with super-vised learning on human-annotated corpora (Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021; Cattan et al., 2021). Achieving top results with such models still requires high-quality labeled data for fine-tuning (Gururangan et al., 2020), yet the size of current corpora is insufficient for training gold-standard CDCR corpora, large systems. Secondly, because corpora need C3. out-of-domain transfer experiments with a to make compromises on domain coverage, the state-of-the-art CDCR system, certifying the domain coverage of all existing CDCR corpora quality of this data. is limited even when combined. This holds back research on open-domain CDCR systems which [Cite_Footnote_2] Fundamentals could increase the (currently limited) applicability",補足資料,Website,False,Introduce（引用目的）,False,2021.emnlp-main.38_1_0,2021,Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News,Footnote
1656,11660," https://pypi.org/project/newspaper3k/"," ['from Common Crawl 2 and apply it to create the']","We download these web pages, remove excess markup and detect publication dates with the newspaper3k framework [Cite_Footnote_3] .",3 https://pypi.org/project/newspaper3k/(cf. Pustejovsky et al. (2003)) and mentions using,"2. We download these web pages, remove excess markup and detect publication dates with the newspaper3k framework [Cite_Footnote_3] .",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.38_2_0,2021,Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News,Footnote
1657,11661," https://en.wikipedia.org/wiki/"," ['5 Discussion and Future Work']","[Cite] https://en.wikipedia.org/wiki/ preconditions of actions and events, and Cham- Wikipedia:Notability_(events) be transferable to the cross-document case.",,"Recently, Choubey and Huang (2021) investi-mentions available). Future work may exploit the gated automated retrieval of annotations for within-entirety of H YPER C OREF for training large, truly document event coreference resolution. Their open-domain CDCR models, potentially including method is applicable on plaintext news articles, additional languages beyond English by adapting but requires a database of lexical paraphrases for our pipeline. mention identification and a discourse parsing sys- 6 Related Work tem for filtering. It is unclear whether this method can be successfully applied for CDCR, since the Harvesting NLP datasets from the web has a long employed discourse-based filtering rules may not history. Sil et al. (2010) extract sequences of verb constructions from webpages to learn common 8 See [Cite] https://en.wikipedia.org/wiki/ preconditions of actions and events, and Cham- Wikipedia:Notability_(events) be transferable to the cross-document case. on Empirical Methods in Natural Language Process-To the best of our knowledge, we are the first to ing: System Demonstrations, pages 205–215, On-propose a cheap and high-quality data extraction approach specifically for cross-document event ex-traction and coreference which does not depend on pre-existing resources, combining past work on event extraction from raw newswire text and min-ing of hyperlinks.",補足資料,Document,True,Introduce（引用目的）,False,2021.emnlp-main.38_3_0,2021,Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News,Body
1658,11662," https://github.com/ns-moosavi/coval"," ['A.4 Full Experiment Results']",We use the scorer implementation from [Cite] https://github.com/ns-moosavi/coval.,,"We measure coreference resolution performance with the MUC (Vilain et al., 1995), CEAF e (Luo, 2005), B 3 (Bagga and Baldwin, 1998), CoNLL F1 (Pradhan et al., 2012) and LEA (Moosavi and Strube, 2016) metrics. We use the scorer implementation from [Cite] https://github.com/ns-moosavi/coval. Tables 12 to 16 report the full P/R/F1 scores of the coreference resolu-tion experiments reported in Section 4.1 for each of these metrics respectively. Table 17 reports the best training epochs, clustering thresholds τ and development set LEA F1 scores of each model and independent trial.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.38_4_0,2021,Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News,Body
1659,11663," http://ml.nec-labs.com/senna/"," ['2 Approach', '2.2 Using Word Representations to Identify Event Roles']","First, we extract the noun chunks using SENNA [Cite_Footnote_2] parser (Collobert et al., 2011; Collobert, 2011) and we build a representation for these chunks defined as the maximum, per column, of the vector represen-tations of the words it contains.",2 Code and resources can be found at http://ml.nec-labs.com/senna/,"After having generated for each word their vec-tor representation, we use them as features for the annotated data to classify event roles. However, event role fillers are not generally single words but noun phrases that can be, in some cases, identi-fied as named entities. For identifying the event roles, we therefore apply a two-step strategy. First, we extract the noun chunks using SENNA [Cite_Footnote_2] parser (Collobert et al., 2011; Collobert, 2011) and we build a representation for these chunks defined as the maximum, per column, of the vector represen-tations of the words it contains. Second, we use a statistical classifier to recognize the slot fillers, using this representation as features. We chose the extra-trees ensemble classifier (Geurts et al., 2006), which is a meta estimator that fits a num-ber of randomized decision trees (extra-trees) on various sub-samples of the data set and use averag-ing to improve the predictive accuracy and control over-fitting.",Method,Code,True,Produce（引用目的）,True,D14-1199_0_0,2014,Event Role Extraction using Domain-Relevant Word Representations,Footnote
1660,11664," http://metaoptimize.com/projects/wordreprs"," ['3 Experiments and Results', '3.2 Experiments']","We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representa-tions (DRVR-50) to more generic word embed-dings (C&W50, HLBL-50) [Cite_Footnote_3] and finally to another word representation construction on the domain-specific data (W2V-50) 4 .","3 C&W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embed-dings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/projects/wordreprs induced from the Reuters-RCV1","We used the extra-trees ensemble classifier im-plemented in (Pedregosa et al., 2011), with hyper-parameters optimized on the validation data: for-est of 500 trees and the maximum number of features√to consider when looking for the best split is number features. We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representa-tions (DRVR-50) to more generic word embed-dings (C&W50, HLBL-50) [Cite_Footnote_3] and finally to another word representation construction on the domain-specific data (W2V-50) 4 .",Material,Knowledge,False,Use（引用目的）,True,D14-1199_1_0,2014,Event Role Extraction using Domain-Relevant Word Representations,Footnote
1661,11665," https://code.google.com/p/word2vec/"," ['3 Experiments and Results', '3.2 Experiments']","We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representa-tions (DRVR-50) to more generic word embed-dings (C&W50, HLBL-50) 3 and finally to another word representation construction on the domain-specific data (W2V-50) [Cite_Footnote_4] .","4 W2V-50 are the embeddings induced from the MUC4 data set using the negative sampling training algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), available at https://code.google.com/p/word2vec/","We used the extra-trees ensemble classifier im-plemented in (Pedregosa et al., 2011), with hyper-parameters optimized on the validation data: for-est of 500 trees and the maximum number of features√to consider when looking for the best split is number features. We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representa-tions (DRVR-50) to more generic word embed-dings (C&W50, HLBL-50) 3 and finally to another word representation construction on the domain-specific data (W2V-50) [Cite_Footnote_4] .",Material,Dataset,True,Use（引用目的）,True,D14-1199_2_0,2014,Event Role Extraction using Domain-Relevant Word Representations,Footnote
1662,11666," http://computing.open.ac.uk/coda/"," ['Acknowledgements']",The research reported in this pa-per was carried out as part of the CODA re-search project ( [Cite] http://computing.open.ac.uk/coda/) which was funded by the UK’s Engineering and Physical Sciences Research Council under Grant EP/G020981/1.,,We would like to thank the three anonymous reviewers for their helpful comments and sug-gestions. We are also grateful to our col-leagues in the Open University’s Natural Lan-guage Generation group for stimulating discussions and feedback. The research reported in this pa-per was carried out as part of the CODA re-search project ( [Cite] http://computing.open.ac.uk/coda/) which was funded by the UK’s Engineering and Physical Sciences Research Council under Grant EP/G020981/1.,補足資料,Website,True,Introduce（引用目的）,True,P11-2042_0_0,2011,Data-oriented Monologue-to-Dialogue Generation,Body
1663,11667," http://cond.org/schbase.html"," ['References']",The com-plete SCHB ASE dataset is available at: [Cite] http://cond.org/schbase.html.,,"Extracted keyphrases can enhance numer-ous applications ranging from search to tracking the evolution of scientific dis-course. We present SCHB ASE , a hier-archical database of keyphrases extracted from large collections of scientific liter-ature. SCHB ASE relies on a tendency of scientists to generate new abbrevia-tions that “extend” existing forms as a form of signaling novelty. We demon-strate how these keyphrases/concepts can be extracted, and their viability as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The com-plete SCHB ASE dataset is available at: [Cite] http://cond.org/schbase.html.",Material,Dataset,True,Produce（引用目的）,True,P15-1059_0_0,2015,Building a Scientific Concept Hierarchy Database (SCHB ASE ),Body
1664,11668," http://cond.org/schbase.html"," ['1 Introduction']","Finally, we illustrate how authors build upon each others’ terminology over time to create new keyphrases. [Cite_Footnote_1]",1 Full database available at: http://cond.org/schbase.html,"In this paper we introduce SCHB ASE , a hi-erarchical database of keyphrases. We demon-strate how we can simply, but effectively, extract keyphrases by mining abbreviations from scien-tific literature and composing those keyphrases into semantically-meaningful hierarchies. We fur-ther show that abbreviations are a viable mech-anism for building a domain-specific keyphrase database by comparing our extracted keyphrases to a number of author-defined and automatically-created keyphrase corpora. Finally, we illustrate how authors build upon each others’ terminology over time to create new keyphrases. [Cite_Footnote_1]",Material,Dataset,True,Produce（引用目的）,True,P15-1059_1_0,2015,Building a Scientific Concept Hierarchy Database (SCHB ASE ),Footnote
1665,11669," http://academic.research.microsoft.com"," ['4 Overlap with Keyphrase Corpora', '4.2 Microsoft Academic (MSRAC ORPUS )']","Our second keyphrase dataset comes from the Mi-crosoft Academic (MSRA) search corpus (Mi-crosoft, 2015) [Cite_Ref] .",Microsoft. 2015. Microsoft academic search. http://academic.research.microsoft.com. Accessed: 2015-2-26.,"Our second keyphrase dataset comes from the Mi-crosoft Academic (MSRA) search corpus (Mi-crosoft, 2015) [Cite_Ref] . While particularly focused on",Material,DataSource,True,Use（引用目的）,True,P15-1059_2_0,2015,Building a Scientific Concept Hierarchy Database (SCHB ASE ),Reference
1666,11670," http://snowball.tartarus.org/texts/introduction.html"," ['3 Keyphrases and Hierarchies', '3.1 Abbreviation Extraction', '3.1.1 The A BBREV C ORPUS']","We further normalize our keyphrases by lowercasing, removing hyphens, and using the Snowball stemmer (Porter, 2001) [Cite_Ref] to merge plu-ral variants.",Martin F. Porter. 2001. Snowball: A language for stemming algorithms. http://snowball.tartarus.org/texts/introduction.html. Accessed: 2015-2-26.,"In addition to the filtering rules described above, we manually constructed a set of fil-ter terms to remove publication venues, agen-cies, and other institutions: ‘university’, ‘confer-ence’, ‘symposium’, ‘journal’, ‘foundation’, ‘con-sortium’, ‘agency’, ‘institute’ and ‘school’ are dis-carded. We further normalize our keyphrases by lowercasing, removing hyphens, and using the Snowball stemmer (Porter, 2001) [Cite_Ref] to merge plu-ral variants. After stemming and normalizing, we found a total of 155,957 unique abbreviation ex-pansions. Among these, 48,890 expansions occur more than once, 25,107 expansions thrice or more and 16,916 expansions four or more times. We re-fer to this collection as the A BBREV C ORPUS .",Method,Tool,True,Use（引用目的）,True,P15-1059_3_0,2015,Building a Scientific Concept Hierarchy Database (SCHB ASE ),Reference
1667,11671," http://en.wiki-pedia.org/wiki/List"," ['3 Keyphrases and Hierarchies']","Specifically, we looked at the 85 unique keyphrases (in this case, article titles) listed in the Wikipedia entry for List of Machine Learning Concepts (Wikipedia, 2014) [Cite_Ref] .",Wikipedia. 2014. Wikipedia: List of ma-chine learning concepts. http://en.wiki-pedia.org/wiki/List of machine learning concepts. Accessed: 2015-2-26.,"Our high level strategy for finding an initial set of keyphrases is to mine a corpus for abbrevia-tion expansions. This is a simple strategy, but as we show below, highly effective. Though the idea that abbreviations and keyphrases are linked fits within our understanding of scientific writing, we confirmed our intuition through a small exper-iment. Specifically, we looked at the 85 unique keyphrases (in this case, article titles) listed in the Wikipedia entry for List of Machine Learning Concepts (Wikipedia, 2014) [Cite_Ref] . These ranged from well known terms (e.g., Support Vector Machines and Autoencoders) to less known (e.g., Informa-tion fuzzy networks). In all 85 cases we were able to find an abbreviation on the Web (using Google) alongside the expansion (e.g., searching for the phrases “Support Vector Machines (SVMs)” or “Information Fuzzy Networks (IFN)”). Though there may be bias in the use of abbreviations in the Machine Learning literature, our experience has been that this holds in other domains as well. When a scientific keyphrase is used often enough, someone, somewhere, will have abbreviated it.",補足資料,Document,True,Introduce（引用目的）,True,P15-1059_4_0,2015,Building a Scientific Concept Hierarchy Database (SCHB ASE ),Reference
1668,11672," https://github.com/shivashankarrs/classimb_fairness"," ['References']",We empirically show through controlled experi-ments that the proposed approaches help miti-gate both class imbalance and demographic bi-ases. [Cite_Footnote_1],1 Code available at: https://github.com/shivashankarrs/classimb_fairness,"Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between re-search on class-imbalanced learning and mit-igating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning meth-ods for tweet sentiment and occupation clas-sification, and extend a margin-loss based ap-proach with methods to enforce fairness. We empirically show through controlled experi-ments that the proposed approaches help miti-gate both class imbalance and demographic bi-ases. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.155_0_0,2021,Fairness-aware Class Imbalanced Learning,Footnote
1669,11673," https://wit3.fbk.eu/"," ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']","German→English translation (IWSLT14 De-En) [Cite_Footnote_1] , WMT16 English↔Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English↔German translation (WMT14 En-De/De-En) .",1 https://wit3.fbk.eu/,"We evaluate our method on five widely used benchmark tasks: IWSLT14 German→English translation (IWSLT14 De-En) [Cite_Footnote_1] , WMT16 English↔Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English↔German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works. For the IWSLT14 De-En task, we train the model on its training set with 157k training samples, and evaluate on its test set. For the WMT14 En-De/De-En task, we train the model on the training set with 4.5M training samples, where newstest2013 and newstest2014 are used as the validation and test set respectively. As for the WMT16 En-Ro task which has 610k training pairs, we utilize newsdev2016 and newstest2016 as the validation and test set. For each dataset, we tokenize the sentences by Moses (Koehn et al., 2007) and segment each word into subwords using Byte-Pair Encoding (BPE) (Sennrich et al., 2015), resulting in a 32k vocabulary shared by source and target languages.",補足資料,Website,False,Use（引用目的）,True,2020.acl-main.36_0_0,2020,Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation,Footnote
1670,11674," https://www.statmt.org/wmt16/translation-task"," ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']","German→English translation (IWSLT14 De-En) , WMT16 English↔Romanian translation (WMT16 En-Ro/Ro-En) [Cite_Footnote_2] , and WMT14 English↔German translation (WMT14 En-De/De-En) .",2 https://www.statmt.org/wmt16/ translation-task,"We evaluate our method on five widely used benchmark tasks: IWSLT14 German→English translation (IWSLT14 De-En) , WMT16 English↔Romanian translation (WMT16 En-Ro/Ro-En) [Cite_Footnote_2] , and WMT14 English↔German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works. For the IWSLT14 De-En task, we train the model on its training set with 157k training samples, and evaluate on its test set. For the WMT14 En-De/De-En task, we train the model on the training set with 4.5M training samples, where newstest2013 and newstest2014 are used as the validation and test set respectively. As for the WMT16 En-Ro task which has 610k training pairs, we utilize newsdev2016 and newstest2016 as the validation and test set. For each dataset, we tokenize the sentences by Moses (Koehn et al., 2007) and segment each word into subwords using Byte-Pair Encoding (BPE) (Sennrich et al., 2015), resulting in a 32k vocabulary shared by source and target languages.",補足資料,Website,False,Use（引用目的）,True,2020.acl-main.36_1_0,2020,Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation,Footnote
1671,11675," https://www.statmt.org/wmt14/translation-task"," ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']","German→English translation (IWSLT14 De-En) , WMT16 English↔Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English↔German translation (WMT14 En-De/De-En) [Cite_Footnote_3] .",3 https://www.statmt.org/wmt14/ translation-task,"We evaluate our method on five widely used benchmark tasks: IWSLT14 German→English translation (IWSLT14 De-En) , WMT16 English↔Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English↔German translation (WMT14 En-De/De-En) [Cite_Footnote_3] . We strictly follow the dataset configurations of previous works. For the IWSLT14 De-En task, we train the model on its training set with 157k training samples, and evaluate on its test set. For the WMT14 En-De/De-En task, we train the model on the training set with 4.5M training samples, where newstest2013 and newstest2014 are used as the validation and test set respectively. As for the WMT16 En-Ro task which has 610k training pairs, we utilize newsdev2016 and newstest2016 as the validation and test set. For each dataset, we tokenize the sentences by Moses (Koehn et al., 2007) and segment each word into subwords using Byte-Pair Encoding (BPE) (Sennrich et al., 2015), resulting in a 32k vocabulary shared by source and target languages.",補足資料,Website,False,Use（引用目的）,True,2020.acl-main.36_2_0,2020,Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation,Footnote
1672,11676," https://github.com/lemmonation/jm-nat"," ['5 Experiments', '5.1.5 Training and Inference']","Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [Cite] https://github.com/lemmonation/jm-nat.",,"As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 † 404 † ms 1.50×25.48 29.32 30.19 25.94 30.42 ∗ 62 ∗ ms 9.79×29.90 32.53 33.23 27.03 31.71 ∗ 161 ∗ ms 3.77×30.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5× 27.69 106 ms 5.73×32.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [Cite] https://github.com/lemmonation/jm-nat.",Material,Knowledge,False,Produce（引用目的）,True,2020.acl-main.36_3_0,2020,Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation,Body
1673,11677," https://github.com/jcyk/AMR-parser"," ['5 Experiments', '5.1 Setup']","During testing, we use a beam size of 8 for generating graphs. [Cite_Footnote_5]",5 Our code can be found at https://github.com/jcyk/AMR-parser.,"We use Stanford CoreNLP (Manning et al., 2014) for text preprocessing, including tokeniza-tion, lemmatization, part-of-speech, and named-entity tagging. The input for sentence encoder consists of the randomly initialized lemma, part-of-speech tag, and named-entity tag embeddings, as well as the output from a learnable CNN with character embeddings as inputs. The graph en-coder uses randomly initialized concept embed-dings and another char-level CNN. Model hyper-parameters are chosen by experiments on the de-velopment set. The details of the hyper-parameter settings are provided in the Appendix. During testing, we use a beam size of 8 for generating graphs. [Cite_Footnote_5]",Method,Tool,True,Use（引用目的）,True,D19-1393_0_0,2019,Core Semantic First: A Top-down Approach for AMR Parsing ∗,Footnote
1674,11678," https://github.com/DeepLearnXMU/CG-RL"," ['References']",We have released our code at [Cite] https: //github.com/,,"Due to the great potential in facilitating soft-ware development, code generation has at-tracted increasing attention recently. Gener-ally, dominant models are Seq2Tree models, which convert the input natural language de-scription into a sequence of tree-construction actions corresponding to the pre-order traver-sal of an Abstract Syntax Tree (AST). How-ever, such a traversal order may not be suit-able for handling all multi-branch nodes. In this paper, we propose to equip the Seq2Tree model with a context-based Branch Selector, which is able to dynamically determine opti-mal expansion orders of branches for multi-branch nodes. Particularly, since the selec-tion of expansion orders is a non-differentiable multi-step operation, we optimize the selector through reinforcement learning, and formulate the reward function as the difference of model losses obtained through different expansion or-ders. Experimental results and in-depth analy-sis on several commonly-used datasets demon-strate the effectiveness and generality of our ap-proach. We have released our code at [Cite] https: //github.com/DeepLearnXMU/CG-RL .",Method,Code,True,Produce（引用目的）,True,2021.acl-long.394_0_0,2021,Exploring Dynamic Selection of Branch Expansion Orders for Code Generation,Body
1675,11679," http://www.cis.hut.fi/ahonkela/dippa/dippa.html"," ['2 Bayesian Model of Summaries']",The expectation and variance of Dirichlet(θ|v) are given as follows. [Cite_Footnote_3],3 http://www.cis.hut.fi/ahonkela/dippa/dippa.html,The expectation and variance of Dirichlet(θ|v) are given as follows. [Cite_Footnote_3] where v 0 = i v i . Therefore the variance of a scaled Dirichlet is:,Material,Knowledge,False,Produce（引用目的）,True,H05-1032_0_0,2005,Bayesian Learning in Text Summarization,Footnote
1676,11680," https://cocolab.stanford.edu/datasets/colors.html"," ['References']","Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public [Cite_Footnote_7] with our code and trained model parameters.",7 https://cocolab.stanford.edu/datasets/colors.html,"Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public [Cite_Footnote_7] with our code and trained model parameters. setting up the data collection platform, and mem-bers of the Stanford NLP group—particularly Reid Pryzant, Sebastian Schuster, and Reuben Cohn-Gordon—for valuable feedback on earlier drafts. This material is based in part upon work supported by the Stanford Data Science Initiative and by the NSF under Grant Nos. BCS-1456077 and SMA-1659585.",Mixed,Mixed,True,Produce（引用目的）,True,N18-1196_0_0,2018,Generating Bilingual Pragmatic Color References,Footnote
1677,11681," https://github.com/futurulus/colors-in-context"," ['References']","Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public with our code and trained model parameters. [Cite_Footnote_8]",8 https://github.com/futurulus/ colors-in-context,"Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public with our code and trained model parameters. [Cite_Footnote_8] setting up the data collection platform, and mem-bers of the Stanford NLP group—particularly Reid Pryzant, Sebastian Schuster, and Reuben Cohn-Gordon—for valuable feedback on earlier drafts. This material is based in part upon work supported by the Stanford Data Science Initiative and by the NSF under Grant Nos. BCS-1456077 and SMA-1659585.",Mixed,Mixed,True,Produce（引用目的）,True,N18-1196_1_0,2018,Generating Bilingual Pragmatic Color References,Footnote
1678,11682," https://slack.com/"," ['1 Introduction']","With the continuing growth of Internet and social media, online group chat channels, e.g., Slack [Cite_Footnote_1] and Whatsapp , among many others, have become in-creasingly popular and played a significant social and economic role.",1 https://slack.com/,"With the continuing growth of Internet and social media, online group chat channels, e.g., Slack [Cite_Footnote_1] and Whatsapp , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.",補足資料,Website,True,Introduce（引用目的）,True,2021.emnlp-main.181_0_0,2021,Unsupervised Conversation Disentanglement through Co-Training,Footnote
1679,11683," https://www.whatsapp.com/"," ['1 Introduction']","With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [Cite_Footnote_2] , among many others, have become in-creasingly popular and played a significant social and economic role.",2 https://www.whatsapp.com/,"With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [Cite_Footnote_2] , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.",補足資料,Website,True,Introduce（引用目的）,True,2021.emnlp-main.181_1_0,2021,Unsupervised Conversation Disentanglement through Co-Training,Footnote
1680,11684," https://github.com/LayneIns/Unsupervised_dialo_disentanglement"," ['1 Introduction']","Moreover, we apply the disentangled conversations predicted by our method to the downstream task of multi-party response selection and get significant improvements compared to a baseline system. [Cite_Footnote_3]",3 Code will be publicly available at https: //github.com/LayneIns/Unsupervised_ dialo_disentanglement,"We conduct experiments on the large public Movie Dialogue Dataset (Liu et al., 2020). Ex-perimental results demonstrate that our proposed method outperforms strong baselines based on BERT (Devlin et al., 2019) in two-step settings, and achieves competitive results compared to those of the state-of-the-art supervised end-to-end methods. Moreover, we apply the disentangled conversations predicted by our method to the downstream task of multi-party response selection and get significant improvements compared to a baseline system. [Cite_Footnote_3] In summary, our main contributions are three-fold:",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.181_2_0,2021,Unsupervised Conversation Disentanglement through Co-Training,Footnote
1681,11685," https://catalog.ldc.upenn.edu/LDC2005T23"," ['3 Experiments', '3.1 Experimental Setting']","To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0. [Cite_Footnote_1]",1 https://catalog.ldc.upenn.edu/LDC2005T23,"To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0. [Cite_Footnote_1] We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid. The test set includes 72 files, which are chtb 001.fid to chtb 040.fid, and chtb 900.fid to chtb 931.fid. We use another annotated corpus with distinct se-mantic role labels and annotation schema, which is designed by ourselves for other projects, as hetero-geneous resource. This labeled dataset has 17,308 annotated sentences, and the semantic roles con-cerned are like “agent” and “patient”, resulting in 21 kinds of types, which are all distinct from the semantic roles defined in CPB. We use the devel-opment set of CPB for model selection, and the hyper parameter setting of our model is reported in Table 1.",Material,Dataset,True,Use（引用目的）,True,D15-1186_0_0,2015,Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks,Footnote
1682,11686," https://code.google.com/p/word2vec/"," ['3 Experiments', '3.2 Chinese SRL Performance']","With the result in Table 2, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec [Cite_Footnote_3] on Chinese Gigaword Corpus for word pre-training).",3 https://code.google.com/p/word2vec/,"Further, we conduct experiments with the intro-duction of heterogenous resource. Previous work found that the performance can be improved by pre-training the word embeddings on large unla-beled data and using the obtained embeddings to make initialization. With the result in Table 2, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec [Cite_Footnote_3] on Chinese Gigaword Corpus for word pre-training). However, as shown in Table 2, com-pared to standard pre-training, the influence of het-erogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improve-ment. The information provided by standard pre-training with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident.",Method,Tool,True,Use（引用目的）,True,D15-1186_1_0,2015,Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks,Footnote
1683,11687," http://www.sighan.org/bakeoff2005/"," ['2 System Architecture', '2.2 MSU Based Tagging', '2.2.1 Minimum Semantic Unit']",We collect all the MSUs from the benchmark datasets provided by the second International Chi-nese Word Segmentation Bakeoff [Cite_Footnote_2] .,2 http://www.sighan.org/bakeoff2005/,"We collect all the MSUs from the benchmark datasets provided by the second International Chi-nese Word Segmentation Bakeoff [Cite_Footnote_2] . We choose the Peking University (PKU) data because it is more fine-grained than all other corpora. Suppose we represent the segmented data as L (In our case L is the PKU word segmentation data), the MSU se-lecting algorithm is shown in T ABLE 3.",補足資料,Website,True,Introduce（引用目的）,True,D14-1147_0_0,2014,Predicting Chinese Abbreviations with Minimum Semantic Unit and Global Constraints,Footnote
1684,11688," http://nlp.stanford.edu/software/segmenter.shtml"," ['2 System Architecture', '2.2 MSU Based Tagging', '2.2.1 Minimum Semantic Unit']",Here we use the Stan-ford Chinese Word Segmenter [Cite_Footnote_3] .,3 http://nlp.stanford.edu/software/segmenter.shtml,"For a given full form, we first segment it us-ing a standard word segmenter to get a coarse-grained segmentation result. Here we use the Stan-ford Chinese Word Segmenter [Cite_Footnote_3] . Then we use the MSU set to segment each word using the strategy of “Maximum Forward Matching” to get the fine-grained MSU segmentation result.",Method,Tool,True,Use（引用目的）,True,D14-1147_1_0,2014,Predicting Chinese Abbreviations with Minimum Semantic Unit and Global Constraints,Footnote
1685,11689," http://nlp.stanford.edu/software/tagger.shtml"," ['2 System Architecture', '2.2 MSU Based Tagging', '2.2.3 Feature templates']","In MSU-based tagging, we can uti-lize the POS information, which we get from the Stanford Chinese POS Tagger [Cite_Footnote_6] .",6 http://nlp.stanford.edu/software/tagger.shtml,"Templates 1, 2 and 3 express word uni-grams and bi-grams. In MSU-based tagging, we can uti-lize the POS information, which we get from the Stanford Chinese POS Tagger [Cite_Footnote_6] . In template 4, the type of word refers to whether it is a number, an English word or a Chinese word. Because the ba-sic tagging unit is MSU, which carries word infor-mation, we can use many features that are infeasi-ble in character-based tagging.",Method,Tool,True,Use（引用目的）,True,D14-1147_2_0,2014,Predicting Chinese Abbreviations with Minimum Semantic Unit and Global Constraints,Footnote
1686,11690," http://crfpp.sourceforge.net/"," ['3 Experiments', '3.1 Data and Evaluation Metric']","CRF++ [Cite_Footnote_7] , an open source linear chain CRF tool, is used in the sequence labeling part.",7 http://crfpp.sourceforge.net/,"CRF++ [Cite_Footnote_7] , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.",Method,Tool,True,Use（引用目的）,True,D14-1147_3_0,2014,Predicting Chinese Abbreviations with Minimum Semantic Unit and Global Constraints,Footnote
1687,11691," http://lpsolve.sourceforge.net/5.5/"," ['3 Experiments', '3.1 Data and Evaluation Metric']","For ILP part, we use lpsolve [Cite_Footnote_8] , which is also an open source tool.",8 http://lpsolve.sourceforge.net/5.5/,"CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve [Cite_Footnote_8] , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.",Method,Tool,True,Use（引用目的）,True,D14-1147_4_0,2014,Predicting Chinese Abbreviations with Minimum Semantic Unit and Global Constraints,Footnote
1688,11692," http://en.wikipedia.org/wiki/SinaWeibo"," ['1 Introduction']","Weibo, in particular Sina Weibo [Cite_Footnote_1] , has attracted more than 30% of Internet users (Yang et al., 2012), making it one of the most popular social media services in the world.",1 http://en.wikipedia.org/wiki/Sina Weibo,"Weibo, in particular Sina Weibo [Cite_Footnote_1] , has attracted more than 30% of Internet users (Yang et al., 2012), making it one of the most popular social media services in the world. While Weibo posts are abundantly available, NLP techniques for ana-lyzing Weibo posts have not been well-studied in the past.",補足資料,Document,True,Introduce（引用目的）,True,D14-1122_0_0,2014,Dependency Parsing for Weibo: An Efficient Probabilistic Logic Programming Approach,Footnote
1689,11693," http://www.cs.cmu.edu/~yww/data/WeiboTreebank.zip"," ['1 Introduction']","• We present a freely available Chinese Weibo dependency treebank [Cite_Footnote_3] , manually annotated with more than 18,000 tokens;",3 http://www.cs.cmu.edu/˜yww/data/WeiboTreebank.zip,"• We present a freely available Chinese Weibo dependency treebank [Cite_Footnote_3] , manually annotated with more than 18,000 tokens;",Method,Tool,True,Produce（引用目的）,True,D14-1122_1_0,2014,Dependency Parsing for Weibo: An Efficient Probabilistic Logic Programming Approach,Footnote
1690,11694," http://github.com/gabedoyle/acl2017"," ['3 Data: Corporate Email Corpus']","We can, however, share the code and dummy test data, both of which can be accessed at [Cite] http: //github.com/gabedoyle/acl2017.",,"Privacy protections and ethical considerations Research based on employees’ archived electronic communications in organizational settings poses potential threats to employee privacy and com-pany confidentiality. To address these concerns, and following established ethical guidelines for the conduct of such research (Borgatti and Molina, 2003), we implemented the following procedures: (a) raw data were stored on secure research servers behind the company’s firewall; (b) messages ex-changed with individuals outside the firm were eliminated; (c) all identifying information such as email addresses was transformed into hashed identifiers, with the company retaining access to the key code linking identifying information to hashed identifiers; and (d) raw message content was transformed into linguistic categories so that identities could not be inferred from message con-tent. Per terms of the non-disclosure agreement we signed with the firm, we are not able to share the data underlying the analyses reported below. We can, however, share the code and dummy test data, both of which can be accessed at [Cite] http: //github.com/gabedoyle/acl2017.",Mixed,Mixed,True,Produce（引用目的）,True,P17-1056_0_0,2017,Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,Body
1691,11695," http://groups.csail.mit.edu/rbg/code/dependency/"," ['References']",Across six lan-guages our approach outperforms state-of-the-art unsupervised methods by a significant mar-gin. [Cite_Footnote_1],1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/,"We present an approach to grammar induc-tion that utilizes syntactic universals to im-prove dependency parsing across a range of languages. Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies be-tween pairs of syntactic categories that com-monly occur across languages. During infer-ence of the probabilistic model, we use pos-terior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also auto-matically refine the syntactic categories given in our coarsely tagged input. Across six lan-guages our approach outperforms state-of-the-art unsupervised methods by a significant mar-gin. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D10-1120_0_0,2010,Using Universal Linguistic Knowledge to Guide Grammar Induction,Footnote
1692,11696," https://github.com/eladsegal/tag-based-multi-span-extraction"," ['6 Conclusion']",Our code can be downloaded from [Cite] https://github.com/eladsegal/tag-based-multi-span-extraction.,,"In this work, we cast the task of answering multi-span questions as a sequence tagging prob-lem, and present a simple corresponding multi-span architecture. We show that replacing the standard single-span architecture with our multi-span architecture dramatically improves results on multi-span questions, without harming per-formance on single-span questions, leading to state-of-the-art results on Q UOREF . In addition, integrating our multi-span architecture into ex-isting models further improves performance on DROP, as is evident from the leading models on DROP’s leaderboard. Our code can be downloaded from [Cite] https://github.com/eladsegal/tag-based-multi-span-extraction.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.248_0_0,2020,A Simple and Effective Model for Answering Multi-span Questions,Body
1693,11697," https://github.com/raylin1000/drop-bert"," ['Acknowledgements']","For DROP, we additionally use arithmetic and count heads based on (Dua et al., 2019; Kin-ley and Lin, 2019 [Cite_Ref] ).",Jambay Kinley and Raymond Lin. 2019. NABERT+: Improving numerical reason-ing in reading comprehension. URL https: //github.com/raylin1000/drop-bert .,"This research was partially supported by The Israel Science Foundation grants 942/16 and 1186/18, The Yandex Initiative for Machine Learning and the European Research Council (ERC) under the European Union Horizons 2020 research and inno-vation programme (grant ERC DELPHI 802800). Appendix for “A Simple and Effective Model for Answering Multi-span Questions” A Experimental Setup We experiment with model variations that use either SSE, TASE, or their combination as a multi-head model. For DROP, we additionally use arithmetic and count heads based on (Dua et al., 2019; Kin-ley and Lin, 2019 [Cite_Ref] ). Our model is implemented with PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2017). For f in Eq. (1) we use a 2-layer feed-forward network with ReLU activations and |S| outputs. We use the Hugging Face im-plementation of RoBERTa LARGE (Wolf et al., 2019; Liu et al., 2019) as the encoder in our model. 5% of DROP and 30% of Q UOREF are inputs with over 512 tokens. Due to RoBERTa LARGE ’s limita-tion of 512 positional embeddings, we truncate inputs by removing over-flowing tokens from the passage, both at train and test time. We discard 3.87% of the training examples of DROP and 5.05% of the training example of Q UOREF , which are cases when the answer cannot be outputted by the model (due to a dataset error, or trunca-tion of the correct answer span). For training, the BertAdam 4 optimizer is used with default parame-ters and learning rates of either 5 × 10 −6 or 10 −5 . Hyperparameter search was not performed. We train on a single NVIDIA Titan XP with a batch size of 2 and gradient accumulation of 6, result-ing in an effective batch size of 12, for 20 epochs with an early-stopping patience of 10. The average runtime per epoch is 3.5 hours. Evaluation was performed with the official evaluation scripts of DROP and Q UOREF . Our full implementation can be found at https://github.com/eladsegal/tag-based-multi-span-extraction.",補足資料,Paper,True,Use（引用目的）,True,2020.emnlp-main.248_1_0,2020,A Simple and Effective Model for Answering Multi-span Questions,Reference
1694,11698," https://github.com/huggingface/transformers/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md#optimizers"," ['7:453–466.']","For training, the BertAdam [Cite_Footnote_4] optimizer is used with default parame-ters and learning rates of either 5 × 10 −6 or 10 −5 .",4 https://github.com/huggingface/transformers/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md#optimizers,"This research was partially supported by The Israel Science Foundation grants 942/16 and 1186/18, The Yandex Initiative for Machine Learning and the European Research Council (ERC) under the European Union Horizons 2020 research and inno-vation programme (grant ERC DELPHI 802800). Appendix for “A Simple and Effective Model for Answering Multi-span Questions” A Experimental Setup We experiment with model variations that use either SSE, TASE, or their combination as a multi-head model. For DROP, we additionally use arithmetic and count heads based on (Dua et al., 2019; Kin-ley and Lin, 2019). Our model is implemented with PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2017). For f in Eq. (1) we use a 2-layer feed-forward network with ReLU activations and |S| outputs. We use the Hugging Face im-plementation of RoBERTa LARGE (Wolf et al., 2019; Liu et al., 2019) as the encoder in our model. 5% of DROP and 30% of Q UOREF are inputs with over 512 tokens. Due to RoBERTa LARGE ’s limita-tion of 512 positional embeddings, we truncate inputs by removing over-flowing tokens from the passage, both at train and test time. We discard 3.87% of the training examples of DROP and 5.05% of the training example of Q UOREF , which are cases when the answer cannot be outputted by the model (due to a dataset error, or trunca-tion of the correct answer span). For training, the BertAdam [Cite_Footnote_4] optimizer is used with default parame-ters and learning rates of either 5 × 10 −6 or 10 −5 . Hyperparameter search was not performed. We train on a single NVIDIA Titan XP with a batch size of 2 and gradient accumulation of 6, result-ing in an effective batch size of 12, for 20 epochs with an early-stopping patience of 10. The average runtime per epoch is 3.5 hours. Evaluation was performed with the official evaluation scripts of DROP and Q UOREF . Our full implementation can be found at https://github.com/eladsegal/tag-based-multi-span-extraction.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.248_2_0,2020,A Simple and Effective Model for Answering Multi-span Questions,Footnote
1695,11699," https://github.com/eladsegal/tag-based-multi-span-extraction"," ['7:453–466.']",Our full implementation can be found at [Cite] https://github.com/eladsegal/tag-based-multi-span-extraction.,,"This research was partially supported by The Israel Science Foundation grants 942/16 and 1186/18, The Yandex Initiative for Machine Learning and the European Research Council (ERC) under the European Union Horizons 2020 research and inno-vation programme (grant ERC DELPHI 802800). Appendix for “A Simple and Effective Model for Answering Multi-span Questions” A Experimental Setup We experiment with model variations that use either SSE, TASE, or their combination as a multi-head model. For DROP, we additionally use arithmetic and count heads based on (Dua et al., 2019; Kin-ley and Lin, 2019). Our model is implemented with PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2017). For f in Eq. (1) we use a 2-layer feed-forward network with ReLU activations and |S| outputs. We use the Hugging Face im-plementation of RoBERTa LARGE (Wolf et al., 2019; Liu et al., 2019) as the encoder in our model. 5% of DROP and 30% of Q UOREF are inputs with over 512 tokens. Due to RoBERTa LARGE ’s limita-tion of 512 positional embeddings, we truncate inputs by removing over-flowing tokens from the passage, both at train and test time. We discard 3.87% of the training examples of DROP and 5.05% of the training example of Q UOREF , which are cases when the answer cannot be outputted by the model (due to a dataset error, or trunca-tion of the correct answer span). For training, the BertAdam 4 optimizer is used with default parame-ters and learning rates of either 5 × 10 −6 or 10 −5 . Hyperparameter search was not performed. We train on a single NVIDIA Titan XP with a batch size of 2 and gradient accumulation of 6, result-ing in an effective batch size of 12, for 20 epochs with an early-stopping patience of 10. The average runtime per epoch is 3.5 hours. Evaluation was performed with the official evaluation scripts of DROP and Q UOREF . Our full implementation can be found at [Cite] https://github.com/eladsegal/tag-based-multi-span-extraction.",Mixed,Mixed,False,Produce（引用目的）,True,2020.emnlp-main.248_3_0,2020,A Simple and Effective Model for Answering Multi-span Questions,Body
1696,11700," https://github.com/huggingface/neuralcoref"," ['4 Experiments']",Training Multee: For OpenBookQA we use cross entropy loss for labels corresponding to [Cite_Footnote_4] an-swer choices.,4 https://github.com/huggingface/neuralcoref,"Training Multee: For OpenBookQA we use cross entropy loss for labels corresponding to [Cite_Footnote_4] an-swer choices. For MultiRC, we use binary cross entropy loss for each answer-choice separately since in MultiRC each question can have more than one correct answer choice. The entailment components are pre-trained on sentence-level en-tailment tasks and then fine-tuned as part of end-to-end QA training. The MultiRC dataset includes sentence-level relevance labels. We supervise the Sentence Relevance module with a binary cross entropy loss for predicting these relevance labels when available. We used PyTorch (Paszke et al., 2017) and AllenNLP to implement our models and ran them on Beaker 7 . For pre-training we use the same hyper-parameters of ESIM(Chen et al., 2017) as available in implementation of AllenNLP (Gardner et al., 2017) and fine-tune the model pa-rameters. We do not perform any hyper-parameter tuning for any of our models. We fine-tune all lay-ers in ESIM except for the embedding layer.",Material,Knowledge,True,Use（引用目的）,True,N19-1302_0_0,2019,Repurposing Entailment for Multi-Hop Question Answering Tasks,Footnote
1697,11701," https://github.com/allenai/OpenBookQA"," ['4 Experiments']","For Open-BookQA, we use the exact same retrieval as re-leased by the authors of OpenBookQA [Cite_Footnote_6] and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query.",6 https://github.com/allenai/OpenBookQA,"Datasets: We evaluate Multee on two datasets, OpenBookQA (Mihaylov et al., 2018) and Mul-tiRC (Khashabi et al., 2018), both of which are specifically designed to test reasoning over multiple sentences. MultiRC is paragraph-based multiple-choice QA dataset derived from varying topics where the questions are answerable based on information from the paragraph. In MultiRC, each question can have more than one correct an-swer choice, and so it can be viewed as a bi-nary classification task (one prediction per an-swer choice), with 4,848 / 4,583 examples in Dev/Test sets. OpenBookQA, on the other hand, has multiple-choice science questions with exactly one correct answer choice and no associated para-graph. As a result, this dataset requires the rele-vant facts to be retrieved from auxiliary resources including the open book of facts released with the paper and other sources such as WordNet (Miller, 1995) and ConceptNet (Speer and Havasi, 2012). It contains 500 questions in the Dev and Test sets. Preprocessing: For each question and answer choice, we create an answer hypothesis statement using a modified version of the script used in Sc-iTail (Khot et al., 2018) construction. We wrote a handful of rules to better convert the question and answer to a hypothesis. We also mark the span of answer in the hypothesis with special begin and end tokens, @@@answer and answer@@@ re-spectively . For MultiRC, we also apply an off-the-shelf coreference resolution model 4 and re-place the mentions when they resolve to pronouns occurring in a different sentence . For Open-BookQA, we use the exact same retrieval as re-leased by the authors of OpenBookQA [Cite_Footnote_6] and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query.",Material,Knowledge,True,Use（引用目的）,True,N19-1302_1_0,2019,Repurposing Entailment for Multi-Hop Question Answering Tasks,Footnote
1698,11702," https://beaker.org/"," ['4 Experiments']","We used PyTorch (Paszke et al., 2017) and AllenNLP to implement our models and ran them on Beaker [Cite_Footnote_7] .",7 https://beaker.org/,"Training Multee: For OpenBookQA we use cross entropy loss for labels corresponding to 4 an-swer choices. For MultiRC, we use binary cross entropy loss for each answer-choice separately since in MultiRC each question can have more than one correct answer choice. The entailment components are pre-trained on sentence-level en-tailment tasks and then fine-tuned as part of end-to-end QA training. The MultiRC dataset includes sentence-level relevance labels. We supervise the Sentence Relevance module with a binary cross entropy loss for predicting these relevance labels when available. We used PyTorch (Paszke et al., 2017) and AllenNLP to implement our models and ran them on Beaker [Cite_Footnote_7] . For pre-training we use the same hyper-parameters of ESIM(Chen et al., 2017) as available in implementation of AllenNLP (Gardner et al., 2017) and fine-tune the model pa-rameters. We do not perform any hyper-parameter tuning for any of our models. We fine-tune all lay-ers in ESIM except for the embedding layer.",Method,Tool,True,Use（引用目的）,True,N19-1302_2_0,2019,Repurposing Entailment for Multi-Hop Question Answering Tasks,Footnote
1699,11703," http://www.aclweb.org/anthology/D14-1162"," ['4 Experiments']","Models Compared: We experiment with Glove (Pennington et al., 2014) [Cite_Ref] and ELMo (Peters et al., 2018) embeddings for Multee and compare with following three types of systems: (A) Baselines using entailment as a black-box We use the pre-trained entailment model as a black-box in two ways: concatenate premises (Concat) and aggregate sentence level decisions with a max operation (Max).","Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532– 1543. http://www.aclweb.org/anthology/D14-1162.","Models Compared: We experiment with Glove (Pennington et al., 2014) [Cite_Ref] and ELMo (Peters et al., 2018) embeddings for Multee and compare with following three types of systems: (A) Baselines using entailment as a black-box We use the pre-trained entailment model as a black-box in two ways: concatenate premises (Concat) and aggregate sentence level decisions with a max operation (Max). Both models were also pre-trained on SNLI and MultiNLI datasets and fine-tuned on the target QA datasets with same pre-processing. (B) Previously published results: For MultiRC, there are two published baselines: IR (Information Retrieval) and LR (Logistic Regression). These simple models turn out to be strong baselines on this relatively smaller sized dataset. For Open-BookQA, we report published baselines from (Mihaylov et al., 2018): Question Match with ELMo (QM + ELMo), Question to Answer ESIM with ELMo (ESIM + ELMo) and their best result with the Knowledge Enhanced Reader (KER). (C) Large Transformer based models: We com-pare with OpenAI-Transformer (OFT), pre-trained on large-scale language modeling task and fine-tuned on respective datasets. A contemporaneous work, 8 which published these transformer results, also fine-tuned this transformer further on a large scale reading comprehension dataset, RACE (Lai et al., 2017), before fine-tuning on the target QA datasets with their method, Reading Strategies.",補足資料,Paper,True,Compare（引用目的）,True,N19-1302_8_0,2019,Repurposing Entailment for Multi-Hop Question Answering Tasks,Reference
1700,11704," https://github.com/nanguoshun/LSR"," ['1 Introduction']",The code and pretrained model are available at [Cite] https: //github.com/nanguoshun/LSR 1 .,,"Experiments show that our model significantly outperforms the existing approaches on DocRED, a large-scale document-level relation extraction dataset with a large number of entities and re-lations, and also yields new state-of-the-art re-sults on two popular document-level relation ex-traction datasets in the biomedical domain. The code and pretrained model are available at [Cite] https: //github.com/nanguoshun/LSR 1 . els in various settings. We demonstrate that our model is capable of discovering more ac-curate inter-sentence relations by utilizing a multi-hop reasoning module.",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.141_0_0,2020,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,Body
1701,11705," https://spacy.io/"," ['3 Experiments', '3.2 Setup']",We use spaCy [Cite_Footnote_2] to get the meta dependency paths of sentences in a document.,2 https://spacy.io/,"We use spaCy [Cite_Footnote_2] to get the meta dependency paths of sentences in a document. Following Yao et al. (2019) and Wang et al. (2019), we use the GloVe (Pennington et al., 2014) embedding with BiLSTM, and Uncased BERT-Base (Devlin et al., 2019) as the context encoder. All hyper-parameters are tuned based on the development set. We list some of the important hyper-parameters in Table 1.",Method,Tool,True,Use（引用目的）,True,2020.acl-main.141_1_0,2020,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,Footnote
1702,11706," https://competitions.codalab.org/competitions/20717"," ['3 Experiments', '3.2 Setup']",Evaluation on the test set is done through CodaLab [Cite_Footnote_3] .,3 https://competitions.codalab.org/competitions/20717,"Following Yao et al. (2019), we use F 1 and Ign F 1 as the evaluation metrics. Ign F 1 denotes F 1 scores excluding relational facts shared by the train-ing and dev/test sets. F 1 scores for intra- and inter-sentence entity pairs are also reported. Evaluation on the test set is done through CodaLab [Cite_Footnote_3] .",補足資料,Website,False,Use（引用目的）,True,2020.acl-main.141_2_0,2020,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,Footnote
1703,11707," https://amzn.to/2qDjNcJ"," ['1 Introduction']","The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the ∼20 SLU domains [Cite_Footnote_1] .",1 https://amzn.to/2qDjNcJ,"The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the ∼20 SLU domains [Cite_Footnote_1] . In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and third-party applications. We have manually annotated data for 20k examples using the Alexa ontology. This data includes the annotation of ∼100 actions, ∼500 types, ∼20 roles and ∼172 properties.",補足資料,Website,False,Introduce（引用目的）,False,N18-3022_0_0,2018,The Alexa Meaning Representation Language,Footnote
1704,11708," https://amzn.to/2qDjNcJ"," ['6 Conclusions and Future Work']",The representation has been released at AWS Re:Invent 2016 [Cite_Footnote_2] .,2 https://amzn.to/2qDjNcJ,"This paper develops AMRL, a meaning represen-tation for spoken language. We have shown how it can be used to expand the set of supported use-cases to complex and cross-domain utterances, while leveraging a single compositional seman-tics. The representation has been released at AWS Re:Invent 2016 [Cite_Footnote_2] . It is also being used as a rep-resentation for expanded support for complex ut-terances, such as those with sequential composi-tion. Continued development of a common mean-ing representation for spoken language will enable Alexa to become capable and accurate, expanding the set of functionality for all Alexa users.",補足資料,Website,True,Introduce（引用目的）,True,N18-3022_1_0,2018,The Alexa Meaning Representation Language,Footnote
1705,11709," https://doi.org/10.1002/9781444395037.ch5"," ['5 Related Work']","[Cite_Ref] (Bos et al., 2017)2, universal dependencies (Bos et al., 2017)3 are all related representations.","Mark Steedman and Jason Baldridge. 2011. Combina-tory Categorial Grammar, Wiley-Blackwell, pages 181–224. https://doi.org/10.1002/9781444395037.ch5.","Semantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002). OntoNotes (Hovy et al., 2006), lambda- DCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and Baldridge, 2011) [Cite_Ref] (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations. A comparison of semantic representations for natural language se-mantics is described in Abend and Rappoport. Un-like these meaning representations for written lan-guage, AMRL covers question answering, imper-ative actions, and a wide range of new types and properties (e.g., smart home, timers, etc.).",補足資料,Paper,True,Introduce（引用目的）,True,N18-3022_10_0,2018,The Alexa Meaning Representation Language,Reference
1706,11710," http://www.leffingwell.com/top_10.htm"," ['1 Introduction']",The flavor and fragrance industry is estimated to be worth over $20 billion in 2015 [Cite_Footnote_1] .,1 http://www.leffingwell.com/top_10.htm,"The lack of vocabulary for smells and flavors contrasts starkly with the interest people in the West have for flavors and fragrances, and what they are willing to spend on such products. The flavor and fragrance industry is estimated to be worth over $20 billion in 2015 [Cite_Footnote_1] . In this context, experts’ recommendations are used by the pub-lic in order to help them make decisions about purchases. But are the expert recommendations meaningful, given the limitations of language for smells and flavors?",補足資料,Document,True,Introduce（引用目的）,True,P16-2050_0_0,2016,Very quaffable and great fun: Applying NLP to wine reviews,Footnote
1707,11711," http://www.vivino.com"," ['1 Introduction']","Current recom-mender systems such as the mobile apps Vivino [Cite_Footnote_2] or Delectable work with metadata and user-based filtering, i.e. the principle of ‘other users also bought . . .’.",2 Vivino: http://www.vivino.com,"In the long run, as we are training automatic systems to predict wine properties, we could use such systems for automatic metadata prediction and error correction in wine review databases. These systems are also a first step towards a recommender system for wines based on review content and flavor descriptions. Current recom-mender systems such as the mobile apps Vivino [Cite_Footnote_2] or Delectable work with metadata and user-based filtering, i.e. the principle of ‘other users also bought . . .’. So there is potential here for content-based recommender systems to be developed.",Method,Tool,True,Introduce（引用目的）,True,P16-2050_1_0,2016,Very quaffable and great fun: Applying NLP to wine reviews,Footnote
1708,11712," http://www.delectable.com"," ['1 Introduction']","Current recom-mender systems such as the mobile apps Vivino or Delectable [Cite_Footnote_3] work with metadata and user-based filtering, i.e. the principle of ‘other users also bought . . .’.",3 Delectable: http://www.delectable.com,"In the long run, as we are training automatic systems to predict wine properties, we could use such systems for automatic metadata prediction and error correction in wine review databases. These systems are also a first step towards a recommender system for wines based on review content and flavor descriptions. Current recom-mender systems such as the mobile apps Vivino or Delectable [Cite_Footnote_3] work with metadata and user-based filtering, i.e. the principle of ‘other users also bought . . .’. So there is potential here for content-based recommender systems to be developed.",Method,Tool,True,Introduce（引用目的）,True,P16-2050_2_0,2016,Very quaffable and great fun: Applying NLP to wine reviews,Footnote
1709,11713," http://www.winemag.com/"," ['3 Data set']","The website [Cite] http://www.winemag.com/, owned by Wine Enthusiast Companies, hosts a substantial catalog of wine descriptions.",,"The website [Cite] http://www.winemag.com/, owned by Wine Enthusiast Companies, hosts a substantial catalog of wine descriptions. We downloaded the available reviews 4 and gathered a total of 76,585 wine reviews. The catalog data is structured and contains information about the wine such as the producer, appellation region and country, grape variety, color, alcohol percentage, price, and where to buy it. The expert who writes the wine review also rates the wine by assigning it a score between 80 and 100. The reviews are writ-ten by 33 different experts, and can be considered concise, with an average length of 39 words.",補足資料,Website,True,Introduce（引用目的）,True,P16-2050_3_0,2016,Very quaffable and great fun: Applying NLP to wine reviews,Body
1710,11714," http://mallet.cs.umass.edu"," ['4 Methodology']","For LDA (McCal-lum, 2002) [Cite_Ref] , we also varied the threshold to assign a topic only to a text when it covered 1%, 2%, or 5% of the text.",Andrew McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.,"As the reviews are short and only contain about 23 content words on average, we decided to also add semantic features to reduce data sparsity. As shown by Kusner and colleagues (2015), seman-tic representations such as Latent Semantic In-dexing and Latent Dirichlet Allocation (LDA) can outperform a BoW representation. For our sec-ond experimental setup, we combined our set of BoW features with (1) 100 topics generated with Latent Dirichlet Allocation (Blei, 2012), and (2) 100 clusters based on word embeddings gener-ated with Word2Vec (Mikolov et al., 2013). We ran initial experiments with exemplar-based clas-sification and experimented with different cluster (Word2Vec) and topic (LDA) sizes of 100, 500, 1000, 2000 on the training set. For LDA (McCal-lum, 2002) [Cite_Ref] , we also varied the threshold to assign a topic only to a text when it covered 1%, 2%, or 5% of the text. The best results were obtained with 100 topics and a proportion threshold of 1%. We used these settings throughout our experiments. Two examples of LDA topics are shown here:",補足資料,Paper,True,Introduce（引用目的）,True,P16-2050_5_0,2016,Very quaffable and great fun: Applying NLP to wine reviews,Reference
1711,11715," https://www.wiktionary.org/"," ['2 A Collection of BV–PV Analogies']","The target selection was re-stricted to PV 1 /PV 2 combinations with identical particles, and where the two PVs were deemed (near-)synonyms according to the German stan-dard dictionary DUDEN or the German Wik-tionary [Cite_Footnote_2] , as we were interested in BV–PV analo-gies with semantically highly similar PVs.",2 https://www.wiktionary.org/,"BV 1 : P V 1 :: BV 2 : P V 2 such as klappern:abklappern::klopfen:abklopfen. We aimed for ≈200 analogies per particle type, focusing on the four highly frequent particle types ab, an, auf, aus. The target selection was re-stricted to PV 1 /PV 2 combinations with identical particles, and where the two PVs were deemed (near-)synonyms according to the German stan-dard dictionary DUDEN or the German Wik-tionary [Cite_Footnote_2] , as we were interested in BV–PV analo-gies with semantically highly similar PVs.",Material,Knowledge,True,Use（引用目的）,True,N18-2024_0_0,2018,Analogies in Complex Verb Meaning Shifts: The Effect of Affect in Semantic Similarity Models,Footnote
1712,11716," https://github.com/Babylonpartners/fastText_multilingual"," ['3 Representations of BV–PV Analogies', '3.3 Affect Models']","We took off-the-shelf word represen-tations [Cite_Footnote_4] for German and English that live in the same semantic space, learned a regression model based on the English data, and applied it to the German data by relying on findings from Köper and Schulte im Walde (2017), who showed that a feed-forward neural network obtained a high cor-relation with human-annotated ratings.",4 https://github.com/Babylonpartners/ fastText_multilingual,"We enriched the basic similarity model by inte-grating affective information from human-created lexicons. Since affective datasets are typically small-scale and mostly exist for English, we ap-plied a cross-lingual approach (Smith et al., 2017) to learn a linear transformation that aligns mono-lingual vectors from two languages in a single vec-tor space. We took off-the-shelf word represen-tations [Cite_Footnote_4] for German and English that live in the same semantic space, learned a regression model based on the English data, and applied it to the German data by relying on findings from Köper and Schulte im Walde (2017), who showed that a feed-forward neural network obtained a high cor-relation with human-annotated ratings.",Material,Knowledge,False,Use（引用目的）,True,N18-2024_1_0,2018,Analogies in Complex Verb Meaning Shifts: The Effect of Affect in Semantic Similarity Models,Footnote
1713,11717," https://allennlp.org/mocha"," ['References']",MOCHA presents a chal-lenging problem for developing accurate and robust generative reading comprehension met-rics. [Cite_Footnote_1],"1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha.","Posing reading comprehension as a genera-tion problem provides a great deal of flexibil-ity, allowing for open-ended questions with few restrictions on possible answers. How-ever, progress is impeded by existing gen-eration metrics, which rely on token over-lap and are agnostic to the nuances of read-ing comprehension. To address this, we in-troduce a benchmark for training and evalu-ating generative reading comprehension met-rics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 di-verse question answering datasets and an addi-tional set of minimal pairs for evaluation. Us-ing MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annota-tions. When we evaluate robustness on mini-mal pairs, LERC achieves 80% accuracy, out-performing baselines by 14 to 26 absolute per-centage points while leaving significant room for improvement. MOCHA presents a chal-lenging problem for developing accurate and robust generative reading comprehension met-rics. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.528_0_0,2020,MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,Footnote
1714,11718," https://github.com/raylin1000/drop-bert"," ['2 A Description of MOCHA', '2.2 Collecting Candidates']","We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT [Cite_Footnote_2] models for DROP.",2 https://github.com/raylin1000/drop-bert,"Candidates on all four generative datasets are gen-erated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for Nar-rativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each ques-tion has two references. We treat the second ref-erence as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT [Cite_Footnote_2] models for DROP.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.528_1_0,2020,MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,Footnote
1715,11719," https://github.com/salaniz/pycocoevalcap"," ['B Details on Baselines']","We use implementations of BLEU, METEOR, and ROUGE using Microsoft MS COCO evaluation scripts [Cite_Footnote_4] .",4 https://github.com/salaniz/pycocoevalcap,"We use implementations of BLEU, METEOR, and ROUGE using Microsoft MS COCO evaluation scripts [Cite_Footnote_4] . We removed question marks, periods, and exclamation marks from references and candi-dates when evaluating with BLEU, METEOR, and ROUGE.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.528_2_0,2020,MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,Footnote
1716,11720," http://www.cl.cam.ac.uk/~ah433/mcrae-quantified-majority.txt"," ['1 Introduction']","Using a publicly available dataset of feature norms annotated with quantifiers [Cite_Footnote_1] (Herbelot and Vec-chi, 2015), we show that human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data.",1 Data available at http://www.cl.cam.ac.uk/˜ah433/mcrae-quantified-majority.txt,"In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers. To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vec-tors – giving us a common representation across for-malisms. Using a publicly available dataset of feature norms annotated with quantifiers [Cite_Footnote_1] (Herbelot and Vec-chi, 2015), we show that human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data.",Material,Dataset,True,Use（引用目的）,True,D15-1003_0_0,2015,Building a shared world: Mapping distributional to model-theoretic semantic spaces,Footnote
1717,11721," http://www.cl.cam.ac.uk/~ah433/material/herbelot_iwcs13_data.txt"," ['3 Annotated datasets', '3.2 Additional animal data']",AD [Cite_Footnote_3] is a set of 72 animal concepts with quantification annotations along 54 features.,3 Data available at http://www.cl.cam.ac.uk/˜ah433/material/herbelot_iwcs13_data.txt.,"QMR gives us an average of 11 features per con-cept. This results in fairly sparse vectors in the model-theoretic semantic space (see §4). In order to remedy data sparsity, we consider the use of additional data in the form of the animal dataset from Herbelot (2013) (henceforth AD). AD [Cite_Footnote_3] is a set of 72 animal concepts with quantification annotations along 54 features. The main differences between QMR and AD are as follows:",Material,Dataset,True,Produce（引用目的）,True,D15-1003_1_0,2015,Building a shared world: Mapping distributional to model-theoretic semantic spaces,Footnote
1718,11722," http://wacky.sslmit.unibo.it"," ['4 Semantic spaces', '4.1 The distributional semantic space']","As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [Cite_Footnote_4] , which consists of about 2.8 billion tokens.","4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk","We consider two distributional semantic space archi-tectures which have each shown to have considerable success in a number of semantic tasks. First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [Cite_Footnote_4] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimen-sions to 300 through Singular Value Decomposition.",Material,DataSource,True,Use（引用目的）,True,D15-1003_2_0,2015,Building a shared world: Mapping distributional to model-theoretic semantic spaces,Footnote
1719,11723," http://www.natcorp.ox.ac.uk"," ['4 Semantic spaces', '4.1 The distributional semantic space']","As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [Cite_Footnote_4] , which consists of about 2.8 billion tokens.","4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk","We consider two distributional semantic space archi-tectures which have each shown to have considerable success in a number of semantic tasks. First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [Cite_Footnote_4] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimen-sions to 300 through Singular Value Decomposition.",Material,DataSource,True,Use（引用目的）,True,D15-1003_3_0,2015,Building a shared world: Mapping distributional to model-theoretic semantic spaces,Footnote
1720,11724," https://code.google.com/p/word2vec"," ['4 Semantic spaces', '4.1 The distributional semantic space']","Next we consider the context-predicting vectors (DS Mikolov ) available as part of the word2vec [Cite_Footnote_6] project (Mikolov et al., 2013a).",6 https://code.google.com/p/word2vec,"Next we consider the context-predicting vectors (DS Mikolov ) available as part of the word2vec [Cite_Footnote_6] project (Mikolov et al., 2013a). We use the publicly avail-able vectors which were trained on a Google News dataset of circa 100 billion tokens. Baroni et al. (2014b) showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this op-tion as a valid distributional representation of a word’s semantics.",Method,Tool,True,Introduce（引用目的）,False,D15-1003_4_0,2015,Building a shared world: Mapping distributional to model-theoretic semantic spaces,Footnote
1721,11725," https://utexas.box.com/s/ekznoh08afi1kpkbf0hb"," ['2 Related Work', '2.1 Formal Distributional Semantics']",Our account is also similar to that proposed by Erk (2015) [Cite_Ref] .,Katrin Erk. 2015. What do you know about an alli-gator when you know the company it keeps? Un-published draft. https://utexas.box.com/s/ekznoh08afi1kpkbf0hb.,"Our account is also similar to that proposed by Erk (2015) [Cite_Ref] . Erk suggests that distributional data influences semantic ‘knowledge’ : specifically, while a speaker may not know the extension of the word alligator, they maintain an information state which models properties of alligators (for instance, that they are animals). This information state is described in terms of probabilistic logic, which accounts for an agent’s uncertainty about what the world is like. The probability of a sentence is the summed probability of the possible worlds that make it true. Similarly, we assume a systematic relation between distributional information and world knowl-edge, expressed set-theoretically. The knowledge rep-resentation we derive is not a model proper: it cannot be said to be a description of a world – either the real one or a speaker’s set of beliefs (c.f. §4 for more de-tails). But it is a good approximation of the shared in-tuitions people have about the world, in the way that distributional representations are an averaged represen-tation of how a group of speakers use their language.",補足資料,Document,True,Compare（引用目的）,True,D15-1003_5_0,2015,Building a shared world: Mapping distributional to model-theoretic semantic spaces,Reference
1722,11726," http://www.jstatsoft.org/v18/i02/"," ['References']",[Cite] http://www.,,"Björn-Helge Mevik and Ron Wehrens. 2007. The pls package: Principal component and partial least squares regression in R. Journal of Statistical Soft-ware, 18(2). Published online: [Cite] http://www.jstatsoft.org/v18/i02/.",補足資料,Paper,True,Introduce（引用目的）,True,D15-1003_6_0,2015,Building a shared world: Mapping distributional to model-theoretic semantic spaces,Body
1723,11727," https://github.com/webis-de/acl20-editorials-style-persuasive-effect"," ['1 Introduction']","Based on the NY-Times editorial corpus of El Baff et al. (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented clas-sifiers for persuasive effect (Section 5). [Cite_Footnote_1]","1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect","This paper analyzes the persuasive effect of style in news editorial argumentation on readers with dif-ferent political ideologies (conservative vs. liberal). We model style with widely-used features captur-ing argumentativeness (Somasundaran et al., 2007), psychological meaning (Tausczik and Pennebaker, 2010), and similar (Section 3). Based on the NY-Times editorial corpus of El Baff et al. (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented clas-sifiers for persuasive effect (Section 5). [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.acl-main.287_0_0,2020,Analyzing the Persuasive Effect of Style in News Editorial Argumentation,Footnote
1724,11728," http://home.uchicago.edu/~cbs2/banglainstruction.html"," ['3 Resource Preparation']",These lists have been converted to Bengali us-ing English to Bengali bilingual dictionary [Cite_Footnote_1] .,1 http://home.uchicago.edu/~cbs2/banglainstruction.html,"These lists have been converted to Bengali us-ing English to Bengali bilingual dictionary [Cite_Footnote_1] . These six lists have been termed as Emotion lists. A Bengali SentiWordNet is being developed by replacing each word entry in the synonymous set of the English SentiWordNet (Esuli et al., 2006) by its equivalent Bengali meaning using the same English to Bengali bilingual dictionary.",Material,Knowledge,True,Use（引用目的）,True,P09-2038_0_0,2009,Word to Sentence Level Emotion Tagging for Bengali Blogs,Footnote
1725,11729," http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf"," ['4 Word Level Emotion Classification', '4.1 Feature Selection and Training']","The POS tagger was developed with a tagset of 26 POS tags [Cite_Footnote_2] , defined for the Indian languages.",2 http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf," POS information: We are interested with the verb, noun, adjective and adverb words as these are emotion informative constitu-ents. For this feature, total 1300 sentences has been passed through a Bengali part of speech tagger (Ekbal et al. 2008) based on Support Vector Machine (SVM) tech-nique. The POS tagger was developed with a tagset of 26 POS tags [Cite_Footnote_2] , defined for the Indian languages. The POS tagger has demonstrated an overall accuracy of ap-proximately 90%.",Material,Knowledge,True,Produce（引用目的）,True,P09-2038_1_0,2009,Word to Sentence Level Emotion Tagging for Bengali Blogs,Footnote
1726,11730," https://github.com/UKPLab/emnlp2017-cmapsum-corpus"," ['References']",We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [Cite_Footnote_1],1 Available at https://github.com/UKPLab/ emnlp2017-cmapsum-corpus,"Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created cor-pus of concept maps that summarize het-erogeneous collections of web documents on educational topics. It was created us-ing a novel crowdsourcing approach that allows us to efficiently determine impor-tant elements in large document collec-tions. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [Cite_Footnote_1]",Material,Dataset,True,Produce（引用目的）,True,D17-1320_0_0,2017,Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps,Footnote
1727,11731," https://github.com/knowitall/openie"," ['5 Corpus Creation', '5.2 Proposition Extraction']","We used Open IE 4 [Cite_Footnote_7] , a state-of-the-art system (Stanovsky and Dagan, 2016) to process all sentences.",7 https://github.com/knowitall/openie,"While the relation phrase is similar to a relation in a concept map, many arguments in these tuples represent useful concepts. We used Open IE 4 [Cite_Footnote_7] , a state-of-the-art system (Stanovsky and Dagan, 2016) to process all sentences. After removing du-plicates, we obtained 4137 tuples per topic.",Method,Tool,True,Use（引用目的）,True,D17-1320_1_0,2017,Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps,Footnote
1728,11732," https://github.com/kermitt2/grobid"," ['4 Methodology', '4.3 Preprocessing and Feature Extraction']",We extracted the text from PDFs using Grobid. [Cite_Footnote_2],2 https://github.com/kermitt2/grobid,We extracted the text from PDFs using Grobid. [Cite_Footnote_2] Several preprocessing steps were necessary before using the articles’ text as features in our model.,Method,Tool,True,Use（引用目的）,True,D19-1236_0_0,2019,The Myth of Double-Blind Review Revisited: ACL vs. EMNLP,Footnote
1729,11733," https://github.com/NP-NET-research/Recursive-Semi-Markov-Model"," ['1 Introduction']","Our code is released at [Cite] https://github.com/NP-NET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).",,"Our code is released at [Cite] https://github.com/NP-NET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).",Method,Code,True,Produce（引用目的）,True,2021.acl-long.205_0_0,2021,N-ary Constituent Tree Parsing with Recursive Semi-Markov Model,Body
1730,11734," https://nlp.cs.nyu.edu/evalb"," ['5 Experiments', '5.1 Experimental Setup']","Standard precision, recall and F1-measure are em-ployed as evaluation metrics, where the EVALB [Cite_Footnote_1] tool is employed in the single task.",1 https://nlp.cs.nyu.edu/evalb,"We evaluate the proposed framework in both En-glish and Chinese, on the datasets of PTB (WSJ sections (Marcus et al., 1993)) and CTB 5.1 (Xue et al., 2005), respectively. For Chinese, we evaluate both the single task of constituent parsing and the joint task with word segmentation and POS tagging. We follow the standard split of the datasets (Kitaev et al., 2019). In the single task for Chinese, some previous work utilize the Stanford tagger (Toutano-va et al., 2003) to generate the POS tags as input, which leads to a fixed error propagation. In this paper, POS tags are removed and not used as input features in both training and testing in CTB 5.1, following the previous work in (Zhang et al., 2020). Standard precision, recall and F1-measure are em-ployed as evaluation metrics, where the EVALB [Cite_Footnote_1] tool is employed in the single task. The hyper-parameters in the implementation are shown in Ta-ble. 1. Most of them are set following the Berkeley parser (Kitaev and Klein, 2018). When choosing the pre-train models (Wolf et al., 2020), “bert-large-cased” is utilized for English with a single RTX 3090, “bert-base-chinese” is utilized for Chinese with a single RTX 1080TI.",Method,Tool,True,Use（引用目的）,False,2021.acl-long.205_1_0,2021,N-ary Constituent Tree Parsing with Recursive Semi-Markov Model,Footnote
1731,11735," https://github.com/venkatasg/Advice-EMNLP2020"," ['1 Introduction']",We make all of our data and code available online [Cite_Footnote_1] .,1 https://github.com/venkatasg/ Advice-EMNLP2020,"We establish benchmarks for this task with BERT (Devlin et al., 2019), a large pre-trained language model, to identify sentences that consti-tute advice. We find that it is substantially better than a rule-based approach (§5). In an in-depth analysis, we find that BERT re-discovers some lin-guistic rules that have been previously proposed for identifying advice, but struggles with advice that is more implicit, for example in the form of a narrative, like in (2) (§7). Our results also show that r/AskParents is more challenging for advice identification, despite the fact that r/needadvice has a wider range of topics. We make all of our data and code available online [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.427_0_0,2020,Help! Need Advice on Identifying Advice,Footnote
1732,11736," http://github.com/ajaech/querycompletion"," ['4 Experiments', '4.1 Implementation Details']","We use Adadelta (Zeiler, 2012) and tune the online learn-ing rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task. [Cite_Footnote_2]",2 Code at http://github.com/ajaech/query completion,"The vocabulary consists of 79 characters including special start and stop tokens. Models were trained for six epochs. The Adam optimizer is used dur-ing training with a learning rate of 10 −3 (Kingma and Ba, 2014). When updating the user embed-dings during evaluation, we found that it is easier to use an optimizer without momentum. We use Adadelta (Zeiler, 2012) and tune the online learn-ing rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task. [Cite_Footnote_2]",Method,Code,True,Produce（引用目的）,True,P18-2111_0_0,2018,Personalized Language Model for Query Auto-Completion,Footnote
1733,11737," https://hackingsemantics.xyz/2019/leaderboards/"," ['3 Utilitarian Critiques', '3.2 Prediction Cost']","(Rogers, 2019) [Cite_Ref] , since the resource de-mands would dwarf any utility from the model.",Anna Rogers. 2019. How the transformers broke NLP leaderboards. https://hackingsemantics.xyz/2019/leaderboards/. Accessed: 2020-05- 20.,"This means that a SOTA model can simultane-ously provide high utility to a leaderboard and zero utility to a practitioner, by virtue of being too impractical to use. For some time, this was true of the 175 billion parameter GPT-3 (Brown et al., 2020), which achieved SOTA on several few-shot tasks, but whose sheer size precludes it from be-ing fully reproduced by researchers. Even today, practitioners can only use GPT-3 through an API, access to which is restricted. The cost-ignorance of leaderboards disproportionately affects practi-tioners with fewer resources (e.g., independent re-searchers) (Rogers, 2019) [Cite_Ref] , since the resource de-mands would dwarf any utility from the model.",補足資料,Paper,True,Introduce（引用目的）,True,2020.emnlp-main.393_0_0,2020,Utility is in the Eye of the User: A Critique of NLP Leaderboards,Reference
1734,11738," https://hackingsemantics.xyz/2019/leaderboards/"," ['3 Utilitarian Critiques', '3.2 Prediction Cost']","If lower prediction costs also improved leaderboard utility, then there would be more interest in creating them (Linzen, 2020; Rogers, 2019 [Cite_Ref] ; Dodge et al., 2019).",Anna Rogers. 2019. How the transformers broke NLP leaderboards. https://hackingsemantics.xyz/2019/leaderboards/. Accessed: 2020-05- 20.,"Our point is not that there is no incentive at all to build cheaper models, but rather that this incen-tive is not baked into leaderboards, which are an important artefact of the NLP community. Because lower prediction costs improve practitioner utility, practitioners build them despite the lack of incen-tive from leaderboards. If lower prediction costs also improved leaderboard utility, then there would be more interest in creating them (Linzen, 2020; Rogers, 2019 [Cite_Ref] ; Dodge et al., 2019). At the very least, making prediction costs publicly available would allow users to better estimate the utility that they will get from a model, given that the leaderboard’s cost-ignorant ranking may be a poor proxy for their preferences.",補足資料,Paper,True,Introduce（引用目的）,True,2020.emnlp-main.393_0_1,2020,Utility is in the Eye of the User: A Critique of NLP Leaderboards,Reference
1735,11739," https://hackingsemantics.xyz/2019/leaderboards/"," ['4 The Future of Leaderboards', '4.1 A Leaderboard for Every User']","For example, reporting the costs of making predictions would put large institutions and poorly-resourced model creators on more equal footing (Rogers, 2019) [Cite_Ref] .",Anna Rogers. 2019. How the transformers broke NLP leaderboards. https://hackingsemantics.xyz/2019/leaderboards/. Accessed: 2020-05- 20.,"This would have beneficial second-order effects as well. For example, reporting the costs of making predictions would put large institutions and poorly-resourced model creators on more equal footing (Rogers, 2019) [Cite_Ref] . This might motivate the creation of simpler methods whose ease-of-use makes up for weaker performance, such as weighted-average sen-tence embeddings (Arora et al., 2019; Ethayarajh, 2018). Even if a poorly-resourced creator could not afford to train the SOTA model du jour, they could at least compete on the basis of efficiency or create a minimally viable system that meets some desired threshold (Dodge et al., 2019; Dorr, 2011). Reporting the performance on the worst-off group, in the spirit of Rawlsian fairness (Rawls, 2001; Hashimoto et al., 2018), would also incentivize creators to improve worst-case performance.",補足資料,Paper,True,Introduce（引用目的）,True,2020.emnlp-main.393_0_2,2020,Utility is in the Eye of the User: A Critique of NLP Leaderboards,Reference
1736,11740," https://hackingsemantics.xyz/2019/leaderboards/"," ['5 Conclusion']","We were not the first to criticize NLP leaderboards (Rogers, 2019 [Cite_Ref] ; Crane, 2018; Linzen, 2020), but we were the first to do so under a framework of utility, which we used to study the divergence between what is incentivized by leaderboards and what is valued by practitioners.",Anna Rogers. 2019. How the transformers broke NLP leaderboards. https://hackingsemantics.xyz/2019/leaderboards/. Accessed: 2020-05- 20.,"In this work, we offered several criticisms of leader-board design in NLP. While it has helped create more accurate models, we argued that this has been at the expense of fairness, efficiency, and robust-ness, among other desiderata. We were not the first to criticize NLP leaderboards (Rogers, 2019 [Cite_Ref] ; Crane, 2018; Linzen, 2020), but we were the first to do so under a framework of utility, which we used to study the divergence between what is incentivized by leaderboards and what is valued by practitioners. Given the diversity of NLP practitioners, there is no one-size-fits-all solution; rather, leaderboards should demand transparency, requiring the report-ing of statistics that may be of practical concern. Equipped with these statistics, each user could then estimate the utility that each model provides to them and then re-rank accordingly, effectively cre-ating a custom leaderboard for everyone.",補足資料,Paper,True,Introduce（引用目的）,True,2020.emnlp-main.393_0_3,2020,Utility is in the Eye of the User: A Critique of NLP Leaderboards,Reference
1737,11741," https://hackingsemantics.xyz/2020/reviewing-models/"," ['4 The Future of Leaderboards', '4.2 A Leaderboard for Every Type of User']","In contrast, researchers submitting their work to a con-ference may place more value on accuracy, given that a potential reviewer may reject a model that is not SOTA (Rogers, 2020) [Cite_Ref] .",Anna Rogers. 2020. Peer review in NLP: reject-if-not-SOTA. https://hackingsemantics.xyz/2020/reviewing-models/. Accessed: 2020-05- 20.,"While each practitioner may have their own utility function, groups of practitioners – characterized by a shared goal – can be modelled with a single function. For example, programmers working on low latency applications (e.g., multiplayer games) will place more value on latency than others. In contrast, researchers submitting their work to a con-ference may place more value on accuracy, given that a potential reviewer may reject a model that is not SOTA (Rogers, 2020) [Cite_Ref] . Although there is vari-ance within any group, this approach is tractable when there are many points of consensus.",補足資料,Paper,True,Introduce（引用目的）,True,2020.emnlp-main.393_1_0,2020,Utility is in the Eye of the User: A Critique of NLP Leaderboards,Reference
1738,11742," http://goo.gl/language/wiki-split"," ['1 Introduction']","• Public release of the English WikiSplit dataset, containing one million rewrites: [Cite] http://goo.gl/language/wiki-split",,"• Public release of the English WikiSplit dataset, containing one million rewrites: [Cite] http://goo.gl/language/wiki-split",Material,Dataset,True,Introduce（引用目的）,False,D18-1080_0_0,2018,Learning To Split and Rephrase From Wikipedia Edit History,Body
1739,11743," http://github.com/shashiongithub/Split-and-Rephrase"," ['2 The WikiSplit Corpus', '2.3 Comparison to WebSplit']",This is to be expected given that WebSplit’s small vocabulary of 7k words must account for the 344k tokens that make up the distinct complex sen-tences themselves. [Cite_Footnote_3],"3 We use WebSplit v1.0 throughout, which is the scaled-up re-release by Narayan et al. (2017) at http://github.com/shashiongithub/Split-and-Rephrase, commit a9a288c. Preliminary experiments showed the same trends on the smaller v0.1 corpus, as resplit by Aharoni and Goldberg (2018).",This is to be expected given that WebSplit’s small vocabulary of 7k words must account for the 344k tokens that make up the distinct complex sen-tences themselves. [Cite_Footnote_3],Method,Tool,True,Use（引用目的）,True,D18-1080_1_0,2018,Learning To Split and Rephrase From Wikipedia Edit History,Footnote
1740,11744," https://github.com/ccsasuke/man"," ['References']","In this work, we propose a multino-mial adversarial network [Cite_Footnote_1] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains.",1 The source code of MAN is available at https://github.com/ccsasuke/man.,"Many text classification tasks are known to be highly domain-dependent. Unfortunately, the availability of training data can vary drasti-cally across domains. Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multino-mial adversarial network [Cite_Footnote_1] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theo-retical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multi-ple probability distributions. MANs are thus a theoretically sound generalization of tradi-tional adversarial networks that discriminate over two distributions. More specifically, for the MDTC task, MAN learns features that are invariant across multiple domains by resort-ing to its ability to reduce the divergence among the feature distributions of each do-main. We present experimental results show-ing that MANs significantly outperform the prior art on the MDTC task. We also show that MANs achieve state-of-the-art performance for domains with no labeled data.",Method,Code,True,Produce（引用目的）,True,N18-1111_0_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Footnote
1741,11745," https://doi.org/10.1007/978-3-540-"," ['3 Theories of Multinomial Adversarial Networks']","We then have the following main theo-rems for the domain loss for F s : to its optimality, if D adopts the NLL loss: where JSD(·) is the generalized Jensen-Shannon Divergence (Lin, 1991) among multiple distribu-tions, defined as the average Kullback-Leibler di-vergence of each P i to the centroid P (Aslam and Pavlu, 2007) [Cite_Ref] .","Javed A. Aslam and Virgil Pavlu. 2007. Query hardness estimation using jensen-shannon di-vergence among multiple scoring functions. In Advances in Information Retrieval. Springer Berlin Heidelberg, Berlin, Heidelberg, pages 198–209. https://doi.org/10.1007/978-3-540- 71496-5 20.","We first derive the optimal D for any fixed F s . Lemma 1. For any fixed F s , with either NLL or L2 loss, the optimum domain discriminator D ⇤ is: grangian Multiplier to solve the minimum value of J D , and the details can be found in the Ap-pendix. We then have the following main theo-rems for the domain loss for F s : to its optimality, if D adopts the NLL loss: where JSD(·) is the generalized Jensen-Shannon Divergence (Lin, 1991) among multiple distribu-tions, defined as the average Kullback-Leibler di-vergence of each P i to the centroid P (Aslam and Pavlu, 2007) [Cite_Ref] .",補足資料,Paper,False,Introduce（引用目的）,False,N18-1111_2_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1742,11746,https://doi.org/10.1007/978-3-540-,[\'References\'],"There are two equivalent definitions of the generalized Jensen-Shannon divergence: the original def-inition based on Shannon entropy (Lin, 1991), and a reshaped one expressed as the average Kullback-Leibler divergence of each P i to the centroid P (Aslam and Pavlu, 2007) [Cite_Ref] .","Javed A. Aslam and Virgil Pavlu. 2007. Query hardness estimation using jensen-shannon di-vergence among multiple scoring functions. In Advances in Information Retrieval. Springer Berlin Heidelberg, Berlin, Heidelberg, pages 198–209. https://doi.org/10.1007/978-3-540- 71496-5 20.","There are two equivalent definitions of the generalized Jensen-Shannon divergence: the original def-inition based on Shannon entropy (Lin, 1991), and a reshaped one expressed as the average Kullback-Leibler divergence of each P i to the centroid P (Aslam and Pavlu, 2007) [Cite_Ref] . We adopt the latter one here: f⇠P i "" # f⇠P i 4log P i (f) 35 E f⇠P i \uf8ff",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_2_1,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1743,11747," http://papers.nips.cc/paper/6254-domain-separation-networks.pdf"," ['1 Introduction']","This paradigm is sometimes known as the Shared-Private model (Bousmalis et al., 2016) [Cite_Ref] .","Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343–351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.","One state-of-the-art system for MDTC, the CMSC system of Wu and Huang (2015), com-bines a classifier that is shared across all do-mains (for learning domain-invariant knowledge) with a set of classifiers, one per domain, each of which captures domain-specific text classification knowledge. This paradigm is sometimes known as the Shared-Private model (Bousmalis et al., 2016) [Cite_Ref] . CMSC, however, lacks an explicit mech-anism to ensure that the shared classifier captures only domain-independent knowledge: the shared classifier may well also acquire some domain-specific features that are useful for a subset of the domains. We hypothesize that better performance can be obtained if this constraint were explicitly enforced.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_6_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1744,11748," http://papers.nips.cc/paper/6254-domain-separation-networks.pdf"," ['2 Model', '2.1 Model Architecture']","As shown in Figure 1, the Multinomial Adver-sarial Network (MAN) adopts the Shared-Private paradigm of Bousmalis et al. (2016) [Cite_Ref] and consists of four components: a shared feature extractor F s , a domain feature extractor F d for each labeled domain d i 2 L , a text classifier C, and a domain discriminator D.","Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343–351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.","As shown in Figure 1, the Multinomial Adver-sarial Network (MAN) adopts the Shared-Private paradigm of Bousmalis et al. (2016) [Cite_Ref] and consists of four components: a shared feature extractor F s , a domain feature extractor F d for each labeled domain d i 2 L , a text classifier C, and a domain discriminator D. The main idea of MAN is to ex-plicitly model the domain-invariant features that are beneficial to the main classification task across all domains (i.e. the shared features, extracted by F s ), as well as the domain-specific features that mainly contribute to the classification in its own domain (the domain features, extracted by F d ). Here, the adversarial domain discriminator D has a multinomial output that takes a shared feature training on a mini-batch of data from one domain. One training iteration consists of one such mini-batch train-ing from each domain. The parameters of F s , F d , C are updated together, and the training flows are illustrated by the green arrows. The parameters of D are updated separately, shown in red arrows. Solid lines indicate forward passes while dotted lines are backward passes. J FD is the domain loss for F s , which is anticorrelated vector and predicts the likelihood of that sample coming from each domain. As seen in Figure 1, during the training of F s (green arrows denote the training flow), F s aims to confuse D by minimiz-ing J FD , which is anticorrelated to J D (detailed in s §2.2), so that D cannot predict the domain of a sample given its shared features. The intuition is that if even a strong discriminator D cannot tell the domain of a sample from the extracted features, those features F s learned are essentially domain invariant. By enforcing domain-invariant features to be learned by F s , when trained jointly via back-propagation, the set of domain feature extractors F d will each learn domain-specific features bene-ficial within its own domain.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_6_1,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1745,11749," http://papers.nips.cc/paper/6254-domain-separation-networks.pdf"," ['6 Related Work']","Bousmalis et al. (2016) [Cite_Ref] utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting.","Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343–351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.","Domain Adaptation Domain Adaptation at-tempts to transfer the knowledge from a source domain to a target one, and the traditional form is the single-source, single-target (SS,ST) adapta-tion (Blitzer et al., 2006). Another variant is the SS,MT adaptation (Yang and Eisenstein, 2015), which tries to simultaneously transfer the knowl-edge to multiple target domains from a single source. However, it cannot fully take advantage the training data if it comes from multiple source domains. MS,ST adaptation (Mansour et al., 2009; Zhao et al., 2017) can deal with multiple source domains but only transfers to a single target domain. Therefore, when multiple target domains exist, they need to treat them as independent prob-lems, which is more expensive and cannot utilize the additional unlabeled data in these domains. Fi-nally, MDTC can be viewed as MS,MT adapta-tion, which is arguably more general and realistic. Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al. (2016) first used it for the SS,ST domain adaptation followed by many others. Bousmalis et al. (2016) [Cite_Ref] utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss. We propose MAN as a more gen-eral framework with alternative architectures for the adversarial component, and for the first time provide theoretical justifications the multinomial adversarial nets. Moreover, Liu et al. (2017) used a LSTM without attention as their feature extrac-tor, which we found to perform sub-optimal in the experiments. We instead chose Convolutional Neural Nets as our feature extractor that achieves higher accuracy while running an order of magni-tude faster (see §4.3).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_6_2,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1746,11750," https://dl.acm.org/citation.cfm?id=601016"," ['3 Theories of Multinomial Adversarial Networks']","Consequently, by the non-negativity and joint convexity of the f-divergence (Csiszar and Korner, 1982) [Cite_Ref] , we have: Corollary 1.","Imre Csiszar and Janos Korner. 1982. Infor-mation Theory: Coding Theorems for Discrete Memoryless Systems. Academic Press, Inc., Orlando, FL, USA. https://dl.acm.org/citation.cfm?id=601016.","Consequently, by the non-negativity and joint convexity of the f-divergence (Csiszar and Korner, 1982) [Cite_Ref] , we have: Corollary 1. The optimum of J D is N logN when using NLL loss, and 0 for the F s L2 loss. The optimum value above is achieved if and only if P 1 = P 2 = · · · = P N = P for either loss.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_9_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1747,11751," http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf"," ['1 Introduction']","In contrast to standard adversarial networks (Goodfellow et al., 2014) [Cite_Ref] , which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions.","Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672–2680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.","In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014) [Cite_Ref] , which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g. image generation (Goodfellow et al., 2014), domain adaptation (Ganin et al., 2016), cross-lingual text classification (Chen et al., 2016)), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_12_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1748,11752," http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf"," ['1 Introduction']","And just as binomial adversar-ial networks have been applied to numerous tasks (e.g. image generation (Goodfellow et al., 2014) [Cite_Ref] , domain adaptation (Ganin et al., 2016), cross-lingual text classification (Chen et al., 2016)), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.","Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672–2680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.","In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014), which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g. image generation (Goodfellow et al., 2014) [Cite_Ref] , domain adaptation (Ganin et al., 2016), cross-lingual text classification (Chen et al., 2016)), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_12_1,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1749,11753," http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf"," ['6 Related Work']","The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [Cite_Ref] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017).","Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672–2680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.","Domain Adaptation Domain Adaptation at-tempts to transfer the knowledge from a source domain to a target one, and the traditional form is the single-source, single-target (SS,ST) adapta-tion (Blitzer et al., 2006). Another variant is the SS,MT adaptation (Yang and Eisenstein, 2015), which tries to simultaneously transfer the knowl-edge to multiple target domains from a single source. However, it cannot fully take advantage the training data if it comes from multiple source domains. MS,ST adaptation (Mansour et al., 2009; Zhao et al., 2017) can deal with multiple source domains but only transfers to a single target domain. Therefore, when multiple target domains exist, they need to treat them as independent prob-lems, which is more expensive and cannot utilize the additional unlabeled data in these domains. Fi-nally, MDTC can be viewed as MS,MT adapta-tion, which is arguably more general and realistic. Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [Cite_Ref] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al. (2016) first used it for the SS,ST domain adaptation followed by many others. Bousmalis et al. (2016) utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss. We propose MAN as a more gen-eral framework with alternative architectures for the adversarial component, and for the first time provide theoretical justifications the multinomial adversarial nets. Moreover, Liu et al. (2017) used a LSTM without attention as their feature extrac-tor, which we found to perform sub-optimal in the experiments. We instead chose Convolutional Neural Nets as our feature extractor that achieves higher accuracy while running an order of magni-tude faster (see §4.3).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_12_2,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1750,11754," http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf"," ['6 Related Work']","MS,ST adaptation (Mansour et al., 2009 [Cite_Ref]","Yishay Mansour, Mehryar Mohri, and Afshin Ros-tamizadeh. 2009. Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems 21, Curran Associates, Inc., pages 1041–1048. http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf.","Domain Adaptation Domain Adaptation at-tempts to transfer the knowledge from a source domain to a target one, and the traditional form is the single-source, single-target (SS,ST) adapta-tion (Blitzer et al., 2006). Another variant is the SS,MT adaptation (Yang and Eisenstein, 2015), which tries to simultaneously transfer the knowl-edge to multiple target domains from a single source. However, it cannot fully take advantage the training data if it comes from multiple source domains. MS,ST adaptation (Mansour et al., 2009 [Cite_Ref] ; Zhao et al., 2017) can deal with multiple source domains but only transfers to a single target domain. Therefore, when multiple target domains exist, they need to treat them as independent prob-lems, which is more expensive and cannot utilize the additional unlabeled data in these domains. Fi-nally, MDTC can be viewed as MS,MT adapta-tion, which is arguably more general and realistic. Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al. (2016) first used it for the SS,ST domain adaptation followed by many others. Bousmalis et al. (2016) utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss. We propose MAN as a more gen-eral framework with alternative architectures for the adversarial component, and for the first time provide theoretical justifications the multinomial adversarial nets. Moreover, Liu et al. (2017) used a LSTM without attention as their feature extrac-tor, which we found to perform sub-optimal in the experiments. We instead chose Convolutional Neural Nets as our feature extractor that achieves higher accuracy while running an order of magni-tude faster (see §4.3).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_22_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1751,11755," https://doi.org/10.1109/ICCV.2017.304"," ['2 Model', '2.2 Training']","The first one is to use the NLL loss same as C which suits the classification task; while an-other option is to use the Least-Square (L2) loss that was shown to be able to alleviate the gradient vanishing problem when using the NLL loss in the adversarial setting (Mao et al., 2017) [Cite_Ref] : where d is the domain index of some sample and dˆis the prediction.","Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, and Stephen Paul Smolley. 2017. Least squares generative adversarial networks. In IEEE International Conference on Computer Vision (ICCV). pages 2813–2821. https://doi.org/10.1109/ICCV.2017.304.","In Algorithm 1, L C and L D are the loss func-tions of the text classifier C and the domain dis-criminator D, respectively. As mentioned in §2.1, C has a softmax layer on top for classifica-tion. We hence adopt the canonical negative log-likelihood (NLL) loss: where y is the true label and ŷ is the softmax predictions. For D, we consider two variants of MAN. The first one is to use the NLL loss same as C which suits the classification task; while an-other option is to use the Least-Square (L2) loss that was shown to be able to alleviate the gradient vanishing problem when using the NLL loss in the adversarial setting (Mao et al., 2017) [Cite_Ref] : where d is the domain index of some sample and dˆis the prediction. Without loss of generality, we",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_23_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1752,11756," http://papers.nips.cc/paper/"," ['1 Introduction']","In contrast to standard adversarial networks (Goodfellow et al., 2014), which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016) [Cite_Ref] , MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions.","Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., pages 271– 279. http://papers.nips.cc/paper/ 6066-f-gan-training-generative- neural-samplers-using-variational-divergence-minimization.pdf.","In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014), which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016) [Cite_Ref] , MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g. image generation (Goodfellow et al., 2014), domain adaptation (Ganin et al., 2016), cross-lingual text classification (Chen et al., 2016)), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_27_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1753,11757," http://papers.nips.cc/paper/"," ['3 Theories of Multinomial Adversarial Networks']","Binomial adversarial nets are known to have theoretical connections to the minimization of various f-divergences between two distribu-tions (Nowozin et al., 2016) [Cite_Ref] .","Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., pages 271– 279. http://papers.nips.cc/paper/ 6066-f-gan-training-generative- neural-samplers-using-variational-divergence-minimization.pdf.","Binomial adversarial nets are known to have theoretical connections to the minimization of various f-divergences between two distribu-tions (Nowozin et al., 2016) [Cite_Ref] . However, for ad-versarial training among multiple distributions, no theoretical justifications have been provided to our best knowledge, despite that this idea has recently been explored empirically (Liu et al., 2017).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_27_1,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1754,11758," https://openreview.net/pdf?id=BJJsrmfCZ"," ['5 Implementation Details']","MAN is implemented using PyTorch (Paszke et al., 2017) [Cite_Ref] .","Adam Paszke, Sam Gross, Soumith Chintala, Gre-gory Chanan, Edward Yang, Zachary DeVito, Zem-ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in py-torch. NIPS 2017 Autodiff Workshop https://openreview.net/pdf?id=BJJsrmfCZ.","For the CNN feature extractor used in the FDU-MTL experiment, a single convolution layer is used. The kernel sizes are 3, 4, and 5, and the num-ber of kernels are 200. The convolution layers take as input the 100d word embeddings of each word in the input sequence. We use word2vec word em-beddings (Mikolov et al., 2013) trained on a bunch of unlabeled raw Amazon reviews (Blitzer et al., 2007). After convolution, the outputs go through a ReLU layer before fed into a max pooling layer. The pooled output is then fed into a single fully connected layer to be converted into a feature vec-tor of size either 128 or 64. More details of us-ing CNN for text classification can be found in the original paper (Kim, 2014). MAN is implemented using PyTorch (Paszke et al., 2017) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_28_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1755,11759," https://doi.org/10.1109/ICDM.2015.68"," ['1 Introduction']","One state-of-the-art system for MDTC, the CMSC system of Wu and Huang (2015) [Cite_Ref] , com-bines a classifier that is shared across all do-mains (for learning domain-invariant knowledge) with a set of classifiers, one per domain, each of which captures domain-specific text classification knowledge.",Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459–468. https://doi.org/10.1109/ICDM.2015.68.,"One state-of-the-art system for MDTC, the CMSC system of Wu and Huang (2015) [Cite_Ref] , com-bines a classifier that is shared across all do-mains (for learning domain-invariant knowledge) with a set of classifiers, one per domain, each of which captures domain-specific text classification knowledge. This paradigm is sometimes known as the Shared-Private model (Bousmalis et al., 2016). CMSC, however, lacks an explicit mech-anism to ensure that the shared classifier captures only domain-independent knowledge: the shared classifier may well also acquire some domain-specific features that are useful for a subset of the domains. We hypothesize that better performance can be obtained if this constraint were explicitly enforced.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_29_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1756,11760," https://doi.org/10.1109/ICDM.2015.68"," ['1 Introduction']","We find first that MAN significantly outperforms the state-of-the-art CMSC method (Wu and Huang, 2015) [Cite_Ref] on the widely used multi-domain Amazon review dataset, and does so without relying on external re-sources such as sentiment lexica (§4.1).",Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459–468. https://doi.org/10.1109/ICDM.2015.68.,"We then validate the effectiveness of MAN in experiments on two MDTC data sets. We find first that MAN significantly outperforms the state-of-the-art CMSC method (Wu and Huang, 2015) [Cite_Ref] on the widely used multi-domain Amazon review dataset, and does so without relying on external re-sources such as sentiment lexica (§4.1). When ap-plied to the second dataset, FDU-MTL (§4.3), we obtain similar results: MAN achieves substantially higher accuracy than the previous top-performing method, ASP-MTL (Liu et al., 2017). ASP-MTL is the first empirical attempt to use a multinomial adversarial network for multi-task learning, but is more restricted and can be viewed as a special case of MAN. In addition, we provide the first theoretical guarantees for multinomial adversarial networks (§3). Finally, while many MDTC methods such as CMSC require labeled data for each domain, MANs can be applied in cases where no labeled data ex-ists for a subset of domains. To evaluate MAN in this semi-supervised setting, we compare MAN to a method that can accommodate unlabeled data for (only) one domain (Zhao et al., 2017), and show that MAN achieves performance comparable to the state of the art (§4.2).",補足資料,Paper,True,Compare（引用目的）,True,N18-1111_29_1,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1757,11761," https://doi.org/10.1109/ICDM.2015.68"," ['3 Theories of Multinomial Adversarial Networks']",(±0.16) (±0.23) (±0.15) (±0.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) 3 Wu and Huang (2015) [Cite_Ref] 3 Zhao et al. (2017),Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459–468. https://doi.org/10.1109/ICDM.2015.68.,Book DVD Elec. Kit. Avg. Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 ±0 .46 83.98 87.22* 88.53 85.55* .25) (±0.17) (±0.04) (±0.19) (±0.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (±0.28) (±0.16) (±0.23) (±0.15) (±0.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) 3 Wu and Huang (2015) [Cite_Ref] 3 Zhao et al. (2017),補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_29_2,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1758,11762," https://doi.org/10.1109/ICDM.2015.68"," ['3 Theories of Multinomial Adversarial Networks']",Mod-els in bold are ours while the performance of the rest is taken from Wu and Huang (2015) [Cite_Ref] .,Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459–468. https://doi.org/10.1109/ICDM.2015.68.,"Table 1: MDTC results on the Amazon dataset. Mod-els in bold are ours while the performance of the rest is taken from Wu and Huang (2015) [Cite_Ref] . Numbers in paren-theses indicate standard errors, calculated based on 5 runs. Bold numbers indicate the highest performance in each domain, and ⇤ shows statistical significance (p < 0.05) over CMSC under a one-sample T-Test.",補足資料,Paper,True,Compare（引用目的）,True,N18-1111_29_3,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1759,11763," https://doi.org/10.1109/ICDM.2015.68"," ['4 Experiments', '4.1 Multi-Domain Text Classification']","Following Wu and Huang (2015) [Cite_Ref] , we con-duct 5-way cross validation.",Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459–468. https://doi.org/10.1109/ICDM.2015.68.,"The Amazon dataset contains 2000 samples for each of the four domains: book, DVD, electron-ics, and kitchen, with binary labels (positive, neg-ative). Following Wu and Huang (2015) [Cite_Ref] , we con-duct 5-way cross validation. Three out of the five folds are treated as the training set, one serves as the validation set, while the remaining is the test set. The 5-fold average test accuracy is reported.",補足資料,Paper,True,Compare（引用目的）,True,N18-1111_29_4,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1760,11764," https://doi.org/10.1109/ICDM.2015.68"," ['6 Related Work']","The prior art of MDTC (Wu and Huang, 2015) [Cite_Ref] decomposes the text classifier into a gen-eral one and a set of domain-specific ones.",Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459–468. https://doi.org/10.1109/ICDM.2015.68.,"Multi-Domain Text Classification The MDTC task was first examined by Li and Zong (2008), who proposed to fuse the training data from multi-ple domains either at the feature level or the classi-fier level. The prior art of MDTC (Wu and Huang, 2015) [Cite_Ref] decomposes the text classifier into a gen-eral one and a set of domain-specific ones. How-ever, the general classifier is learned by param-eter sharing and domain-specific knowledge may sneak into it. They also require external resources to help improve accuracy and compute domain similarities.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_29_5,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1761,11765," https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344"," ['6 Related Work']","The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017 [Cite_Ref] ).","Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence genera-tive adversarial nets with policy gradient. In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344.","Domain Adaptation Domain Adaptation at-tempts to transfer the knowledge from a source domain to a target one, and the traditional form is the single-source, single-target (SS,ST) adapta-tion (Blitzer et al., 2006). Another variant is the SS,MT adaptation (Yang and Eisenstein, 2015), which tries to simultaneously transfer the knowl-edge to multiple target domains from a single source. However, it cannot fully take advantage the training data if it comes from multiple source domains. MS,ST adaptation (Mansour et al., 2009; Zhao et al., 2017) can deal with multiple source domains but only transfers to a single target domain. Therefore, when multiple target domains exist, they need to treat them as independent prob-lems, which is more expensive and cannot utilize the additional unlabeled data in these domains. Fi-nally, MDTC can be viewed as MS,MT adapta-tion, which is arguably more general and realistic. Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017 [Cite_Ref] ). Ganin et al. (2016) first used it for the SS,ST domain adaptation followed by many others. Bousmalis et al. (2016) utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss. We propose MAN as a more gen-eral framework with alternative architectures for the adversarial component, and for the first time provide theoretical justifications the multinomial adversarial nets. Moreover, Liu et al. (2017) used a LSTM without attention as their feature extrac-tor, which we found to perform sub-optimal in the experiments. We instead chose Convolutional Neural Nets as our feature extractor that achieves higher accuracy while running an order of magni-tude faster (see §4.3).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_31_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1762,11766," http://www.public.asu.edu/~jye02/Software/MALSAR"," ['3 Theories of Multinomial Adversarial Networks']",(±0.16) (±0.23) (±0.15) (±0.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [Cite_Ref] 3 Wu and Huang (2015) 3 Zhao et al. (2017),"J. Zhou, J. Chen, and J. Ye. 2011. MAL-SAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University. http://www.public.asu.edu/˜jye02/Software/MALSAR.",Book DVD Elec. Kit. Avg. Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 ±0 .46 83.98 87.22* 88.53 85.55* .25) (±0.17) (±0.04) (±0.19) (±0.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (±0.28) (±0.16) (±0.23) (±0.15) (±0.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [Cite_Ref] 3 Wu and Huang (2015) 3 Zhao et al. (2017),補足資料,Paper,True,Introduce（引用目的）,True,N18-1111_33_0,2018,Multinomial Adversarial Networks for Multi-Domain Text Classification,Reference
1763,11767," https://github.com/salesforce/cove"," ['2 Proposed model: SAN']","We concatenate a pre-trained 600-dimensional CoVe vectors [Cite_Footnote_1] (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the fi-nal input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer.",1 https://github.com/salesforce/cove,"Contextual Encoding Layer. Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings. We concatenate a pre-trained 600-dimensional CoVe vectors [Cite_Footnote_1] (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the fi-nal input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer. To reduce the parameter size, we use a maxout layer (Goodfellow et al., 2013) at each BiLSTM layer to shrink its dimension. By a concatena-tion of the outputs of two BiLSTM layers, we obtain H q ∈ R 2d×m as representation of Q and H p ∈ R 2d×n as representation of P, where d is the hidden size of the BiLSTM.",Material,Knowledge,False,Use（引用目的）,True,P18-1157_0_0,2018,Stochastic Answer Networks for Machine Reading Comprehension,Footnote
1764,11768," https://spacy.io"," ['3 Experiment Setup']","Implementation details: The spaCy tool [Cite_Footnote_2] is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags.",2 https://spacy.io,"Implementation details: The spaCy tool [Cite_Footnote_2] is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encod-ing. The mini-batch size is set to 32 and Adamax (Kingma and Ba, 2014) is used as our optimizer. The learning rate is set to 0.002 at first and de-creased by half after every 10 epochs. We set the dropout rate for all the hidden units of LSTM, and the answer module output layer to 0.4. To prevent degenerate output, we ensure that at least one step in the answer module is active during training.",Method,Tool,True,Use（引用目的）,True,P18-1157_1_0,2018,Stochastic Answer Networks for Machine Reading Comprehension,Footnote
1765,11769," https://aclweb.org/anthology/D16-1264"," ['1 Introduction']","For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) [Cite_Ref] illustrates the need for synthesis of information across sentences and multiple steps of reasoning:","Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383–2392. https://aclweb.org/anthology/D16-1264.","Machine reading comprehension (MRC) is a chal-lenging task: the goal is to have machines read a text passage and then answer any question about the passage. This task is an useful benchmark to demonstrate natural language understanding, and also has important applications in e.g. conversa-tional agents and customer service support. It has been hypothesized that difficult MRC problems re-quire some form of multi-step synthesis and rea-soning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) [Cite_Ref] illustrates the need for synthesis of information across sentences and multiple steps of reasoning:",Material,Dataset,True,Introduce（引用目的）,False,P18-1157_5_0,2018,Stochastic Answer Networks for Machine Reading Comprehension,Reference
1766,11770," https://aclweb.org/anthology/D16-1264"," ['3 Experiment Setup']","Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) [Cite_Ref] .","Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383–2392. https://aclweb.org/anthology/D16-1264.","Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) [Cite_Ref] . This contains about 23K passages and 100K questions. The passages come from approx-imately 500 Wikipedia articles and the questions and answers are obtained by crowdsourcing. The crowdsourced workers are asked to read a passage (a paragraph), come up with questions, then mark the answer span. All results are on the official de-velopment set, unless otherwise noted.",Material,Dataset,True,Use（引用目的）,True,P18-1157_5_1,2018,Stochastic Answer Networks for Machine Reading Comprehension,Reference
1767,11771," https://aclweb.org/anthology/D16-1264"," ['6 Related Work']","The recent big progress on MRC is largely due to the availability of the large-scale datasets (Ra-jpurkar et al., 2016 [Cite_Ref] ; Nguyen et al., 2016; Richard-son et al., 2013; Hill et al., 2016), since it is possi-ble to train large end-to-end neural network mod-els.","Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383–2392. https://aclweb.org/anthology/D16-1264.","The recent big progress on MRC is largely due to the availability of the large-scale datasets (Ra-jpurkar et al., 2016 [Cite_Ref] ; Nguyen et al., 2016; Richard-son et al., 2013; Hill et al., 2016), since it is possi-ble to train large end-to-end neural network mod-els. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model first maps the symbolic rep-resentation of the documents and questions into a neural space, then search answers on top of it. We categorize these models into two groups based on the difference of the answer module: single-step and multi-step reasoning. The key difference between the two is what strategies are applied to search the final answers in the neural space.",Material,Dataset,True,Introduce（引用目的）,True,P18-1157_5_2,2018,Stochastic Answer Networks for Machine Reading Comprehension,Reference
1768,11772,"http://www.rd.nacsis.ac.jp/-ntcadm/,Sep",[\'3 Burstiness\'],"(Fujita, 1999) [Cite_Ref] BKJJBIDS used performance of the best method (fit-E) to nearly Berkeley\'s logistic regression methods (with about the level of JCB1.","Sumio Fujita. 1999. Notes on phrasal index-ing: Jscb evaluation experiments at ntcir ad hoc\'. In NTCIR Workshop 1, pages 101-108, http://www.rd.nacsis.ac.jp/-ntcadm/,Sep.","120 use training (with the possible exception of JCB1); docs brought in by query expansion (el(t) > methods below the line do not. k). pend on many factors such as language, collection, Table 9: Filters: results vary somewhat depending typical query patterns and so on. To cope with on these choices, though not too much, which is such complications, we believe that it is safer to fortunate, since since we don\'t understand stop use histogram methods than to try to account for lists very well. all of these interactions at once in a single multiple regression. The next section will show that fit-E filter trained on sys. 11 R has very encouraging performance. 5 Experiments 2+, E1 tf,where,ef fit-E .354 .363 2+, E2 tf,where,ef fit-E .350 .359 2+, E4 tf,where,ef fit-E .333 .341 Two measures of performance are reported: (1) 11 2+ tf,where,ef fit-E .332 .366 point average precision and (2) R, precision after .360 .351NA NA JCB1 retrieving Nrd documents, where Nrd is the num-ber of relevant documents. We used the ""short query"" condition of the NACSIS NTCIR-1 Test Table 10: The best filters (Ek) improve the per- Collection (Kando et al., 1999) which consists of formance of the best method (fit-E) to nearly the about 300,000 documents in Japanese, plus about level of JCB1. 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements the K filter is slightly unhelpful. for testing. The result of ""short query"" is shown in A number of filters have been considered (ta-page 25 of(Kando et al., 1999), which shows that ble 9). Results vary somewhat depending on these ""short query"" is hard for statistical methods. choices, though not too much, which is fortunate, Two previously published systems are included since since we don\'t understand stop lists very in the tables below: JCB1 and BKJJBIDS. JCB1, well. To the extent that there is a pattern, we sus-submitted by Just System, a company with a com-pect that words axe slightly better than bigrams, mercially successful product for Japanese word-and that the E filter is slightly better than the B processing, produced the best results using sophis-filter which is slightly better than the K filter. Ta-ticated (and proprietary) natural language pro-ble 10 shows that the best filters (Ek) improve the cessing techniques.(Fujita, 1999) [Cite_Ref] BKJJBIDS used performance of the best method (fit-E) to nearly Berkeley\'s logistic regression methods (with about the level of JCB1. half a dozen variables) to fit term weights to the labeled training material. filter sys. UL LL II R Table 8 shows that training often helps. The eliminate Japanese function words. Although the slightly better than one, and oneis slightly bet- K filter does not change performance very much, ter than none. (UL = upper limit of ~ < idf; LL the use of this filter changes the relative order of fit-B and BKJJBIDS. These results suggest that = lower limit of 0 _ 122",補足資料,Paper,True,Introduce（引用目的）,True,W00-1315_0_0,2000,Empirical Term Weighting,Reference
1769,11773,"http://www.rd.nacsis.ac.jp/""ntcadm/,Sep",[\'3 Burstiness\'],"Test Table 10: The best filters (Ek) improve the per- Collection (Kando et al., 1999) [Cite_Ref] which consists of formance of the best method (fit-E) to nearly the about 300,000 documents in Japanese, plus about level of JCB1.","Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, Koji Eguchi, and Hiroyuki Katoand Souichiro Hi-daka. 1999. Overview of ir tasks at the first nt-cir workshop. In NTCIR Workshop 1, pages 11-44, http://www.rd.nacsis.ac.jp/""ntcadm/,Sep.","120 use training (with the possible exception of JCB1); docs brought in by query expansion (el(t) > methods below the line do not. k). pend on many factors such as language, collection, Table 9: Filters: results vary somewhat depending typical query patterns and so on. To cope with on these choices, though not too much, which is such complications, we believe that it is safer to fortunate, since since we don\'t understand stop use histogram methods than to try to account for lists very well. all of these interactions at once in a single multiple regression. The next section will show that fit-E filter trained on sys. 11 R has very encouraging performance. 5 Experiments 2+, E1 tf,where,ef fit-E .354 .363 2+, E2 tf,where,ef fit-E .350 .359 2+, E4 tf,where,ef fit-E .333 .341 Two measures of performance are reported: (1) 11 2+ tf,where,ef fit-E .332 .366 point average precision and (2) R, precision after .360 .351NA NA JCB1 retrieving Nrd documents, where Nrd is the num-ber of relevant documents. We used the ""short query"" condition of the NACSIS NTCIR-1 Test Table 10: The best filters (Ek) improve the per- Collection (Kando et al., 1999) [Cite_Ref] which consists of formance of the best method (fit-E) to nearly the about 300,000 documents in Japanese, plus about level of JCB1. 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements the K filter is slightly unhelpful. for testing. The result of ""short query"" is shown in A number of filters have been considered (ta-page 25 of(Kando et al., 1999), which shows that ble 9). Results vary somewhat depending on these ""short query"" is hard for statistical methods. choices, though not too much, which is fortunate, Two previously published systems are included since since we don\'t understand stop lists very in the tables below: JCB1 and BKJJBIDS. JCB1, well. To the extent that there is a pattern, we sus-submitted by Just System, a company with a com-pect that words axe slightly better than bigrams, mercially successful product for Japanese word-and that the E filter is slightly better than the B processing, produced the best results using sophis-filter which is slightly better than the K filter. Ta-ticated (and proprietary) natural language pro-ble 10 shows that the best filters (Ek) improve the cessing techniques.(Fujita, 1999) BKJJBIDS used performance of the best method (fit-E) to nearly Berkeley\'s logistic regression methods (with about the level of JCB1. half a dozen variables) to fit term weights to the labeled training material. filter sys. UL LL II R Table 8 shows that training often helps. The eliminate Japanese function words. Although the slightly better than one, and oneis slightly bet- K filter does not change performance very much, ter than none. (UL = upper limit of ~ < idf; LL the use of this filter changes the relative order of fit-B and BKJJBIDS. These results suggest that = lower limit of 0 _ 122",補足資料,Paper,True,Introduce（引用目的）,True,W00-1315_1_0,2000,Empirical Term Weighting,Reference
1770,11774,"http://www.rd.nacsis.ac.jp/""ntcadm/,Sep",[\'3 Burstiness\'],"The result of ""short query"" is shown in A number of filters have been considered (ta-page 25 of(Kando et al., 1999) [Cite_Ref] , which shows that ble 9).","Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, Koji Eguchi, and Hiroyuki Katoand Souichiro Hi-daka. 1999. Overview of ir tasks at the first nt-cir workshop. In NTCIR Workshop 1, pages 11-44, http://www.rd.nacsis.ac.jp/""ntcadm/,Sep.","120 use training (with the possible exception of JCB1); docs brought in by query expansion (el(t) > methods below the line do not. k). pend on many factors such as language, collection, Table 9: Filters: results vary somewhat depending typical query patterns and so on. To cope with on these choices, though not too much, which is such complications, we believe that it is safer to fortunate, since since we don\'t understand stop use histogram methods than to try to account for lists very well. all of these interactions at once in a single multiple regression. The next section will show that fit-E filter trained on sys. 11 R has very encouraging performance. 5 Experiments 2+, E1 tf,where,ef fit-E .354 .363 2+, E2 tf,where,ef fit-E .350 .359 2+, E4 tf,where,ef fit-E .333 .341 Two measures of performance are reported: (1) 11 2+ tf,where,ef fit-E .332 .366 point average precision and (2) R, precision after .360 .351NA NA JCB1 retrieving Nrd documents, where Nrd is the num-ber of relevant documents. We used the ""short query"" condition of the NACSIS NTCIR-1 Test Table 10: The best filters (Ek) improve the per- Collection (Kando et al., 1999) which consists of formance of the best method (fit-E) to nearly the about 300,000 documents in Japanese, plus about level of JCB1. 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements the K filter is slightly unhelpful. for testing. The result of ""short query"" is shown in A number of filters have been considered (ta-page 25 of(Kando et al., 1999) [Cite_Ref] , which shows that ble 9). Results vary somewhat depending on these ""short query"" is hard for statistical methods. choices, though not too much, which is fortunate, Two previously published systems are included since since we don\'t understand stop lists very in the tables below: JCB1 and BKJJBIDS. JCB1, well. To the extent that there is a pattern, we sus-submitted by Just System, a company with a com-pect that words axe slightly better than bigrams, mercially successful product for Japanese word-and that the E filter is slightly better than the B processing, produced the best results using sophis-filter which is slightly better than the K filter. Ta-ticated (and proprietary) natural language pro-ble 10 shows that the best filters (Ek) improve the cessing techniques.(Fujita, 1999) BKJJBIDS used performance of the best method (fit-E) to nearly Berkeley\'s logistic regression methods (with about the level of JCB1. half a dozen variables) to fit term weights to the labeled training material. filter sys. UL LL II R Table 8 shows that training often helps. The eliminate Japanese function words. Although the slightly better than one, and oneis slightly bet- K filter does not change performance very much, ter than none. (UL = upper limit of ~ < idf; LL the use of this filter changes the relative order of fit-B and BKJJBIDS. These results suggest that = lower limit of 0 _ 122",補足資料,Paper,True,Introduce（引用目的）,True,W00-1315_1_1,2000,Empirical Term Weighting,Reference
1771,11775," http://groups.csail.mit.edu/rbg/code/wordprobs/"," ['1 Introduction']",This paper studies the task of learning to automatically solve such problems given only the natural language. [Cite_Footnote_1],1 The code and data for this work are available at http://groups.csail.mit.edu/rbg/code/wordprobs/.,"Algebra word problems concisely describe a world state and pose questions about it. The described state can be modeled with a system of equations whose solution specifies the questions’ answers. For example, Figure 1 shows one such problem. The reader is asked to infer how many children and adults were admitted to an amusement park, based on constraints provided by ticket prices and overall sales. This paper studies the task of learning to automatically solve such problems given only the natural language. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P14-1026_0_0,2014,Learning to Automatically Solve Algebra Word Problems,Footnote
1772,11776," https://code.google.com/p/efficient-java-matrix-library/"," ['7 Experimental Setup']","The set of mathematical relations supported by our im-plementation is {+, −, ×, /}.Our implementation uses the Gaussian Elimination function in the Effi-cient Java Matrix Library (EJML) (Abeles, 2014) [Cite_Ref] to generate answers given a set of equations.",Peter Abeles. 2014. Efficient java matrix library. https://code.google.com/p/efficient-java-matrix-library/.,"Parameters and Solver In our experiments we set k in our beam search algorithm (Section 5) to 200, and l to 20. We run the L-BFGS computation for 50 iterations. We regularize our learning objec-tive using the L 2 -norm and a λ value of 0.1. The set of mathematical relations supported by our im-plementation is {+, −, ×, /}.Our implementation uses the Gaussian Elimination function in the Effi-cient Java Matrix Library (EJML) (Abeles, 2014) [Cite_Ref] to generate answers given a set of equations.",Method,Code,True,Use（引用目的）,True,P14-1026_1_0,2014,Learning to Automatically Solve Algebra Word Problems,Reference
1773,11777," https://github.com/facebookresearch/PyTorch-BigGraph"," ['4 Experiments', '4.1 Experimental Setup']",Early experiments with embeddings derived from Wikidata relations [Cite_Footnote_4] did not improve results.,4 https://github.com/facebookresearch/ PyTorch-BigGraph,"KnowBert-Wiki The entity linker in KnowBert-Wiki borrows both the entity can-didate selectors and embeddings from Ganea and Hofmann (2017). The candidate selectors and priors are a combination of CrossWikis, a large, precomputed dictionary that combines statistics from Wikipedia and a web corpus (Spitkovsky and Chang, 2012), and the YAGO dictionary (Hoffart et al., 2011). The entity embeddings use a skip-gram like objective (Mikolov et al., 2013b) to learn 300-dimensional embeddings of Wikipedia page titles directly from Wikipedia descriptions without using any explicit graph structure between nodes. As such, nodes in the KB are Wikipedia page titles, e.g., Prince (musician). Ganea and Hofmann (2017) provide pretrained embed-dings for a subset of approximately 470K entities. Early experiments with embeddings derived from Wikidata relations [Cite_Footnote_4] did not improve results.",Material,Knowledge,False,Use（引用目的）,True,D19-1005_0_0,2019,Knowledge Enhanced Contextual Word Representations,Footnote
1774,11778," https://spacy.io/"," ['4 Experiments', '4.1 Experimental Setup']",The candidate se-lector uses a rule-based lemmatizer without part-of-speech (POS) information. [Cite_Footnote_5],5 https://spacy.io/,"KnowBert-WordNet Our WordNet KB com-bines synset metadata, lemma metadata and the re-lational graph. To construct the graph, we first ex-tracted all synsets, lemmas, and their relationships from WordNet 3.0 using the nltk interface. After disregarding certain symmetric relationships (e.g., we kept the hypernym relationship, but removed the inverse hyponym relationship) we were left with 28 synset-synset and lemma-lemma relation-ships. From these, we constructed a graph where each node is either a synset or lemma, and intro-duced the special lemma in synset relation-ship to link synsets and lemmas. The candidate se-lector uses a rule-based lemmatizer without part-of-speech (POS) information. [Cite_Footnote_5]",Method,Tool,True,Use（引用目的）,True,D19-1005_1_0,2019,Knowledge Enhanced Contextual Word Representations,Footnote
1775,11779," https://github.com/thunlp/ERNIE"," ['4 Experiments', '4.3 Downstream Tasks']","To directly compare to ERNIE, we adopted the evaluation protocol in Zhang et al. (2019) which considers the nine general entity types. [Cite_Footnote_7]",7 Data obtained from https://github.com/thunlp/ERNIE,"Entity typing We also evaluated KnowBert-W+W using the entity typing dataset from Choi et al. (2018). To directly compare to ERNIE, we adopted the evaluation protocol in Zhang et al. (2019) which considers the nine general entity types. [Cite_Footnote_7] Our model marks the location of a target span with the special [E] and [/E] tokens and uses the representation of the [E] token to predict the type. As shown in Table 7, KnowBert-W+W shows an improvement of 0.6% F 1 over ERNIE and 2.5% over BERT BASE .",Material,DataSource,True,Use（引用目的）,True,D19-1005_2_0,2019,Knowledge Enhanced Contextual Word Representations,Footnote
1776,11780," http://lingpy.org"," ['3 Methods', '3.5 Implementation']","The code was implemented in Python, as part of the LingPy library (Version 2.5, List and Forkel (2016), [Cite] http://lingpy.org).",,"The code was implemented in Python, as part of the LingPy library (Version 2.5, List and Forkel (2016), [Cite] http://lingpy.org). The Igraph soft-ware package (Csárdi and Nepusz 2006) is needed to apply the Infomap algorithm.",Method,Tool,False,Use（引用目的）,True,P16-2097_0_0,2016,Using Sequence Similarity Networks to Identify Partial Cognates in Multilingual Wordlists,Body
1777,11781," https://zenodo.org/record/51328"," ['-', 'Supplementary Material']","Material contains results, benchmark datasets, and code, downloadable at: [Cite] https://zenodo.org/record/51328.",,"The Sup. Material contains results, benchmark datasets, and code, downloadable at: [Cite] https://zenodo.org/record/51328.",Mixed,Mixed,True,Produce（引用目的）,True,P16-2097_1_0,2016,Using Sequence Similarity Networks to Identify Partial Cognates in Multilingual Wordlists,Body
1778,11782," http://sequencecomparison.github.io"," ['3 Methods', '3.1 Sequence Similarity']","An example for language-specific similarity measures is the LexStat algorithm, first proposed in List (2012a) and later refined in List (2014b) [Cite_Ref] .","Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. Düsseldorf Uni-versity Press, Düsseldorf. URL: http://sequencecomparison.github.io.","There are various ways to determine the similar-ity or distance between words and morphemes. A general distinction can be made between language-independent and language-specific ap-proaches. The former determine the word simi-larity independently of the languages to which the words belong. As a result, the scores only depend on the substantial and structural differences be-tween words. Examples for language-independent similarity measures are SCA distances, as pro-duced by the Sound-Class-Based Phonetic Align-ment algorithm (List 2012b), or PMI similarities as produced by the Weighted String Alignment algorithm (Jäger 2013). Language-specific ap-proaches, on the other hand, are based on pre-viously identified recurring correspondences be-tween the languages from which the words are taken (List 2014b: 48-50) and may differ across languages. An example for language-specific similarity measures is the LexStat algorithm, first proposed in List (2012a) and later refined in List (2014b) [Cite_Ref] . As a general rule, language-specific ap-proaches outperform language-independent ones, provided the sample size is large enough (List 2014a).",Method,Code,False,Introduce（引用目的）,True,P16-2097_2_0,2016,Using Sequence Similarity Networks to Identify Partial Cognates in Multilingual Wordlists,Reference
1779,11783," http://sequencecomparison.github.io"," ['3 Methods', '3.1 Sequence Similarity']","This distribution is then compared with the actual distribution re-trieved from aligned words in the word list, and a language-specific scoring function is created List (2014b) [Cite_Ref] .","Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. Düsseldorf Uni-versity Press, Düsseldorf. URL: http://sequencecomparison.github.io.","Two similarity measures are used in this pa-per, one language-independent, and one language-specific one. The above-mentioned SCA method for phonetic alignments (List 2012b, 2014b) re-duces the phonetic space of sound sequences to 28 sound classes. Based on a scoring function which defines transition scores between the sound classes, phonetic sequences are aligned and simi-larity and distance scores can be determined. The LexStat approach List (2012a, 2014b) also uses sound classes, but instead of using a pre-defined scoring function, transition scores between sound classes are determined with help of a permutation test. In this test, words drawn from a random-ized sample are repeatedly aligned with each other in order to create a distribution of sound transi-tions for unrelated languages. This distribution is then compared with the actual distribution re-trieved from aligned words in the word list, and a language-specific scoring function is created List (2014b) [Cite_Ref] . SCA is very fast in computation, but Lex-Stat has a much higher accuracy. Both approaches are freely available as part of the LingPy software package (List and Forkel 2016).",Method,Code,False,Introduce（引用目的）,True,P16-2097_2_1,2016,Using Sequence Similarity Networks to Identify Partial Cognates in Multilingual Wordlists,Reference
1780,11784,http://sequencecomparison.github.io,"[\'3 Methods\', \'3.4 Analyses and Evaluation\']","With SCA and LexStat, two classical meth-ods for cognate detection were tested List (2014b) [Cite_Ref] , and their underlying models for phonetic similar-ity (see Sec. 3.1) were used as basis for the par-tial cognate detection algorithm.","Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. Düsseldorf Uni-versity Press, Düsseldorf. URL: http://sequencecomparison.github.io.","With SCA and LexStat, two classical meth-ods for cognate detection were tested List (2014b) [Cite_Ref] , and their underlying models for phonetic similar-ity (see Sec. 3.1) were used as basis for the par-tial cognate detection algorithm. All in all, this yielded four different methods: LexStat, LexStat-Partial, SCA, and SCA-Partial. Since our new algorithms yield partial cognates, while LexStat and SCA yield ``complete"" cognates, it is not pos-sible to compare them directly. In order to al-low for a direct comparison, partial cognate sets were converted into ``complete"" cognate sets us-ing the above-mentioned strict coding approach proposed by Ben Hamed and Wang (2006): only those words in which all morphemes are cognate were assigned to the cognate same set. With a total of three different clustering algorithms (UPGMA, Markov Clustering, and Infomap), we thus carried out twelve tests on complete cognacy (three for each of our four approaches), and six additional tests on pure partial cognate detection, in which we compared the suitability of SCA and LexStat as string similarity measures. on all datasets. The table shows for each of the 18 different methods the threshold (T) for which the best B-Cubed F-Score was determined, as well as the B-Cubed precision (P), recall (R), and F-score (FS). The best result in each block is shaded in gray.",Method,Code,False,Introduce（引用目的）,True,P16-2097_2_2,2016,Using Sequence Similarity Networks to Identify Partial Cognates in Multilingual Wordlists,Reference
1781,11785," http://lingpy.org"," ['3 Methods', '3.5 Implementation']","The code was implemented in Python, as part of the LingPy library (Version 2.5, List and Forkel (2016) [Cite_Ref] , http://lingpy.org).","Johann-Mattis List and Robert Forkel. 2016. LingPy. A Python library for historical linguis-tics. Max Planck Institute for the Science of Hu-man History, Jena. Version 2.5. URL: http: //lingpy.org. With contributions by Steven Moran, Peter Bouda, Johannes Dellert, Taraka Rama, Frank Nagel, and Simon Greenhill.","The code was implemented in Python, as part of the LingPy library (Version 2.5, List and Forkel (2016) [Cite_Ref] , http://lingpy.org). The Igraph soft-ware package (Csárdi and Nepusz 2006) is needed to apply the Infomap algorithm.",Method,Tool,False,Introduce（引用目的）,True,P16-2097_4_0,2016,Using Sequence Similarity Networks to Identify Partial Cognates in Multilingual Wordlists,Reference
1782,11786," https://amr.isi.edu/download.html"," ['1 Introduction']","Annotating natural language with AMR is a complex task and training datasets only exist for English [Cite_Footnote_1] , so previous work on AMR-to-text gen-eration has overwhelmingly focused on English.",1 AMR datasets from the LDC can be found at https://amr.isi.edu/download.html,"Annotating natural language with AMR is a complex task and training datasets only exist for English [Cite_Footnote_1] , so previous work on AMR-to-text gen-eration has overwhelmingly focused on English. We create training data for multilingual AMR-to- Text models, by taking the EUROPARL multilin-gual corpus and automatically annotating the En-glish data with AMRs using the jamr semantic parser. We then use the English AMRs as the in-put for all generation tasks. To improve quality, we leverage recent advances in natural language processing such as cross-lingual embeddings, pre-training and multilingual learning. Cross-lingual embeddings have shown striking improvements on a range of cross-lingual natural language under-standing tasks (Devlin et al., 2019; Conneau et al., 2019; Wu and Dredze, 2019; Pires et al., 2019). Other work has shown that the pre-training and fine-tuning approaches also help improve genera-tion performance (Dong et al., 2019; Song et al., 2019; Lawrence et al., 2019; Rothe et al., 2019). Finally, multilingual models, where a single model is trained to translate from multiple source lan-guages into multiple target languages, are achiev-ing increasingly better results in machine transla-tion (Johnson et al., 2017; Firat et al., 2017; Aha-roni et al., 2019; Arivazhagan et al., 2019).",Material,Dataset,True,Introduce（引用目的）,True,2020.emnlp-main.231_0_0,2020,Multilingual AMR-to-Text Generation,Footnote
1783,11787," https://github.com/facebookresearch/cc_net"," ['4 Experimental Setting', '4.1 Data']","Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET [Cite_Footnote_2] (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web).",2 https://github.com/facebookresearch/ cc_net,"Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET [Cite_Footnote_2] (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web). We use jamr to parse En-glish sentences into AMR. For multilingual de-coder pretraining, we take thirty million sentences from each language split of CCNET .",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.231_1_0,2020,Multilingual AMR-to-Text Generation,Footnote
1784,11788," https://github.com/jflanigan/jamr"," ['4 Experimental Setting', '4.1 Data']",We use jamr [Cite_Footnote_3] to parse En-glish sentences into AMR.,3 https://github.com/jflanigan/jamr,"Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web). We use jamr [Cite_Footnote_3] to parse En-glish sentences into AMR. For multilingual de-coder pretraining, we take thirty million sentences from each language split of CCNET .",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.231_2_0,2020,Multilingual AMR-to-Text Generation,Footnote
1785,11789," https://github.com/facebookresearch/m-amr2text"," ['4 Experimental Setting', '4.2 Models']",Our pretrained mod-els are available for download. [Cite_Footnote_5],5 https://github.com/facebookresearch/ m-amr2text,"We implement our models in fairseq-py (Ott et al., 2019). We use large Transformer (Vaswani et al., 2017) sequence-to-sequence models and train all models for 50 epochs with LayerDrop (Fan et al., 2019b), which takes around 2 days. We initialize all weights with the pretrained models. When com-bining crosslingual word embeddings and encoder and decoder pretraining, we initialize all weights with pretraining, then use crosslingual word embed-dings. We do not perform extensive hyperparame-ter search, but experimented with various learning rate values to maintain stable training with pre-trained initialization. To generate, we decode with beam search with beam size 5. Our pretrained mod-els are available for download. [Cite_Footnote_5]",Material,Knowledge,True,Produce（引用目的）,True,2020.emnlp-main.231_3_0,2020,Multilingual AMR-to-Text Generation,Footnote
1786,11790," http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html"," ['6 Experiment', '6.2 Baseline Models']","We use the publicly available implementation of MSTParser [Cite_Footnote_7] (with modifications to the feature com-putation) and its default settings, so the feature weights of the projective and non-projective parsers are trained by the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2006).",7 http://www.seas.upenn.edu/˜strctlrn/MSTParser/MSTParser.html,"We use the publicly available implementation of MSTParser [Cite_Footnote_7] (with modifications to the feature com-putation) and its default settings, so the feature weights of the projective and non-projective parsers are trained by the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2006).",Method,Tool,True,Use（引用目的）,True,D13-1152_0_0,2013,Dynamic Feature Selection for Dependency Parsing,Footnote
1787,11791," https://github.com/BLLIP/bllip-parser"," ['4 Datasets and models']","In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [Cite_Footnote_1] (Charniak and Johnson, 2005).",1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model ”WSJ+Gigaword-v2”.,"We use English-French and English-German data from WMT2014 (Bojar et al., 2014). We take 4M English sentences from the English-German data to train E2E and PE2PE. For the neural parser (E2P), we construct the training corpus following the recipe of Vinyals et al. (2015). We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [Cite_Footnote_1] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus. We use WSJ Section 22 as our development set and section 23 as the test set, where we obtain an F1-score of 89.6, competitive with the previously-published 90.5 (Table 4).",Method,Tool,True,Use（引用目的）,True,D16-1159_0_0,2016,Does String-Based Neural MT Learn Source Syntax?,Footnote
1788,11792," https://github.com/isi-nlp/ZophRNN"," ['4 Datasets and models']","For all experiments [Cite_Footnote_2] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997).",2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN,"Model Architecture. For all experiments [Cite_Footnote_2] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.",Method,Tool,True,Use（引用目的）,True,D16-1159_1_0,2016,Does String-Based Neural MT Learn Source Syntax?,Footnote
1789,11793," http://nlp.cs.nyu.edu/evalb/"," ['6 Extract Syntactic Trees from Encoder', '6.2 Evaluation']",The EVALB tool [Cite_Footnote_3] to calculate the labeled bracketing F1-score.,3 http://nlp.cs.nyu.edu/evalb/,1. The EVALB tool [Cite_Footnote_3] to calculate the labeled bracketing F1-score.,Method,Tool,True,Use（引用目的）,False,D16-1159_2_0,2016,Does String-Based Neural MT Learn Source Syntax?,Footnote
1790,11794," https://github.com/timtadh/zhang-shasha"," ['6 Extract Syntactic Trees from Encoder', '6.2 Evaluation']","The zxx package [Cite_Footnote_4] to calculate Tree edit dis-tance (TED) (Zhang and Shasha, 1989).",4 https://github.com/timtadh/zhang-shasha,"2. The zxx package [Cite_Footnote_4] to calculate Tree edit dis-tance (TED) (Zhang and Shasha, 1989).",Method,Tool,True,Use（引用目的）,False,D16-1159_3_0,2016,Does String-Based Neural MT Learn Source Syntax?,Footnote
1791,11795," https://github.com/jkkummerfeld/berkeley-parser-analyser"," ['6 Extract Syntactic Trees from Encoder', '6.2 Evaluation']","The Berkeley Parser Analyser [Cite_Footnote_5] (Kummerfeld et al., 2012) to analyze parsing error types.",5 https://github.com/jkkummerfeld/berkeley-parser-analyser,"3. The Berkeley Parser Analyser [Cite_Footnote_5] (Kummerfeld et al., 2012) to analyze parsing error types.",Method,Tool,True,Use（引用目的）,False,D16-1159_4_0,2016,Does String-Based Neural MT Learn Source Syntax?,Footnote
1792,11796," http://www-01.ibm.com/software/data/infosphere/streams/"," ['3 The System']","For accuracy and speed, we built our real-time data processing infrastructure on the IBM’s InfoSphere Streams platform (IBM, 2012) [Cite_Ref] , which enables us to write our own analysis and visualization modules and assemble them into a real-time processing pipeline.","IBM. (2012). InfoSphere Streams, from http://www-01.ibm.com/software/data/infosphere/streams/","For accuracy and speed, we built our real-time data processing infrastructure on the IBM’s InfoSphere Streams platform (IBM, 2012) [Cite_Ref] , which enables us to write our own analysis and visualization modules and assemble them into a real-time processing pipeline. Streams applications are highly scalable so we can adjust our system to handle higher volume of data by adding more servers and by distributing processing tasks. Twitter traffic often balloons during big events (e.g. televised debates or primary election days) and stays low between events, making high scalability strongly desirable. Figure 1 shows our system’s architecture and its modules. Next, we introduce our data source and each individual module.",Method,Tool,True,Use（引用目的）,True,P12-3020_0_0,2012,A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle,Reference
1793,11797," http://tweetcongress.org/members/"," ['1 Introduction']","More than two thirds of U.S. congress members have created a Twitter account and many are actively using Twitter to reach their constituents (Lassen & Brown, 2010; TweetCongress, 2012 [Cite_Ref] ).","TweetCongress. (2012). Congress Members on Twitter Retrieved Mar 18, 2012, from http://tweetcongress.org/members/","Twitter allows users to post tweets, messages of up to 140 characters, on its social network. Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day (Twitter, 2012). It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011). More than two thirds of U.S. congress members have created a Twitter account and many are actively using Twitter to reach their constituents (Lassen & Brown, 2010; TweetCongress, 2012 [Cite_Ref] ). Since October 12, 2012, we have gathered over 36 million tweets about the 2012 U.S. presidential candidates, a quarter million per day on average. During one of the key political events, the Dec 15, 2011 primary debate in Iowa, we collected more than half a million relevant tweets in just a few hours. This kind of ‘big data’ vastly outpaces the capacity of traditional content analysis approaches, calling for novel computational approaches.",補足資料,Document,True,Introduce（引用目的）,True,P12-3020_2_0,2012,A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle,Reference
1794,11798," https://business.twitter.com/en/basics/what-is-twitter/"," ['1 Introduction']","The company reports over 100 million active users worldwide, together sending over 250 million tweets each day (Twitter, 2012) [Cite_Ref] .","Twitter. (2012). What is Twitter Retrieved Mar 18, 2012, from https://business.twitter.com/en/basics/what-is-twitter/","Twitter allows users to post tweets, messages of up to 140 characters, on its social network. Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day (Twitter, 2012) [Cite_Ref] . It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011). More than two thirds of U.S. congress members have created a Twitter account and many are actively using Twitter to reach their constituents (Lassen & Brown, 2010; TweetCongress, 2012). Since October 12, 2012, we have gathered over 36 million tweets about the 2012 U.S. presidential candidates, a quarter million per day on average. During one of the key political events, the Dec 15, 2011 primary debate in Iowa, we collected more than half a million relevant tweets in just a few hours. This kind of ‘big data’ vastly outpaces the capacity of traditional content analysis approaches, calling for novel computational approaches.",補足資料,Document,True,Introduce（引用目的）,True,P12-3020_3_0,2012,A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle,Reference
1795,11799," https://ai.tencent.com/ailab/nlp/dialogue/datasets/grayscale_data_release.zip"," ['4 Experimental Setup', '4.3 Implementation Details']","To facilitate further research, we have made our col-lected grayscale data publicly available. [Cite_Footnote_1]",1 Related resources can be found at https: //ai.tencent.com/ailab/nlp/dialogue/ datasets/grayscale_data_release.zip,"For grayscale data construction, we train a seq2seq generation model and build a BM25 retrieval sys-tem using the training set for each dataset. We con-sider the top 100 responses from BM25 retrieval and the top 5 responses from seq2seq generation (via beam search) as the grayscale responses. To facilitate further research, we have made our col-lected grayscale data publicly available. [Cite_Footnote_1] During training, we use these grayscale responses in a way adaptive to the training matching model. At each training epoch, ten different grayscale responses are used: the top 5 retrieval responses ranked by the current matching model and all 5 seq2seq gen-eration responses. We experiment our new training approach on four latest state-of-the-art models as follows: Specifically, we first pre-train a model with objec-tive L ran only then switch to L Uni . We find that such a treatment makes the training process more stable.",Material,Dataset,True,Produce（引用目的）,True,2020.emnlp-main.741_0_0,2020,The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection,Footnote
1796,11800," https://github.com/huminghao16/RE3QA"," ['1 Introduction']",Source code is released for fu-ture research exploration [Cite_Footnote_1] .,1 https://github.com/huminghao16/RE3QA,"We evaluate our approach on four datasets. On TriviaQA-Wikipedia and TriviaQA-unfiltered datasets (Joshi et al., 2017), we achieve 75.2 F1 and 71.2 F1 respectively, outperforming previ-ous best approaches. On SQuAD-document and SQuAD-open datasets, both of which are modified versions of SQuAD (Rajpurkar et al., 2016), we obtain 14.8 and 4.1 absolute gains on F1 score over prior state-of-the-art results. Moreover, our ap-proach surpasses the pipelined baseline with faster inference speed on both TriviaQA-Wikipedia and SQuAD-document. Source code is released for fu-ture research exploration [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,P19-1221_0_0,2019,"Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension",Footnote
1797,11801," https://github.com/google-research/bert"," ['4 Experimental Setup']","us-ing two publicly available uncased versions of BERT [Cite_Footnote_5] : BERT BASE and BERT LARGE , and refer readers to Devlin et al. (2018) for details on model sizes.",5 https://github.com/google-research/bert,"Data preprocessing Following Clark and Gard-ner (2018), we merge small paragraphs into a sin-gle paragraph of up to a threshold length in Triv-iaQA and SQuAD-open. The threshold is set as 200 by default. We manually tune the number of retrieved paragraphs K for each dataset, and set the number of retrieved segments N as 8. Follow-ing Devlin et al. (2018), we set the window length l as 384 L q 3 so that L x is 384 and set the stride r as 128, where L q is the question length. We also calculate the answer recall after document prun-ing, which indicates the performance upper bound. Model settings We initialize our model us-ing two publicly available uncased versions of BERT [Cite_Footnote_5] : BERT BASE and BERT LARGE , and refer readers to Devlin et al. (2018) for details on model sizes. We use Adam optimizer with a learning rate of 3e-5 and warmup over the first 10% steps to fine-tune the network for 2 epochs. The batch size is 32 and a dropout probability of 0.1 is used. The number of blocks J used for early-stopped re-triever is 3 for base model and 6 for large model by default. The number of proposed answers M is 20, while the threshold of NMS M ⇤ is 5. Dur-ing inference, we tune the weights for retrieving, reading, and reranking, and set them as 1.4, 1, 1.4. Evaluation metrics We use mean average pre-cision (MAP) and top-N to evaluate the retriev-ing component. As for evaluating the performance of reading and reranking, we measure the exact match (EM) accuracy and F1 score calculated be-tween the final prediction and gold answers.",Method,Tool,False,Introduce（引用目的）,True,P19-1221_1_0,2019,"Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension",Footnote
1798,11802," https://github.com/kovvalsky/LangPro"," ['References']",L ang P ro is an automated theorem prover for natural language. [Cite_Footnote_1],1 https://github.com/kovvalsky/LangPro,"L ang P ro is an automated theorem prover for natural language. [Cite_Footnote_1] Given a set of premises and a hypothesis, it is able to prove semantic relations between them. The prover is based on a version of an-alytic tableau method specially designed for natural logic. The proof procedure op-erates on logical forms that preserve lin-guistic expressions to a large extent. The nature of proofs is deductive and transpar-ent. On the FraCaS and SICK textual en-tailment datasets, the prover achieves high results comparable to state-of-the-art.",Method,Tool,True,Produce（引用目的）,True,D17-2020_0_0,2017,LANGPRO : Natural Language Theorem Prover,Footnote
1799,11803," http://goo.gl/language/wiki-atomic-edits"," ['1 Introduction']",• A new corpus (WikiAtomicEdits) of 26M atomic insertions and 17M atomic deletions covering 8 languages (§3 and §4): [Cite] http: //goo.gl/language/wiki-atomic-edits .,,• A new corpus (WikiAtomicEdits) of 26M atomic insertions and 17M atomic deletions covering 8 languages (§3 and §4): [Cite] http: //goo.gl/language/wiki-atomic-edits .,Material,Dataset,True,Produce（引用目的）,False,D18-1028_0_0,2018,WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse,Body
1800,11804," https://fasttext.cc/docs/en/crawl-vectors.html"," ['6 Language Modeling Analysis', '6.1 Predicting Insertion Locations']","We use a 256-dimensional 2-layer biLSTM encoder, initial-ized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018). [Cite_Footnote_7]",7 https://fasttext.cc/docs/en/crawl-vectors.html,"Models. We evaluate two models. First, we evaluate a standard language modeling baseline (General LM), in which we simply insert the phrase p at every possible point in s and chose the index which yields the lowest perplexity. We use the LSTM language model from Jozefowicz et al. (2016), which obtained SOTA results on lan-guage modeling on the one billion words bench-mark for English (Chelba et al., 2013). We train this language model for each language on an aver-age of ∼ 500 million tokens from Wikipedia. Sec-ond, we evaluate a discriminative model specifi-cally trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a context-dependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the rep-resentation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initial-ized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018). [Cite_Footnote_7] We hold out 50K and 10K insertion edits for each lan-guage as development and test sets, and use the re-maining edits (insertions and deletions) as training data. This provides us with at least 1 million ex-amples for training in each language (cf. Table 2). See Supplementary Material for additional details. Results. Table 8 shows the accuracy of each model for each language. We see that the discri-minitve model trained on insertions directly per-forms better than the general LM by at least 1% absolute accuracy on every language, and by 3.8% absolute on average. It is worth emphasizing that this performance improvement is despite the fact that the general LM was trained with, on average, four times the number of tokens and is a much larger model–the general LM has ∼ 2 billion pa-rameters (Jozefowicz et al., 2016) compared to ∼ 1 million for the discriminative model.",Material,Knowledge,True,Use（引用目的）,True,D18-1028_1_0,2018,WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse,Footnote
1801,11805," http://goo.gl/language/wiki-atomic-edits"," ['Description of Data Release']",Our full corpus is available for download at [Cite] http: //goo.gl/language/wiki-atomic-edits .,,"Our full corpus is available for download at [Cite] http: //goo.gl/language/wiki-atomic-edits . The data contains 26M atomic insertions and 17M atomic deletions covering 8 languages. All sen-tences (both the original sentence s, and the edited sentence e(s)) have been POS-tagged and depen-dency parsed (Andor et al., 2016) as well as scored using a SOTA LM (Jozefowicz et al., 2016). We also release the 5K 5-way human insertion annota-tions for English, and 1K 3-way annotations each for Spanish and German, as described in §4.",Material,Dataset,True,Produce（引用目的）,True,D18-1028_2_0,2018,WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse,Body
1802,11806," https://github.com/i-machine-think/machine-tasks/tree/master/LongLookupTables"," ['6 Experiments', '6.1 Datasets']","Based on this observation, we evaluate on a compositionality-specific artificial task, lookup ta-bles (Liska et al., 2018), but extend it to better quantify extrapolation. [Cite_Footnote_6]",6 The extended datasets as well as scripts to gener-ate them can be found at https://github.com/i-machine-think/machine-tasks/tree/master/LongLookupTables,"Based on this observation, we evaluate on a compositionality-specific artificial task, lookup ta-bles (Liska et al., 2018), but extend it to better quantify extrapolation. [Cite_Footnote_6] This task is especially interesting to us, as there is a clear notion of what a good attention pattern should look like, making it easy to qualitatively and quantitatively analyze attentive models. It is a well-controlled task, which allows us to uncover challenges that prevent models from extrapolating on real-world data.",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.39_0_0,2020,Location Attention for Extrapolation to Longer Sequences,Footnote
1803,11807," https://dev.twitter.com/docs/streaming-api/methods"," ['4 Contextually-similar Pair Generation']",All tweets were col-lected from September 2010 to January 2011 via the Twitter API. [Cite_Footnote_1],1 https://dev.twitter.com/docs/streaming-api/methods,"We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were col-lected from September 2010 to January 2011 via the Twitter API. [Cite_Footnote_1] From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06) to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length ≥ 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are mor-phophonemically similar to the OOV type, follow-ing settings in Han and Baldwin (2011).",Method,Code,True,Use（引用目的）,True,D12-1039_0_0,2012,Automatically Constructing a Normalisation Dictionary for Microblogs,Footnote
1804,11808," http://aspell.net/"," ['4 Contextually-similar Pair Generation']","[Cite_Footnote_2] to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length ≥ 4, both of which were determined through empirical observation.",2 http://aspell.net/,"We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were col-lected from September 2010 to January 2011 via the Twitter API. From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06) [Cite_Footnote_2] to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length ≥ 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are mor-phophonemically similar to the OOV type, follow-ing settings in Han and Baldwin (2011).",Material,Knowledge,True,Use（引用目的）,True,D12-1039_1_0,2012,Automatically Constructing a Normalisation Dictionary for Microblogs,Footnote
1805,11809," https://www.mturk.com/mturk/welcome"," ['4 Contextually-similar Pair Generation']","We set up an annotation task on Amazon Mechanical Turk, [Cite_Footnote_4] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate.",4 https://www.mturk.com/mturk/welcome,"In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [Cite_Footnote_4] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of “to-morrow”. For correct OOV words like iPad, on the other hand, we would expect them to leave the word unchanged. If 3 or more of the 5 annotators make the same suggestion (in the form of either a canoni-cal spelling or leaving the word unchanged), we in-clude this in our gold standard for evaluation. In total, this resulted in 351 lexical variants and 282 correct OOV words, accounting for 63.3% of the 1000 OOV words. These 633 OOV words were used as (OOV,IV) pairs for parameter tuning. The re-mainder of the 1000 OOV words were ignored on the grounds that there was not sufficient consensus amongst the annotators.",補足資料,Website,False,Use（引用目的）,True,D12-1039_2_0,2012,Automatically Constructing a Normalisation Dictionary for Microblogs,Footnote
1806,11810," https://github.com/RUC-WSM/WD-Match"," ['1 Introduction']",The source code of WD-Match is available at [Cite] https://github.com/RUC-WSM/WD-Match,,The source code of WD-Match is available at [Cite] https://github.com/RUC-WSM/WD-Match,Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.239_0_0,2020,Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,Body
1807,11811," https://nlp.stanford.edu/projects/snli"," ['4 Experiments', '4.1 Datasets and Metrics']",SNLI [Cite_Footnote_3] is a benchmark for natural language in-ference.,3 https://nlp.stanford.edu/projects/ snli,"SNLI [Cite_Footnote_3] is a benchmark for natural language in-ference. In SNLI, each data record is a premise-hypothesis-label triple. The premise and hypoth-esis are two sentences and the label could be “en-tailment”, “neutral”, “contradiction”, or “-”. In our experiments, following the practices in (Bowman et al., 2015), the data with label “-” are ignored. We follow the original dataset partition. Accuracy is used as the evaluation metric for this dataset.",Material,Dataset,True,Introduce（引用目的）,False,2020.emnlp-main.239_1_0,2020,Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,Footnote
1808,11812," http://data.allenai.org/scitail/"," ['4 Experiments', '4.1 Datasets and Metrics']",SciTail [Cite_Footnote_4] is an entailment dataset based on multiple-choice science exams and web sentences.,4 http://data.allenai.org/scitail/,"SciTail [Cite_Footnote_4] is an entailment dataset based on multiple-choice science exams and web sentences. Each record is a premise-hypothesis-label triple. The label is “entailment” or “neutral”, because sci-entific factors cannot contradict. We follow the original dataset partition. Accuracy are used as the evaluation metric for this dataset.",Material,Dataset,True,Introduce（引用目的）,False,2020.emnlp-main.239_2_0,2020,Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,Footnote
1809,11813," https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA"," ['4 Experiments', '4.1 Datasets and Metrics']",TrecQA [Cite_Footnote_5] is a answer sentence selection dataset designed for the open-domain question answering setting.,5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA,"TrecQA [Cite_Footnote_5] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included. The raw version has 82 ques-tions in the development set and 100 questions in the test set. Mean average precision (MAP) and mean reciprocal rank (MRR) are used as the evalu-ation metrics for this task.",Material,Dataset,True,Introduce（引用目的）,False,2020.emnlp-main.239_3_0,2020,Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,Footnote
1810,11814," https://www.microsoft.com/en-us/download/details.aspx?id=52419"," ['4 Experiments', '4.1 Datasets and Metrics']",WikiQA [Cite_Footnote_6] is a retrieval-based question answer-ing dataset based on Wikipedia.,6 https://www.microsoft.com/en-us/download/details.aspx?id=52419,"WikiQA [Cite_Footnote_6] is a retrieval-based question answer-ing dataset based on Wikipedia. We follow the data split of original paper. This dataset consists of 20.4k training pairs, 2.7k development pairs, and 6.2k testing pairs. We use MAP and MRR as the evaluation metrics for this task.",Material,Dataset,True,Introduce（引用目的）,False,2020.emnlp-main.239_4_0,2020,Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,Footnote
1811,11815," https://github.com/google-research/bert"," ['4 Experiments', '4.2 Experimental Setup']","Please note that we remove the character embed-ding and position embedding in our experiments; in WD-Match(BERT), F is a pre-trained BERT-base [Cite_Footnote_7] model, M is an MLP.",7 https://github.com/google-research/ bert,"Please note that we remove the character embed-ding and position embedding in our experiments; in WD-Match(BERT), F is a pre-trained BERT-base [Cite_Footnote_7] model, M is an MLP. Please note that for easing of combining with WD-Match, BERT was only used to extract the sentence features separately in our experiments. The G module for four models are identical: a non-linear projection layer and a linear projection layer.",Method,Tool,False,Use（引用目的）,True,2020.emnlp-main.239_5_0,2020,Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,Footnote
1812,11816," http://www.ehealthmd.com/library/acltears"," ['1 Introduction']","[Cite] http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee.",,"First, our proposed method obtains Web pages that include both an acronym and its definitions. Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. [Cite] http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user’s effective permissions requires more than simply looking up that user’s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym “ACL” without its definition in three different meanings found in blogs",Material,Knowledge,False,Introduce（引用目的）,False,N06-2041_0_0,2006,Using the Web to Disambiguate Acronyms,Body
1813,11817," http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html"," ['1 Introduction']",( [Cite] http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html),,"First, our proposed method obtains Web pages that include both an acronym and its definitions. Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. ( [Cite] http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user’s effective permissions requires more than simply looking up that user’s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym “ACL” without its definition in three different meanings found in blogs",補足資料,Document,False,Introduce（引用目的）,True,N06-2041_1_0,2006,Using the Web to Disambiguate Acronyms,Body
1814,11818," http://en.wikipedia.org/wiki"," ['1 Introduction']",[Cite] http://en.wikipedia.org/wiki,,"First, our proposed method obtains Web pages that include both an acronym and its definitions. Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List [Cite] http://en.wikipedia.org/wiki Calculating a user’s effective permissions requires more than simply looking up that user’s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym “ACL” without its definition in three different meanings found in blogs",補足資料,Document,True,Introduce（引用目的）,False,N06-2041_2_0,2006,Using the Web to Disambiguate Acronyms,Body
1815,11819," http://www.mcsa-exam.com/2006/02/02/effective-permissions.html"," ['1 Introduction']",( [Cite] http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/,,"First, our proposed method obtains Web pages that include both an acronym and its definitions. Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user’s effective permissions requires more than simply looking up that user’s name in the ACL. ( [Cite] http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym “ACL” without its definition in three different meanings found in blogs",補足資料,Document,False,Introduce（引用目的）,False,N06-2041_3_0,2006,Using the Web to Disambiguate Acronyms,Body
1816,11820," http://www.aclweb.org/"," ['1 Introduction']",(http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics [Cite] http://www.aclweb.org/,,"First, our proposed method obtains Web pages that include both an acronym and its definitions. Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user’s effective permissions requires more than simply looking up that user’s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics [Cite] http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym “ACL” without its definition in three different meanings found in blogs",補足資料,Document,False,Introduce（引用目的）,False,N06-2041_4_0,2006,Using the Web to Disambiguate Acronyms,Body
1817,11821," http://pahendra.blogspot.com/2005/06/june-14th.html"," ['1 Introduction']",( [Cite] http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym “ACL” without its definition in three different meanings found in blogs,,"First, our proposed method obtains Web pages that include both an acronym and its definitions. Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user’s effective permissions requires more than simply looking up that user’s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. ( [Cite] http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym “ACL” without its definition in three different meanings found in blogs",補足資料,Document,True,Introduce（引用目的）,False,N06-2041_5_0,2006,Using the Web to Disambiguate Acronyms,Body
1818,11822," http://www.acronymsearch.com/"," ['1 Introduction']",Some sites such as [Cite] http://www.acronymsearch.com/ or http://www.findacronym.com/ provide us with this function.,,"Here, we assume that the list of possible defi-nitions for an acronym is given from sources ex-ternal to this work. Listing pairs of acronyms and their original definitions, on which many studies have been done, such as Nadeau and Turney (2005), results in high performance. Some sites such as [Cite] http://www.acronymsearch.com/ or http://www.findacronym.com/ provide us with this function.",補足資料,Website,True,Use（引用目的）,False,N06-2041_6_0,2006,Using the Web to Disambiguate Acronyms,Body
1819,11823," http://www.findacronym.com/"," ['1 Introduction']",Some sites such as http://www.acronymsearch.com/ or [Cite] http://www.findacronym.com/ provide us with this function.,,"Here, we assume that the list of possible defi-nitions for an acronym is given from sources ex-ternal to this work. Listing pairs of acronyms and their original definitions, on which many studies have been done, such as Nadeau and Turney (2005), results in high performance. Some sites such as http://www.acronymsearch.com/ or [Cite] http://www.findacronym.com/ provide us with this function.",補足資料,Website,True,Use（引用目的）,False,N06-2041_7_0,2006,Using the Web to Disambiguate Acronyms,Body
1820,11824," http://www.acronymsearch.com/"," ['3 Experiment', '3.1 Acronym and definition preparation']",Then we obtained definitions for each acronym from [Cite] http://www.acronymsearch.com/ and dis-carded acronyms that have less than five defini-tions.,,"We downloaded a list of acronyms in capital let-ters only from Wikipedia and filtered them by eliminating acronyms shorter than three letters. Then we obtained definitions for each acronym from [Cite] http://www.acronymsearch.com/ and dis-carded acronyms that have less than five defini-tions. Finally, we randomly selected 20 acronyms.",補足資料,Website,True,Use（引用目的）,True,N06-2041_8_0,2006,Using the Web to Disambiguate Acronyms,Body
1821,11825," http://www.d.umn.edu/~tpederse/WSDTutorial.html"," ['2 The proposal']","Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005) [Cite_Ref] .","Ted Pedersen and Rada. F. Mihalcea, “Advances in Word Sense Disambiguation,” tutorial at ACL 2005. http://www.d.umn.edu/~tpederse/WSDTutorial.html.","Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005) [Cite_Ref] . The hit pages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of a classifier. Features used in classification will be explained in the latter half of this subsection.",補足資料,Paper,True,Introduce（引用目的）,True,N06-2041_9_0,2006,Using the Web to Disambiguate Acronyms,Reference
1822,11826," https://en.wikipedia.org/wiki/Irina_Dunn"," ['2 Motivation']",Consider the sarcastic sen-tence ‘A woman needs a man like a fish needs bicycle’ [Cite_Footnote_2] .,"2 This quote is attributed to Irina Dunn, an Australian writer (https://en.wikipedia.org/wiki/Irina_Dunn","In our literature survey of sarcasm detection (Joshi et al., 2016), we observe that a popular trend is semi-supervised extraction of patterns with implicit sentiment. One such work is by Riloff et al. (2013) who give a bootstrap-ping algorithm that discovers a set of positive verbs and negative/undesirable situations. However, this simplifi-cation (of representing sarcasm merely as positive verbs followed by negative situation) may not capture difficult forms of context incongruity. Consider the sarcastic sen-tence ‘A woman needs a man like a fish needs bicycle’ [Cite_Footnote_2] . The sarcasm in this sentence is understood from the fact that a fish does not need bicycle - and hence, the sentence ridicules the target ‘a man’. However, this sentence does not contain any sentiment-bearing word. Existing sar-casm detection systems relying on sentiment incongruity (as in the case of our past work reported as Joshi et al. (2015)) may not work well in such cases of sarcasm.",補足資料,Document,True,Introduce（引用目的）,True,D16-1104_0_0,2016,Are Word Embedding-based Features Useful for Sarcasm Detection?,Footnote
1823,11827," http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/"," ['5 Experiment Setup']",We use pre-trained word em-beddings based on LSA [Cite_Footnote_5] .,5 http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/,"1. LSA: This approach was reported in Landauer and Dumais (1997). We use pre-trained word em-beddings based on LSA [Cite_Footnote_5] . The vocabulary size is 100,000.",Material,Knowledge,False,Use（引用目的）,True,D16-1104_1_0,2016,Are Word Embedding-based Features Useful for Sarcasm Detection?,Footnote
1824,11828," http://nlp.stanford.edu/projects/glove/"," ['5 Experiment Setup']",GloVe: We use pre-trained vectors avaiable from the GloVe project [Cite_Footnote_6] .,6 http://nlp.stanford.edu/projects/glove/,"2. GloVe: We use pre-trained vectors avaiable from the GloVe project [Cite_Footnote_6] . The vocabulary size in this case is 2,195,904.",Material,DataSource,True,Use（引用目的）,True,D16-1104_2_0,2016,Are Word Embedding-based Features Useful for Sarcasm Detection?,Footnote
1825,11829," https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/"," ['5 Experiment Setup']","Dependency Weights: We use pre-trained vectors [Cite_Footnote_7] weighted using dependency distance, as given in Levy and Goldberg (2014).",7 https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/,"3. Dependency Weights: We use pre-trained vectors [Cite_Footnote_7] weighted using dependency distance, as given in Levy and Goldberg (2014). The vocabulary size is 174,015.",Material,Knowledge,True,Use（引用目的）,True,D16-1104_3_0,2016,Are Word Embedding-based Features Useful for Sarcasm Detection?,Footnote
1826,11830," https://code.google.com/archive/p/Word2Vec/"," ['5 Experiment Setup']",These were trained using Word2Vec tool [Cite_Footnote_8] on the Google News corpus.,8 https://code.google.com/archive/p/Word2Vec/,"4. Word2Vec: use pre-trained Google word vectors. These were trained using Word2Vec tool [Cite_Footnote_8] on the Google News corpus. The vocabulary size for Word2Vec is 3,000,000. To interact with these pre-trained vectors, as well as compute various features, we use gensim library (Řehůřek and Sojka, 2010).",Method,Tool,True,Use（引用目的）,False,D16-1104_4_0,2016,Are Word Embedding-based Features Useful for Sarcasm Detection?,Footnote
1827,11831," https://github.com/luheng/lsgn"," ['References']",Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates. [Cite_Footnote_1],1 Code and models: https://github.com/luheng/lsgn,"Recent BIO-tagging-based neural seman-tic role labeling models are very high per-forming, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the rela-tions between them. The model makes in-dependent decisions about what relation-ship, if any, holds between every possi-ble word-span pair, and learns contextu-alized span representations that provide rich, shared input features for each deci-sion. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P18-2058_0_0,2018,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,Footnote
1828,11832," http://sary.sourceforge.net"," ['4 Experimental Settings']","The system was implemented in Java, however it handled the suffix arrays through an external C li-brary called Sary. [Cite_Footnote_1]",1 http://sary.sourceforge.net,"The system was implemented in Java, however it handled the suffix arrays through an external C li-brary called Sary. [Cite_Footnote_1] All experiments were conducted on a 2 GHz Core2Duo T7200 machine with 2 GB RAM.",Material,Knowledge,False,Produce（引用目的）,True,D10-1081_0_0,2010,An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL,Footnote
1829,11833," https://worksheets.codalab.org/worksheets/0x593676a278fc4e5abe2d8bac1e3df486/"," ['6 Discussion and related work']","All code, data, and experiments for this paper are avail-able on the CodaLab platform at [Cite] https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/ .",,"Reproducibility. All code, data, and experiments for this paper are avail-able on the CodaLab platform at [Cite] https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/ .",Mixed,Mixed,True,Produce（引用目的）,True,P16-1090_0_0,2016,Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings,Body
1830,11834," https://github.com/declare-lab/"," ['References']",The implementations are available at [Cite] https://github.com/declare-lab/ identifiable-transformers .,,"Interpretability is an important aspect of the trustworthiness of a model’s predic-tions. Transformer’s predictions are widely explained by the attention weights, i.e., a probability distribution generated at its self-attention unit (head). Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique. A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights. For a given input to a head and its output, if the attention weights generated in it are unique, we call the weights identifiable. In this work, we provide deeper theoretical analysis and empirical observa-tions on the identifiability of attention weights. Ignored in the previous works, we find the at-tention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector. However, the weights are still prone to be non-unique attentions that make them unfit for interpretation. To tackle this issue, we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input. We prove the applicability of such variations by providing empirical justifications on varied text classification tasks. The implementations are available at [Cite] https://github.com/declare-lab/ identifiable-transformers .",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.94_0_0,2021,More Identifiable yet Equally Performant Transformers for Text Classification,Body
1831,11835," https://pytorch.org/text/_modules/torchtext/data/utils.html"," ['6 Experimental Setup']","We normalize the text by lower casing, removing special characters, etc. [Cite_Footnote_9]",9 https://pytorch.org/text/_modules/torchtext/data/utils.html,"Setting up the encoder. We normalize the text by lower casing, removing special characters, etc. [Cite_Footnote_9] For each task, we construct separate 1-Gram vo-cabulary (U) and initialize a trainable randomly sampled token embedding (U × d e ) from N (0, 1). Similarly, we randomly initialize a (d s-max × d e ) positional embedding.",Method,Code,True,Produce（引用目的）,True,2021.acl-long.94_1_0,2021,More Identifiable yet Equally Performant Transformers for Text Classification,Footnote
1832,11836," https://nlp.stanford.edu/projects/nmt/"," ['6 Experiments']","For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [Cite_Footnote_2] .","2 Datasets used in this work can be found at https://nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense","The proposed approach provides a new paradigm and understanding of data augmentation for text generation. To evaluate that our approach can mimic the effect of data augmentation, we con-duct experiments on two text generation tasks – neural machine translation and conversational re-sponse generation. We compare our approach with two most popular data augmentation methods (one token-level and one sentence-level augmentation method) that can be applied on various text genera-tion tasks: • Masked Language model (MLM): We use a pre-trained BERT (Devlin et al., 2019; Wolf et al., 2020) and randomly choose 15% of the words for each sentence. BERT takes in these masked words to predict these masked positions with new words. We augment one sample from each original train-ing sample. Thus the data size increases to twice of the original one. Note that we only augment the English side of translation datasets. • Back-translation (BT): For neural machine trans-lation, we employ a fixed target-to-source transla-tion model trained on the original dataset. For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [Cite_Footnote_2] . We again augment one sam-ple from each original training sample.",Material,Dataset,True,Produce（引用目的）,True,2021.acl-long.173_0_0,2021,Data Augmentation for Text Generation Without Any Augmented Data,Footnote
1833,11837," http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense"," ['6 Experiments']","For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [Cite_Footnote_2] .","2 Datasets used in this work can be found at https://nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense","The proposed approach provides a new paradigm and understanding of data augmentation for text generation. To evaluate that our approach can mimic the effect of data augmentation, we con-duct experiments on two text generation tasks – neural machine translation and conversational re-sponse generation. We compare our approach with two most popular data augmentation methods (one token-level and one sentence-level augmentation method) that can be applied on various text genera-tion tasks: • Masked Language model (MLM): We use a pre-trained BERT (Devlin et al., 2019; Wolf et al., 2020) and randomly choose 15% of the words for each sentence. BERT takes in these masked words to predict these masked positions with new words. We augment one sample from each original train-ing sample. Thus the data size increases to twice of the original one. Note that we only augment the English side of translation datasets. • Back-translation (BT): For neural machine trans-lation, we employ a fixed target-to-source transla-tion model trained on the original dataset. For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [Cite_Footnote_2] . We again augment one sam-ple from each original training sample.",Material,Dataset,True,Produce（引用目的）,True,2021.acl-long.173_1_0,2021,Data Augmentation for Text Generation Without Any Augmented Data,Footnote
1834,11838," https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh"," ['6 Experiments', '6.1 Neural Machine Translation']",The datasets of IWSLT14 are pre-processed with the script in Fairseq [Cite_Footnote_3] .,3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh,"We use translation benchmarks IWSLT14 En–De, En–Fr, En–It, and IWSLT15 En–Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [Cite_Footnote_3] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set. The IWSLT15 dataset is the same as that used in Luong et al. (2015a), and the validation and test sets are tst2012 and tst2013, respectively.",Material,Dataset,False,Use（引用目的）,True,2021.acl-long.173_2_0,2021,Data Augmentation for Text Generation Without Any Augmented Data,Footnote
1835,11839," https://github.com/SapienzaNLP/genesis"," ['References']","We release the fine-tuned models, the generated datasets and the code to repro-duce the experiments at [Cite] https://github.com/SapienzaNLP/genesis.",,"The lexical substitution task aims at generat-ing a list of suitable replacements for a tar-get word in context, ideally keeping the mean-ing of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetun-ing of neural models on the task, hindering the full fruition of recently introduced power-ful architectures such as language models. Fur-thermore, lexical substitution is usually evalu-ated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, sub-stitutes. To assess these issues, we propose G ENE S IS (Generating Substitutes in contexts), the first generative approach to lexical substi-tution. Thanks to a seq2seq model, we gen-erate substitutes for a word according to the context it appears in, attaining state-of-the-art results on different benchmarks. More-over, our approach allows silver data to be produced for further improving the perfor-mances of lexical substitution systems. Along with an extensive analysis of G ENE S IS results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets and the code to repro-duce the experiments at [Cite] https://github.com/SapienzaNLP/genesis.",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.844_0_0,2021,G ENE S IS : A Generative Approach to Substitutes in Context,Body
1836,11840," http://nlp.uniroma1.it/wsdeval/training-data"," ['5 Dataset Generation']","As source dataset C we exploit SemCor (Miller et al., 1993), a manually annotated corpus where instances are sense-tagged according to the WordNet sense in-ventory [Cite_Footnote_4] .","4 We use the version released at http://nlp.uniroma1.it/wsdeval/training-data, with sense annotations that leverage WordNet 3.0.","G ENE S IS is able to generate substitutes starting from a word in its context. Thus, starting from a source dataset C of target words in context, we can exploit G ENE S IS to produce ranked lists of sub-stitutes and, associating the generated substitutes with the targets, obtain silver datasets for the lexi-cal substitution task. To this end, first, we finetune G ENE S IS on a gold dataset for the lexical substi-tution task; then, we give as input to the finetuned model the corpus C, generating as output a list of replacements for each input instance. The input instances, associated with the generated substitutes, constitute the silver corpus. To ensure the quality of the generated substitutes, we apply a similar-ity threshold λ on the ranking step of G ENE S IS (cf. Section 3), keeping only the substitutes whose similarity to the target is higher than λ. As source dataset C we exploit SemCor (Miller et al., 1993), a manually annotated corpus where instances are sense-tagged according to the WordNet sense in-ventory [Cite_Footnote_4] . While it is typically used as a training corpus for English Word Sense Disambiguation (WSD), as we show, its manually-curated sense dis-tribution is also beneficial for lexical substitution. Indeed, having a frequency of target words that follows a sense distribution involves the generation of substitutes for different senses of the same word, helping lexical substitution systems finetuned on the silver dataset to generalize more effectively.",Material,Knowledge,True,Use（引用目的）,True,2021.emnlp-main.844_1_0,2021,G ENE S IS : A Generative Approach to Substitutes in Context,Footnote
1837,11841," https://github.com/heartexlabs/label-studio#try-out-label-studio"," ['9 Qualitative Evaluation']",We select three annotators with certified proficiency in English and previous experi-ence in linguistic annotation tasks and present them with a sample of 322 test instances drawn from the LST dataset [Cite_Footnote_8] .,"8 The sample size is significant with respect to the source dataset with confidence level of 95% and a margin error of ± 5. The annotation interface was developed through Label Studio https://github.com/heartexlabs/label-studio#try-out-label-studio. layer dropout, when keeping dropout fixed at 0.1. The chosen value is 0.6.","Annotation Task We set up a test where an anno-tator is provided with a target word in context and a set of substitutes that are equally distributed among the gold and the generated ones. The annotator is required to select, if there are any, all the substitutes that are not suitable replacements for the target in the given context. We select three annotators with certified proficiency in English and previous experi-ence in linguistic annotation tasks and present them with a sample of 322 test instances drawn from the LST dataset [Cite_Footnote_8] . The annotators are asked to select the inappropriate substitutes from an anonymized shuffled set of three gold and three generated substi-tutes, obtained with G ENE S IS trained on CT T . For all the instances, the gold substitutes do not appear in the generated ones and vice versa. The annota-tion guidelines are reported in the supplementary material (Section D).",Material,Dataset,True,Use（引用目的）,False,2021.emnlp-main.844_2_0,2021,G ENE S IS : A Generative Approach to Substitutes in Context,Footnote
1838,11842," https://github.com/doug919/multi_relational_script_learning"," ['4 Experiments']",The source code and pre-trained models are publicly available [Cite_Footnote_2] .,2 https://github.com/doug919/multi_ relational_script_learning,"For training, we use the New York Times (NYT) section of the English Gigaword (Parker et al., 2011). It contains 2M newswire articles and splits into train/dev/test sets, replicating the setup given by Granroth-Wilding and Clark (2016). 500M triplets are extracted from the training set. All the experimental results are averaged over 5 runs. We leave the details about hyperparameter tuning in the appendix. The source code and pre-trained models are publicly available [Cite_Footnote_2] .",Mixed,Mixed,True,Produce（引用目的）,True,P19-1413_0_0,2019,Multi-Relational Script Learning for Discourse Relations,Footnote
1839,11843," http://mallet.cs.umass.edu"," ['3 Discourse vs. non-discourse usage']",Here we report results using a maximum entropy classifier [Cite_Footnote_2] using ten-fold cross-validation over sections 2-22.,2 http://mallet.cs.umass.edu,Sections 0 and 1 of the PDTB were used for de-velopment of the features described in the previous section. Here we report results using a maximum entropy classifier [Cite_Footnote_2] using ten-fold cross-validation over sections 2-22.,Method,Tool,True,Use（引用目的）,True,P09-2004_0_0,2009,Using Syntax to Disambiguate Explicit Discourse Connectives in Text ∗,Footnote
1840,11844," http://sourceforge.net/projects/zgen/"," ['1 Introduction']",Our linearizer is publicly available under GPL at [Cite] http://sourceforge.,,"For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under lin-ear complexity. Compared with parsers that use dy-namic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard lineariza-tion task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addi-tion to syntactic tree construction. Experimental re-sults show that the transition-based linearization sys-tem runs an order of magnitude faster than a state-of-the-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at [Cite] http://sourceforge.net/projects/zgen/.",Method,Tool,True,Produce（引用目的）,True,N15-1012_0_0,2015,Transition-Based Syntactic Linearization,Body
1841,11845," http://stp.lingfil.uu.se/~nivre/research/Penn2Malt.html"," ['4 Experiments']","Gold-standard de-pendency trees are derived from bracketed sentences in the treebank using Penn2Malt [Cite_Footnote_1] , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013).",1 http://stp.lingfil.uu.se/˜nivre/research/Penn2Malt.html,"We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour-nal sections 2–21 for training, 22 for development testing and 23 for final testing. Gold-standard de-pendency trees are derived from bracketed sentences in the treebank using Penn2Malt [Cite_Footnote_1] , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of lin-earization, which has been adopted in former liter-als (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline.",Material,Dataset,True,Extend（引用目的）,False,N15-1012_1_0,2015,Transition-Based Syntactic Linearization,Footnote
1842,11846," http://sourceforge.net/projects/zgen/"," ['6 Conclusion']",We publicly release our code at [Cite] http: //sourceforge.net/projects/zgen/.,,"We studied transition-based syntactic linearization as an extension to transition-based parsing. Com-pared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sen-tences when given a bag of words. Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-first baseline. We publicly release our code at [Cite] http: //sourceforge.net/projects/zgen/.",Method,Code,True,Produce（引用目的）,True,N15-1012_2_0,2015,Transition-Based Syntactic Linearization,Body
1843,11847," http://github.com/chaitanyamalaviya/lang-reps"," ['References']","Experiments show that the pro-posed method is able to infer not only syn-tactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages’ geographic and phy-logenetic neighbors. [Cite_Footnote_1]",1 Code and learned vectors are available at http://github.com/chaitanyamalaviya/lang-reps,"One central mystery of neural NLP is what neural models “know” about their subject matter. When a neural machine transla-tion system learns to translate from one language to another, does it learn the syn-tax or semantics of the languages? Can this knowledge be extracted from the sys-tem to fill holes in human scientific knowl-edge? Existing typological databases con-tain relatively full feature specifications for only a few hundred languages. Ex-ploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one neural machine translation (NMT) system from 1017 lan-guages into English, and use this to pre-dict information missing from typological databases. Experiments show that the pro-posed method is able to infer not only syn-tactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages’ geographic and phy-logenetic neighbors. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D17-1268_0_0,2017,Learning Language Representations for Typology Prediction,Footnote
1844,11848," https://github.com/wenlinyao/EMNLP20-SubeventAcquisition"," ['References']",The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations [Cite_Footnote_1] .,1 Code and the knowledge base are avail-able at https://github.com/wenlinyao/ EMNLP20-SubeventAcquisition,"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly su-pervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two obser-vations that 1) subevents are temporally con-tained by the parent event, and 2) the defini-tions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the ini-tial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evalua-tion showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.430_0_0,2020,Weakly Supervised Subevent Knowledge Acquisition,Footnote
1845,11849," https://github.com/huggingface/transformers"," ['5 The Contextual Classifier Using BERT']","In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [Cite_Footnote_8] .",8 Our implementation was based on https://github.com/huggingface/transformers.,"In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [Cite_Footnote_8] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs.",Method,Code,False,Use（引用目的）,True,2020.emnlp-main.430_1_0,2020,Weakly Supervised Subevent Knowledge Acquisition,Footnote
1846,11850," http://en.wikipedia.org/wiki/Metaphone"," ['3 Our Method', '3.1 UrduPhone']","Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caver-phone (Wang, 2009) and Double Metaphone. [Cite_Footnote_2]",2 http://en.wikipedia.org/wiki/ Metaphone,"Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caver-phone (Wang, 2009) and Double Metaphone. [Cite_Footnote_2] These schemes encode words based on their sound which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other family of languages like Urdu.",補足資料,Document,True,Introduce（引用目的）,True,D15-1097_0_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1847,11851," http://www.shashca.com"," ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']","The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news [Cite_Footnote_5] , poetry , SMS and blog .","5 http://www.shashca.com,http://stepforwardpak.com/","The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news [Cite_Footnote_5] , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard.",Material,DataSource,True,Use（引用目的）,True,D15-1097_1_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1848,11852," http://stepforwardpak.com/"," ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']","The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news [Cite_Footnote_5] , poetry , SMS and blog .","5 http://www.shashca.com,http://stepforwardpak.com/","The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news [Cite_Footnote_5] , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard.",Material,DataSource,True,Use（引用目的）,True,D15-1097_2_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1849,11853," https://hadi763.wordpress.com/"," ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']","The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry [Cite_Footnote_6] , SMS and blog .",6 https://hadi763.wordpress.com/,"The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry [Cite_Footnote_6] , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard.",Material,DataSource,True,Use（引用目的）,True,D15-1097_3_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1850,11854," http://www.replysms.com/"," ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']","The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS [Cite_Footnote_7] and blog .",7 http://www.replysms.com/,"The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS [Cite_Footnote_7] and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard.",Material,DataSource,True,Use（引用目的）,True,D15-1097_4_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1851,11855," http://roman.urdu.co/"," ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']","The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS and blog [Cite_Footnote_8] .",8 http://roman.urdu.co/,"The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS and blog [Cite_Footnote_8] . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard.",Material,DataSource,True,Use（引用目的）,True,D15-1097_5_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1852,11856," http://chopaal.org"," ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']","The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service [Cite_Footnote_9] .",9 http://chopaal.org,"The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service [Cite_Footnote_9] . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold standard.",補足資料,Website,True,Use（引用目的）,True,D15-1097_6_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1853,11857," http://bit.ly/1OJGL9Q"," ['4 Experimental Evaluation', '4.2 UrduPhone Evaluation']",We compare UrduPhone with Soundex and its variants. [Cite_Footnote_10],10 We use NLTK-Trainer’s phonetic library http://bit.ly/1OJGL9Q,"We compare UrduPhone with Soundex and its variants. [Cite_Footnote_10] These algorithms are used to group words based on their encoding and then evalu-ated against the gold standard. Table 2 shows the results on Web dataset. UrduPhone out-performs Soundex, Caverphone, and Metaphone while Nysiis’s f-measure is comparable to that of UrduPhone. We observe that Nysiis produces a large number of single word clusters (out of 6,943 clusters produced 5,159 have only one word). This gives high precision but recall is low. UrduPhone produces fewer clusters (and fewer one word clus-ters) with high recall.",Material,Knowledge,True,Use（引用目的）,True,D15-1097_7_0,2015,An Unsupervised Method for Discovering Lexical Variations in Roman Urdu Informal Text,Footnote
1854,11858," https://github.com/neemakot/Health-Fact-Checking"," ['References']","To sup-port this case study we construct a new dataset P UB H EALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check la-bels for claims [Cite_Footnote_1] .",1 Data and code are available here: https://github.com/neemakot/Health-Fact-Checking,"Fact-checking is the task of verifying the ve-racity of claims by assessing their assertions against credible evidence. The vast major-ity of fact-checking studies focus exclusively on political claims. Very little research ex-plores fact-checking for other topics, specif-ically subject matters for which expertise is required. We present the first study of ex-plainable fact-checking for claims which re-quire specific expertise. For our case study we choose the setting of public health. To sup-port this case study we construct a new dataset P UB H EALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check la-bels for claims [Cite_Footnote_1] . We explore two tasks: ve-racity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.623_0_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1855,11859," https://www.snopes.com/"," ['3 The P UB H EALTH dataset', '3.1 Data collection']","Initially, we scraped 39,301 claims, amounting to: 27,578 fact-checked claims from five fact-checking websites (Snopes [Cite_Footnote_2] , Politifact 3 , Truthor-Fiction 4 , FactCheck 5 , and FullFact 6 ); 9,023 news headline claims from the health section and health tags of Associated Press 7 and Reuters News 8 web-sites; and 2,700 claims from the news review site Health News Review (HNR) 9 .","2 https://www.snopes.com/claims). We compute the mean and standard deviation for Flesch-Kincaid and Dale-Chall scores of claims for LIAR (Wang, 2017), FEVER (Thorne et al., 2018), MultiFC (Augenstein et al., 2019), FAKENEWSNET (Shu et al., 2019b), and also our own fact-checking dataset. The sample sizes used for evaluation for each dataset are as follows, LIAR: 12,791, MultiFC: 34,842, FAKENEWSNET: 23,196, FEVER: 145,449, and 11,832 for our dataset.","Initially, we scraped 39,301 claims, amounting to: 27,578 fact-checked claims from five fact-checking websites (Snopes [Cite_Footnote_2] , Politifact 3 , Truthor-Fiction 4 , FactCheck 5 , and FullFact 6 ); 9,023 news headline claims from the health section and health tags of Associated Press 7 and Reuters News 8 web-sites; and 2,700 claims from the news review site Health News Review (HNR) 9 .",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_1_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1856,11860," https://medlineplus.gov/encyclopedia.html"," ['References']","These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, [Cite_Footnote_12] Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic.",12 https://medlineplus.gov/encyclopedia. html,"Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, [Cite_Footnote_12] Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. ‘Centers for Disease Control and Prevention’, ‘abscess’, ‘adolescence’, ‘airborne’, ‘alimentation’, ‘alopecia’, ‘aneurysm’, ‘anorexia’, ‘anti-vaxxer’, ‘arrhythmia’, ‘bacteria’, ‘bacterium’, ‘biohazard’, ‘bioterrorism’, ‘bleeding’, ‘blood pressure’, ‘chickenpox’, ‘chloroquine’, ‘con-tagious’, ‘death’, ‘disease’, ‘embolism’, ‘endemic’, ‘environment’, ‘epidemiology’, ‘first aid’, ‘flatten the curve’, ‘flu’, ‘gallbladder’, ‘gangrene’, ‘heart at-tack’, ‘heparin’, ‘hospital’, ‘hydroxychloroquine’, ‘hygiene’, ‘hypertension’, ‘illness’, ‘immune’, ‘in-fant mortality rate’, ‘infect’, ‘influenza’, ‘lactose intolerance’, ‘liver’, ‘medicine’, ‘menstruation’, ‘mental health’, ‘nurse’, ‘organs’, outbreak, pace-maker, ‘pandemic’, ‘pathogen’, ‘patients’, ‘period poverty’, ‘public health’, ‘quarantine’, ‘sickness’, ‘smoking’, ‘stroke’, ‘surgical’, ‘tumour’, ‘vaccine’, ‘ventilator’, ‘virus’, ‘x-ray’. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_11_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1857,11861," https://www.thinklocalactpersonal.org.uk/Browse/Informationandadvice/CareandSupportJargonBuster/"," ['References']","These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, [Cite_Footnote_13] National Careers Healthcare Job, and the Mayo Clinic.",13 https://www.thinklocalactpersonal.org.uk/Browse/Informationandadvice/CareandSupportJargonBuster/,"Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, [Cite_Footnote_13] National Careers Healthcare Job, and the Mayo Clinic. Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. ‘Centers for Disease Control and Prevention’, ‘abscess’, ‘adolescence’, ‘airborne’, ‘alimentation’, ‘alopecia’, ‘aneurysm’, ‘anorexia’, ‘anti-vaxxer’, ‘arrhythmia’, ‘bacteria’, ‘bacterium’, ‘biohazard’, ‘bioterrorism’, ‘bleeding’, ‘blood pressure’, ‘chickenpox’, ‘chloroquine’, ‘con-tagious’, ‘death’, ‘disease’, ‘embolism’, ‘endemic’, ‘environment’, ‘epidemiology’, ‘first aid’, ‘flatten the curve’, ‘flu’, ‘gallbladder’, ‘gangrene’, ‘heart at-tack’, ‘heparin’, ‘hospital’, ‘hydroxychloroquine’, ‘hygiene’, ‘hypertension’, ‘illness’, ‘immune’, ‘in-fant mortality rate’, ‘infect’, ‘influenza’, ‘lactose intolerance’, ‘liver’, ‘medicine’, ‘menstruation’, ‘mental health’, ‘nurse’, ‘organs’, outbreak, pace-maker, ‘pandemic’, ‘pathogen’, ‘patients’, ‘period poverty’, ‘public health’, ‘quarantine’, ‘sickness’, ‘smoking’, ‘stroke’, ‘surgical’, ‘tumour’, ‘vaccine’, ‘ventilator’, ‘virus’, ‘x-ray’. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_12_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1858,11862," https://nationalcareers.service.gov.uk/job-categories/healthcare"," ['References']","These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, [Cite_Footnote_14] and the Mayo Clinic.",14 https://nationalcareers.service.gov.uk/job-categories/healthcare,"Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, [Cite_Footnote_14] and the Mayo Clinic. Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. ‘Centers for Disease Control and Prevention’, ‘abscess’, ‘adolescence’, ‘airborne’, ‘alimentation’, ‘alopecia’, ‘aneurysm’, ‘anorexia’, ‘anti-vaxxer’, ‘arrhythmia’, ‘bacteria’, ‘bacterium’, ‘biohazard’, ‘bioterrorism’, ‘bleeding’, ‘blood pressure’, ‘chickenpox’, ‘chloroquine’, ‘con-tagious’, ‘death’, ‘disease’, ‘embolism’, ‘endemic’, ‘environment’, ‘epidemiology’, ‘first aid’, ‘flatten the curve’, ‘flu’, ‘gallbladder’, ‘gangrene’, ‘heart at-tack’, ‘heparin’, ‘hospital’, ‘hydroxychloroquine’, ‘hygiene’, ‘hypertension’, ‘illness’, ‘immune’, ‘in-fant mortality rate’, ‘infect’, ‘influenza’, ‘lactose intolerance’, ‘liver’, ‘medicine’, ‘menstruation’, ‘mental health’, ‘nurse’, ‘organs’, outbreak, pace-maker, ‘pandemic’, ‘pathogen’, ‘patients’, ‘period poverty’, ‘public health’, ‘quarantine’, ‘sickness’, ‘smoking’, ‘stroke’, ‘surgical’, ‘tumour’, ‘vaccine’, ‘ventilator’, ‘virus’, ‘x-ray’. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_13_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1859,11863," https://www.mayoclinic.org/"," ['References']","These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15]","15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements","Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. ‘Centers for Disease Control and Prevention’, ‘abscess’, ‘adolescence’, ‘airborne’, ‘alimentation’, ‘alopecia’, ‘aneurysm’, ‘anorexia’, ‘anti-vaxxer’, ‘arrhythmia’, ‘bacteria’, ‘bacterium’, ‘biohazard’, ‘bioterrorism’, ‘bleeding’, ‘blood pressure’, ‘chickenpox’, ‘chloroquine’, ‘con-tagious’, ‘death’, ‘disease’, ‘embolism’, ‘endemic’, ‘environment’, ‘epidemiology’, ‘first aid’, ‘flatten the curve’, ‘flu’, ‘gallbladder’, ‘gangrene’, ‘heart at-tack’, ‘heparin’, ‘hospital’, ‘hydroxychloroquine’, ‘hygiene’, ‘hypertension’, ‘illness’, ‘immune’, ‘in-fant mortality rate’, ‘infect’, ‘influenza’, ‘lactose intolerance’, ‘liver’, ‘medicine’, ‘menstruation’, ‘mental health’, ‘nurse’, ‘organs’, outbreak, pace-maker, ‘pandemic’, ‘pathogen’, ‘patients’, ‘period poverty’, ‘public health’, ‘quarantine’, ‘sickness’, ‘smoking’, ‘stroke’, ‘surgical’, ‘tumour’, ‘vaccine’, ‘ventilator’, ‘virus’, ‘x-ray’. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_14_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1860,11864," https://www.mayoclinic.org/symptoms"," ['References']","These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15]","15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements","Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. ‘Centers for Disease Control and Prevention’, ‘abscess’, ‘adolescence’, ‘airborne’, ‘alimentation’, ‘alopecia’, ‘aneurysm’, ‘anorexia’, ‘anti-vaxxer’, ‘arrhythmia’, ‘bacteria’, ‘bacterium’, ‘biohazard’, ‘bioterrorism’, ‘bleeding’, ‘blood pressure’, ‘chickenpox’, ‘chloroquine’, ‘con-tagious’, ‘death’, ‘disease’, ‘embolism’, ‘endemic’, ‘environment’, ‘epidemiology’, ‘first aid’, ‘flatten the curve’, ‘flu’, ‘gallbladder’, ‘gangrene’, ‘heart at-tack’, ‘heparin’, ‘hospital’, ‘hydroxychloroquine’, ‘hygiene’, ‘hypertension’, ‘illness’, ‘immune’, ‘in-fant mortality rate’, ‘infect’, ‘influenza’, ‘lactose intolerance’, ‘liver’, ‘medicine’, ‘menstruation’, ‘mental health’, ‘nurse’, ‘organs’, outbreak, pace-maker, ‘pandemic’, ‘pathogen’, ‘patients’, ‘period poverty’, ‘public health’, ‘quarantine’, ‘sickness’, ‘smoking’, ‘stroke’, ‘surgical’, ‘tumour’, ‘vaccine’, ‘ventilator’, ‘virus’, ‘x-ray’. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_15_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1861,11865," https://www.mayoclinic.org/tests-procedures"," ['References']","These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15]","15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements","Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. ‘Centers for Disease Control and Prevention’, ‘abscess’, ‘adolescence’, ‘airborne’, ‘alimentation’, ‘alopecia’, ‘aneurysm’, ‘anorexia’, ‘anti-vaxxer’, ‘arrhythmia’, ‘bacteria’, ‘bacterium’, ‘biohazard’, ‘bioterrorism’, ‘bleeding’, ‘blood pressure’, ‘chickenpox’, ‘chloroquine’, ‘con-tagious’, ‘death’, ‘disease’, ‘embolism’, ‘endemic’, ‘environment’, ‘epidemiology’, ‘first aid’, ‘flatten the curve’, ‘flu’, ‘gallbladder’, ‘gangrene’, ‘heart at-tack’, ‘heparin’, ‘hospital’, ‘hydroxychloroquine’, ‘hygiene’, ‘hypertension’, ‘illness’, ‘immune’, ‘in-fant mortality rate’, ‘infect’, ‘influenza’, ‘lactose intolerance’, ‘liver’, ‘medicine’, ‘menstruation’, ‘mental health’, ‘nurse’, ‘organs’, outbreak, pace-maker, ‘pandemic’, ‘pathogen’, ‘patients’, ‘period poverty’, ‘public health’, ‘quarantine’, ‘sickness’, ‘smoking’, ‘stroke’, ‘surgical’, ‘tumour’, ‘vaccine’, ‘ventilator’, ‘virus’, ‘x-ray’. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_16_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1862,11866," https://www.mayoclinic.org/drugs-supplements"," ['References']","These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15]","15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements","Building the public health lexicon. In order to compile the lexicon we scraped health related terms from the following website sources. In total we scraped vocabulary from a number of pages across six websites. These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [Cite_Footnote_15] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped. ‘Centers for Disease Control and Prevention’, ‘abscess’, ‘adolescence’, ‘airborne’, ‘alimentation’, ‘alopecia’, ‘aneurysm’, ‘anorexia’, ‘anti-vaxxer’, ‘arrhythmia’, ‘bacteria’, ‘bacterium’, ‘biohazard’, ‘bioterrorism’, ‘bleeding’, ‘blood pressure’, ‘chickenpox’, ‘chloroquine’, ‘con-tagious’, ‘death’, ‘disease’, ‘embolism’, ‘endemic’, ‘environment’, ‘epidemiology’, ‘first aid’, ‘flatten the curve’, ‘flu’, ‘gallbladder’, ‘gangrene’, ‘heart at-tack’, ‘heparin’, ‘hospital’, ‘hydroxychloroquine’, ‘hygiene’, ‘hypertension’, ‘illness’, ‘immune’, ‘in-fant mortality rate’, ‘infect’, ‘influenza’, ‘lactose intolerance’, ‘liver’, ‘medicine’, ‘menstruation’, ‘mental health’, ‘nurse’, ‘organs’, outbreak, pace-maker, ‘pandemic’, ‘pathogen’, ‘patients’, ‘period poverty’, ‘public health’, ‘quarantine’, ‘sickness’, ‘smoking’, ‘stroke’, ‘surgical’, ‘tumour’, ‘vaccine’, ‘ventilator’, ‘virus’, ‘x-ray’. coronavirusdeaths covid-19virus abortionhospital vaccine marijuanaoutbreak weightflu pandemichealthy hiv infectionprostate respiratoryillness medicinesmoking suicide immuneabuse pregnancydiabetes surgical epidemicmeasles bacteria sleepaids strokebone environmentobesity fever insulinstress quarantine cardiovasculardepression hormoneanxiety liver influenzatumor chemotherapycholesterol count (# 800 text 600 of 400 Length",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.623_17_0,2020,Explainable Automated Fact-Checking for Public Health Claims,Footnote
1863,11867," https://chywang.github.io/data/emnlp17.zip"," ['1 Introduction']",The ex-tracted relations and the labeled test set are pub-licly available [Cite_Footnote_2] .,2 https://chywang.github.io/data/emnlp17.zip,"In the experiments, given only 0.6M entities and their respective 2.4M categories in Chinese Wikipedia, our method extracts 1.52M relations with an overall accuracy of 93.6%. The exper-iments also show that our approach outperforms previous methods for both is-a and non-taxonomic relation extraction from Chinese UGCs. The ex-tracted relations and the labeled test set are pub-licly available [Cite_Footnote_2] .",Material,Knowledge,True,Produce（引用目的）,True,D17-1273_0_0,2017,Learning Fine-grained Relations from Chinese User Generated Categories,Footnote
1864,11868," http://download.wikipedia.com/zhwiki/20170120/"," ['6 Experiments', '6.1 Data Source and Experimental Settings']","The data source is downloaded from the Chi-nese Wikipedia dump of the version January 20th, 2017 [Cite_Footnote_8] .",8 http://download.wikipedia.com/zhwiki/20170120/,"The data source is downloaded from the Chi-nese Wikipedia dump of the version January 20th, 2017 [Cite_Footnote_8] . Because some Wikipedia pages are not related to entities, we use heuristic rules to fil-ter out disambiguation, redirect, template and list pages. Finally, we obtain 0.6M entities and 2.4M entity-category pairs. The open-source toolkit Fu-danNLP (Qiu et al., 2013) is employed for Chinese NLP analysis. The word embeddings are trained via a Skip-gram model using a large corpus from Wang and He (2016) and set to 100 dimensions.",Material,DataSource,True,Use（引用目的）,True,D17-1273_1_0,2017,Learning Fine-grained Relations from Chinese User Generated Categories,Footnote
1865,11869," http://knowledgeworks.cn:20313/cndbpedia/api/entityAVP"," ['6 Experiments', '6.3 Non-taxonomic Relation Extraction']",We use the CN-DBpedia API [Cite_Footnote_10] to ob-tain relations for each entity and report the cover-age of relation r as:,10 http://knowledgeworks.cn:20313/cndbpedia/api/entityAVP,"Evaluation To evaluate the correctness of ex-tracted relations, we carry out two experimental tests: accuracy test and coverage test. Follow-ing Suchanek et al. (2007), in the accuracy test, we randomly sample 200 relation instances for each relation type and ask human annotators to label. We discard the results if human annota-tors disagree. The coverage test is to determine whether the extracted relations already exist in Chinese knowledge bases. Low coverage score means these relations are not present in existing Chinese knowledge bases. In the experiments, we take CN-DBpedia V2.0 (Xu et al., 2017) as the ground truth knowledge base. Up till February 2017, it contains 41M explicit semantic relations of 9M entities, excluding entity summaries, syn-onyms, etc. We use the CN-DBpedia API [Cite_Footnote_10] to ob-tain relations for each entity and report the cover-age of relation r as:",Method,Code,True,Use（引用目的）,True,D17-1273_2_0,2017,Learning Fine-grained Relations from Chinese User Generated Categories,Footnote
1866,11870," https://code.google.com/p/word2vec/"," ['4 Training']",Our experiments are based on the publicly avail-able word2vec [Cite_Footnote_1] and GloVe packages.,1 https://code.google.com/p/word2vec/,"Our experiments are based on the publicly avail-able word2vec [Cite_Footnote_1] and GloVe packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009. On the smallest 17M word corpus we explored a range of vector dimen-sionalities from 10 to 1000. On the larger 120M and 1.6B word corpus, we trained extended mod-els with a 200-dimensional semantic component and a 100-dimensional syntactic component com-paring to 300-dimensional CBOW, Skip-gram and GloVe models. The parameter, λ, in Equation 11 was set to 0.1 and the recommended window sizes of 5, 10 and 15 words either side of the central word were used as context for the CBOW, Skip-gram and GloVe models respectively.",Method,Tool,True,Use（引用目的）,True,P15-1126_0_0,2015,Orthogonality of Syntax and Semantics within Distributional Spaces,Footnote
1867,11871," http://nlp.stanford.edu/projects/glove/"," ['4 Training']",Our experiments are based on the publicly avail-able word2vec and GloVe [Cite_Footnote_2] packages.,2 http://nlp.stanford.edu/projects/glove/,"Our experiments are based on the publicly avail-able word2vec and GloVe [Cite_Footnote_2] packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009. On the smallest 17M word corpus we explored a range of vector dimen-sionalities from 10 to 1000. On the larger 120M and 1.6B word corpus, we trained extended mod-els with a 200-dimensional semantic component and a 100-dimensional syntactic component com-paring to 300-dimensional CBOW, Skip-gram and GloVe models. The parameter, λ, in Equation 11 was set to 0.1 and the recommended window sizes of 5, 10 and 15 words either side of the central word were used as context for the CBOW, Skip-gram and GloVe models respectively.",Method,Tool,True,Use（引用目的）,True,P15-1126_1_0,2015,Orthogonality of Syntax and Semantics within Distributional Spaces,Footnote
1868,11872," http://mattmahoney.net/dc/text.html"," ['4 Training']","We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files [Cite_Footnote_3] suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009.",3 http://mattmahoney.net/dc/text.html,"Our experiments are based on the publicly avail-able word2vec and GloVe packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files [Cite_Footnote_3] suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009. On the smallest 17M word corpus we explored a range of vector dimen-sionalities from 10 to 1000. On the larger 120M and 1.6B word corpus, we trained extended mod-els with a 200-dimensional semantic component and a 100-dimensional syntactic component com-paring to 300-dimensional CBOW, Skip-gram and GloVe models. The parameter, λ, in Equation 11 was set to 0.1 and the recommended window sizes of 5, 10 and 15 words either side of the central word were used as context for the CBOW, Skip-gram and GloVe models respectively.",Material,Dataset,True,Use（引用目的）,True,P15-1126_2_0,2015,Orthogonality of Syntax and Semantics within Distributional Spaces,Footnote
1869,11873," https://github.com/facebookresearch/InferSent"," ['3 Experiments']",[Cite_Footnote_1] .,"1 The official implementation available at https: //github.com/facebookresearch/InferSent is used. Reported hyperparameters are used except LSTM hid-den state, 1024d is chosen due to hardware limitations.","On the SST dataset, our model (86.66% Acc.) is again on par with tree LSTM (87.27% Acc.) and better than Transformer (85.38% Acc.) as well as Infersent (86.00% Acc.) [Cite_Footnote_1] .",Method,Tool,True,Use（引用目的）,True,P19-1030_0_0,2019,You Only Need Attention to Traverse Trees,Footnote
1870,11874," https://github.com/ChenWu98/Point-Then-Operate"," ['References']",Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [Cite_Footnote_1],1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.,"Unsupervised text style transfer aims to al-ter text styles while preserving the content, without aligned data for supervision. Exist-ing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) gener-ated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence op-eration method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We pro-vide comprehensive training objectives to con-trol the fluency, style, and content of the out-puts and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,P19-1482_0_0,2019,A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer,Footnote
1871,11875," http://www.geohive.com/"," ['3 Determining Word Feature', '3.1 Internal Sub-Features']","In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive [Cite_Footnote_3] , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems .",3 http://www.geohive.com/,"Our model captures three types of internal sub-features: 1) f 1 : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2 : internal semantic feature of important triggers; 3) f 3 : internal gazetteer feature. 1) f 1 is the basic sub-feature exploited in this look-up gazetteers: lists of names of persons, organizations, locations and other kinds of named entities. This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive [Cite_Footnote_3] , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems . Gazetters have been widely used in NER systems to improve performance.",Material,DataSource,True,Use（引用目的）,True,P02-1060_0_0,2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 473-480.",Footnote
1872,11876," http://www.yahoo.com/"," ['3 Determining Word Feature', '3.2 External Sub-Features']","For external evidence, only one external macro context feature f [Cite_Footnote_4] , as shown in Table 5, is captured in our model.",4 http://www.yahoo.com/,"For external evidence, only one external macro context feature f [Cite_Footnote_4] , as shown in Table 5, is captured in our model. f 4 is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document, as shown in Table 5 (n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type.). This sub-feature is unique to our system. The intuition behind this is the phenomena of name alias.",補足資料,Document,False,Introduce（引用目的）,False,P02-1060_1_0,2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 473-480.",Footnote
1873,11877," http://www.scopesys.com/"," ['3 Determining Word Feature', '3.1 Internal Sub-Features']","In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems [Cite_Footnote_5] .",5 http://www.scopesys.com/,"Our model captures three types of internal sub-features: 1) f 1 : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2 : internal semantic feature of important triggers; 3) f 3 : internal gazetteer feature. 1) f 1 is the basic sub-feature exploited in this look-up gazetteers: lists of names of persons, organizations, locations and other kinds of named entities. This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems [Cite_Footnote_5] . Gazetters have been widely used in NER systems to improve performance.",Material,DataSource,True,Use（引用目的）,True,P02-1060_2_0,2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 473-480.",Footnote
1874,11878," https://code.google.com/p/word2vec/"," ['4 Features']","develop the word embeddings using word2vec [Cite_Footnote_6] from a cor-pus of about 2.8 billion tokens, using the Continuous Bag-of-Words ( CBOW ) model proposed by Mikolov et al. (2013).",6 https://code.google.com/p/word2vec/,"The second feature captures finer-grained sim-ilarities between related words (e.g., cell and organism). Given the 400-dimensional embed-ding (Baroni et al., 2014) of each content word (lem-matized) in an input sentence, we compute a sentence vector by adding its content lemma vectors. The co-sine similarity between the S (1) and S (2) vectors is then used as an STS feature. Baroni et al. develop the word embeddings using word2vec [Cite_Footnote_6] from a cor-pus of about 2.8 billion tokens, using the Continuous Bag-of-Words ( CBOW ) model proposed by Mikolov et al. (2013).",Method,Tool,True,Use（引用目的）,True,N16-1107_0_0,2016,Bayesian Supervised Domain Adaptation for Short Text Similarity,Footnote
1875,11879," http://fever.ai"," ['1 Introduction']","In addition to pub-lishing the data via our website [Cite_Footnote_1] , we also publish the annotation interfaces and the baseline system to stimulate further research on verification.",1 http://fever.ai,"To characterize the challenges posed by FEVER we develop a pipeline approach which, given a claim, first identifies relevant documents, then se-lects sentences forming the evidence from the doc-uments and finally classifies the claim w.r.t. ev-idence. The best performing version achieves 31.87% accuracy in verification when requiring correct evidence to be retrieved for claims S UP - PORTED or R EFUTED , and 50.91% if the correct-ness of the evidence is ignored, both indicating the difficulty but also the feasibility of the task. We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website [Cite_Footnote_1] , we also publish the annotation interfaces and the baseline system to stimulate further research on verification.",補足資料,Website,True,Produce（引用目的）,True,N18-1074_0_0,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Footnote
1876,11880," https://github.com/awslabs/fever"," ['1 Introduction']","In addition to pub-lishing the data via our website , we also publish the annotation interfaces [Cite_Footnote_2] and the baseline system to stimulate further research on verification.",2 https://github.com/awslabs/fever,"To characterize the challenges posed by FEVER we develop a pipeline approach which, given a claim, first identifies relevant documents, then se-lects sentences forming the evidence from the doc-uments and finally classifies the claim w.r.t. ev-idence. The best performing version achieves 31.87% accuracy in verification when requiring correct evidence to be retrieved for claims S UP - PORTED or R EFUTED , and 50.91% if the correct-ness of the evidence is ignored, both indicating the difficulty but also the feasibility of the task. We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces [Cite_Footnote_2] and the baseline system to stimulate further research on verification.",Method,Tool,True,Produce（引用目的）,True,N18-1074_1_0,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Footnote
1877,11881," https://github.com/sheffieldnlp/fever-baselines"," ['1 Introduction']","In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [Cite_Footnote_3] to stimulate further research on verification.",3 https://github.com/sheffieldnlp/ fever-baselines,"To characterize the challenges posed by FEVER we develop a pipeline approach which, given a claim, first identifies relevant documents, then se-lects sentences forming the evidence from the doc-uments and finally classifies the claim w.r.t. ev-idence. The best performing version achieves 31.87% accuracy in verification when requiring correct evidence to be retrieved for claims S UP - PORTED or R EFUTED , and 50.91% if the correct-ness of the evidence is ignored, both indicating the difficulty but also the feasibility of the task. We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [Cite_Footnote_3] to stimulate further research on verification.",Method,Tool,True,Produce（引用目的）,True,N18-1074_2_0,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Footnote
1878,11882," https://doi.org/10.1017/S1351324909990209"," ['1 Introduction']","When compared to textual entailment (TE)/natural language infer-ence (Dagan et al., 2009 [Cite_Ref] ; Bowman et al., 2015), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in veri-fication systems it is retrieved from a large set of documents in order to form the evidence.","Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i–xvii. https://doi.org/10.1017/S1351324909990209.","In this paper we focus on verification of textual claims against textual sources. When compared to textual entailment (TE)/natural language infer-ence (Dagan et al., 2009 [Cite_Ref] ; Bowman et al., 2015), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in veri-fication systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to han-dle large-scale resources such as Wikipedia (Chen et al., 2017). However, questions typically pro-vide the information needed to identify the answer, while information missing from a claim can of-ten be crucial in retrieving refuting evidence. For example, a claim stating “Fiji’s largest island is Kauai.” can be refuted by retrieving “Kauai is the oldest Hawaiian Island.” as evidence.",補足資料,Paper,True,Compare（引用目的）,True,N18-1074_4_0,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Reference
1879,11883," https://doi.org/10.1017/S1351324909990209"," ['2 Related Works']","Similar to recognizing textual entailment (RTE) (Dagan et al., 2009) [Cite_Ref] , the systems were provided with the sources to verify against, instead of having to retrieve them.","Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i–xvii. https://doi.org/10.1017/S1351324909990209.","The Fake News challenge (Pomerleau and Rao, 2017) modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by jour-nalists in the context of the Emergent Project (Sil-verman, 2015), and the dataset was first proposed by Ferreira and Vlachos (2016), who only classi-fied the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009) [Cite_Ref] , the systems were provided with the sources to verify against, instead of having to retrieve them.",補足資料,Paper,True,Compare（引用目的）,True,N18-1074_4_1,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Reference
1880,11884," http://fakenewschallenge.org/"," ['1 Introduction']","Indicatively, the recently con-ducted Fake News Challenge (Pomerleau and Rao, 2017) [Cite_Ref] with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 as-sociated news articles which is orders of magni-tude smaller than those used for TE and QA.",Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.,"Progress on the aforementioned tasks has bene-fited from the availability of large-scale datasets (Bowman et al., 2015; Rajpurkar et al., 2016). However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims. Indicatively, the recently con-ducted Fake News Challenge (Pomerleau and Rao, 2017) [Cite_Ref] with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 as-sociated news articles which is orders of magni-tude smaller than those used for TE and QA.",補足資料,Website,True,Introduce（引用目的）,True,N18-1074_7_0,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Reference
1881,11885," http://fakenewschallenge.org/"," ['2 Related Works']","The Fake News challenge (Pomerleau and Rao, 2017) [Cite_Ref] modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim.",Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.,"The Fake News challenge (Pomerleau and Rao, 2017) [Cite_Ref] modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by jour-nalists in the context of the Emergent Project (Sil-verman, 2015), and the dataset was first proposed by Ferreira and Vlachos (2016), who only classi-fied the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009), the systems were provided with the sources to verify against, instead of having to retrieve them.",補足資料,Website,True,Introduce（引用目的）,True,N18-1074_7_1,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Reference
1882,11886," http://towcenter.org/research/lies-damn-lies-and-viral-content/"," ['2 Related Works']","The claims and the articles were curated and labeled by jour-nalists in the context of the Emergent Project (Sil-verman, 2015) [Cite_Ref] , and the dataset was first proposed by Ferreira and Vlachos (2016), who only classi-fied the claim w.r.t.","Craig Silverman. 2015. Lies, Damn Lies and Viral Content. http: //towcenter.org/research/ lies-damn-lies-and-viral-content/.","The Fake News challenge (Pomerleau and Rao, 2017) modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by jour-nalists in the context of the Emergent Project (Sil-verman, 2015) [Cite_Ref] , and the dataset was first proposed by Ferreira and Vlachos (2016), who only classi-fied the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009), the systems were provided with the sources to verify against, instead of having to retrieve them.",補足資料,Website,True,Introduce（引用目的）,True,N18-1074_11_0,2018,FEVER: a large-scale dataset for Fact Extraction and VERification,Reference
1883,11887," https://github.com/Adapter-Hub/adapter-transformers"," ['1 Introduction']",[Cite] https://github.com/Adapter-Hub/adapter-transformers this issue by allowing the model to additionally adapt to the particular target language.,,"Standard Transfer Setup The standard way of 2015; Glavaš et al., 2019; Ruder et al., 2019; Wang performing cross-lingual transfer with a state-of-et al., 2020) and later on the full-sentence level the-art large multilingual model such as multilin- (Devlin et al., 2019; Conneau and Lample, 2019; gual BERT or XLM-R is 1) to fine-tune it on la- Cao et al., 2020). More recent models such as mul-belled data of a downstream task in a source lan-tilingual BERT (Devlin et al., 2019)—large Trans-guage and then 2) apply it directly to perform in-former (Vaswani et al., 2017) models pretrained ference in a target language (Hu et al., 2020). A on large amounts of multilingual data—have been downside of this setting is that the multilingual ini-observed to perform surprisingly well when trans-tialisation balances many languages. It is thus not ferring to other languages (Pires et al., 2019; Wu suited to excel at a specific language at inference and Dredze, 2019; Wu et al., 2020) and the cur-time. We propose a simple method to ameliorate 1 [Cite] https://github.com/Adapter-Hub/adapter-transformers this issue by allowing the model to additionally adapt to the particular target language.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.617_0_0,2020,MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer,Body
1884,11888," https://github.com/raosudha89/GYAFC-corpus"," ['4 Experimental setup', '4.1 Datasets']","For the intrinsic evaluation, we used the GYAFC dataset. [Cite_Footnote_1]",1 https://github.com/raosudha89/GYAFC-corpus,"For the intrinsic evaluation, we used the GYAFC dataset. [Cite_Footnote_1] It consists of handcrafted informal-formal sentence pairs in two domains, namely, Entertain-ment & Music (E&M) and Family & Relationship (F&R). Table 1 shows the statistics of the training, validation, and test sets for the GYAFC dataset. In the validation and test sets of GYAFC, each sen-tence has four references. For better exploring the data requirements of different methods to combine rules, we followed Zhang et al. (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, , and Grammarly to help us create a set of rules.",Material,Dataset,True,Use（引用目的）,True,2021.acl-long.124_0_0,2021,Improving Formality Style Transfer with Context-Aware Rule Injection,Footnote
1885,11889," https://www.grammarbot.io/"," ['4 Experimental setup', '4.1 Datasets']","For rule detection system, we used the grammarbot API, [Cite_Footnote_2] , and Grammarly to help us create a set of rules.",2 https://www.grammarbot.io/,"For the intrinsic evaluation, we used the GYAFC dataset. It consists of handcrafted informal-formal sentence pairs in two domains, namely, Entertain-ment & Music (E&M) and Family & Relationship (F&R). Table 1 shows the statistics of the training, validation, and test sets for the GYAFC dataset. In the validation and test sets of GYAFC, each sen-tence has four references. For better exploring the data requirements of different methods to combine rules, we followed Zhang et al. (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, [Cite_Footnote_2] , and Grammarly to help us create a set of rules.",Method,Code,True,Use（引用目的）,True,2021.acl-long.124_1_0,2021,Improving Formality Style Transfer with Context-Aware Rule Injection,Footnote
1886,11890," https://www.grammarly.com/"," ['4 Experimental setup', '4.1 Datasets']","For rule detection system, we used the grammarbot API, , and Grammarly [Cite_Footnote_3] to help us create a set of rules.",3 https://www.grammarly.com/,"For the intrinsic evaluation, we used the GYAFC dataset. It consists of handcrafted informal-formal sentence pairs in two domains, namely, Entertain-ment & Music (E&M) and Family & Relationship (F&R). Table 1 shows the statistics of the training, validation, and test sets for the GYAFC dataset. In the validation and test sets of GYAFC, each sen-tence has four references. For better exploring the data requirements of different methods to combine rules, we followed Zhang et al. (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, , and Grammarly [Cite_Footnote_3] to help us create a set of rules.",Method,Code,True,Use（引用目的）,True,2021.acl-long.124_2_0,2021,Improving Formality Style Transfer with Context-Aware Rule Injection,Footnote
1887,11891," https://slack.com"," ['1 Introduction']","Online conversations are increasingly significant for communication, e.g., Slack [Cite_Footnote_1] for work com-munication and Reddit for general discussion.",1 https://slack.com,"Online conversations are increasingly significant for communication, e.g., Slack [Cite_Footnote_1] for work com-munication and Reddit for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014). State-of-the-art models in both abstractive (Rush et al., 2015) and extractive (Cheng and Lapata, 2016) summarization tasks are based on neural networks, but these models re-quire large amounts of training data. In previous research, these data were created automatically by retrieving headlines and highlights of news articles edited by news editors. However, these method-ologies cannot be applied to the summarization of online conversations because of a lack of summary annotations.",補足資料,Website,True,Introduce（引用目的）,True,D18-1144_0_0,2018,Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations,Footnote
1888,11892," https://www.reddit.com"," ['1 Introduction']","Online conversations are increasingly significant for communication, e.g., Slack for work com-munication and Reddit [Cite_Footnote_2] for general discussion.",2 https://www.reddit.com,"Online conversations are increasingly significant for communication, e.g., Slack for work com-munication and Reddit [Cite_Footnote_2] for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014). State-of-the-art models in both abstractive (Rush et al., 2015) and extractive (Cheng and Lapata, 2016) summarization tasks are based on neural networks, but these models re-quire large amounts of training data. In previous research, these data were created automatically by retrieving headlines and highlights of news articles edited by news editors. However, these method-ologies cannot be applied to the summarization of online conversations because of a lack of summary annotations.",補足資料,Website,True,Introduce（引用目的）,True,D18-1144_1_0,2018,Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations,Footnote
1889,11893," https://twitter.com"," ['2 Related Work']","For a sentiment analysis on Twitter [Cite_Footnote_3] , Davidov(2010) used hashtags, and Gui-bon (2017) used emoji.",3 https://twitter.com,"Many researchers used user-contributed labels from social media as distant labels. Xiong (2014) used review scores on a movie-rating site for a summarization task. For a sentiment analysis on Twitter [Cite_Footnote_3] , Davidov(2010) used hashtags, and Gui-bon (2017) used emoji. In our study, we leverage a popularity measure for a summarization task.",補足資料,Website,True,Compare（引用目的）,True,D18-1144_2_0,2018,Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations,Footnote
1890,11894," http://academicworks.cuny.edu/gc"," ['1 Introduction and Motivation', '1.2 Prior Work']","A survey of recent work of several researchers on producing animations of sign language with fa-cial expressions appears in (Kacorri, 2015) [Cite_Ref] .","Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.","A survey of recent work of several researchers on producing animations of sign language with fa-cial expressions appears in (Kacorri, 2015) [Cite_Ref] . There is recent interest in data-driven approaches using facial motion-capture of human performances to generate sign language animations: For example, (Schmidt et al., 2013) used clustering techniques to select facial expressions that co-occur with indi-vidual lexical items, and (Gibet et al., 2011) stud-ied how to map facial motion-capture data to ani-mation controls.",補足資料,Paper,True,Introduce（引用目的）,True,P16-1196_1_0,2016,Continuous Profile Models in ASL Syntactic Facial Expression Synthesis,Reference
1891,11895," http://academicworks.cuny.edu/gc"," ['3 Evaluation', '3.2 User Evaluation']","Our recent work (Kacorri et al., 2015) [Cite_Ref] has established a set of de-mographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to eval-uate animation.","Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.","In prior work (Huenerfauth and Kacorri, 2015b), we investigated key methodological con-siderations in conducting a study to evaluate sign language animations with deaf users, including the use of appropriate baselines for comparison, appropriate presentation of questions and instruc-tions, demographic and technology experience factors influencing acceptance of signing avatars, and other factors that we have considered in the design of this current study. Our recent work (Kacorri et al., 2015) [Cite_Ref] has established a set of de-mographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to eval-uate animation. Specifically, we screened for par-ticipants that identified themselves as “deaf/Deaf” or “hard-of-hearing,” who had grown up using ASL at home or had attended an ASL-based school as a young child, such as a residential or daytime school.",補足資料,Paper,True,Produce（引用目的）,True,P16-1196_1_1,2016,Continuous Profile Models in ASL Syntactic Facial Expression Synthesis,Reference
1892,11896," http://academicworks.cuny.edu/gc"," ['3 Evaluation', '3.2 User Evaluation']","In prior work, we (Kacorri et al., 2015) [Cite_Ref] have advocated that participants in studies eval-uating sign language animation complete a two standardized surveys about their technology ex-perience (MediaSharing and AnimationAttitude) and that researchers report these values for partici-pants, to enable comparison across studies.","Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.","Deaf researchers (all fluent ASL signers) re-cruited and collected data from participants, dur-ing meetings conducted in ASL. Initial advertise-ments were sent to local email distribution lists and Facebook groups. A total of 17 participants met the above criteria, where 14 participants self-identified as deaf/Deaf and 3 as hard-of-hearing. Of our participants in the study, 10 had attended a residential school for deaf students, and 7, a day-time school for deaf students. 14 participants had learned ASL prior to age 5, and the remaining 3 had been using ASL for over 7 years. There were 8 men and 9 women of ages 19-29 (average age 22.8). In prior work, we (Kacorri et al., 2015) [Cite_Ref] have advocated that participants in studies eval-uating sign language animation complete a two standardized surveys about their technology ex-perience (MediaSharing and AnimationAttitude) and that researchers report these values for partici-pants, to enable comparison across studies. In our study, participant scores for MediaSharing varied between 3 and 6, with a mean score of 4.3, and scores for AnimationAttitude varied from 2 to 6, with a mean score of 3.8.",補足資料,Paper,True,Introduce（引用目的）,True,P16-1196_1_2,2016,Continuous Profile Models in ASL Syntactic Facial Expression Synthesis,Reference
1893,11897," https://visagetechnologies.com/products-and-services/visagesdk/facetrack"," ['2 Method', '2.1 Dataset and Feature Extraction']","To extract face and head movement information from the video, a face-tracker (Visage, 2016) [Cite_Ref] was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea-tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow.",Technologies Visage. 2016. Face tracking. https://visagetechnologies.com/products-and-services/visagesdk/facetrack. Accessed: 2016-03- 10.,"To extract face and head movement information from the video, a face-tracker (Visage, 2016) [Cite_Ref] was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea-tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow. The first six values represent head location and orientation. The next six values represent vertical movement of the outer (“o ”), middle (“m ”), or inner (“i ”) portion of the right (“r ”) or left (“l ”) eyebrows. The final values rep-resent horizontal movement of the eyebrows.",Method,Tool,True,Use（引用目的）,True,P16-1196_2_0,2016,Continuous Profile Models in ASL Syntactic Facial Expression Synthesis,Reference
1894,11898," http://www.cs.toronto.edu/"," ['References']","A Appendix: Supplemental Material In Section 2.3, we made use of a freely available CPM implementation available from [Cite] http://www.cs.toronto.edu/ ∼ jenn/CPM/ in MAT-LAB, Version 8.5.0.197613 (R2015a).",,"A Appendix: Supplemental Material In Section 2.3, we made use of a freely available CPM implementation available from [Cite] http://www.cs.toronto.edu/ ∼ jenn/CPM/ in MAT-LAB, Version 8.5.0.197613 (R2015a).",Method,Tool,True,Use（引用目的）,True,P16-1196_3_0,2016,Continuous Profile Models in ASL Syntactic Facial Expression Synthesis,Body
1895,11899," https://github.com/levguy/talksumm"," ['1 Introduction']",We make our dataset and related code publicly available [Cite_Footnote_3] .,3 https://github.com/levguy/talksumm,"Our main contributions are as follows: (1) we propose a new approach to automatically gener-ate summaries for scientific papers based on video talks; (2) we create a new dataset, that contains 1716 summaries for papers from several computer science conferences, that can be used as training data; (3) we show both automatic and human eval-uations for our approach. We make our dataset and related code publicly available [Cite_Footnote_3] . To our knowl-edge, this is the first approach to automatically cre-ate extractive summaries for scientific papers by utilizing the videos of conference talks.",Mixed,Mixed,True,Produce（引用目的）,True,P19-1204_0_0,2019,T ALK S UMM : A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks,Footnote
1896,11900," http://wikipedia.org/"," ['1 Introduction']",We fo-cus on Wikipedia [Cite_Footnote_1] as the resource for acquiring NEs.,1 http://wikipedia.org/,"In recent years, NE extraction has been performed with machine learning based methods. However, such methods cannot cover all of NEs in texts. Therefore, it is necessary to extract NEs from ex-isting resources and use them to identify more NEs. There are many useful resources on the Web. We fo-cus on Wikipedia [Cite_Footnote_1] as the resource for acquiring NEs. Wikipedia is a free multilingual online encyclope-dia and a rapidly growing resource. In Wikipedia, a large number of NEs are described in titles of ar-ticles with useful information such as HTML tree structures and categories. Each article links to other related articles. According to these characteristics, they could be an appropriate resource for extracting NEs.",Material,DataSource,True,Use（引用目的）,True,D07-1068_0_0,2007,A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields,Footnote
1897,11901," http://mallet.cs.umass.edu"," ['3 Graph-based CRFs for NE Categorization in Wikipedia']",[Cite] http://mallet.cs.umass.edu.,,Charles Sutton. 2006. GRMM: A graphical models toolkit. [Cite] http://mallet.cs.umass.edu.,Method,Tool,True,Use（引用目的）,False,D07-1068_3_0,2007,A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields,Body
1898,11902," http://www.chasen.org/~taku/software/TinySVM/"," ['4 Experiments', '4.2 Experimental settings']","To perform training and testing with SVMs, we use TinySVM [Cite_Footnote_2] with a linear-kernel, and one-versus-rest is used for multi-class classification.",2 http://www.chasen.org/˜taku/software/TinySVM/,"S model C model R R model I model Figure 3: An example of graphs constructed by combination of defined cliques. S, C, R in the model names mean that corresponding model has Sibling, Cousin, Relative cliques respectively. In each model, classification is performed on each con-nected subgraph. SVMs We introduce two models by SVMs (model I and model P). In model I, each anchor text is clas-sified independently. In model P, we ordered the anchor texts in a linear-chain sequence. Then, we perform a history-based classification along the se-quence, in which j − 1-th classification result is used in j-th classification. We use TinySVM with a linear-kernel. One-versus-rest method is used for multi-class classification. To perform training and testing with SVMs, we use TinySVM [Cite_Footnote_2] with a linear-kernel, and one-versus-rest is used for multi-class classification. We used the cost of constraint vio-lation C = 1.",Method,Tool,True,Use（引用目的）,True,D07-1068_4_0,2007,A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields,Footnote
1899,11903," http://mecab.sourceforge.net/"," ['4 Experiments', '4.2 Experimental settings']",Japanese morphological analyzer MeCab [Cite_Footnote_3] is used to obtain morphemes.,3 http://mecab.sourceforge.net/,Features for CRFs and SVMs The features used in the classification with CRFs and SVMs are shown in Table 2. Japanese morphological analyzer MeCab [Cite_Footnote_3] is used to obtain morphemes.,Method,Tool,True,Use（引用目的）,True,D07-1068_5_0,2007,A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields,Footnote
1900,11904," http://mallet.cs.umass.edu"," ['3 Graph-based CRFs for NE Categorization in Wikipedia']","To perform training and testing with CRFs, we use GRMM (Sutton, 2006) [Cite_Ref] with TRP.",Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.,"CRFs In order to investigate which type of clique boosts classification performance, we perform ex-periments on several CRFs models that are con-structed from combinations of defined cliques. Re-sulting models of CRFs evaluated on this experi-ments are SCR, SC, SR, CR, S, C, R and I (indepen-dent). Figure 3 shows representative graphs of the eight models. When the graph are disconnected by reducing the edges, the classification is performed on each connected subgraph. We call it an example. We name the examples according the graph struc-ture: ”loopy examples” are subgraphs including at least one cycle; ”linear chain or tree examples” are subgraphs including not a cycle but at least an edge; ”one node examples” are subgraphs without edges. Table 1 shows the distribution of the examples of each model. Since SCR, SC, SR and CR model have loopy examples, TRP approximate inference is nec-essary. To perform training and testing with CRFs, we use GRMM (Sutton, 2006) [Cite_Ref] with TRP. We set the Gaussian Prior variances for weights as σ 2 = 10 in all models.",Method,Tool,True,Use（引用目的）,True,D07-1068_6_0,2007,A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields,Reference
1901,11905," http://www.chasen.org/~taku/software/TinySVM/"," ['4 Experiments', '4.2 Experimental settings']","To perform training and testing with SVMs, we use TinySVM [Cite_Footnote_2] with a linear-kernel, and one-versus-rest is used for multi-class classification.",2 http://www.chasen.org/˜taku/software/TinySVM/,"S model C model R R model I model Figure 3: An example of graphs constructed by combination of defined cliques. S, C, R in the model names mean that corresponding model has Sibling, Cousin, Relative cliques respectively. In each model, classification is performed on each con-nected subgraph. SVMs We introduce two models by SVMs (model I and model P). In model I, each anchor text is clas-sified independently. In model P, we ordered the anchor texts in a linear-chain sequence. Then, we perform a history-based classification along the se-quence, in which j − 1-th classification result is used in j-th classification. We use TinySVM with a linear-kernel. One-versus-rest method is used for multi-class classification. To perform training and testing with SVMs, we use TinySVM [Cite_Footnote_2] with a linear-kernel, and one-versus-rest is used for multi-class classification. We used the cost of constraint vio-lation C = 1.",Method,Tool,True,Use（引用目的）,True,D07-1068_7_0,2007,A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields,Footnote
1902,11906," http://mecab.sourceforge.net/"," ['4 Experiments', '4.2 Experimental settings']",Japanese morphological analyzer MeCab [Cite_Footnote_3] is used to obtain morphemes.,3 http://mecab.sourceforge.net/,Features for CRFs and SVMs The features used in the classification with CRFs and SVMs are shown in Table 2. Japanese morphological analyzer MeCab [Cite_Footnote_3] is used to obtain morphemes.,Method,Tool,True,Use（引用目的）,True,D07-1068_8_0,2007,A Graph-based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields,Footnote
1903,11907," http://cnts.uia.ac.be/conll2003/ner/"," ['6 Experiments', '6.1 Dataset and Evaluation']","We test the effectiveness of our technique on the CoNLL 2003 English named en-tity recognition dataset downloadable from [Cite] http://cnts.uia.ac.be/conll2003/ner/. The data comprises Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC).",,"We test the effectiveness of our technique on the CoNLL 2003 English named en-tity recognition dataset downloadable from [Cite] http://cnts.uia.ac.be/conll2003/ner/. The data comprises Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). The data is separated into a training set, a development set (testa), and a test set (testb). The training set contains 945 documents, and approximately 203,000 tokens and the test set has 231 documents and approximately 46,000 tokens. Performance on this task is evaluated by measuring the precision and recall of annotated entities (and not tokens), combined into an F1 score. There is no partial credit for labeling part of an entity sequence correctly; an incorrect entity boundary is penalized as both a false positive and as a false negative.",Material,Knowledge,True,Use（引用目的）,True,P06-1141_0_0,2006,An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition,Body
1904,11908," https://github.com/XinnuoXu/AugNLG"," ['References']",Our code and data are available at [Cite] https: //github.com/XinnuoXu/AugNLG .,,"Intent: request NLG which city are you interested in? component in a task-oriented dialogue system, which converts the structured meaning repre-sentation (MR) to the natural language. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based ap-proaches nor model-based approaches are scal-able. Recently, neural NLGs started lever-aging transfer learning and showed promis-ing results in few-shot settings. This paper proposes A UG NLG, a novel data augmenta-tion approach that combines a self-trained neu-ral retrieval model with a few-shot learned NLU model, to automatically create MR-to- Text data from open-domain texts. The pro-posed system mostly outperforms the state-of-the-art methods on the F EW S HOT WOZ data in both BLEU and Slot Error Rate. We fur-ther confirm improved results on the F EW - S HOT SGD data and provide comprehensive analysis results on key components of our sys-tem. Our code and data are available at [Cite] https: //github.com/XinnuoXu/AugNLG .",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.95_0_0,2021,A UG NLG: Few-shot Natural Language Generation using Self-trained Data Augmentation,Body
1905,11909," https://github.com/XinnuoXu/"," ['1 Introduction']","However, GPT-2 is pre-trained on Open- lease the data from all intermediate steps for WebText and the language style and topics thereof each domain at [Cite] https://github.com/XinnuoXu/ are quite different from those of daily conversa- AugNLG/tree/master/augmented_data .",,"Specifically, in Step (1), we adopt GPT-2 (Rad- retrieving keyword-matching utterances and fil-ford et al., 2019) as our base language model since tering out domain-irrelevant instances, (2) gen- GPT-2 has demonstrated a remarkable performance erating synthetic MR annotations. Figure 3 il-on auto-regressive text generation tasks, which is lustrates the overall pipeline with some exam-close to MR-to-Text generation, in a variety of ples. For further analysis and studies, we re-domains. However, GPT-2 is pre-trained on Open- lease the data from all intermediate steps for WebText and the language style and topics thereof each domain at [Cite] https://github.com/XinnuoXu/ are quite different from those of daily conversa- AugNLG/tree/master/augmented_data .",補足資料,Website,False,Introduce（引用目的）,False,2021.acl-long.95_1_0,2021,A UG NLG: Few-shot Natural Language Generation using Self-trained Data Augmentation,Body
1906,11910," https://github.com/lifengjin/dimi_emnlp18"," ['5 Model']","Specifically, τeach tree is a set {τ , τ , τ [Cite_Footnote_2] , τ 11 , τ 12 , τ 21 , ...} of category node labels τ η where η ∈ {1,2} ∗ defines a path of left or right branches from the root to that node.",2 https://github.com/lifengjin/dimi_emnlp18.,"Specifically, τeach tree is a set {τ , τ , τ [Cite_Footnote_2] , τ 11 , τ 12 , τ 21 , ...} of category node labels τ η where η ∈ {1,2} ∗ defines a path of left or right branches from the root to that node. Category labels for every pair of left and right children τ η1 ,τ η2 are drawn from a multinomial distribution defined by the grammar G and the category of the parent τ η : where δ x is a Kronecker delta function equal to 1 at value x and 0 elsewhere, and terminals have null expansions P G (a b | w) = P G (a b | ⊥) = ~a, b=⊥, ⊥ for w ∈ W. 4",Material,Knowledge,False,Produce（引用目的）,False,P19-1235_0_0,2019,Variance of average surprisal: a better predictor for quality of grammar from unsupervised PCFG induction,Footnote
1907,11911,http://osc.edu/ark:/19495/f5s1ph73,['Acknowledgments'],Computations for this project were partly run on the Ohio Super-computer Center (1987) [Cite_Ref] .,The Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. \\url{http://osc.edu/ark:/19495/f5s1ph73}.,"The authors would like to thank the anonymous re-viewers for their helpful comments. Computations for this project were partly run on the Ohio Super-computer Center (1987) [Cite_Ref] . This research was par-tially funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022. The content of the information does not necessarily re-flect the position or the policy of the Government, and no official endorsement should be inferred. This work was also supported by the National Sci-ence Foundation grant 1816891. All views ex-pressed are those of the authors and do not nec-essarily reflect the views of the National Science Foundation.",補足資料,Website,True,Introduce（引用目的）,True,P19-1235_1_0,2019,Variance of average surprisal: a better predictor for quality of grammar from unsupervised PCFG induction,Reference
1908,11912," https://pan.webis.de/clef16/pan16-web/author-profiling.html"," ['B Additional Experimental Details', 'B.1 Self-attentive Sentence Classification']",Author profiling (Age dataset [Cite_Footnote_1] ) is to predict the age range of the user by giving their tweets.,1 https://pan.webis.de/clef16/pan16-web/author-profiling.html,"Dataset Three tasks are conducted on three pub-lic sentence classification datasets. Author profiling (Age dataset [Cite_Footnote_1] ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset ) is to predict the number of stars the user assigned to by analysis their reviews. Tex-tual entailment (SNLI dataset 3 ) is to tell whether the semantics in the two sentences are entailment or contradiction or neutral. Following Lin et al. (2017), the train / validate / test split of Age is 68485 / 4000 / 4000, Yelp is 500K / 2000 / 2000, SNLI is 550K / 10K / 10K.",補足資料,Website,False,Introduce（引用目的）,False,2020.emnlp-main.17_0_0,2020,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,Footnote
1909,11913," https://www.yelp.com/dataset/download"," ['B Additional Experimental Details', 'B.1 Self-attentive Sentence Classification']",Sentiment analysis (Yelp dataset [Cite_Footnote_2] ) is to predict the number of stars the user assigned to by analysis their reviews.,2 https://www.yelp.com/dataset/download,"Dataset Three tasks are conducted on three pub-lic sentence classification datasets. Author profiling (Age dataset ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset [Cite_Footnote_2] ) is to predict the number of stars the user assigned to by analysis their reviews. Tex-tual entailment (SNLI dataset 3 ) is to tell whether the semantics in the two sentences are entailment or contradiction or neutral. Following Lin et al. (2017), the train / validate / test split of Age is 68485 / 4000 / 4000, Yelp is 500K / 2000 / 2000, SNLI is 550K / 10K / 10K.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.17_1_0,2020,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,Footnote
1910,11914," https://skylion007.github.io/OpenWebTextCorpus/"," ['B Additional Experimental Details', 'B.3 ELECTRA']","Dataset Following the official code of Clark et al. (2020), ELECTRA models are pretrained on the OpenWebTextCorpus [Cite_Footnote_9] dataset, an open source ef-fort to reproduce OpenAI’s WebText dataset.",9 https://skylion007.github.io/OpenWebTextCorpus/,"Dataset Following the official code of Clark et al. (2020), ELECTRA models are pretrained on the OpenWebTextCorpus [Cite_Footnote_9] dataset, an open source ef-fort to reproduce OpenAI’s WebText dataset. Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents. The pretrained model is then finetuned and evaluated on GLUE benchmark . GLUE contains a variety of tasks covering tex-tual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), senti-ment (SST), and linguistic acceptability (CoLA). Our evaluation metrics are Spearman correlation for STS, Matthews correlation for CoLA, and ac-curacy for the other GLUE tasks.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.17_7_0,2020,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,Footnote
1911,11915," https://gluebenchmark.com/"," ['B Additional Experimental Details', 'B.3 ELECTRA']",The pretrained model is then finetuned and evaluated on GLUE benchmark [Cite_Footnote_10] .,10 https://gluebenchmark.com/,"Dataset Following the official code of Clark et al. (2020), ELECTRA models are pretrained on the OpenWebTextCorpus dataset, an open source ef-fort to reproduce OpenAI’s WebText dataset. Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents. The pretrained model is then finetuned and evaluated on GLUE benchmark [Cite_Footnote_10] . GLUE contains a variety of tasks covering tex-tual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), senti-ment (SST), and linguistic acceptability (CoLA). Our evaluation metrics are Spearman correlation for STS, Matthews correlation for CoLA, and ac-curacy for the other GLUE tasks.",補足資料,Website,True,Use（引用目的）,True,2020.emnlp-main.17_8_0,2020,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,Footnote
1912,11916," https://github.com/google-research/electra"," ['B Additional Experimental Details', 'B.3 ELECTRA']",Experiment settings The ELECTRA-small model we implemented follow all official settings [Cite_Footnote_11] except that it is fully-trained on one GTX 1080Ti GPU for 6 days.,11 https://github.com/google-research/electra,"Experiment settings The ELECTRA-small model we implemented follow all official settings [Cite_Footnote_11] except that it is fully-trained on one GTX 1080Ti GPU for 6 days. The ELECTRA-small model has 12 layers with 4 heads in every layer’s attention. For our method, the stepsize is set to 0.01 and the repulsive term α is set to 0.1. The repulsive learning of attention is only applied to the pre-training stage. The fine-tuning remains the same with the original one. B.4 GraphWriter Dataset Experiments are conducted on the Ab-stract GENeration DAtaset (AGENDA) (Koncel-Kedziorski et al., 2019), a dataset of knowledge graphs paired with scientific abstracts. It consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences. We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings 12 in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer. We reproduce their results and keep all settings the same when applying the proposed repulsive attention. The SVGD update rule is used in our algorithm and applied to all lay-ers. The stepsize is set to 0.1 and the repulsive weight is set to 0.01 in this experiment. The model is trained on one TITAN Xp GPU.",補足資料,Website,True,Use（引用目的）,True,2020.emnlp-main.17_9_0,2020,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,Footnote
1913,11917," https://github.com/rikdz/GraphWriter"," ['B Additional Experimental Details', 'B.3 ELECTRA']",Experimental settings We follow the official settings [Cite_Footnote_12] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer.,12 https://github.com/rikdz/GraphWriter,"Experiment settings The ELECTRA-small model we implemented follow all official settings 11 except that it is fully-trained on one GTX 1080Ti GPU for 6 days. The ELECTRA-small model has 12 layers with 4 heads in every layer’s attention. For our method, the stepsize is set to 0.01 and the repulsive term α is set to 0.1. The repulsive learning of attention is only applied to the pre-training stage. The fine-tuning remains the same with the original one. B.4 GraphWriter Dataset Experiments are conducted on the Ab-stract GENeration DAtaset (AGENDA) (Koncel-Kedziorski et al., 2019), a dataset of knowledge graphs paired with scientific abstracts. It consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences. We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings [Cite_Footnote_12] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer. We reproduce their results and keep all settings the same when applying the proposed repulsive attention. The SVGD update rule is used in our algorithm and applied to all lay-ers. The stepsize is set to 0.1 and the repulsive weight is set to 0.01 in this experiment. The model is trained on one TITAN Xp GPU.",Material,Knowledge,False,Use（引用目的）,True,2020.emnlp-main.17_10_0,2020,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,Footnote
1914,11918," http://www.isi.edu/~cyl/ROUGE/"," ['3 Evaluation', '-', 'the DUC 2002 document summarization evaluations.']","The table also lists baseline results, obtained on summaries generated by 1 R OUGE is available at [Cite] http://www.isi.edu/˜cyl/ROUGE/. taking the first sentences in each document.",,"Table 2 shows the results obtained on these two data sets for different graph settings. The table also lists baseline results, obtained on summaries generated by 1 R OUGE is available at [Cite] http://www.isi.edu/˜cyl/ROUGE/. taking the first sentences in each document. By ways of comparison, the best participating system in DUC 2002 was a supervised system that led to a ROUGE score of 0.5011.",補足資料,Document,True,Produce（引用目的）,True,P05-3013_0_0,2005,Language Independent Extractive Summarization,Body
1915,11919," http://www-nlpir.nist.gov/projects/duc/"," ['3 Evaluation']","In particular, we use the data set of 567 news articles made available during the DUC 2002 evaluations (DUC, 2002) [Cite_Ref] , and the correspond-ing 100-word summaries generated for each of these documents.",DUC. 2002. Document understanding conference 2002. http://www-nlpir.nist.gov/projects/duc/.,"English document summarization experiments are run using the summarization test collection provided in the framework of the Document Understanding Con-ference (DUC). In particular, we use the data set of 567 news articles made available during the DUC 2002 evaluations (DUC, 2002) [Cite_Ref] , and the correspond-ing 100-word summaries generated for each of these documents. This is the single document summariza-tion task undertaken by other systems participating in",補足資料,Website,True,Introduce（引用目的）,True,P05-3013_1_0,2005,Language Independent Extractive Summarization,Reference
1916,11920," http://taku910.github.io/cabocha/"," ['1 Introduction']","Most publicly available Japanese parsers, including CaboCha [Cite_Footnote_1] (Kudo et al., 2002) and KNP (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure.",1 http://taku910.github.io/cabocha/.,"Syntactic structures are usually represented as dependencies between chunks called bunsetsus. A bunsetsu is a Japanese grammatical and phono-logical unit that consists of one or more con-tent words such as a noun, verb, or adverb fol-lowed by a sequence of zero or more function words such as auxiliary verbs, postpositional par-ticles, or sentence-final particles. Most publicly available Japanese parsers, including CaboCha [Cite_Footnote_1] (Kudo et al., 2002) and KNP (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure. Such parsers are generally highly accurate and have been widely used in various NLP applications.",Method,Tool,True,Introduce（引用目的）,True,P15-2039_0_0,2015,Word-based Japanese typed dependency parsing with grammatical function analysis,Footnote
1917,11921," http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP"," ['1 Introduction']","Most publicly available Japanese parsers, including CaboCha (Kudo et al., 2002) and KNP [Cite_Footnote_2] (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure.",2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.,"Syntactic structures are usually represented as dependencies between chunks called bunsetsus. A bunsetsu is a Japanese grammatical and phono-logical unit that consists of one or more con-tent words such as a noun, verb, or adverb fol-lowed by a sequence of zero or more function words such as auxiliary verbs, postpositional par-ticles, or sentence-final particles. Most publicly available Japanese parsers, including CaboCha (Kudo et al., 2002) and KNP [Cite_Footnote_2] (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure. Such parsers are generally highly accurate and have been widely used in various NLP applications.",Method,Tool,True,Introduce（引用目的）,True,P15-2039_1_0,2015,Word-based Japanese typed dependency parsing with grammatical function analysis,Footnote
1918,11922," http://www.cl.cs.titech.ac.jp/∼ryu-i/syncha/"," ['1 Introduction']","Therefore, predicate-argument structure analysis is usually implemented as a post-processor of bunsetsu-based syntactic parser, not just for as-signing grammatical functions, but for identifying constituents, such as an analyzer SynCha [Cite_Footnote_3] (Iida et al., 2011), which uses the parsing results from CaboCha.",3 http://www.cl.cs.titech.ac.jp/∼ryu-i/syncha/.,"Therefore, predicate-argument structure analysis is usually implemented as a post-processor of bunsetsu-based syntactic parser, not just for as-signing grammatical functions, but for identifying constituents, such as an analyzer SynCha [Cite_Footnote_3] (Iida et al., 2011), which uses the parsing results from CaboCha. We assume that using a word as a pars-ing unit instead of a bunsetsu chunk helps to main-tain consistency between syntactic structure anal-ysis and predicate-argument structure analysis.",Method,Tool,True,Introduce（引用目的）,True,P15-2039_2_0,2015,Word-based Japanese typed dependency parsing with grammatical function analysis,Footnote
1919,11923," http://universaldependencies.github.io/docs/"," ['2 Related work']","The Universal dependencies (UD) (McDonald et al., 2013; de Marneffe et al., 2014) has been developed based on SD in order to design the cross-linguistically consistent treebank annotation [Cite_Footnote_4] .",4 http://universaldependencies.github.io/docs/.,"We proposed a typed dependency scheme based on the well-known and widely used Stanford typed dependencies (SD), which originated in English and has since been extended to many languages, but not to Japanese. The Universal dependencies (UD) (McDonald et al., 2013; de Marneffe et al., 2014) has been developed based on SD in order to design the cross-linguistically consistent treebank annotation [Cite_Footnote_4] . The UD for Japanese has also been discussed, but no treebanks have been provided yet. We focus on the feasibility of word-based Japanese typed dependency parsing rather than on cross-linguistic consistency. We plan to examine the conversion between UD and our scheme in the future.",補足資料,Website,True,Introduce（引用目的）,True,P15-2039_3_0,2015,Word-based Japanese typed dependency parsing with grammatical function analysis,Footnote
1920,11924," https://github.com/nmrksic/attract-repel"," ['3 Experimental Setup']","The constraints are initially injected into the dis-tributional vector space (see Figure 1 again) us-ing ATTRACT - REPEL , a state-of-the-art specializa-tion model, for which we adopt the original sug-gested model setup (Mrkšić et al., 2017). [Cite_Footnote_6]",6 https://github.com/nmrksic/ attract-repel,"The constraints are initially injected into the dis-tributional vector space (see Figure 1 again) us-ing ATTRACT - REPEL , a state-of-the-art specializa-tion model, for which we adopt the original sug-gested model setup (Mrkšić et al., 2017). [Cite_Footnote_6] Hyper-parameter values are set to: δ A = 0.6, δ R = 0.0, λ P = 10 −9 . The models are trained for 5 epochs with Adagrad (Duchi et al., 2011), with batch sizes set to k A = k R = 50, again as in the original work. AuxGAN Setup and Hyper-Parameters. Both the generator and the discriminator are feed-forward nets with l = 2 hidden layers, each of size h = 2048, and LeakyReLU as non-linear ac-tivation (Maas et al., 2013). The dropout for the input and hidden layers of the generator is 0.2 and for the input layer of the discriminator 0.1. In eval-uation, the noise is blanketed out in order to ensure a deterministic mapping (Isola et al., 2017). More-over, we smooth the golden labels for prediction by a factor of 0.1 to make the model less vulnerable to adversarial examples (Szegedy et al., 2016).",Mixed,Mixed,True,Use（引用目的）,True,D18-1026_0_0,2018,Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization,Footnote
1921,11925," https://github.com/nmrksic/"," ['4 Results and Discussion', '4.2 Downstream Tasks', '4.2.2 Dialog State Tracking']","To evaluate the effects of spe-cialized word vectors on DST, following prior work we utilize the Neural Belief Tracker (NBT), a sta-tistical DST model that makes inferences purely based on pre-trained word vectors (Mrkšić et al., 2017). [Cite_Footnote_9]","9 https://github.com/nmrksic/ neural-belief-tracker; For full model details, we refer the reader to the original paper.","Evaluation Setup. To evaluate the effects of spe-cialized word vectors on DST, following prior work we utilize the Neural Belief Tracker (NBT), a sta-tistical DST model that makes inferences purely based on pre-trained word vectors (Mrkšić et al., 2017). [Cite_Footnote_9] Again, as in prior work the DST evalu-ation is based on the Wizard-of-Oz (WOZ) v2.0 dataset (Wen et al., 2017; Mrkšić et al., 2017), com-prising 1,200 dialogues split into training (600 di-alogues), development (200), and test data (400). We report the standard DST metric: joint goal ac-curacy (JGA), the proportion of dialog turns where all the user’s search goal constraints were correctly identified, computed as average over 5 NBT runs. Results and Analysis. We show English DST performance in the FULL setting in Table 3. Only NBT performance with GLOVE - CC vectors is re-ported for brevity, as similar performance gains are observed with the other two pre-trained vec-tor collections. The results confirm our findings established in the other two tasks: a) initial AR spe-cialization of distributional vectors is useful, but b) it is crucial to specialize the full vocabulary for im-proved performance (e.g., 57% of all WOZ words are present in the constraints), and c) the more so-phisticated AUXGAN model yields additional gains.",補足資料,Document,False,Introduce（引用目的）,False,D18-1026_1_0,2018,Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization,Footnote
1922,11926," https://github.com/cambridgeltl/adversarial-postspec"," ['6 Conclusion and Future Work']",The code is available at [Cite] https://github.com/cambridgeltl/adversarial-postspec.,,"In future work, we will explore more sophis-ticated adversarial models such as Cycle-GAN (Zhu et al., 2017). Moreover, we will experiment with bootstrapping approaches to extract new lexi-cal constraints from post-specialized embeddings. We also plan to extend the method to asymmet-ric relations (e.g., hypernymy) and to more target (resource-lean) languages. The code is available at [Cite] https://github.com/cambridgeltl/adversarial-postspec.",Method,Code,True,Produce（引用目的）,True,D18-1026_2_0,2018,Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization,Body
1923,11927," https://www.statmt.org/wmt14/translation-task.html"," ['4 Experimentation', '4.1 Experimental Settings']",4 We use WMT14 EN-DE translation dataset as the EN-DE sentence-level parallel dataset which consists of 4.4M sentence pairs. [Cite_Footnote_5],5 https://www.statmt.org/wmt14/translation-task.html,Pre-training data settings. The ZH-EN sentence-level parallel dataset contains 2.0M sentence pairs with 54.8M Chinese words and 60.8M English words. 4 We use WMT14 EN-DE translation dataset as the EN-DE sentence-level parallel dataset which consists of 4.4M sentence pairs. [Cite_Footnote_5],Material,Dataset,True,Use（引用目的）,True,2021.acl-long.222_0_0,2021,Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training,Footnote
1924,11928," https://github.com/messense/jieba-rs"," ['4 Experimentation', '4.1 Experimental Settings']","All Chinese sentences are segmented by Jieba [Cite_Footnote_6] while all English and German sentences are tok-enized by Moses scripts (Koehn et al., 2007).",6 https://github.com/messense/jieba-rs,"All Chinese sentences are segmented by Jieba [Cite_Footnote_6] while all English and German sentences are tok-enized by Moses scripts (Koehn et al., 2007). For ZH-EN (EN-DE) translation, we merge the source and target sentences of the parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) opera-tions (Sennrich et al., 2016).",Method,Tool,True,Use（引用目的）,True,2021.acl-long.222_1_0,2021,Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training,Footnote
1925,11929," http://www.casmacat.eu/corpus/news-commentary.html"," ['4 Experimentation', '4.1 Experimental Settings']","• News, which is from News Commentary v11 corpus. [Cite_Footnote_9]",9 http://www.casmacat.eu/corpus/news-commentary.html,"• News, which is from News Commentary v11 corpus. [Cite_Footnote_9] We use news-test2015 and news-",Material,DataSource,True,Use（引用目的）,True,2021.acl-long.222_2_0,2021,Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training,Footnote
1926,11930," https://github.com/sameenmaruf/selective-attn/tree/master/data"," ['4 Experimentation', '4.1 Experimental Settings']",All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019). [Cite_Footnote_10],10 https://github.com/sameenmaruf/selective-attn/tree/master/data,"All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019). [Cite_Footnote_10] Simi-lar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pre-training datasets. See Appendix A for more statis-tics of the fine-tuning datasets.",Material,DataSource,True,Use（引用目的）,True,2021.acl-long.222_3_0,2021,Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training,Footnote
1927,11931," https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT"," ['4 Experimentation', '4.1 Experimental Settings']","We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [Cite_Footnote_11]",11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT,"Model settings. We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [Cite_Footnote_11] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively. The number of heads in multi-head attention is 8 and the dropout rate is 0.1. In pre-training, we train the models for 500K steps on four V100 GPUs with batch-size 8192. We use Adam (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 for optimiza-tion, and learning rate as 1, the warm-up step as 16K. In fine-tuning, we fine-tune the models for 200K steps on a single V100 GPU with batch-size 8192, learning rate 0.3, and warm-up step 4K. In inferring, we set the beam size to 5. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality.",Method,Code,True,Produce（引用目的）,True,2021.acl-long.222_4_0,2021,Breaking Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training,Footnote
1928,11932," https://github.com/williamFalcon/"," ['References']","For the sake of easy reproducibility, we use Pytorch Lightning (Falcon, 2019) [Cite_Ref] framework.",W.A. Falcon. 2019. Pytorch lightning. https://github.com/williamFalcon/ pytorch-lightning .,"Since we essentially cherry-picked good results, its only fair to show a similarly cherry-picked negative example of M A U DE . We sampled from responses where M A U DE scores are negatively correlated with human annotations on Inquisitive-ness metric (5% of cases), and we show one of those responses in Figure 6. We notice how both DistilBERT-NLI and M A U DE fails to recognize the duplication of utterances which leads to a low overall score. This suggests there still exists room for improvement in developing M A U DE , possibly by training the model to detect degeneracy in the context. E Hyperparameters and Training Details We performed rigorous hyperparameter search to tune our model M A U DE . We train M A U DE with downsampling, as we observe poor results when we run the recurrent network on top of 768 dimensions. Specifically, we downsample to 300 dimensions, which is the same used by our baselines RUBER and InferSent in their respective encoder represen-tations. We also tested with the choice of either learning a PCA to downsample the BERT represen-tations vs learning the mapping D g (Equation 4), and found the latter producing better results. We keep the final decoder same for all models, which is a two layer MLP with hidden layer of size 200 dimensions and dropout 0.2. For BERT-based mod-els (DistilBERT-NLI and M A U DE ), we use Hug-gingFace Transformers (Wolf et al., 2019) to first fine-tune the training dataset on language model objective. We tested with training on frozen fine-tuned representations in our initial experiments, but fine-tuning end-to-end lead to better ablation scores. For all models we train using Adam optimizer with 0.0001 as the learning rate, early stopping till vali-dation loss doesn’t improve. For the sake of easy reproducibility, we use Pytorch Lightning (Falcon, 2019) [Cite_Ref] framework. We used 8 Nvidia-TitanX GPUs on a DGX Server Workstation to train faster using Pytorch Distributed Data Parallel (DDP).",Method,Tool,True,Use（引用目的）,True,2020.acl-main.220_1_0,2020,Learning an Unreferenced Metric for Online Dialogue Evaluation,Reference
1929,11933," https://github.com/gucci-j/light-transformer-emnlp2021"," ['E Performance in SQ U AD']","We further validate our methods using smaller models, showing that pretrain-ing a model with 41% of the B ERT - BASE ’s pa-rameters, B ERT - MEDIUM results in only a 1% drop in G LUE scores with our best objective. [Cite_Footnote_1]",1 Our code is publicly available here: https://github.com/gucci-j/light-transformer-emnlp2021,"Masked language modeling ( MLM ), a self-supervised pretraining objective, is widely used in natural language processing for learn-ing text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocab-ulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve down-stream performance (e.g. next sentence predic-tion). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretrain-ing objectives based on token-level classifica-tion tasks as replacements of MLM . Empirical results on G LUE and SQ U AD show that our proposed methods achieve comparable or bet-ter performance to MLM using a B ERT - BASE architecture. We further validate our methods using smaller models, showing that pretrain-ing a model with 41% of the B ERT - BASE ’s pa-rameters, B ERT - MEDIUM results in only a 1% drop in G LUE scores with our best objective. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.249_0_0,2021,Frustratingly Simple Pretraining Alternatives to Masked Language Modeling,Footnote
1930,11934," https://www.nltk.org/"," ['2 Pretraining Tasks']","Our fourth objective is a four-way classi-fication, aiming to predict whether a token is a stop word, [Cite_Footnote_2] a digit, a punctuation mark, or a content word.",2 We use the Natural Language Toolkit’s stop word list: https://www.nltk.org/.,"Masked Token Type Classification (T OKEN T YPE ): Our fourth objective is a four-way classi-fication, aiming to predict whether a token is a stop word, [Cite_Footnote_2] a digit, a punctuation mark, or a content word. Therefore, the task can be seen as a simpli-fied version of POS tagging. We regard any tokens that are not included in the first three categories as content words. We mask 15% of tokens in each sample with a special [MASK] token and compute the cross-entropy loss over the masked ones only not to make the task trivial. For example, if we com-pute the token-level loss over unmasked tokens, a model can easily recognize the four categories as we only have a small number of non-content words in the vocabulary.",Material,Knowledge,True,Use（引用目的）,True,2021.emnlp-main.249_1_0,2021,Frustratingly Simple Pretraining Alternatives to Masked Language Modeling,Footnote
1931,11935," https://github.com/huggingface/datasets"," ['3 Experimental Setup']","Pretraining Data: We pretrain all models on the English Wikipedia and BookCorpus (Zhu et al., 2015) (WikiBooks) using the datasets library. [Cite_Footnote_4]",4 https://github.com/huggingface/ datasets,"Models: We use B ERT (Devlin et al., 2019) ( BASE ) as our basis model by replacing the MLM and NSP objectives with one of our five token-level pretraining tasks in all our experiments. We also consider two smaller models from Turc et al. (2019), MEDIUM and SMALL , where we reduce the size of the following components compared to the BASE model: (1) hidden layers; (2) hidden size; (3) feed-forward layer size; and (4) attention heads. More specifically, MEDIUM has eight hidden layers and attention heads, while SMALL has four hid-den layers and eight attention heads. The size of feed-forward and hidden layers for both models are 2048 and 512, respectively. Pretraining Data: We pretrain all models on the English Wikipedia and BookCorpus (Zhu et al., 2015) (WikiBooks) using the datasets library. [Cite_Footnote_4] Implementation Details: We pretrain and fine-tune our models with two NVIDIA Tesla V100 (SXM2 - 32GB) with a batch size of 32 for BASE and 64 for MEDIUM and SMALL . We pretrain all our models for up to five days each due to limited access to computational resources and funds for running experiments. We save a checkpoint of each model every 24 hours. Evaluation: We evaluate our approaches on G LUE (Wang et al., 2019) and SQ U AD (Rajpurkar et al., 2016) benchmarks. To measure performance in downstream tasks, we fine-tune all models for five times each with a different random seed.",Material,Dataset,True,Use（引用目的）,True,2021.emnlp-main.249_2_0,2021,Frustratingly Simple Pretraining Alternatives to Masked Language Modeling,Footnote
1932,11936," https://www.nltk.org/"," ['E Performance in SQ U AD']","Our fourth task is a four-way classifi-cation task that identifies whether a token is a stop word [Cite_Footnote_7] , a digit, a punctuation mark, or a content word.",7 A stop word category is based on the Natural Language Toolkit’s stop word list: https://www.nltk.org/.,"Masked Token Type Classification (T OKEN T YPE ): Our fourth task is a four-way classifi-cation task that identifies whether a token is a stop word [Cite_Footnote_7] , a digit, a punctuation mark, or a content word. We regard any tokens that are not included in the first three categories as content words. We mask 15% of tokens in each sample with a special [MASK] token and compute the cross-entropy loss over the masked ones only not to make the task trivial: if we compute the token-level loss, includ-ing unmasked tokens, a model can easily recognize the four categories of tokens as we have a small number of tokens for non-content words. In this task, a model should be able to identify the distinc-tion between different types of tokens; therefore, the task can be seen as a simplified version of POS tagging.",Material,Knowledge,True,Use（引用目的）,True,2021.emnlp-main.249_3_0,2021,Frustratingly Simple Pretraining Alternatives to Masked Language Modeling,Footnote
1933,11937," https://github.com/huggingface/datasets"," ['C Experimental Setup', 'C.1 Model Architecture']","C.2 Data Following Devlin et al. (2019), we use the English Wikipedia and BookCorpus (Zhu et al., 2015) data (WikiBooks) downloaded from the datasets library [Cite_Footnote_8] .",8 https://github.com/huggingface/ datasets,"C.2 Data Following Devlin et al. (2019), we use the English Wikipedia and BookCorpus (Zhu et al., 2015) data (WikiBooks) downloaded from the datasets library [Cite_Footnote_8] . We remove headers for the English Wikipedia and extract training samples with a max-imum length of 512. For the BookCorpus, we concatenate sentences such that the total number of tokens is less than 512. For the English Wikipedia, we extract one sample from articles whose length is less than 512. We tokenize text using byte-level Byte-Pair-Encoding (Sennrich et al., 2016). The resulting corpus consists of 8.1 million samples and 2.7 billion tokens in total.",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.249_4_0,2021,Frustratingly Simple Pretraining Alternatives to Masked Language Modeling,Footnote
1934,11938," https://github.com/gucci-j/light-transformer-emnlp2021"," ['C Experimental Setup', 'C.3 Implementation Details']",Our code is publicly available on GitHub: [Cite] https://github.com/gucci-j/light-transformer-emnlp2021.,,"We implement our models using PyTorch (Paszke et al., 2019) and the transformers library (Wolf et al., 2020). We pretrain our models with two NVIDIA Tesla V100 (SXM2 - 32GB) and use one for fine-tuning. Our code is publicly available on GitHub: [Cite] https://github.com/gucci-j/light-transformer-emnlp2021.",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.249_5_0,2021,Frustratingly Simple Pretraining Alternatives to Masked Language Modeling,Body
1935,11939," https://github.com/craigastewart/qe"," ['References']","In experiments over five settings in three language pairs, we extend a QE pipeline to estimate in-terpreter performance (as approximated by the METEOR evaluation metric) and pro-pose novel features reflecting interpreta-tion strategy and evaluation measures that further improve prediction accuracy. [Cite_Footnote_1]",1 https://github.com/craigastewart/qe sim interp Proceedings of the 56th Annual Meeting of the Association for,"Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically de-manding. Methods to predict interpreter confidence and the adequacy of the in-terpreted message have a number of po-tential applications, such as in computer-assisted interpretation interfaces or ped-agogical tools. We propose the task of predicting simultaneous interpreter perfor-mance by building on existing methodol-ogy for quality estimation (QE) of ma-chine translation output. In experiments over five settings in three language pairs, we extend a QE pipeline to estimate in-terpreter performance (as approximated by the METEOR evaluation metric) and pro-pose novel features reflecting interpreta-tion strategy and evaluation measures that further improve prediction accuracy. [Cite_Footnote_1]",補足資料,Document,True,Introduce（引用目的）,False,P18-2105_0_0,2018,Automatic Estimation of Simultaneous Interpreter Performance,Footnote
1936,11940," https://github.com/artetxem/vecmap"," ['5 Experiments']","We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictio-nary consisting only of numeral identity transla-tions (such as 2-2, 3-3, et cetera) as in Artetxe et al. (2017). [Cite_Footnote_1]",1 https://github.com/artetxem/vecmap,"We use the datasets used by Artetxe et al. (2017), consisting of three language pairs: English-Italian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014); the other datasets were created by Artetxe et al. (2017). Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b)) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictio-nary consisting only of numeral identity transla-tions (such as 2-2, 3-3, et cetera) as in Artetxe et al. (2017). [Cite_Footnote_1] However, because the methods pre-sented in this work feature tunable hyperparame-ters, we use a portion of the training set as devel-opment data. 2 In all experiments, a single target word is predicted for each source word, and full points are awarded if it is one of the listed correct translations. On average, the number of transla-tions for each source (non-English) word was 1.2 for English-Italian, 1.3 for English-German, and 1.4 for English-Finnish.",Material,Knowledge,True,Produce（引用目的）,False,P18-2062_0_0,2018,Orthographic Features for Bilingual Lexicon Induction,Footnote
1937,11941," http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/"," ['4 Orthographic Similarity Adjustment']","This subset of word pairs was chosen using an adaptation of the Symmetric Delete spelling correction algorithm described by Garbe (2012) [Cite_Ref] , which we denote as symDelete(·,·,·).",Wolf Garbe. 2012. 1000x faster spelling correc-tion algorithm. http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/. Accessed: 2018-02-12.,"This subset of word pairs was chosen using an adaptation of the Symmetric Delete spelling correction algorithm described by Garbe (2012) [Cite_Ref] , which we denote as symDelete(·,·,·). This al-gorithm takes as arguments the target vocabulary, source vocabulary, and a constant k, and identifies all source-target word pairs that are identical af-ter k or fewer deletions from each word; that is, all pairs where each is reachable from the other with no more than k insertions and k deletions. For example, the Italian-English pair moderno-modern will be identified with k = 1, and the pair tollerante-tolerant will be identified with k = 2.",補足資料,Paper,True,Introduce（引用目的）,True,P18-2062_1_0,2018,Orthographic Features for Bilingual Lexicon Induction,Reference
1938,11942," http://L2R.cs.uiuc.edu/~cogcomp/software.php"," ['5 Experiments', '5.2 Textual Entailment']","For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger [Cite_Footnote_3] ) to generate features.",3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php,"The second column of Table 2 lists the resources used to generate features corresponding to each hid-den variable type. For word-mapping variables, the features include a WordNet based metric (WNSim), indicators for the POS tags and negation identifiers. We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger [Cite_Footnote_3] ) to generate features. For edge the entailment and paraphrase identification tasks. See Section 4 for an explanation of the hidden variable types. The linguistic resources used to generate features are abbreviated as follows – POS: Part of speech, Coref: Canonical coreferent entities; NE: Named Entity, ED: Edit distance, Neg: Negation markers, DEP: mapping variables, we include the features of the corresponding word mapping variables, scaled by the word similarity of the words forming the edge.",Method,Tool,True,Use（引用目的）,True,N10-1066_0_0,2010,Discriminative Learning over Constrained Latent Representations,Footnote
1939,11943," http://groups.google.com/group/hoo-nlp/"," ['3 Experiments and Results']","The HOO test data [Cite_Footnote_2] consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details).",2 Available at http://groups.google.com/group/hoo-nlp/ after registration.,"We experimentally test our M 2 method in the con-text of the HOO shared task. The HOO test data [Cite_Footnote_2] consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff to extract system edits. We obtain each system’s output and segment it at the sentence level accord-ing to the gold standard sentence segmentation. The source sentences, system hypotheses, and correc-tions are tokenized using the Penn Treebank stan-dard (Marcus et al., 1993). The character edit offsets are automatically converted to token offsets. We set the parameter u to 2, allowing up to two unchanged words per edit. The results are shown in Table 1. Note that the M 2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. We can see that the M 2 scorer results in higher scores than the offi-cial scorer for all systems, showing that the official scorer missed some valid edits. For example, the M 2 scorer finds 155 valid edits for the UI system compared to 141 found by the official scorer, and 83 valid edits for the NU system, compared to 78 by the official scorer. We manually inspect the output of the scorers and find that the M 2 scorer indeed ex-tracts the correct edits matching the gold standard where possible. Examples are shown in Table 2.",Material,Dataset,True,Use（引用目的）,True,N12-1067_0_0,2012,Better Evaluation for Grammatical Error Correction,Footnote
1940,11944," http://www.gnu.org/s/wdiff/"," ['3 Experiments and Results']","We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff [Cite_Footnote_4] to extract system edits.",4 http://www.gnu.org/s/wdiff/,"We experimentally test our M 2 method in the con-text of the HOO shared task. The HOO test data consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff [Cite_Footnote_4] to extract system edits. We obtain each system’s output and segment it at the sentence level accord-ing to the gold standard sentence segmentation. The source sentences, system hypotheses, and correc-tions are tokenized using the Penn Treebank stan-dard (Marcus et al., 1993). The character edit offsets are automatically converted to token offsets. We set the parameter u to 2, allowing up to two unchanged words per edit. The results are shown in Table 1. Note that the M 2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. We can see that the M 2 scorer results in higher scores than the offi-cial scorer for all systems, showing that the official scorer missed some valid edits. For example, the M 2 scorer finds 155 valid edits for the UI system compared to 141 found by the official scorer, and 83 valid edits for the NU system, compared to 78 by the official scorer. We manually inspect the output of the scorers and find that the M 2 scorer indeed ex-tracts the correct edits matching the gold standard where possible. Examples are shown in Table 2.",Method,Tool,True,Use（引用目的）,True,N12-1067_1_0,2012,Better Evaluation for Grammatical Error Correction,Footnote
1941,11945," http://nlp.comp.nus.edu.sg/software/"," ['5 Conclusion']",The M 2 scorer is available for download at [Cite] http://nlp.comp.nus.edu.sg/software/.,,"We have presented a novel method, called Max-Match (M 2 ), for evaluating grammatical error cor-rection. Our method computes the sequence of phrase-level edits that achieves the highest over-lap with the gold-standard annotation. Experi-ments on the HOO data show that our method overcomes deficiencies in the current evaluation method. The M 2 scorer is available for download at [Cite] http://nlp.comp.nus.edu.sg/software/.",Method,Tool,True,Use（引用目的）,True,N12-1067_2_0,2012,Better Evaluation for Grammatical Error Correction,Body
1942,11946," https://stackexchange.com/sites"," ['6 Experimental Setup']","MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset where [Cite_Footnote_2] to 10 news articles share a single summary.",2 https://stackexchange.com/sites,"MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset where [Cite_Footnote_2] to 10 news articles share a single summary. We consider every pair of articles that share a summary to be a similar document pair. Table 2 shows summary statistics of the two document ranking datasets.",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.496_1_0,2020,Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport,Footnote
1943,11947," https://archive.org/details/stackexchange"," ['6 Experimental Setup']",We took the June 2019 data dumps [Cite_Footnote_3] of the AskUbuntu and Su-perUser subdomains of the platform and combined them to form our dataset.,3 https://archive.org/details/ stackexchange,"StackExchange 2 is an online question answer-ing platform and has been used as a benchmark in previous work (dos Santos et al., 2015; Shah et al., 2018; Perkins and Yang, 2019). We took the June 2019 data dumps [Cite_Footnote_3] of the AskUbuntu and Su-perUser subdomains of the platform and combined them to form our dataset.",Material,Dataset,True,Extend（引用目的）,True,2020.acl-main.496_2_0,2020,Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport,Footnote
1944,11948," https://www.nltk.org/"," ['C Implementation Details']","Python package [Cite_Footnote_6] (Bird et al., 2009).",6 https://www.nltk.org/,"Text Span Extraction. Sentences are ex-tracted from the documents using the sentence tokenizer from the nltk Python package [Cite_Footnote_6] (Bird et al., 2009).",Method,Tool,True,Use（引用目的）,True,2020.acl-main.496_3_0,2020,Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport,Footnote
1945,11949," https://Whoosh.bitbucket.org/mchaput/whoosh"," ['3 System', '3.2 Key features']","Otherwise, the dataset can be saved into an index schema of documents which is com-patible with the Whoosh (Chaput, 2007) [Cite_Ref] library.",Matt Chaput. 2007. https://Whoosh.bitbucket.org/mchaput/whoosh.,"A pre-trained model is loaded from a Keras check-point: the weights are obtained from a .h5 file and the architecture from a .json. In case of an error, a message is shown to the user with the description. The dataset with the texts of question-answer pairs is loaded in either of two ways (denoted as D-I and D-II). It can be preloaded from a custom pickle file containing a pandas data frame. This method is suitable if the candidate answers are known at the test phase. Otherwise, the dataset can be saved into an index schema of documents which is com-patible with the Whoosh (Chaput, 2007) [Cite_Ref] library. This will allow the system to retrieve candidate answers based on a keyword match. It should be noted that it is relatively easy to modify the code to load other datasets as long as they have the fields pool (array of incorrect answer ids), answer ids (array of correct answer ids), question (question text) and answer (candidate answer text). Lastly, pre-trained word embeddings and the tokeniser are loaded from pickled Keras objects.",補足資料,Website,True,Compare（引用目的）,True,D18-2006_0_0,2018,An Interactive Web-Interface for Visualizing the Inner Workings of the Question Answering LSTM,Reference
1946,11950," https://github.com/fchollet/keras"," ['3 System', '3.3 Technical implementation']","The main deep learning framework is Keras (Chollet, 2015) [Cite_Ref] , but there is also a preliminary attempt to include PyTorch (Paszke et al., 2017) models.",Franois Chollet. 2015. keras. https://github.com/fchollet/keras.,"The visualisation application is a client-server sys-tem with a web interface. It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002). Heatmaps and t-SNE results are plotted with the matplotlib (Hunter, 2007) library and sklearn (Pedregosa et al., 2011). The main deep learning framework is Keras (Chollet, 2015) [Cite_Ref] , but there is also a preliminary attempt to include PyTorch (Paszke et al., 2017) models. Statisti-cal measures were calculated using scipy (Jones et al., 2001–).",Method,Tool,False,Use（引用目的）,False,D18-2006_1_0,2018,An Interactive Web-Interface for Visualizing the Inner Workings of the Question Answering LSTM,Reference
1947,11951," https://github.com/pallets/flask"," ['3 System', '3.3 Technical implementation']","The ap-plication is built with the Flask (Ronacher, 2018) [Cite_Ref] framework.",Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.,"The visualisation application is a client-server sys-tem with a web interface. It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) [Cite_Ref] framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002). Heatmaps and t-SNE results are plotted with the matplotlib (Hunter, 2007) library and sklearn (Pedregosa et al., 2011). The main deep learning framework is Keras (Chollet, 2015), but there is also a preliminary attempt to include PyTorch (Paszke et al., 2017) models. Statisti-cal measures were calculated using scipy (Jones et al., 2001–).",Method,Tool,True,Use（引用目的）,True,D18-2006_2_0,2018,An Interactive Web-Interface for Visualizing the Inner Workings of the Question Answering LSTM,Reference
1948,11952," http://www.cs.utexas.edu/users/ml/nldata/geoquery"," ['4 Experiment']","To test the coverage and precision of Locutus, I have customized it to answer questions from the G EOQUERY 250 corpus (Mooney, 1996) [Cite_Ref] , which consists of a database of geographical information paired with 250 English sentences requesting in-formation from that database.","Mooney, Raymond. Geoquery 1996.Data. http://www.cs.utexas.edu/users/ml/nldata/geoquery. html (accessed February 13, 2010).","To test the coverage and precision of Locutus, I have customized it to answer questions from the G EOQUERY 250 corpus (Mooney, 1996) [Cite_Ref] , which consists of a database of geographical information paired with 250 English sentences requesting in-formation from that database. 25 of these sentences are held out for the purposes of another study, and I have not examined the behavior of Locutus with respect to these sentences. I ran the other 225 sen-tences through Locutus, keeping track of which sentences Locutus built at least one query for. For each of those sentences, I also tracked the follow-ing:",Material,DataSource,True,Use（引用目的）,True,N10-3004_0_0,2010,Extrinsic Parse Selection,Reference
1949,11953," https://github.com/mlpc-ucsd/BERT_Convolutions"," ['3 Integrating lightweight convolutions', '3.2 Composite attention and lightweight convolution experiments']",Full pre-training and fine-tuning details are outlined in Appendix A.1. [Cite_Footnote_2],"2 Code is available at https://github.com/mlpc-ucsd/BERT_Convolutions, built upon the Huggingface Transformers library (Wolf et al., 2020).","Pre-training To maximize similarity with De-vlin et al. (2019), we pre-trained models on the BookCorpus (Zhu et al., 2015) and WikiText-103 datasets (Merity et al., 2017) using masked lan-guage modeling. Small models were pre-trained for 125,000 steps, with batch size 128 and learn-ing rate 0.0003. Full pre-training and fine-tuning details are outlined in Appendix A.1. [Cite_Footnote_2] Evaluation Models were evaluated on the GLUE benchmark, a suite of sentence classification tasks including natural language inference (NLI), gram-maticality judgments, sentiment classification, and textual similarity (Wang et al., 2018). For each task, we ran ten fine-tuning runs and used the model with the best score on the development set. We report scores on the GLUE test set. Development scores and statistics for all experiments are reported in Appendix A.2.",Method,Code,True,Produce（引用目的）,True,2021.acl-long.333_0_0,2021,Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models,Footnote
1950,11954," https://www.millforbusiness.com/how-many-websites-are-there/"," ['1 Introduction']","Today the largest medium of written communication is the internet, with approximately 380 new websites created every minute. [Cite_Footnote_1]",1 https://www.millforbusiness.com/how-many-websites-are-there/,"Grammatical error correction (GEC) is the task of automatically editing text to remove grammatical errors; for example: [A link to registration can also be found at on the same page.]. GEC systems so far have primarily focused on correcting essays produced by English-as-a-second-language (ESL) learners, providing fast and inexpensive feedback to facilitate language learning. However, this is only one target domain in the full spectrum of GEC applications. GEC models can also help to improve written communication outside of the formal edu-cation setting. Today the largest medium of written communication is the internet, with approximately 380 new websites created every minute. [Cite_Footnote_1] Ensuring grammatical correctness of websites helps facilitate clear communication and a professional commer-cial presentation. Therefore, it is important that tokens 15 erroneous 10 % 5",補足資料,Document,True,Introduce（引用目的）,True,2020.emnlp-main.680_0_0,2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Footnote
1951,11955," https://github.com/SimonHFL/CWEB"," ['1 Introduction']","Contributions: We (i) release a new dataset, CWEB (Corrected Websites), of website data that is corrected for grammatical errors; [Cite_Footnote_3] (ii) system-atically compare it to previously released GEC corpora; (iii) benchmark current state-of-the-art GEC approaches on this data and demonstrate that they are heavily biased towards existing datasets with high error density, even after fine-tuning on our target domain; (iv) perform an analysis showing that a factor behind the performance drop is the inability of systems to rely on a strong internal language model in low error density domains.",3 https://github.com/SimonHFL/CWEB,"Contributions: We (i) release a new dataset, CWEB (Corrected Websites), of website data that is corrected for grammatical errors; [Cite_Footnote_3] (ii) system-atically compare it to previously released GEC corpora; (iii) benchmark current state-of-the-art GEC approaches on this data and demonstrate that they are heavily biased towards existing datasets with high error density, even after fine-tuning on our target domain; (iv) perform an analysis showing that a factor behind the performance drop is the inability of systems to rely on a strong internal language model in low error density domains.",Material,Dataset,True,Produce（引用目的）,True,2020.emnlp-main.680_1_0,2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Footnote
1952,11956," https://commoncrawl.org/"," ['2 CWEB Dataset']","Crawl [Cite_Footnote_4] dataset and represent a wide range of data seen online such as blogs, magazines, corporate or educational websites.",4 https://commoncrawl.org/,"Crawl [Cite_Footnote_4] dataset and represent a wide range of data seen online such as blogs, magazines, corporate or educational websites. These include texts writ-ten by native or non-native English speakers and professional as well as amateur online writers. Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they We compare our data with existing GEC corpora which cover a range of domains and proficiency lev-els. Table 3 presents a number of different statistics and Table 4 their error-type frequencies. 10 3.1 English as a second language (ESL) JFLEG (Napoles et al., 2017) The JHU Fluency-Extended GUG corpus consists of sentences writ-ten by English language learners (with different proficiency levels and L1s) for the TOEFL® exam, tuned on CWEB data. Fine-tuning yields substantial improvements, but scores are still worse than on ESL domains. Scores are calculated against each individual annotator and averaged. sary). vanced speakers. CWEB and AESW in particular stand out, with edits that largely retain the seman-tics of a sentence and that result in more subtle improvements.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.680_2_0,2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Footnote
1953,11957," https://github.com/miso-belica/jusText"," ['2 CWEB Dataset']",We use the jusText [Cite_Footnote_5] tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs).,5 https://github.com/miso-belica/ jusText,"Crawl dataset and represent a wide range of data seen online such as blogs, magazines, corporate or educational websites. These include texts writ-ten by native or non-native English speakers and professional as well as amateur online writers. Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText [Cite_Footnote_5] tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they We compare our data with existing GEC corpora which cover a range of domains and proficiency lev-els. Table 3 presents a number of different statistics and Table 4 their error-type frequencies. 10 3.1 English as a second language (ESL) JFLEG (Napoles et al., 2017) The JHU Fluency-Extended GUG corpus consists of sentences writ-ten by English language learners (with different proficiency levels and L1s) for the TOEFL® exam, tuned on CWEB data. Fine-tuning yields substantial improvements, but scores are still worse than on ESL domains. Scores are calculated against each individual annotator and averaged. sary). vanced speakers. CWEB and AESW in particular stand out, with edits that largely retain the seman-tics of a sentence and that result in more subtle improvements.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.680_3_0,2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Footnote
1954,11958," https://github.com/SimonHFL/CWEB"," ['2 CWEB Dataset']",The data is available at [Cite] https: //github.com/SimonHFL/CWEB .,,"To avoid copyright restrictions, we split the col-lected paragraphs into sentences and shuffle all sentences in order to break the original and co-herent structure that would be needed to repro-duce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Schäfer, 2015; Bie-mann et al., 2007). The data is available at [Cite] https: //github.com/SimonHFL/CWEB . covering a range of topics. Texts have been cor-rected for grammatical errors and fluency.",Material,Dataset,True,Produce（引用目的）,True,2020.emnlp-main.680_4_0,2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Body
1955,11959," https://spacy.io/"," ['2 CWEB Dataset']","The texts are tokenized using SpaCy [Cite_Footnote_9] and au-tomatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017).",9 https://spacy.io/,"The texts are tokenized using SpaCy [Cite_Footnote_9] and au-tomatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017).",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.680_5_0,2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Footnote
1956,11960," https://www.cambridgeenglish.org/exams-and-tests/cefr/"," ['3 GEC Corpora']","Write&Improve (W&I) (Bryant et al., 2019) Cambridge English Write & Improve (Yan-nakoudakis et al., 2018) is an online web platform that automatically provides diagnostic feedback to non-native English-language learners, including an overall language proficiency score based on the Common European Framework of Reference for Languages (CEFR). [Cite_Footnote_11]",11 https://www.cambridgeenglish.org/exams-and-tests/cefr/,"Write&Improve (W&I) (Bryant et al., 2019) Cambridge English Write & Improve (Yan-nakoudakis et al., 2018) is an online web platform that automatically provides diagnostic feedback to non-native English-language learners, including an overall language proficiency score based on the Common European Framework of Reference for Languages (CEFR). [Cite_Footnote_11] The W&I corpus contains 3, 600 texts across 3 different CEFR levels – A (be-ginner), B (intermediate), and C (advanced) – that have been annotated for errors.",補足資料,Document,True,Introduce（引用目的）,True,2020.emnlp-main.680_6_0,2020,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,Footnote
1957,11961," http://www.cs.sunysb.edu/~junkang/connotation_wordnet"," ['1 Introduction']",The lexicon is pub-licly available at: [Cite] http://www.cs.sunysb.,,"ConnotationWordNet, the final outcome of our study, is a new lexical resource that has conno-tation labels over both words and senses follow-ing the structure of WordNet. The lexicon is pub-licly available at: [Cite] http://www.cs.sunysb.edu/˜junkang/connotation_wordnet.) In what follows, we will first describe the net-work of words and senses (Section 2), then intro-duce the representation of the network structure as pairwise Markov Random Fields, and a loopy be-lief propagation algorithm as collective inference (Section 3). We then present comprehensive eval-uation (Section 4 & 5 & 6), followed by related work (Section 7) and conclusion (Section 8).",Material,Knowledge,True,Produce（引用目的）,True,P14-1145_0_0,2014,ConnotationWordNet: Learning Connotation over the Word+Sense Network,Body
1958,11962," http://cs.stanford.edu/"," ['7 Conclusion']","We see many opportunities for improvement, considering the poor performance of oracle training relative to the super-vised state-of-the-art, and in turn the poor perfor-mance of unsupervised state-of-the-art relative to the oracle models. [Cite_Footnote_11]","11 To facilitate future work, all of our models are publicly available at http://cs.stanford.edu/ ∼ valentin/.","Future work could explore unifying these tech-niques with other state-of-the-art approaches. It may be useful to scaffold on both data and model com-plexity, e.g., by increasing head automata’s number of states (Alshawi and Douglas, 2000). We see many opportunities for improvement, considering the poor performance of oracle training relative to the super-vised state-of-the-art, and in turn the poor perfor-mance of unsupervised state-of-the-art relative to the oracle models. [Cite_Footnote_11] To this end, it would be instructive to understand both the linguistic and statistical na-ture of the sweet spot, and to test its universality.",Material,Knowledge,True,Produce（引用目的）,True,N10-1116_0_0,2010,From Baby Steps to Leapfrog: How “Less is More” in Unsupervised Dependency Parsing ∗,Footnote
1959,11963," https://github.com/asiddhant/Active-NLP"," ['1 Introduction']","In this paper, we present a large-scale study [Cite_Footnote_1] , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL).",1 Code for all of our models and for running active learn-ing experiments can be found at https://github.com/asiddhant/Active-NLP,"In this paper, we present a large-scale study [Cite_Footnote_1] , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL). For each task we consider, with multiple datasets, multiple models, and mul-tiple acquisition functions. Moreover, in all ex-periments, we set hyper-parameters on warm-start data, allowing for a more honest assessment. This paper does not seek to champion any one approach but instead to ask, is there any single method that we can reliably expect to work out-of-the-box on a new problem?",Method,Code,True,Produce（引用目的）,True,D18-1318_0_0,2018,Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study,Footnote
1960,11964," http://trec.nist.gov/"," ['1 Introduction']","The conferences such as TREC [Cite_Footnote_1] , CLEF and NTCIR have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents.",1 http://trec.nist.gov/,"One of the major development challenges is eval-uation. The conferences such as TREC [Cite_Footnote_1] , CLEF and NTCIR have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents. However, these conferences have fo-cused mainly on fact-based questions with short an-swers, so called factoid questions. Recently more complex tasks such as list, definition and discourse-based questions have also been included in TREC in a limited fashion (Dang et al., 2007). More complex how- and why-questions (for Asian languages) were also included in the NTCIR07, but the provided data comprised only 100 questions, of which some were also factoids (Fukumoto et al., 2007). Not only is the available non-factoid data quite limited in size, it is also questionable whether the data sets are us-able in development outside the conferences. Lin and Katz (2006) suggest that training data has to be more precise, and, that it should be collected, or at least cleaned, manually.",補足資料,Website,True,Introduce（引用目的）,True,P08-1051_0_0,2008,Collecting a Why-question corpus for development and evaluation of an automatic QA-system,Footnote
1961,11965," http://www.clef-campaign.org/"," ['1 Introduction']","The conferences such as TREC , CLEF [Cite_Footnote_2] and NTCIR have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents.",2 http://www.clef-campaign.org/,"One of the major development challenges is eval-uation. The conferences such as TREC , CLEF [Cite_Footnote_2] and NTCIR have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents. However, these conferences have fo-cused mainly on fact-based questions with short an-swers, so called factoid questions. Recently more complex tasks such as list, definition and discourse-based questions have also been included in TREC in a limited fashion (Dang et al., 2007). More complex how- and why-questions (for Asian languages) were also included in the NTCIR07, but the provided data comprised only 100 questions, of which some were also factoids (Fukumoto et al., 2007). Not only is the available non-factoid data quite limited in size, it is also questionable whether the data sets are us-able in development outside the conferences. Lin and Katz (2006) suggest that training data has to be more precise, and, that it should be collected, or at least cleaned, manually.",補足資料,Website,True,Introduce（引用目的）,True,P08-1051_1_0,2008,Collecting a Why-question corpus for development and evaluation of an automatic QA-system,Footnote
1962,11966," http://research.nii.ac.jp/ntcir/"," ['1 Introduction']","The conferences such as TREC , CLEF and NTCIR [Cite_Footnote_3] have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents.",3 http://research.nii.ac.jp/ntcir/,"One of the major development challenges is eval-uation. The conferences such as TREC , CLEF and NTCIR [Cite_Footnote_3] have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents. However, these conferences have fo-cused mainly on fact-based questions with short an-swers, so called factoid questions. Recently more complex tasks such as list, definition and discourse-based questions have also been included in TREC in a limited fashion (Dang et al., 2007). More complex how- and why-questions (for Asian languages) were also included in the NTCIR07, but the provided data comprised only 100 questions, of which some were also factoids (Fukumoto et al., 2007). Not only is the available non-factoid data quite limited in size, it is also questionable whether the data sets are us-able in development outside the conferences. Lin and Katz (2006) suggest that training data has to be more precise, and, that it should be collected, or at least cleaned, manually.",補足資料,Website,True,Introduce（引用目的）,True,P08-1051_2_0,2008,Collecting a Why-question corpus for development and evaluation of an automatic QA-system,Footnote
1963,11967," http://www.mturk.com"," ['1 Introduction']",We collected our corpus through Amazon Mechanical Turk service [Cite_Footnote_4] (MTurk).,4 http://www.mturk.com,"In addition to providing a new QA corpus, we hope our description of the data collection process will provide insight, resources and motivation for further research and projects using similar collection methods. We collected our corpus through Amazon Mechanical Turk service [Cite_Footnote_4] (MTurk). The MTurk infrastructure allowed us to distribute our tasks to a multitude of workers around the world, without the burden of advertising. The system also allowed us to test the workers suitability, and to reward the work without the bureaucracy of employment. To our knowledge, this is the first time that the MTurk service has been used in equivalent purpose.",補足資料,Website,True,Use（引用目的）,True,P08-1051_3_0,2008,Collecting a Why-question corpus for development and evaluation of an automatic QA-system,Footnote
1964,11968," http://www.cs.unt.edu/~rada/wa"," ['2 Annotation Interface']","The earliest is the Blinker Project (Melamed, 1998); more re-cent systems have been released to support more lan-guages and visualization features (Ahrenberg et al., 2003; Lambert and Castell, 2004). [Cite_Footnote_2]",2 Rada Mihalcea maintains an alignment resource repository (http://www.cs.unt.edu/~rada/wa) that contains other downloadable interface packages that do not have companion papers.,"Because word alignment annotation is a useful re-source for both training and testing, quite a few in-terfaces have already been developed. The earliest is the Blinker Project (Melamed, 1998); more re-cent systems have been released to support more lan-guages and visualization features (Ahrenberg et al., 2003; Lambert and Castell, 2004). [Cite_Footnote_2] Our interface does share some similarities with these systems, but it is designed with additional features to support our experimental goals of guideline development, active learning and resource projection. Following the ex-perimental design proposed by Och and Ney (2000), we instruct the annotators to indicate their level of confidence by choosing sure or unsure for each align-ment they made. This allows researchers to identify areas where the translation may be unclear or diffi-cult. We provide a text area for comments on each sentence so that the annotator may explain any as-sumptions or problems. A hidden timer records how long each user spends on each sentence in order to gauge the difficulty of the sentence; this information will be a useful measurement of the effectiveness of different active learning algorithms. Finally, our in-terface supports cross projection annotation. As an initial study, we have focused on POS tagging, but the framework can be extended for other types of resources such as syntactic and semantic trees and can be configured for languages other than English and Chinese. When words are aligned, the known and displayed English POS tag of the last English word involved in the alignment group is automati-cally projected onto all Chinese words involved, but a drop-down menu allows the user to correct this if the projection is erroneous. A screenshot of the in-terface is provided in Figure 1a.",補足資料,Website,True,Introduce（引用目的）,False,P05-3018_0_0,2005,Word Alignment and Cross-Lingual Resource Acquisition ∗,Footnote
1965,11969," http://flan.cs.pitt.edu/~hwa/align/align.html"," ['4 Conclusion and Future Work']",The system is in place and the annotation process is underway. [Cite_Footnote_4],4 The annotation interface is open to public. Please visit http://flan.cs.pitt.edu/~hwa/align/align.html,"In summary, we have presented an annotation envi-ronment for acquiring word alignments between En-glish and Chinese as well as Part-Of-Speech tags for Chinese. The system is in place and the annotation process is underway. [Cite_Footnote_4]",Method,Tool,True,Produce（引用目的）,True,P05-3018_1_0,2005,Word Alignment and Cross-Lingual Resource Acquisition ∗,Footnote
1966,11970," https://github.com/lxucs/multilingual-sl"," ['4 Experiments']",We implement three different settings for the baseline. [Cite_Footnote_1],1 Code is available at https://github.com/lxucs/multilingual-sl.,"We implement three different settings for the baseline. [Cite_Footnote_1] BL-Direct is the direct zero-shot trans-fer without utilizing unlabeled data of TLs. BL-Single trains gold data of English and silver data of only one TL per model; it simply selects predic-tions of all unlabeled data as silver labels, without considering any uncertainties. BL-Joint is similar to BL-Single but instead train with all TLs jointly.",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.538_0_0,2021,Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation,Footnote
1967,11971," http://www.ethnologue.com"," ['3 Data and Preprocessing', '3.1 Typology Database and Phylogenetic Trees']","For more detailed trees, we used hierarchical classifications provided by Ethno-logue (Lewis et al., 2014) [Cite_Ref] .","M. Paul Lewis, Gary F. Simons, and Charles D. Fen-nig, editors. 2014. Ethnologue: Languages of the World, 17th Edition. SIL International. Online ver-sion: http://www.ethnologue.com.","WALS provides phylogenetic trees but they only have two layers above individual languages: fam-ily and genus. Language families include Indo-European, Austronesian and Niger-Congo, and gen-era within Indo-European include Germanic, In-dic and Slavic. For more detailed trees, we used hierarchical classifications provided by Ethno-logue (Lewis et al., 2014) [Cite_Ref] . The mapping between WALS and Ethnologue was done using ISO 639-3 language codes. We manually corrected some obso-lete language codes used by WALS and dropped lan-guages without language codes. We also excluded languages labeled by Ethnologue as Deaf sign lan-guage, Mixed language, Creole or Unclassified. For both WALS and Ethnologue trees, we removed in-termediate nodes that had only one child. Language isolates were treated as family trees of their own. We obtained 193 family trees for WALS and 189 for Ethnologue.",補足資料,Paper,True,Introduce（引用目的）,True,N15-1036_0_0,2015,Continuous Space Representations of Linguistic Typology and their Application to Phylogenetic Inference,Reference
1968,11972," http://www.cs.waikato.ac.nz/ml/weka/"," ['2 Related Work and Experimental Framework']","To this end, we use SVM-Regression [Cite_Footnote_2] (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system.",2 Weka software (http://www.cs.waikato.ac.nz/ml/weka/),"Section 3 describes the features used by our pre-dictor. Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights. To this end, we use SVM-Regression [Cite_Footnote_2] (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system. We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown cor-pus (Section 5).",Method,Tool,True,Use（引用目的）,True,D08-1093_0_0,2008,Automatic Prediction of Parser Accuracy,Footnote
1969,11973," https://github.com/daiquocnguyen/R-MeN"," ['5 Conclusion']",[Cite] https://github.com/daiquocnguyen/R-MeN.,,"We propose a new KG embedding model, named R-MeN, where we integrate transformer self-attention mechanism-based memory interactions with a CNN decoder to capture the potential dependencies in the KG triples effectively. Experimental results show that our proposed R-MeN obtains the new state-of-the-art performances for both the triple classification and search personalization tasks. In future work, we plan to extend R-MeN for multi-hop knowledge graph reasoning. Our code is available at: [Cite] https://github.com/daiquocnguyen/R-MeN.",Method,Tool,False,Produce（引用目的）,True,2020.acl-main.313_0_0,2020,A Relational Memory-based Embedding Model for Triple Classification and Search Personalization,Body
1970,11974," http://bit.ly/semi-supervised-qa"," ['References']",We are also releasing a set of 3.2M cloze-style questions for practi-tioners to use while building QA systems [Cite_Footnote_1] .,1 http://bit.ly/semi-supervised-qa,"Recent success of deep learning models for the task of extractive Question Answering (QA) is hinged on the availability of large annotated corpora. However, large domain specific an-notated corpora are limited and expensive to construct. In this work, we envision a system where the end user specifies a set of base doc-uments and only a few labelled examples. Our system exploits the document structure to cre-ate cloze-style questions from these base doc-uments; pre-trains a powerful neural network on the cloze style questions; and further fine-tunes the model on the labeled examples. We evaluate our proposed system across three di-verse datasets from different domains, and find it to be highly effective with very little labeled data. We attain more than 50% F1 score on SQuAD and TriviaQA with less than a thou-sand labelled examples. We are also releasing a set of 3.2M cloze-style questions for practi-tioners to use while building QA systems [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,N18-2092_0_0,2018,Simple and Effective Semi-Supervised Question Answering,Footnote
1971,11975," https://stanfordnlp.github.io/CoreNLP/"," ['3 Methodology']","As a post-processing step, we prune out (P,Q,A) triples where the word overlap between the question (Q) and pas-sage (P) is less than [Cite_Footnote_2] words (after excluding the stop words).",2 https://stanfordnlp.github.io/CoreNLP/,"We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }. Addition-ally, let’s say that each sentence q i in introduction is composed of words {w 1 , w 2 , ...w l qi }, where l q is the length of q i . We consider a match(q i ,p j ), if there is an exact string match of a sequence of words {w k , w k+1 , ..w l qi } between the sentence q i and passage p j . If this sequence is either a noun phrase, verb phrase, adjective phrase or a named entity in p j , as recognized by CoreNLP or BAN-NER, we select it as an answer span A. Addition-ally, we use p j as the passage P and form a cloze question Q from the answer bearing sentence q i by replacing A with a placeholder. As a result, we obtain passage-question-answer (P, Q, A) triples (Table 1 shows an example). As a post-processing step, we prune out (P,Q,A) triples where the word overlap between the question (Q) and pas-sage (P) is less than [Cite_Footnote_2] words (after excluding the stop words).",Method,Tool,False,Use（引用目的）,True,N18-2092_1_0,2018,Simple and Effective Semi-Supervised Question Answering,Footnote
1972,11976," http://banner.sourceforge.net"," ['3 Methodology']",[Cite_Footnote_3] (only for PubMed articles) to identify en-tities and phrases.,3 http://banner.sourceforge.net,"We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer [Cite_Footnote_3] (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }. Addition-ally, let’s say that each sentence q i in introduction is composed of words {w 1 , w 2 , ...w l qi }, where l q is the length of q i . We consider a match(q i ,p j ), if there is an exact string match of a sequence of words {w k , w k+1 , ..w l qi } between the sentence q i and passage p j . If this sequence is either a noun phrase, verb phrase, adjective phrase or a named entity in p j , as recognized by CoreNLP or BAN-NER, we select it as an answer span A. Addition-ally, we use p j as the passage P and form a cloze question Q from the answer bearing sentence q i by replacing A with a placeholder. As a result, we obtain passage-question-answer (P, Q, A) triples (Table 1 shows an example). As a post-processing step, we prune out (P,Q,A) triples where the word overlap between the question (Q) and pas-sage (P) is less than words (after excluding the stop words).",Method,Tool,True,Use（引用目的）,True,N18-2092_2_0,2018,Simple and Effective Semi-Supervised Question Answering,Footnote
1973,11977," https://github.com/georgwiese/biomedical-qa"," ['4 Experiments & Results', '4.1 Datasets']","We use the publicly available system [Cite_Footnote_5] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions.",5 https://github.com/georgwiese/ biomedical-qa,"We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [Cite_Footnote_5] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions. For this setting, there are only 899 questions for training. Since this is already a low-resource problem we only report results using 5-fold cross-validation on all the available data. We report Mean Reciprocal Rank (MRR) on the fac-toid questions, and F1 score for the list questions.",Method,Tool,True,Use（引用目的）,True,N18-2092_3_0,2018,Simple and Effective Semi-Supervised Question Answering,Footnote
1974,11978," https://github.com/brmson/question-classification"," ['4 Experiments & Results', '4.3 Analysis']","We consider two classifications of each ques-tion – one determined on the first word (usually a wh-word) of the question (Figure 2 (bottom)) and one based on the output of a separate ques-tion type classifier [Cite_Footnote_7] adapted from (Li and Roth, 2002).",7 https://github.com/brmson/question-classification,"Performance on question types: Figure 2 shows the average gain in F1 score for different types of questions, when we pretrain on the clozes compared to the supervised case. This analysis is done on the 10% split of the SQuAD training set. We consider two classifications of each ques-tion – one determined on the first word (usually a wh-word) of the question (Figure 2 (bottom)) and one based on the output of a separate ques-tion type classifier [Cite_Footnote_7] adapted from (Li and Roth, 2002). We use the coarse grain labels namely Abbreviation (ABBR), Entity (ENTY), Descrip-tion (DESC), Human (HUM), Location (LOC), Numeric (NUM) trained on a Logistic Regres-sion classification system . While there is an im-provement across the board, we find that abbrevi-ation questions in particular receive a large boost. Also, ”why” questions show the least improve-ment, which is in line with our expectation, since these usually require reasoning or world knowl-edge which cloze questions rarely require.",Method,Code,False,Use（引用目的）,True,N18-2092_4_0,2018,Simple and Effective Semi-Supervised Question Answering,Footnote
1975,11979," https://github.com/antonisa/inflection"," ['References']",The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points. [Cite_Footnote_1],1 Our code is available at https://github.com/antonisa/inflection.,"Recent years have seen exceptional strides in the task of automatic morphological inflec-tion generation. However, for a long tail of languages the necessary resources are hard to come by, and state-of-the-art neural methods that work well under higher resource settings perform poorly in the face of a paucity of data. In response, we propose a battery of im-provements that greatly improve performance under such low-resource conditions. First, we present a novel two-step attention architecture for the inflection decoder. In addition, we in-vestigate the effects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination. The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points. [Cite_Footnote_1] Also, we identify the crucial factors for suc-cess with cross-lingual transfer for morpho-logical inflection: typological similarity and a common representation across languages.",Method,Code,True,Produce（引用目的）,True,D19-1091_0_0,2019,Pushing the Limits of Low-Resource Morphological Inflection,Footnote
1976,11980," http://www.inuktitutcomputing.ca/"," ['1 Introduction']","Encouraging examples are the Yupik morphological analyzer (Schwartz et al., 2019) and the Inuktitut educa-tional tools from the respective Native Peoples communities. [Cite_Footnote_2]",2 http://www.inuktitutcomputing.ca/,"Additionally, they could be very useful for building educational applications for languages of under-represented communities (along with their inverse, morphological analyzers). Encouraging examples are the Yupik morphological analyzer (Schwartz et al., 2019) and the Inuktitut educa-tional tools from the respective Native Peoples communities. [Cite_Footnote_2] The social impact of such applica-tions can be enormous, effectively raising the sta-tus of the languages slightly closer to the level of the dominant regional language.",Method,Tool,True,Introduce（引用目的）,True,D19-1091_1_0,2019,Pushing the Limits of Low-Resource Morphological Inflection,Footnote
1977,11981," http://framenet.icsi.berkeley.edu/"," ['Origin Locale Expensiveness']","We use FrameNet-style (Fillmore, 1976) se-mantic frames as our main source of weak supervision. [Cite_Footnote_1]",1 See http://framenet.icsi.berkeley.edu/,"1. Selecting domain-relevant slots from candi-dates provided by weak supervision from domain-generic linguistic annotation tools. We use FrameNet-style (Fillmore, 1976) se-mantic frames as our main source of weak supervision. [Cite_Footnote_1] We also explore named entity recognition (NER).",補足資料,Website,True,Introduce（引用目的）,True,2021.acl-long.189_0_0,2021,Discovering Dialogue Slots with Weak Supervision,Footnote
1978,11982," https://github.com/vojtsek/joint-induction"," ['Origin Locale Expensiveness']",Our experimental code is available on GitHub. [Cite_Footnote_2],2 https://github.com/vojtsek/ joint-induction,Our experimental code is available on GitHub. [Cite_Footnote_2],Method,Code,True,Produce（引用目的）,True,2021.acl-long.189_1_0,2021,Discovering Dialogue Slots with Weak Supervision,Footnote
1979,11983," https://github.com/deepmipt/ner"," ['3 Method', '3.3 Slot Tagger Model Training']","We model the slot tagging task as sequence tag-ging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of re-spective tags (Lample et al., 2016). [Cite_Footnote_7]",7 https://github.com/deepmipt/ner,"We model the slot tagging task as sequence tag-ging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of re-spective tags (Lample et al., 2016). [Cite_Footnote_7] The output layer of the tagger network gives softmax proba-bility distributions over possible tags. To further increase recall, we add an inference-time rule – if slot candidate is split – it is just ranked for relevance multiple times (with respect to multiple contexts). the most probable predicted tag is ‘O’ (i.e., no slot) and the second most probable tag has a probability higher than a preset threshold 𝑇 tag , the second tag is chosen as a prediction instead. As we discuss in Section 6, this threshold is crucial for achieving substantial recall improvement.",Method,Code,False,Extend（引用目的）,True,2021.acl-long.189_2_0,2021,Discovering Dialogue Slots with Weak Supervision,Footnote
1980,11984," https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk"," ['5 Experiments', '5.1 Datasets and Experimental Setup']","• ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [Cite_Footnote_9]",9 We used the ATIS data version from https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk.,"• ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [Cite_Footnote_9]",Material,DataSource,True,Use（引用目的）,True,2021.acl-long.189_3_0,2021,Discovering Dialogue Slots with Weak Supervision,Footnote
1981,11985," https://spacy.io"," ['5 Experiments', '5.1 Datasets and Experimental Setup']","In ad-dition, to explore combined sources on the named-entity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy. [Cite_Footnote_10]",10 https://spacy.io,"As sources of weak supervision providing slot can-didates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) – a union of labels pro-vided by both parsers is used in all our setups. In ad-dition, to explore combined sources on the named-entity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy. [Cite_Footnote_10] To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings.",Method,Code,False,Use（引用目的）,True,2021.acl-long.189_4_0,2021,Discovering Dialogue Slots with Weak Supervision,Footnote
1982,11986," https://github.com/vojtsek/joint-induction"," ['7 Conclusion']",Code used for our experiments is available on GitHub. [Cite_Footnote_16],16 https://github.com/vojtsek/ joint-induction,"We present a novel approach for weakly supervised natural language understanding in dialogue systems that discovers domain-relevant slots and tags them in a standalone fashion. Our method removes the need for annotated training data by using off-the-shelf linguistic annotation models. Experiments on five datasets in four domains mark a signifi-cant improvement in intrinsic NLU performance over previous weakly supervised approaches; in particular, we vastly improve the slot recall. The usefulness of slots discovered by our method is further confirmed in a full dialogue response gener-ation application. Code used for our experiments is available on GitHub. [Cite_Footnote_16]",Method,Code,True,Produce（引用目的）,True,2021.acl-long.189_5_0,2021,Discovering Dialogue Slots with Weak Supervision,Footnote
1983,11987," https://github.com/qiangning/MATRES"," ['1 Introduction']","The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal order-ing of events. [Cite_Footnote_1]",1 https://github.com/qiangning/MATRES,"The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal order-ing of events. [Cite_Footnote_1] It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and 20 more for evaluation) from TimeBank 2 (Puste-jovsky et al., 2003), AQUAINT 3 (Graff, 2002) and Platinum (UzZaman et al., 2013).",Material,Dataset,True,Introduce（引用目的）,False,2020.emnlp-main.436_0_0,2020,Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,Footnote
1984,11988," https://catalog.ldc.upenn.edu/LDC2006T08"," ['2 Our Baseline Model']",Subsequences span 1 and span [Cite_Footnote_2] represent the input pair of argument events e1 and e2 respectively.,2 https://catalog.ldc.upenn.edu/ LDC2006T08,"Our pairwise temporal ordering model re-ceives as input a sequence X [0,n) of n tokens (or subword units for BERT-like models) i.e. {x 0 ,x 1 ,...,x n−1 }, representing the input text. A subsequence span i is defined by start i , end i ∈ [0,n). Subsequences span 1 and span [Cite_Footnote_2] represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2.",Material,Knowledge,False,Produce（引用目的）,True,2020.emnlp-main.436_1_0,2020,Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,Footnote
1985,11989," https://catalog.ldc.upenn.edu/LDC2002T31"," ['1 Introduction']","1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and 20 more for evaluation) from TimeBank 2 (Puste-jovsky et al., 2003), AQUAINT [Cite_Footnote_3] (Graff, 2002) and Platinum (UzZaman et al., 2013).",3 https://catalog.ldc.upenn.edu/ LDC2002T31,"The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal order-ing of events. 1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and 20 more for evaluation) from TimeBank 2 (Puste-jovsky et al., 2003), AQUAINT [Cite_Footnote_3] (Graff, 2002) and Platinum (UzZaman et al., 2013).",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.436_2_0,2020,Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,Footnote
1986,11990," https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-relations-guidelines-v6.2.pdf"," ['3 Multi-task Learning', '3.2 Auxiliary Datasets']",Our first dataset is the ACE relation extrac-tion task. [Cite_Footnote_5],5 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-relations-guidelines-v6.2.pdf,"Our first dataset is the ACE relation extrac-tion task. [Cite_Footnote_5] We hypothesize that this task can add knowledge of different domains and of the con-cept of linking two spans in text given a taxonomy of relations. While this is not directly related to events and our farthest task in terms of similarity, the pairwise span classification is the reason we include this.",補足資料,Document,True,Introduce（引用目的）,True,2020.emnlp-main.436_3_0,2020,Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,Footnote
1987,11991," http://www.timeml.org/publications/timeMLdocs/timeml_1.2.1.html"," ['3 Multi-task Learning', '3.2 Auxiliary Datasets']","We also use a closer and complementary tem-poral annotation dataset, i.e. the Timebank and Aquaint annotations involving timex re-lations (timex-event, event-timex, timex-timex) (Ning et al., 2018; Goyal and Durrett, 2019). [Cite_Footnote_6]",6 http://www.timeml.org/publications/timeMLdocs/timeml_1.2.1.html.,"We also use a closer and complementary tem-poral annotation dataset, i.e. the Timebank and Aquaint annotations involving timex re-lations (timex-event, event-timex, timex-timex) (Ning et al., 2018; Goyal and Durrett, 2019). [Cite_Footnote_6] We expect the model to greatly benefit from being ex-posed to the timex relations in an MTL framework by learning about temporality in general and by adding specificity of the event-event temporal re-lations from the MATRES annotations. Figure 2 shows an example of the data annotated with an event-timex relation.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.436_4_0,2020,Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,Footnote
1988,11992," https://www.macmillandictionary.com/dictionary/american/penumbra"," ['1 Introduction']","[Cite_Footnote_1] Its metaphoric meaning is seldom encountered: Shadows follow objects that cast them, and espe-cially penumbras can be perceived as having fuzzy outlines; attributes which are picked up by the metaphorical sense of a rather unspecified group of people following someone in differing vicinity.",1 https://www.macmillandictionary.com/dictionary/american/penumbra,"The metaphor tight budgets in (1) is an often used collocation and therefore highly conventionalized. While the basic senses of tight—e.g., being phys-ically close together or firmly attached—conflict with the more abstract budget, the metaphoric use as meaning limited can be readily understood. In contrast, the use of penumbra in (2) is more creative and novel. Its literal meaning is “an area covered by the outer part of a shadow.” [Cite_Footnote_1] Its metaphoric meaning is seldom encountered: Shadows follow objects that cast them, and espe-cially penumbras can be perceived as having fuzzy outlines; attributes which are picked up by the metaphorical sense of a rather unspecified group of people following someone in differing vicinity.",補足資料,Document,True,Introduce（引用目的）,True,D18-1171_0_0,2018,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,Footnote
1989,11993," https://github.com/UKPLab/emnlp2018-novel-metaphors"," ['3 Corpus']",We make the annota-tions and scripts to embed them into the original corpus publicly available. [Cite_Footnote_2],2 https://github.com/UKPLab/ emnlp2018-novel-metaphors,"Using crowdsourcing (Amazon Mechanical Turk), we first conduct a pilot study to choose among four different annotations methods. We then employ the best method to collect annotations and create an additional novelty score between 1 (novel) and −1 (conventionalized) for each token labeled as metaphor in the VUAMC. Note that non-content words like prepositions and auxiliary verbs (have, be, do) are filtered out beforehand. Our annotations/scores can be integrated into the original VUAMC resource. We make the annota-tions and scripts to embed them into the original corpus publicly available. [Cite_Footnote_2]",Material,Knowledge,True,Produce（引用目的）,True,D18-1171_1_0,2018,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,Footnote
1990,11994," http://saifmohammad.com/WebPages/BestWorst.html"," ['3 Corpus', '3.2 Corpus Creation']","For creation of the best–worst scaling tuples, and for aggregation of the annotations, we use the scripts provided by Kiritchenko and Mohammad (2016). [Cite_Footnote_3]",3 http://saifmohammad.com/WebPages/BestWorst.html,"After filtering out prepositions and auxiliary verbs (have, be, do) using the POS tags supplied by the VUAMC, we collect annotations covering 15,180 metaphors in total (Table 1). We only in-clude workers located in the US. For creation of the best–worst scaling tuples, and for aggregation of the annotations, we use the scripts provided by Kiritchenko and Mohammad (2016). [Cite_Footnote_3] We use a best-worst scaling factor of 1.5 and four items per tuple. Thus, each metaphor appears in six differ-ent best-worst scaling comparisons. This results in 22,770 best–worst scaling items to be annotated.",Material,Dataset,True,Use（引用目的）,True,D18-1171_2_0,2018,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,Footnote
1991,11995," https://www.macmillandictionary.com/dictionary/american/go-through#go-through"," ['3 Corpus', '3.3 Analysis']",", (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [Cite_Footnote_4]","4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60","Before we conduct a more in-depth analysis of the annotated metaphors, we show some exam-ples. In Table 3, we list four novel and four con-ventionalized metaphors (as annotated). A good example for a novel metaphor is the description of “words [...] as a coat-hanger” in Table 3 (3). This usage cannot be found in dictionaries, and clearly constitutes creative language use. In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [Cite_Footnote_4]",Material,Knowledge,True,Introduce（引用目的）,True,D18-1171_3_0,2018,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,Footnote
1992,11996," https://www.macmillandictionary.com/dictionary/american/"," ['3 Corpus', '3.3 Analysis']",", (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [Cite_Footnote_4]","4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60","Before we conduct a more in-depth analysis of the annotated metaphors, we show some exam-ples. In Table 3, we list four novel and four con-ventionalized metaphors (as annotated). A good example for a novel metaphor is the description of “words [...] as a coat-hanger” in Table 3 (3). This usage cannot be found in dictionaries, and clearly constitutes creative language use. In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [Cite_Footnote_4]",Material,Knowledge,True,Introduce（引用目的）,True,D18-1171_4_0,2018,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,Footnote
1993,11997," https://github.com/spotify/annoy"," ['3 Corpus', 'an indication of novelty of metaphoric use.']","For a given token t, we extract 20 approximate nearest neighbors nn(t) from Google News Embeddings (Mikolov et al., 2013) using Annoy. [Cite_Footnote_5]",5 https://github.com/spotify/annoy,"To analyze the relation between novelty and concreteness, we first extend the concreteness list by Brysbaert et al. (2014) using a technique sim-ilar to Mohler et al. (2014). For a given token t, we extract 20 approximate nearest neighbors nn(t) from Google News Embeddings (Mikolov et al., 2013) using Annoy. [Cite_Footnote_5] The concreteness value for t is then computed by averaging its neighbors’ con-creteness values from the concreteness list.",Method,Code,True,Use（引用目的）,True,D18-1171_5_0,2018,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,Footnote
1994,11998," https://github.com/google-research/electra"," ['3 Experiments', '3.3 Computational Efficiency']","For direct comparison, we modify the ELECTRA reference code [Cite_Footnote_2] .",2 https://github.com/google-research/ electra,"Wall-clock time. We compare the number of train-ing steps per second. For direct comparison, we modify the ELECTRA reference code [Cite_Footnote_2] . For TPU v3 with 8 TPU cores, ELECTRA and SCRIPT achieve 31.3 and 22.7 training iterations per sec-ond with a mean MXU utilization of 14.93% and 17.91% for small models, respectively.",Method,Code,True,Compare（引用目的）,True,2021.naacl-main.409_0_0,2021,SCRIPT: Self-Critic Pretraining of Transformers,Footnote
1995,11999," http://Skylion007.github.io/OpenWebTextCorpus"," ['3 Experiments']","For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [Cite_Ref] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020).",Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.,"Hence, we train and evaluate two SCRIPT mod-els “small” and “base” with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [Cite_Ref] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details.",Material,Dataset,True,Compare（引用目的）,True,2021.naacl-main.409_1_0,2021,SCRIPT: Self-Critic Pretraining of Transformers,Reference
1996,12000," http://Skylion007.github.io/OpenWebTextCorpus"," ['3 Experiments', '3.2 Efficient Pseudo-Log-Likelihood Scoring']","The models were trained on the OpenWebText corpus (Gokaslan and Cohen, 2019) [Cite_Ref] for 1, 000, 000 and 766, 000 steps, respectively.",Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.,"Table 2: Comparison of small and base models on the GLUE dev set. The models were trained on the OpenWebText corpus (Gokaslan and Cohen, 2019) [Cite_Ref] for 1, 000, 000 and 766, 000 steps, respectively. The GLUE task scores are means of 8 runs over a set of random seeds. SCRIPT outperforms ELECTRA while enjoying a simple architecture and learning algorithm.",Material,Dataset,True,Use（引用目的）,True,2021.naacl-main.409_1_1,2021,SCRIPT: Self-Critic Pretraining of Transformers,Reference
1997,12001," http://www.phontron.com/kftt"," ['5 Experiments']","We use the IWSLT 2014 corpus for D E -E N , the KFTT corpus for J A -E N (Neu-big, 2011) [Cite_Ref] , and the WMT 2016 dataset for R O - E N .",Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.,"We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by cover-age mistakes. We use the IWSLT 2014 corpus for D E -E N , the KFTT corpus for J A -E N (Neu-big, 2011) [Cite_Ref] , and the WMT 2016 dataset for R O - E N . The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our rea-son to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units (Sennrich et al., 2016) with a joint vocab-ulary and 32k merge operations. Our implemen-tation was done on a fork of the OpenNMT-py toolkit (Klein et al., 2017) with the default param-eters . We used a validation set to tune hyperpa-rameters introduced by our model. Even though our attention implementations are CPU-based us-ing NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices.",補足資料,Website,True,Use（引用目的）,True,P18-2059_0_0,2018,Sparse and Constrained Attention Chaitanya Malaviya ∗,Reference
1998,12002," https://github.com/wxjiao/UncSamp"," ['References']",Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [Cite_Footnote_1],1 The source code is available at https://github.com/wxjiao/UncSamp,"Self-training has proven effective for improv-ing NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we em-pirically show is sub-optimal. In this work, we propose to improve the sampling proce-dure by selecting the most informative mono-lingual sentences to complement the paral-lel data. To this end, we compute the un-certainty of monolingual sentences using the bilingual dictionary extracted from the paral-lel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not pro-vide additional gains. Accordingly, we de-sign an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the ef-fectiveness of the proposed approach. Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.acl-long.221_0_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
1999,12003," https://github.com/neulab/compare-mt"," ['2 Observing Monolingual Uncertainty', '2.2 Experimental Setup']","We measured the statistical significance of improve-ment with paired bootstrap resampling (Koehn, 2004) using compare-mt [Cite_Footnote_3] (Neubig et al., 2019).",3 https://github.com/neulab/compare-mt,"Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by Sacre-BLEU (Post, 2018) . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improve-ment with paired bootstrap resampling (Koehn, 2004) using compare-mt [Cite_Footnote_3] (Neubig et al., 2019).",Method,Tool,True,Use（引用目的）,True,2021.acl-long.221_1_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
2000,12004," https://github.com/pytorch/fairseq/tree/master/examples/backtranslation"," ['2 Observing Monolingual Uncertainty', '2.3 Effect of Uncertain Data']","We generated translations using beam search with beam width 5, and followed Edunov et al. (2018) [Cite_Footnote_4] to filter the generated sentence pairs (See Ap-pendix A.1).",4 https://github.com/pytorch/fairseq/tree/master/examples/backtranslation,"First of all, we investigated the effect of mono-lingual data uncertainty on the self-training per-formance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -B ASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M mono-lingual sentences for the self-training. To ensure the quality of synthetic parallel data, we trained a T RANSFORMER -B IG model for translating the source monolingual data to the target language. We generated translations using beam search with beam width 5, and followed Edunov et al. (2018) [Cite_Footnote_4] to filter the generated sentence pairs (See Ap-pendix A.1).",補足資料,Document,False,Introduce（引用目的）,False,2021.acl-long.221_2_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
2001,12005," https://github.com/clab/fast_align"," ['2 Observing Monolingual Uncertainty', '2.3 Effect of Uncertain Data']","After that, we ranked all the 40M monolingual sentences and grouped them 38 into [Cite_Footnote_5] equally-sized bins (i.e., 8M sentences per bin) according to their uncertainty scores.",5 https://github.com/clab/fast_align,"Self-training v.s. Uncertainty. In this experi-ment, we first adopted fast-align 5 to establish word alignments between source and target words in the authentic parallel corpus and used the alignments to build the bilingual dictionary A b . Then we used the bilingual dictionary to compute the data uncer-tainty expressed in Equation (3) for the sentences in the monolingual data set. After that, we ranked all the 40M monolingual sentences and grouped them 38 into [Cite_Footnote_5] equally-sized bins (i.e., 8M sentences per bin) according to their uncertainty scores. At last, we performed self-training with each bin of monolingual 36 data.",Method,Tool,True,Use（引用目的）,True,2021.acl-long.221_3_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
2002,12006," https://transmart.qq.com/index"," ['References']","The proposed technology has been applied to TranSmart [Cite_Footnote_6] (Huang et al., 2021), an interactive ma-chine translation system in Tencent, to improve the performance of its core translation engine.",6 https://transmart.qq.com/index,"In this work, we demonstrate the necessity of distin-guishing monolingual sentences for self-training in NMT, and propose an uncertainty-based sampling strategy to sample monolingual data. By sampling monolingual data with relatively high uncertainty, our method outperforms random sampling signifi-cantly on the large-scale WMT English⇒German and English⇒Chinese datasets. Further analyses demonstrate that our uncertainty-based sampling approach does improve the translation quality of high uncertainty sentences and also benefits the prediction of low-frequency words at the target side. The proposed technology has been applied to TranSmart [Cite_Footnote_6] (Huang et al., 2021), an interactive ma-chine translation system in Tencent, to improve the performance of its core translation engine. Future work includes the investigation on the confirmation bias issue of self-training and the effect of decoding strategies on self-training sampling. Grants Council of the Hong Kong Special Admin-istrative Region, China (CUHK 2410021, Research Impact Fund (RIF), R5034-18; CUHK 14210717, General Research Fund), and Tencent AI Lab Rhi-noBird Focused Research Program (GF202036). We sincerely thank the anonymous reviewers for their insightful suggestions on various aspects of this work.",Method,Tool,True,Use（引用目的）,True,2021.acl-long.221_4_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
2003,12007," https://github.com/pytorch/fairseq/tree/master/examples/backtranslation"," ['A Appendix', 'A.1 Synthetic Data']","When performing self-training, we constructed the synthetic data by translating the monolingual sen-tences via beam search with beam width 5, and followed Edunov et al. (2018) [Cite_Footnote_7] to remove sen-tences longer than 250 words as well as sentence-pairs with a source/target length ratio exceeding 1.5.",7 https://github.com/pytorch/fairseq/tree/master/examples/backtranslation,"When performing self-training, we constructed the synthetic data by translating the monolingual sen-tences via beam search with beam width 5, and followed Edunov et al. (2018) [Cite_Footnote_7] to remove sen-tences longer than 250 words as well as sentence-pairs with a source/target length ratio exceeding 1.5. The “teacher” NMT model for self-training is the T RANSFORMER -B IG model to ensure the quality of synthetic data.",補足資料,Document,True,Use（引用目的）,True,2021.acl-long.221_5_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
2004,12008," https://github.com/clab/fast_align"," ['A Appendix', 'A.2 Linguistic Properties']","Firstly, we trained an alignment model on the authentic parallel data by fast-align [Cite_Footnote_8] .",8 https://github.com/clab/fast_align,"Coverage. Coverage measures the ratio of source words being aligned by any target words (Tu et al., 2016). Firstly, we trained an alignment model on the authentic parallel data by fast-align [Cite_Footnote_8] . Then we used the alignment model to force-align the mono-lingual sentences and the synthetic target sentences. Next, we calculated the coverage of each source sentence, and report the averaged coverage of each data bin. The lower coverage of monolingual sen-tences in bin 5 indicates that they are not aligned as well as the other bins.",Method,Tool,True,Use（引用目的）,True,2021.acl-long.221_6_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
2005,12009," https://kheafield.com/code/kenlm/"," ['A Appendix', 'A.3 Comparison with Related Work']","For S RC LM, we trained an N -gram language model (Heafield, 2011) [Cite_Footnote_9] on the source sentences in the bitext and measured the distance between each monolingual sentence to the bitext source sentences by cross-entropy.",9 https://kheafield.com/code/kenlm/,"For DWF, we ranked the monolingual data by word rarity (Platanios et al., 2019) of sentences and also selected the top 80M monolingual data for self-training. For S RC LM, we trained an N -gram language model (Heafield, 2011) [Cite_Footnote_9] on the source sentences in the bitext and measured the distance between each monolingual sentence to the bitext source sentences by cross-entropy. Similarly, we se-lected 8M monolingual data with the lowest cross-entropy for self-training.",Method,Tool,False,Use（引用目的）,True,2021.acl-long.221_7_0,2021,Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation,Footnote
2006,12010," https://github.com/mounicam/lexical_simplification"," ['References']","Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by ap-plying our model to the Paraphrase Database (PPDB). [Cite_Footnote_1]",1 The code and data are publicly available on the au-thors’ homepages and GitHub: https://github.com/mounicam/lexical_simplification.,"Current lexical simplification approaches rely heavily on heuristics and corpus level fea-tures that do not always align with human judgment. We create a human-rated word-complexity lexicon of 15,000 English words and propose a novel neural readability rank-ing model with a Gaussian-based feature vec-torization layer that utilizes these human rat-ings to measure the complexity of any given word or phrase. Our model performs bet-ter than the state-of-the-art systems for dif-ferent lexical simplification tasks and evalua-tion datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by ap-plying our model to the Paraphrase Database (PPDB). [Cite_Footnote_1]",Mixed,Mixed,True,Use（引用目的）,True,D18-1410_0_0,2018,A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,Footnote
2007,12011," http://paraphrase.org"," ['1 Introduction']","In fact, we found that 21% of the 2272 meaning-equivalent word pairs randomly sampled from PPDB [Cite_Footnote_2] (Ganitkevitch et al., 2013) had the simpler word longer than the complex word, while 14% had the simpler word less frequent.",2 PPDB is a large paraphrase database derived from static bilingual translation data available at: http://paraphrase.org,"Most current approaches to lexical simplifica-tion heavily rely on corpus statistics and surface level features, such as word length and corpus-based word frequencies (read more in §5). Two of the most commonly used assumptions are that simple words are associated with shorter lengths and higher frequencies in a corpus. However, these assumptions are not always accurate and are often the major source of errors in the simplifi-cation pipeline (Shardlow, 2014). For instance, the word foolishness is simpler than its meaning-preserving substitution folly even though foolish-ness is longer and less frequent in the Google 1T Ngram corpus (Brants and Franz, 2006). In fact, we found that 21% of the 2272 meaning-equivalent word pairs randomly sampled from PPDB [Cite_Footnote_2] (Ganitkevitch et al., 2013) had the simpler word longer than the complex word, while 14% had the simpler word less frequent.",Material,DataSource,True,Use（引用目的）,True,D18-1410_1_0,2018,A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,Footnote
2008,12012," https://github.com/mounicam/lexical_simplification"," ['2 Constructing A Word-Complexity Lexicon with Human Judgments']","We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators. [Cite_Footnote_3]",3 Download at https://github.com/mounicam/ lexical_simplification,"We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators. [Cite_Footnote_3] Despite the actual larger En-glish vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus was effective for simplifica-tion purposes (see experiments in §4) as our neural ranking model (§3) can estimate the complexity of any word or phrase even out-of-vocabulary.",Material,Knowledge,True,Produce（引用目的）,True,D18-1410_2_0,2018,A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,Footnote
2009,12013," https://catalog.ldc.upenn.edu/ldc2006t13"," ['2 Constructing A Word-Complexity Lexicon with Human Judgments']","Despite the actual larger En-glish vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus [Cite_Footnote_4] was effective for simplifica-tion purposes (see experiments in §4) as our neural ranking model (§3) can estimate the complexity of any word or phrase even out-of-vocabulary.",4 https://catalog.ldc.upenn.edu/ ldc2006t13,"We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators. Despite the actual larger En-glish vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus [Cite_Footnote_4] was effective for simplifica-tion purposes (see experiments in §4) as our neural ranking model (§3) can estimate the complexity of any word or phrase even out-of-vocabulary.",Material,DataSource,True,Use（引用目的）,True,D18-1410_3_0,2018,A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,Footnote
2010,12014," https://code.google.com/archive/p/word2vec/"," ['3 Neural Readability Ranking Model for Words and Phrases', '3.2 Features']","We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package. [Cite_Footnote_7]","7 https://code.google.com/archive/p/word2vec/ ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional rep-resentations by applying Gaussian radial basis functions.","For an input pair of words/phrases hw a ,w b i, we include individual features f(w 1 ), f(w 2 ) and the differences f(w a )−f(w b ). We also use pair-wise features f(hw a , w b i) including cosine simi-larity cos(→−w a , →−w b ) and the difference −→w a − →−w b between the word2vec (Mikolov et al., 2013) em-bedding of the input words. The embeddings for a mutli-word phrase are obtained by averaging the embeddings of all the words in the phrase. We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package. [Cite_Footnote_7]",Material,DataSource,True,Use（引用目的）,True,D18-1410_4_0,2018,A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,Footnote
2011,12015," http://nlp.stanford.edu/software/tmt/"," ['4 Experimental Settings']",the Stanford Topic Modeling Toolbox [Cite_Footnote_3] for L-LDA.,3 http://nlp.stanford.edu/software/tmt/.,"Implementations. To implement our models, we rely on MALLET (McCallum, 2002) for C-LDA and the Stanford Topic Modeling Toolbox [Cite_Footnote_3] for L-LDA. In both cases, we run 1000 iterations of Gibbs sam-pling, using default values for all hyperparameters.",Method,Tool,True,Use（引用目的）,True,D11-1050_0_0,2011,Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases,Footnote
2012,12016," http://wordnetcode.princeton.edu/standoff-files/core-wordnet.txt"," ['4 Experimental Settings']","They are divided into development and test data according to a sampling procedure that respects the following criteria: (i) Both sets must contain all attributes with an equal number of phrases for each attribute; (ii) phrases with both elements con-tained in CoreWordNet [Cite_Footnote_4] are preferred, while others are only considered if necessary to satisfy the first criterion.",4 A subset of WordNet restricted to the 5000 most fre-quently used word senses. Available from: http://wordnetcode.princeton.edu/standoff-files/core-wordnet.txt,"This method yields 7901 labeled adjective-noun phrases. They are divided into development and test data according to a sampling procedure that respects the following criteria: (i) Both sets must contain all attributes with an equal number of phrases for each attribute; (ii) phrases with both elements con-tained in CoreWordNet [Cite_Footnote_4] are preferred, while others are only considered if necessary to satisfy the first criterion. This procedure yields 496/345 phrases in the development/test set, distributed over 206 at-tributes . Training data. The pseudo-documents are collec-ted from dependency paths obtained from section 2 of the parsed pukWaC corpus (Baroni et al., 2009).",Material,DataSource,True,Use（引用目的）,True,D11-1050_1_0,2011,Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases,Footnote
2013,12017," http://mallet.cs.umass.edu"," ['4 Experimental Settings']","To implement our models, we rely on MALLET (McCallum, 2002) [Cite_Ref] for C-LDA and the Stanford Topic Modeling Toolbox 3 for L-LDA.",Andrew Kachites McCallum. 2002. MAL-LET: A machine learning for language toolkit. http://mallet.cs.umass.edu.,"Implementations. To implement our models, we rely on MALLET (McCallum, 2002) [Cite_Ref] for C-LDA and the Stanford Topic Modeling Toolbox 3 for L-LDA. In both cases, we run 1000 iterations of Gibbs sam-pling, using default values for all hyperparameters.",Method,Tool,True,Use（引用目的）,True,D11-1050_2_0,2011,Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases,Reference
2014,12018," http://continual-vista.github.io/"," ['1 Introduction']","Mo-tivated by this finding, we design a set of linguistically-informed experiments: i) to inves-tigate whether the order in which children ac-quire question types facilitates continual learning for computational models and, accordingly, the impact of task order on catastrophic forgetting; ii) to measure how far two well-known CL ap-proaches help to overcome the problem (Robins, 1995; Kirkpatrick et al., 2017) [Cite_Footnote_1] .",1 Code and data are available at the link http://continual-vista.github.io/.,"We investigate catastrophic forgetting in the context of multimodal models for Visual Question Answering (Antol et al., 2015) motivated by ev-idence from psycholinguistics. VQA is the task of answering natural language questions about an image. Evidence from child language acquisi-tion indicates that children learn Wh-questions before polar (Yes/No) questions (Moradlou and Ginzburg, 2016; Moradlou et al., 2018). Mo-tivated by this finding, we design a set of linguistically-informed experiments: i) to inves-tigate whether the order in which children ac-quire question types facilitates continual learning for computational models and, accordingly, the impact of task order on catastrophic forgetting; ii) to measure how far two well-known CL ap-proaches help to overcome the problem (Robins, 1995; Kirkpatrick et al., 2017) [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,P19-1350_0_0,2019,Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering,Footnote
2015,12019," http://www.nltk.org/_modules/nltk/tokenize/casual.html"," ['2 Profiling with Abstract Features']","• PunctA Generalization of PunctC (A for ag-gressive), converting different types of punc-tuation to classes: emoticons [Cite_Footnote_1] to ‘E’ and emo-jis 2 to ‘J’, other punctuation to ‘P’.",1 Using the NLTK tokenizer http://www.nltk.org/_modules/nltk/tokenize/casual.html,"• PunctA Generalization of PunctC (A for ag-gressive), converting different types of punc-tuation to classes: emoticons [Cite_Footnote_1] to ‘E’ and emo-jis 2 to ‘J’, other punctuation to ‘P’.",Method,Tool,True,Use（引用目的）,True,P18-2061_0_0,2018,Bleaching Text: Abstract Features for Cross-lingual Gender Prediction,Footnote
2016,12020," https://pypi.python.org/pypi/emoji/"," ['2 Profiling with Abstract Features']",Repetitions of transformed characters are condensed to a maximum of [Cite_Footnote_2] for greater generalization.,2 https://pypi.python.org/pypi/emoji/,"• Shape Transforms uppercase characters to ‘U’, lowercase characters to ‘L’, digits to ‘D’ and all other characters to ‘X’. Repetitions of transformed characters are condensed to a maximum of [Cite_Footnote_2] for greater generalization.",Method,Code,False,Produce（引用目的）,True,P18-2061_1_0,2018,Bleaching Text: Abstract Features for Cross-lingual Gender Prediction,Footnote
2017,12021," https://github.com/bplank/bleaching-text"," ['3 Experiments', '3.1 Lexical vs Bleached Models']",All code is available at [Cite] https://github.com/bplank/bleaching-text.,,"For the multilingual embeddings model we use the mean embedding representation from the sys-tem of (Plank, 2017) and add max, std and cover-age features. We create multilingual embeddings by projecting monolingual embeddings to a single multilingual space for all five languages using a recently proposed SVD-based projection method with a pseudo-dictionary (Smith et al., 2017). The monolingual embeddings are trained on large amounts of in-house Twitter data (as much data as we had access to, i.e., ranging from 30M tweets for French to 1,500M tweets in Dutch, with a word type coverage between 63 and 77%). This results in an embedding space with a vocabulary size of 16M word types. All code is available at [Cite] https://github.com/bplank/bleaching-text.",Method,Code,True,Produce（引用目的）,True,P18-2061_2_0,2018,Bleaching Text: Abstract Features for Cross-lingual Gender Prediction,Body
2018,12022," http://pan.webis.de"," ['3 Experiments', '3.1 Lexical vs Bleached Models']","For the lexicalized experiments, we adopt the features from the best performing system at the lat-est PAN evaluation campaign [Cite_Footnote_3] (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).",3 http://pan.webis.de,"For the lexicalized experiments, we adopt the features from the best performing system at the lat-est PAN evaluation campaign [Cite_Footnote_3] (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).",補足資料,Website,True,Introduce（引用目的）,True,P18-2061_3_0,2018,Bleaching Text: Abstract Features for Cross-lingual Gender Prediction,Footnote
2019,12023," http://www.hlt.utdallas.edu/∼ady"," ['4 Experiments']","Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus [Cite_Footnote_6] to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution.",6 ECB is available at http://www.hlt.utdallas.edu/∼ady.,"Datasets One dataset we employed is the au-tomatic content extraction (ACE) (ACE-Event, 2005). However, the utilization of the ACE corpus for the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using a restricted set of event types such as LIFE , BUSI - NESS , CONFLICT , and JUSTICE . Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus [Cite_Footnote_6] to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution. One important step in the cre-ation process of this corpus consists in finding sets of related documents that describe the same semi-nal event such that the annotation of coreferential event mentions across documents is possible. For this purpose, we selected from the GoogleNews archive various topics whose description contains keywords such as commercial transaction, attack, death, sports, terrorist act, election, arrest, natu-ral disaster, etc. The entire annotation process for creating the ECB resource is described in (Bejan and Harabagiu, 2008). Table 1 lists several basic statistics extracted from these two corpora.",Material,Dataset,True,Produce（引用目的）,True,P10-1143_0_0,2010,Unsupervised Event Coreference Resolution with Rich Linguistic Features,Footnote
2020,12024," http://news.google.com/"," ['4 Experiments']","For this purpose, we selected from the GoogleNews archive [Cite_Footnote_7] various topics whose description contains keywords such as commercial transaction, attack, death, sports, terrorist act, election, arrest, natu-ral disaster, etc.",7 http://news.google.com/,"Datasets One dataset we employed is the au-tomatic content extraction (ACE) (ACE-Event, 2005). However, the utilization of the ACE corpus for the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using a restricted set of event types such as LIFE , BUSI - NESS , CONFLICT , and JUSTICE . Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution. One important step in the cre-ation process of this corpus consists in finding sets of related documents that describe the same semi-nal event such that the annotation of coreferential event mentions across documents is possible. For this purpose, we selected from the GoogleNews archive [Cite_Footnote_7] various topics whose description contains keywords such as commercial transaction, attack, death, sports, terrorist act, election, arrest, natu-ral disaster, etc. The entire annotation process for creating the ECB resource is described in (Bejan and Harabagiu, 2008). Table 1 lists several basic statistics extracted from these two corpora.",Material,DataSource,True,Use（引用目的）,True,P10-1143_1_0,2010,Unsupervised Event Coreference Resolution with Rich Linguistic Features,Footnote
2021,12025," http://plato.stanford.edu/archives/fall2009/entries/davidson/"," ['2 Event Coreference']","In (Davidson, 1985), Davidson abandoned his sug-gestion to embrace the Quinean theory on event identity (Malpas, 2009) [Cite_Ref] .","Jeff Malpas. 2009. Donald Davidson. In The Stanford Encyclopedia of Philosophy (Fall 2009 Edition), Edward N. Zalta (ed.), http://plato.stanford.edu/archives/fall2009/entries/davidson/.","The problem of determining if two events are iden-tical was originally studied in philosophy. One relevant theory on event identity was proposed by Davidson (1969) who argued that two events are identical if they have the same causes and effects. Later on, a different theory was proposed by Quine (1985) who considered that each event refers to a physical object (which is well defined in space and time), and therefore, two events are identical if they have the same spatiotemporal location. In (Davidson, 1985), Davidson abandoned his sug-gestion to embrace the Quinean theory on event identity (Malpas, 2009) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,P10-1143_2_0,2010,Unsupervised Event Coreference Resolution with Rich Linguistic Features,Reference
2022,12026," http://crfpp.sourceforge.net/"," ['5 Experiments', '5.2 Experimental setup']","The CRF model we compare with was trained with the CRF++ tool, available at [Cite] http://crfpp.sourceforge.net/.",,"The CRF model we compare with was trained with the CRF++ tool, available at [Cite] http://crfpp.sourceforge.net/. The model is equiva-lent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic cat-egories in a window of [-2, +2] with respect to the current token, plus bigrams of concept tags (see (Hahn et al., 2008) and the CRF++ web site for more details).",Method,Tool,True,Use（引用目的）,True,D09-1112_0_0,2009,Re-Ranking Models Based-on Small Training Data for Spoken Language Understanding,Body
2023,12027," http://www3.interscience.wiley.com/cgi-bin/jhome/31248"," ['4 Empirical Evaluation', '4.1 Experimental Setup']",We have obtained the digital publications of 9474 Journal of Comparative Neurology (JCN) [Cite_Footnote_1] articles from 1982 to 2005.,1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248,"We have obtained the digital publications of 9474 Journal of Comparative Neurology (JCN) [Cite_Footnote_1] articles from 1982 to 2005. We have converted the PDF format into plain text, maintaining paragraph breaks (some errors still occur though). A simple heuristic based approach identifies semantic sec-tions of the paper (e.g, Introduction, Results, Dis-cussion). As most experimental descriptions appear in the Results section, we only process the Results section. A neuroscience expert manually annotated the data records in the Results section of 58 re-search articles. The total number of sentences in the Results section of the 58 files is 6630 (averag-ing 114.3 sentences per article).",Material,DataSource,True,Use（引用目的）,True,D07-1088_0_0,2007,Extracting Data Records from Unstructured Biomedical Full Text,Footnote
2024,12028," http://mallet.cs.umass.edu"," ['4 Empirical Evaluation', '4.1 Experimental Setup']","We used the machine learning package MALLET (McCallum, 2002) [Cite_Ref] to conduct the CRF model training and labeling.","McCallum, A.K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.","We used the machine learning package MALLET (McCallum, 2002) [Cite_Ref] to conduct the CRF model training and labeling.",Method,Tool,True,Use（引用目的）,True,D07-1088_1_0,2007,Extracting Data Records from Unstructured Biomedical Full Text,Reference
2025,12029," http://medialab.di.unipi.it/Project/Parser/DgAnnotator/"," ['10 Adaptation Track']","By visual inspection using DgAnnotator (DgAnnotator, 2006) [Cite_Ref] , the parses looked generally correct.",DgAnnotator. 2006. http://medialab.di.unipi.it/Project/Parser/DgAnnotator/.,"By visual inspection using DgAnnotator (DgAnnotator, 2006) [Cite_Ref] , the parses looked generally correct. Most of the errors seemed due to improper handling of conjunctions and disjunctions. The collection in fact contains several phrases like:",Method,Tool,True,Use（引用目的）,True,D07-1119_0_0,2007,Multilingual Dependency Parsing and Domain Adaptation using DeSR,Reference
2026,12030," http://sourceforge.net/projects/mstparser/"," ['8 Performance']","For comparison, we tested the MST parser version 0.4.3 (Mstparser, 2007) [Cite_Ref] , configured for second-order, on the same data: training took 73.9 minutes to perform 10 iterations and parsing took 97.5 sec.",Mstparser 0.4.3. 2007. http://sourceforge.net/projects/mstparser/,"The experiments were performed on a 2.4 Ghz AMD Opteron machine with 32 GB RAM. Train-ing the parser using the 2 nd -order perceptron on the English corpus required less than 3 GB of memory and about one hour for each iteration over the whole dataset. Parsing the English test set required 39.97 sec. For comparison, we tested the MST parser version 0.4.3 (Mstparser, 2007) [Cite_Ref] , configured for second-order, on the same data: training took 73.9 minutes to perform 10 iterations and parsing took 97.5 sec. MST parser achieved:",Method,Tool,True,Compare（引用目的）,True,D07-1119_2_0,2007,Multilingual Dependency Parsing and Domain Adaptation using DeSR,Reference
2027,12031," http://www.clsp.jhu.edu/PIRE/ssr"," ['2 Approach', '2.1 Data']","We conducted our experiments on the recently re-leased Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a medium-sized set of disfluency annotations atop Fisher conversational telephone speech data (Cieri et al., 2004) [Cite_Footnote_1] .",1 The Spontaneous Speech Reconstruction corpus can be downloaded from http://www.clsp.jhu.edu/PIRE/ssr.,"We conducted our experiments on the recently re-leased Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a medium-sized set of disfluency annotations atop Fisher conversational telephone speech data (Cieri et al., 2004) [Cite_Footnote_1] . Advantages of the SSR data include",Material,Dataset,True,Use（引用目的）,True,D09-1080_0_0,2009,Integrating sentence- and word-level error identification for disfluency correction,Footnote
2028,12032," http://www.delph-in.net/"," ['3 Identifying poor constructions', '3.1 SU-level error features']",Each utterance is parsed using the PET deep parser produced by the inter-institutional DELPH-IN group [Cite_Footnote_2] .,2 The DEep Linguistic Processing with HPSG INitiative (see http://www.delph-in.net/),"Head-driven Phrase Structure Grammar (HPSG) is a deep-syntax phrase structure gram-mar which produces rich, non-context-free syntactic analyses of input sentences based on a collection of carefully constructed rules and lexical item structures (Pollard and Sag, 1994; Wahlster, 2000). Each utterance is parsed using the PET deep parser produced by the inter-institutional DELPH-IN group [Cite_Footnote_2] . The manually compiled English Resource Grammar (ERG) (Flickinger, 2002) rules have previously been extended for the Verbmobil (Wahlster, 2000) project to allow for the parsing of basic conversa-tional elements such as SUs with no verb or basic backchannel acknowledgements like “last thursday” or “sure” , but still produce strict HPSG parses based on these rules. We use the binary result of whether or not each SU is parsable by the HPSG ERG as binary indicator functions in our models.",補足資料,Website,True,Introduce（引用目的）,True,D09-1080_1_0,2009,Integrating sentence- and word-level error identification for disfluency correction,Footnote
2029,12033,http://pub.hal3.name\\,['3 Identifying poor constructions'],"To identify speaker errors on the sentence level, we consider and combine a collection of features into a single framework using a maximum entropy model (implemented with the Daumé III (2004) [Cite_Ref] MEGA Model toolkit).","Hal Daumé III. 2004. Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://pub.hal3.name\\ #daume04cg-bfgs, implementation available at http://hal3.name/megam/,August.","To identify speaker errors on the sentence level, we consider and combine a collection of features into a single framework using a maximum entropy model (implemented with the Daumé III (2004) [Cite_Ref] MEGA Model toolkit).",Mixed,Mixed,True,Produce（引用目的）,True,D09-1080_2_0,2009,Integrating sentence- and word-level error identification for disfluency correction,Reference
2030,12034," http://www.uco.es/kdis/mllresources/"," ['5:361–397. Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong']",Datasets: Figure 3 and 4 shows the distribution of the instance numbers of the categories in the datasets RCV1 and Delicious [Cite_Footnote_2] .,2 http://www.uco.es/kdis/mllresources/,"Datasets: Figure 3 and 4 shows the distribution of the instance numbers of the categories in the datasets RCV1 and Delicious [Cite_Footnote_2] . In both datasets, the instance numbers of the categories have long-tail distribution.",Material,Dataset,True,Produce（引用目的）,False,2020.emnlp-main.545_0_0,2020,HSCNN: A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification,Footnote
2031,12035," http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/"," ['3 Corpus Annotation']","The corpus of stories for children was drawn from the fables collection of (McIntyre and Lapata, 2009) [Cite_Footnote_1] and annotated as described in (Bethard et al., 2012).",1 Data available at http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/,"The corpus of stories for children was drawn from the fables collection of (McIntyre and Lapata, 2009) [Cite_Footnote_1] and annotated as described in (Bethard et al., 2012). In this section we illustrate the main annotation princi-ples for coherent temporal annotation. As an example story, consider:",Material,DataSource,True,Use（引用目的）,True,P12-1010_0_0,2012,Extracting Narrative Timelines as Temporal Dependency Structures,Footnote
2032,12036," http://www.bethard.info/data/fables-100-temporal-dependency.xml"," ['3 Corpus Annotation']","Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [Cite_Footnote_2] could be used to train a temporal dependency parsing model.",2 Available from http://www.bethard.info/data/fables-100-temporal-dependency.xml,"Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyre-Lapata collection and measured inter-annotator agree-ment by Krippendorff’s Alpha for nominal data (Krip-pendorff, 2004; Hayes and Krippendorff, 2007). For the resulting annotated corpus annotators achieved Alpha of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering rela-tion labels. Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [Cite_Footnote_2] could be used to train a temporal dependency parsing model.",Material,Dataset,True,Extend（引用目的）,True,P12-1010_1_0,2012,Extracting Narrative Timelines as Temporal Dependency Structures,Footnote
2033,12037," https://github.com/yixinL7/Direct-Style-Transfer"," ['References']","The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [Cite_Footnote_1]",1 Code and data are available at: https://github.com/yixinL7/Direct-Style-Transfer,"In most cases, the lack of parallel corpora makes it impossible to directly train super-vised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that ex-plicitly consider different aspects of the style-transferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose effi-cient strategies of using these metrics for train-ing. The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.337_0_0,2021,On Learning Text Style Transfer with Direct Rewards,Footnote
2034,12038," https://github.com/lijuncen/Sentiment-and-Style-Transfer"," ['3 Experiments', '3.1 Datasets']","We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [Cite_Footnote_2] and the IMDb movie review dataset provided by Dai et al. (2019).",2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer,"We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [Cite_Footnote_2] and the IMDb movie review dataset provided by Dai et al. (2019).",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.337_1_0,2021,On Learning Text Style Transfer with Direct Rewards,Footnote
2035,12039," https://github.com/fastnlp/nlp-dataset"," ['3 Experiments', '3.1 Datasets']","We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), and the IMDb movie review dataset provided by Dai et al. (2019). [Cite_Footnote_3]",3 https://github.com/fastnlp/ nlp-dataset,"We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), and the IMDb movie review dataset provided by Dai et al. (2019). [Cite_Footnote_3]",補足資料,Document,False,Compare（引用目的）,True,2021.naacl-main.337_2_0,2021,On Learning Text Style Transfer with Direct Rewards,Footnote
2036,12040," https://github.com/raosudha89/GYAFC-corpus"," ['3 Experiments', '3.1 Datasets']","We also evaluate our methods on a formality style transfer dataset, Grammarly’s Yahoo Answers Formality Corpus (GYAFC), [Cite_Footnote_4] introduced in Rao and Tetreault (2018).",4 https://github.com/raosudha89/ GYAFC-corpus,"We also evaluate our methods on a formality style transfer dataset, Grammarly’s Yahoo Answers Formality Corpus (GYAFC), [Cite_Footnote_4] introduced in Rao and Tetreault (2018). Although it is a parallel cor-pus, we treat it as an unaligned corpus in our ex-periments. In order to compare to previous work, we chose the Family & Relationships category for our experiments. Datasets statistics are shown in Table 1.",Material,Dataset,True,Compare（引用目的）,True,2021.naacl-main.337_3_0,2021,On Learning Text Style Transfer with Direct Rewards,Footnote
2037,12041," https://fasttext.cc/"," ['3 Experiments', '3.2 Experimental Details']","Following previous work, we measure the style transfer accuracy using a FastText [Cite_Footnote_5] (Joulin et al., 2017) style classifier trained on the respective train-ing set of each dataset.",5 https://fasttext.cc/,"Following previous work, we measure the style transfer accuracy using a FastText [Cite_Footnote_5] (Joulin et al., 2017) style classifier trained on the respective train-ing set of each dataset. To measure content preser-vation, we use SIM and BLEU as metrics where self-SIM and self-BLEU are computed between the source sentences and system outputs, while ref-SIM and ref-BLEU are computed between the system outputs and human references when avail-able. To measure the fluency we use a pre-trained GPT-2 model to compute the perplexity. Our gen-erator, GPT-2, has 1.5 billion parameters, and we train on a GTX 1080 Ti GPU for about 12 hours.",Method,Tool,True,Extend（引用目的）,False,2021.naacl-main.337_4_0,2021,On Learning Text Style Transfer with Direct Rewards,Footnote
2038,12042," https://www.mturk.com/"," ['3 Experiments', '3.5 Human Evaluation']",We use Ama-zon Turk [Cite_Footnote_8] for human evaluation.,8 https://www.mturk.com/ tent preservation. Mean denotes the average of the met-,"We conducted human evaluation on Yelp, Amazon and GYAFC datasets evaluating the style transfer accuracy, content preservation, and fluency sepa-rately. The first two aspects are rated with range 1 - 3 while the fluency is rated with range 0 - 1. We randomly select 100 candidates and compare the outputs of different systems. We use Ama-zon Turk [Cite_Footnote_8] for human evaluation. Each candidate is rated by three annotators and we report the av-erage scores here. We did not evaluate the style transfer accuracy for the GYAMC dataset since it is difficult for human annotators to accurately cap-ture the difference between formal and informal sentences. The results of our human evaluations are shown in Table 7. We additionally report the sample-wise mean score of the metrics where the fluency scores are scaled up to be consistent with other scores. Our model achieves better overall performance when considering all three evaluation metrics on each dataset.",補足資料,Website,True,Use（引用目的）,True,2021.naacl-main.337_5_0,2021,On Learning Text Style Transfer with Direct Rewards,Footnote
2039,12043," http://www.statmt.org/wmt11/"," ['5 Experiments', '5.2 P@k Word Translation']",The words were ex-tracted from the publicly available WMT11 [Cite_Footnote_2] cor-pus.,2 http://www.statmt.org/wmt11/,"Next we evaluated our method on a word transla-tion task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were ex-tracted from the publicly available WMT11 [Cite_Footnote_2] cor-pus. The experiments were done for two sets of translation: English to Spanish and Spanish to En-glish. (Mikolov et al., 2013b) extracted the top 6K most frequent words and translated them with Google Translate. They used the top 5K pairs to train a translation matrix, and evaluated their method on the remaining 1K. As our English and Spanish vectors are already aligned we don’t need the 5K training pairs and use only the 1K test pairs.",Material,DataSource,True,Use（引用目的）,True,D15-1131_0_0,2015,"Trans-gram, Fast Cross-lingual Word-embeddings rey es − Mann de = regina it − femme fr",Footnote
2040,12044," http://www.business2community.com/infographics/impact-online-reviews-"," ['1 Introduction']",The majority of consumers uses such re-views to inform themselves before buying. [Cite_Footnote_1],1 http://www.business2community.com/infographics/impact-online-reviews-customers-buying-decisions-infographic- 01280945,"Online reviews written by customers are a boom-ing market. Several companies cater to a wide variety of audiences, supplying—among others— reviews for restaurants (Yelp), travel (TripAdvi-sor), businesses (Trustpilot), and specialized com-munities, such as beer (RateBeer). While the rev-enue of the providers is in the billions of dollars, the currency this industry is built on is consumer trust. The majority of consumers uses such re-views to inform themselves before buying. [Cite_Footnote_1] On-line review companies therefore put considerable effort into maintaining this trust, by addressing the greatest threat to consumer trust (and therefore income)—fake reviews.",補足資料,Document,True,Introduce（引用目的）,True,P16-2057_0_0,2016,The Enemy in Your Own Camp: How Well Can We Detect Statistically-Generated Fake Reviews – An Adversarial Study,Footnote
2041,12045," https://crowdflower.com"," ['4 Experiments', '4.2 Human Judges']","To answer these questions, we also conduct a human judges study on Crowdflower [Cite_Footnote_6] .",6 https://crowdflower.com,"To answer these questions, we also conduct a human judges study on Crowdflower [Cite_Footnote_6] . We se-lect 200 items at random (100 real reviews and 100 from the conditional model, half of each with meta-information), and ask annotators to rate them as real or fake. Judges were not informed about the nature of the reviews, only advised to use their best judgement. The task involved 8 test questions to bar bad annotators from entering. 76 unique judges participated, and rated the task as relatively difficult (3.5/5).",補足資料,Website,True,Use（引用目的）,True,P16-2057_1_0,2016,The Enemy in Your Own Camp: How Well Can We Detect Statistically-Generated Fake Reviews – An Adversarial Study,Footnote
2042,12046," http://fortune.com/2015/10/19/amazon-fake-reviews/"," ['1 Introduction']","Writing fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detec-tors (Roberts, 2012) [Cite_Ref] .","Jeff John Roberts. 2012. Amazon sues people who charge $5 for fake reviews. Fortune, October 19. http://fortune.com/2015/10/19/amazon-fake-reviews/ Retrieved Feb 27, 2016.","Writing fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detec-tors (Roberts, 2012) [Cite_Ref] . What if fake review writ-ers become aware of the ways to game a detection algorithm? As NLP technology becomes more common, we should expect to also see fake re-views generated by NLP models. This pits tech-nology against technology.",補足資料,Document,True,Introduce（引用目的）,True,P16-2057_2_0,2016,The Enemy in Your Own Camp: How Well Can We Detect Statistically-Generated Fake Reviews – An Adversarial Study,Reference
2043,12047," http://www.nytimes.com/2012/08/26/business/book-reviewers-for-hire-meet-a-demand-for-online-raves.html"," ['1 Introduction']","Writing fake reviews has become a lucrative business (Streitfeld, 2012) [Cite_Ref] , and so there is now an arms race going on between producers and detec-tors (Roberts, 2012).","David Streitfeld. 2012. The Best Book Reviews Money Can Buy. The New York Times, August 25. http://www.nytimes.com/2012/08/26/business/book-reviewers-for-hire-meet-a-demand-for-online-raves.html Retrieved Feb 27, 2016.","Writing fake reviews has become a lucrative business (Streitfeld, 2012) [Cite_Ref] , and so there is now an arms race going on between producers and detec-tors (Roberts, 2012). What if fake review writ-ers become aware of the ways to game a detection algorithm? As NLP technology becomes more common, we should expect to also see fake re-views generated by NLP models. This pits tech-nology against technology.",補足資料,Document,True,Introduce（引用目的）,True,P16-2057_4_0,2016,The Enemy in Your Own Camp: How Well Can We Detect Statistically-Generated Fake Reviews – An Adversarial Study,Reference
2044,12048," https://www.cs.bgu.ac.il/~yoavg/software/projectivize"," ['4 Experiments']",Projectivization is based on Yoav Goldberg’s code. [Cite_Footnote_4],4 https://www.cs.bgu.ac.il/˜yoavg/software/projectivize,"The benefits of non-projective examples for train-ing projective parsers are evaluated on the 73 tree-banks of the UD 2.0 (Nivre et al., 2017b,a). Three methods to exploit non-projective trees (instead of discarding them) are contrasted: learning on the trees projectivized using Eisner (1996)’s algo-rithm, learning on pseudo-projectivized examples (Nivre and Nilsson, 2005) and learning on the non-projective trees, with the minimum-cost oracle de-scribed in §3. Projectivization is based on Yoav Goldberg’s code. [Cite_Footnote_4] For pseudo-projectivization, the M ALT P ARSER 1.9 implementation is used, with the head encoding scheme. For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle).",Method,Code,True,Extend（引用目的）,True,N18-2066_0_0,2018,Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective Trees,Footnote
2045,12049," https://perso.limsi.fr/aufrant"," ['4 Experiments']","For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source [Cite_Footnote_5] implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle).",5 https://perso.limsi.fr/aufrant,"The benefits of non-projective examples for train-ing projective parsers are evaluated on the 73 tree-banks of the UD 2.0 (Nivre et al., 2017b,a). Three methods to exploit non-projective trees (instead of discarding them) are contrasted: learning on the trees projectivized using Eisner (1996)’s algo-rithm, learning on pseudo-projectivized examples (Nivre and Nilsson, 2005) and learning on the non-projective trees, with the minimum-cost oracle de-scribed in §3. Projectivization is based on Yoav Goldberg’s code. For pseudo-projectivization, the M ALT P ARSER 1.9 implementation is used, with the head encoding scheme. For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source [Cite_Footnote_5] implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle).",Method,Tool,True,Use（引用目的）,True,N18-2066_1_0,2018,Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective Trees,Footnote
2046,12050," https://github.com/bionlplab/heart_failure_mortality"," ['References']",We make our work publicly available at [Cite] https://github.com/bionlplab/heart_failure_mortality.,,"Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Fur-thermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict pa-tient survival outcomes. We show that hid-den layers yield notably more accurate pre-dictions than predefined features, outperform-ing the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at [Cite] https://github.com/bionlplab/heart_failure_mortality.",Material,Dataset,True,Produce（引用目的）,True,2021.naacl-main.358_0_0,2021,Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality,Body
2047,12051," https://github.com/sebp/scikit-survival"," ['4 Experiments', '4.2 Metrics']","We measure all-time C-index, C-index at 30 days (C-index@30), and AUC at 30 days and 365 days (AUC@30 and AUC@365) to show the models’ performances dealing with different time-to-events [Cite_Footnote_1] .",1 https://github.com/sebp/ scikit-survival,"Both C-index and AUC assign a random model 0.5 and a perfect model 1. We measure all-time C-index, C-index at 30 days (C-index@30), and AUC at 30 days and 365 days (AUC@30 and AUC@365) to show the models’ performances dealing with different time-to-events [Cite_Footnote_1] . Model C-index C-index@30 AUC@30 AUC@365 CPH + Feature Set 1 0.499 ± 0.025 0.504 ± 0.032 0.503 ± 0.037 CPH + Feature Set 2 0.621 ± 0.014 0.632 ± 0.033 0.642 ± 0.030 CPH + Hidden Features 0.674 ± 0.023 0.696 ± 0.022 0.710 ± 0.022 MLP + Feature Set 1 0.502 ± 0.023 0.509 ± 0.026 0.509 ± 0.030 MLP + Feature Set 2 0.658 ± 0.010 0.671 ± 0.025 0.685 ± 0.023 MLP + Hidden Features 0.704 ± 0.017 0.726 ± 0.020 0.744 ± 0.019 LSTM + Sequential HF 0.709 ± 0.022 0.733 ± 0.031 0.752 ± 0.033 0.501 ± 0.028 0.642 ± 0.030 0.697 ± 0.026 0.501 ± 0.032 0.683 ± 0.008 0.734 ± 0.018 0.742 ± 0.023 Model C-index C-index@30 AUC@30 BERT-Base (Devlin et al., 2019) 0.603 ± 0.115 0.611 ± 0.123 0.618 ± 0.134 BioBert (Lee et al., 2020) 0.701 ± 0.021 0.714 ± 0.027 0.734 ± 0.029 ClinicalBert (Alsentzer et al., 2019) 0.692 ± 0.019 0.705 ± 0.023 0.723 ± 0.025 BlueBert (Peng et al., 2019) 0.713 ± 0.019 0.735 ± 0.024 0.755 ± 0.024 CheXbert (Smit et al., 2020) 0.709 ± 0.022 0.733 ± 0.031 0.752 ± 0.033 AUC@365 0.620 ± 0.136 0.739 ± 0.028 0.727 ± 0.029 0.756 ± 0.021 0.742 ± 0.023",Method,Tool,False,Produce（引用目的）,True,2021.naacl-main.358_1_0,2021,Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality,Footnote
2048,12052," https://github.com/havakv/pycox"," ['4 Experiments', '4.3 Training']",We use pycox and PyTorch to implement the framework [Cite_Footnote_2] .,2 https://github.com/havakv/pycox,"We perform a grid search to find the optimal hyper-parameters based on the metrics and use them for all configurations. The learning rate is set to 0.0001 with an Adam optimizer. We iterate the training process for 100 epochs with batch size 256 and early stop if the validation loss does not decrease. The dropout rate is 0.6. We perform five-fold cross-validation to produce 95% confidence intervals for each metric. The training, validation and test splits are 70%, 10%, 20%, respectively. We use pycox and PyTorch to implement the framework [Cite_Footnote_2] . The end-to-end training takes about 30 minutes with NVIDIA Tesla P100 16 GB GPU, mainly due to feature extraction.",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.358_2_0,2021,Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality,Footnote
2049,12053," https://goo.gl/mKqvro"," ['References']",Code and data are available online [Cite_Footnote_1] .,1 https://goo.gl/mKqvro,"Regularization is a critical step in supervised learning to not only address overfitting, but also to take into account any prior knowledge we may have on the features and their de-pendence. In this paper, we explore state-of-the-art structured regularizers and we pro-pose novel ones based on clusters of words from LSI topics, word2vec embeddings and graph-of-words document representation. We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy. Code and data are available online [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,D16-1188_0_0,2016,Regularizing Text Categorization with Clusters of Words,Footnote
2050,12054," http://qwone.com/~jason/20Newsgroups/"," ['4 Experiments', '4.1 Datasets']","From the 20 Newsgroups [Cite_Footnote_2] dataset, we examine four binary classification tasks.",2 http://qwone.com/~jason/20Newsgroups/,"Topic categorization. From the 20 Newsgroups [Cite_Footnote_2] dataset, we examine four binary classification tasks. We end up with binary classification problems, where we classify a document according to two related categories: comp.sys: ibm.pc.hardware vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space and alt.atheism vs. soc.religion.christian. We use the 20NG dataset from Python.",Material,Dataset,True,Use（引用目的）,True,D16-1188_1_0,2016,Regularizing Text Categorization with Clusters of Words,Footnote
2051,12055,http://www.cs.cornell.edu/~ainur/data.html,"[\'4 Experiments\', \'4.1 Datasets\']","The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) [Cite_Footnote_3] , floor speeches by U.S. Congressmen deciding “yea""/“nay"" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) .",3 http://www.cs.cornell.edu/~ainur/data. html,"Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) [Cite_Footnote_3] , floor speeches by U.S. Congressmen deciding “yea""/“nay"" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) .",Material,Dataset,True,Use（引用目的）,True,D16-1188_2_0,2016,Regularizing Text Categorization with Clusters of Words,Footnote
2052,12056,http://www.cs.jhu.edu/~mdredze/datasets/sentiment/,"[\'4 Experiments\', \'4.1 Datasets\']","The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) , floor speeches by U.S. Congressmen deciding “yea""/“nay"" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) [Cite_Footnote_4] .",4 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/,"Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) , floor speeches by U.S. Congressmen deciding “yea""/“nay"" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) [Cite_Footnote_4] .",Material,Dataset,True,Use（引用目的）,True,D16-1188_3_0,2016,Regularizing Text Categorization with Clusters of Words,Footnote
2053,12057," http://aclweb.org/aclwiki/"," ['6 Experiments', '6.1 Part-of-Speech Tagging']","5,6 To conclude, with valid update methods, we can learn a better tagging model with [Cite_Footnote_5] times faster training than exact search.",5 according to ACL Wiki: http://aclweb.org/aclwiki/.,"Table 2 presents the final tagging results on the test set. For each of the five update methods, we choose the beam size at which it achieves the high-est accuracy on the held-out. For standard update, its best held-out accuracy 97.17 is indeed achieved by exact search (i.e., b = +∞) since it does not work well with beam search, but it costs 2.7 hours (162 minutes) to train. By contrast, the four valid up-date methods handle beam search better. The max-violation method achieves its highest held-out/test accuracies of 97.20 / 97.33 with a beam size of only 2, and only 26 minutes to train. Early up-date achieves the highest held-out/test accuracies of 97.22 / 97.35 across all update methods at the beam size of 4. This test accuracy is even better than Shen et al. (2007), the best tagging accuracy reported on the Penn Treebank to date. 5,6 To conclude, with valid update methods, we can learn a better tagging model with [Cite_Footnote_5] times faster training than exact search.",補足資料,Document,True,Introduce（引用目的）,True,N12-1015_0_0,2012,Structured Perceptron with Inexact Search,Footnote
2054,12058," https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics"," ['References']",Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics. [Cite_Footnote_1],1 The code is available at https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics,"Recent prompt-based approaches allow pre-trained language models to achieve strong per-formances on few-shot finetuning by reformu-lating downstream tasks as a language mod-eling problem. In this work, we demon-strate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., mod-els incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is sig-nificantly less present in the zero-shot evalu-ation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that pre-serves pretraining weights is effective in mit-igating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.713_0_0,2021,"Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning Prasetya Ajie Utama †‡ , Nafise Sadat Moosavi ‡ , Victor Sanh ♣ , Iryna Gurevych ‡",Footnote
2055,12059," http://github.com/lipiji/SongNet"," ['References']",Extensive experiments conducted on two collected corpora demonstrate that our pro-posed framework generates significantly better results in terms of both automatic metrics and the human evaluation. [Cite_Footnote_1],1 Code: http://github.com/lipiji/SongNet,"Neural text generation has made tremendous progress in various tasks. One common char-acteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Al-though they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we pro-pose a simple and elegant framework named SongNet to tackle this problem. The back-bone of the framework is a Transformer-based auto-regressive language model. Sets of sym-bols are tailor-designed to improve the model-ing performance especially on format, rhyme, and sentence integrity. We improve the atten-tion mechanism to impel the model to cap-ture some future information on the format. A pre-training and fine-tuning framework is de-signed to further improve the generation qual-ity. Extensive experiments conducted on two collected corpora demonstrate that our pro-posed framework generates significantly better results in terms of both automatic metrics and the human evaluation. [Cite_Footnote_1]",補足資料,Document,False,Introduce（引用目的）,True,2020.acl-main.68_0_0,2020,Rigid Formats Controlled Text Generation,Footnote
2056,12060," http://en.wikipedia.org/wiki/Ci(poetry"," ['1 Introduction']","Ci is a type of lyric poetry in the tradition of Clas-sical Chinese poetry. [Cite_Footnote_2] , SongCi is the Ci created during Song dynasty), etc., and some examples are illustrated in Figure 1.",2 http://en.wikipedia.org/wiki/Ci (poetry),"In practice we will confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet (say Shakespeare’s Son-nets (Shakespeare, 2000)), SongCi (a kind of Ci. Ci is a type of lyric poetry in the tradition of Clas-sical Chinese poetry. [Cite_Footnote_2] , SongCi is the Ci created during Song dynasty), etc., and some examples are illustrated in Figure 1. The typical characteristics of these text can be categorized into three folds: (1) The assembling of text must comply fully with the predefined rigid formats. Assume that the music “gu”.). The example in the third row of Figure 1 comes from Shakespeare’s “Sonnet 116” (Shake-speare, 2000), the first four sentences. Usually, the rhyming schemes of Shakespeare’s Sonnets is “ABAB CDCD EFEF GG” . In the example, the rhyming words in scheme “ABAB” are “minds”, “love”, “finds”, and “remove”. (3) Even though the format is rigid, the sentence integrity must always be guaranteed. Incomplete sentence such as “love is not the” is inappropriate.",補足資料,Document,True,Introduce（引用目的）,True,2020.acl-main.68_1_0,2020,Rigid Formats Controlled Text Generation,Footnote
2057,12061," http://en.wikipedia.org/wiki/Shakespeare%27ssonnets"," ['1 Introduction']","Usually, the rhyming schemes of Shakespeare’s Sonnets is “ABAB CDCD EFEF GG” [Cite_Footnote_3] .",3 http://en.wikipedia.org/wiki/Shakespeare%27s sonnets,"In practice we will confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet (say Shakespeare’s Son-nets (Shakespeare, 2000)), SongCi (a kind of Ci. Ci is a type of lyric poetry in the tradition of Clas-sical Chinese poetry. , SongCi is the Ci created during Song dynasty), etc., and some examples are illustrated in Figure 1. The typical characteristics of these text can be categorized into three folds: (1) The assembling of text must comply fully with the predefined rigid formats. Assume that the music “gu”.). The example in the third row of Figure 1 comes from Shakespeare’s “Sonnet 116” (Shake-speare, 2000), the first four sentences. Usually, the rhyming schemes of Shakespeare’s Sonnets is “ABAB CDCD EFEF GG” [Cite_Footnote_3] . In the example, the rhyming words in scheme “ABAB” are “minds”, “love”, “finds”, and “remove”. (3) Even though the format is rigid, the sentence integrity must always be guaranteed. Incomplete sentence such as “love is not the” is inappropriate.",補足資料,Document,True,Introduce（引用目的）,True,2020.acl-main.68_2_0,2020,Rigid Formats Controlled Text Generation,Footnote
2058,12062," http://github.com/mozillazg/python-pinyin"," ['4 Experimental Setup', '4.3 Evaluation Metrics']","For the generated samples, we first use the tool pinyin [Cite_Footnote_4] to get the pronunciations (PinYin) of the words in the rhyming positions, and then conduct the evaluation.",4 http://github.com/mozillazg/python-pinyin,"Rhyme For SongCi, usually, there is only one group of rhyming words in one sample. As the example shown in Table 1, the pronunciation of the red rhyming words are “zhu”, “yü”, “du”, and “gu” respectively, and the rhyming phoneme is “u”. For the generated samples, we first use the tool pinyin [Cite_Footnote_4] to get the pronunciations (PinYin) of the words in the rhyming positions, and then conduct the evaluation. For Shakespeare’s Sonnets corpus, the rhyming rule is clear “ABAB CDCD EFEF GG” and there are 7 groups of rhyming tokens. For the generated samples, we employ the CMU Pronounc-ing Dictionary (Speech@CMU, 1998) to obtain the phonemes of the words in the rhyming posi-tions. For example, the phonemes for word “asleep” and “steep” are [’AH0’, ’S’, ’L’, ’IY1’, ’P’] and [’S’, ’T’, ’IY1’, ’P’] respectively. And then we can conduct the evaluation by counting the overlapping units from both the original words and the extracted phonemes group by group. We report the Macro-F1 and Micro-F1 numbers in the results tables as well. Integrity Since the format in our task is strict and rigid, thus the number of words to be predicted is also pre-defined. Our model must organize the language using the limited positions, thus sentence integrity may become a serious issue. For exam-ple, the integrity of “love is not love . h/si” is much better than“love is not the . h/si”. To con-duct the evaluation of sentence integrity, we design a straightforward method by calculating the pre-diction probability of the punctuation characters before h/si given the prefix tokens:",Method,Tool,True,Use（引用目的）,True,2020.acl-main.68_3_0,2020,Rigid Formats Controlled Text Generation,Footnote
2059,12063," http://www.speech.cs.cmu.edu/cgi-bin/cmudict"," ['4 Experimental Setup', '4.3 Evaluation Metrics']","For the generated samples, we employ the CMU Pronounc-ing Dictionary [Cite_Footnote_5] (Speech@CMU, 1998) to obtain the phonemes of the words in the rhyming posi-tions.",5 http://www.speech.cs.cmu.edu/cgi-bin/cmudict,"Rhyme For SongCi, usually, there is only one group of rhyming words in one sample. As the example shown in Table 1, the pronunciation of the red rhyming words are “zhu”, “yü”, “du”, and “gu” respectively, and the rhyming phoneme is “u”. For the generated samples, we first use the tool pinyin to get the pronunciations (PinYin) of the words in the rhyming positions, and then conduct the evaluation. For Shakespeare’s Sonnets corpus, the rhyming rule is clear “ABAB CDCD EFEF GG” and there are 7 groups of rhyming tokens. For the generated samples, we employ the CMU Pronounc-ing Dictionary [Cite_Footnote_5] (Speech@CMU, 1998) to obtain the phonemes of the words in the rhyming posi-tions. For example, the phonemes for word “asleep” and “steep” are [’AH0’, ’S’, ’L’, ’IY1’, ’P’] and [’S’, ’T’, ’IY1’, ’P’] respectively. And then we can conduct the evaluation by counting the overlapping units from both the original words and the extracted phonemes group by group. We report the Macro-F1 and Micro-F1 numbers in the results tables as well. Integrity Since the format in our task is strict and rigid, thus the number of words to be predicted is also pre-defined. Our model must organize the language using the limited positions, thus sentence integrity may become a serious issue. For exam-ple, the integrity of “love is not love . h/si” is much better than“love is not the . h/si”. To con-duct the evaluation of sentence integrity, we design a straightforward method by calculating the pre-diction probability of the punctuation characters before h/si given the prefix tokens:",Material,Knowledge,True,Use（引用目的）,True,2020.acl-main.68_4_0,2020,Rigid Formats Controlled Text Generation,Footnote
2060,12064," http://www.berro.com/Jokes"," ['2 Humorous and Non-humorous Data Sets', '2.1 Humorous Data']","For example, [Cite] http://www.berro.com/Jokes or http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.",,"The first constraint is implemented using a set of keywords of which at least one has to appear in the URL of a retrieved webpage, thus poten-tially limiting the content of the webpage to a theme related to that keyword. The set of key-words used in the current implementation consists of six words that explicitly indicate humor-related content: oneliner, one-liner, humor, humour, joke, funny. For example, [Cite] http://www.berro.com/Jokes or http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.",補足資料,Website,True,Introduce（引用目的）,True,H05-1067_0_0,2005,Making Computers Laugh: Investigations in Automatic Humor Recognition,Body
2061,12065," http://www.mutedfaith.com/funny/life.htm"," ['2 Humorous and Non-humorous Data Sets', '2.1 Humorous Data']","For example, http://www.berro.com/Jokes or [Cite] http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.",,"The first constraint is implemented using a set of keywords of which at least one has to appear in the URL of a retrieved webpage, thus poten-tially limiting the content of the webpage to a theme related to that keyword. The set of key-words used in the current implementation consists of six words that explicitly indicate humor-related content: oneliner, one-liner, humor, humour, joke, funny. For example, http://www.berro.com/Jokes or [Cite] http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.",補足資料,Website,True,Introduce（引用目的）,True,H05-1067_1_0,2005,Making Computers Laugh: Investigations in Automatic Humor Recognition,Body
2062,12066," http://www.speech.cs.cmu.edu/cgi-bin/cmudict"," ['3 Automatic Humor Recognition', '3.1 Humor-Specific Stylistic Features']",The chains are automatically ex-tracted using an index created on top of the CMU pronunciation dictionary [Cite_Footnote_2] .,2 Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict,"To extract this feature, we identify and count the number of alliteration/rhyme chains in each exam-ple in our data set. The chains are automatically ex-tracted using an index created on top of the CMU pronunciation dictionary [Cite_Footnote_2] .",Material,Knowledge,True,Use（引用目的）,True,H05-1067_2_0,2005,Making Computers Laugh: Investigations in Automatic Humor Recognition,Footnote
2063,12067," http://wndomains.itc.it"," ['3 Automatic Humor Recognition', '3.1 Humor-Specific Stylistic Features']","To form a lexicon required for the identification of this feature, we extract from W ORD N ET D OMAINS [Cite_Footnote_3] all the synsets labeled with the domain S EXUALITY .","3 W ORD N ET D OMAINS assigns each synset in W ORD N ET with one or more “domain” labels, such as S PORT , M EDICINE , E CONOMY . See http://wndomains.itc.it.","To form a lexicon required for the identification of this feature, we extract from W ORD N ET D OMAINS [Cite_Footnote_3] all the synsets labeled with the domain S EXUALITY . The list is further processed by removing all words with high polysemy (≥ 4). Next, we check for the presence of the words in this lexicon in each sen-tence in the corpus, and annotate them accordingly. Note that, as in the case of antonymy, W ORD N ET coverage is not complete, and the adult slang fea-ture cannot always be identified. antonymy, adult slang) are present in the same sen-tence, as for instance the following one-liner:",Material,Knowledge,True,Use（引用目的）,True,H05-1067_3_0,2005,Making Computers Laugh: Investigations in Automatic Humor Recognition,Footnote
2064,12068," https://pushshift.io/"," ['A.4 Reddit Dataset']","We use a subset of Reddit comments from 2006- 2018 scraped from [Cite] https://pushshift.io/. We con-struct a dictionary containing the 10,000 most popular words and preprocess the dataset by re-moving deleted posts, out-of-vocabulary tokens, profanity, comments with less than 10 upvotes, and comments with over 400 tokens.",,"We use a subset of Reddit comments from 2006- 2018 scraped from [Cite] https://pushshift.io/. We con-struct a dictionary containing the 10,000 most popular words and preprocess the dataset by re-moving deleted posts, out-of-vocabulary tokens, profanity, comments with less than 10 upvotes, and comments with over 400 tokens.",Material,DataSource,True,Use（引用目的）,True,N19-1169_0_0,2019,Unifying Human and Statistical Evaluation for Natural Language Generation,Body
2065,12069," https://github.com/raylin1000/dropbert"," ['3 Experiments']","We use the NABERT [Cite_Footnote_3] (Numerically Augmented BERT) model with an additional reasoning type to allow “No Answer” as an answer to accommodate the SQuAD 2.0 dataset which has about 40,000 “No Answer” questions.",3 https://github.com/raylin1000/drop bert,"Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Kočisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 2.0 (Rajpurkar et al., 2018). We use the NABERT [Cite_Footnote_3] (Numerically Augmented BERT) model with an additional reasoning type to allow “No Answer” as an answer to accommodate the SQuAD 2.0 dataset which has about 40,000 “No Answer” questions. Each training session lasted 30 epochs with 50,000 instances sampled per epoch. Three training ses-sions were conducted per sampling method and the EM and F1 scores shown are averaged over those three sessions. Note that NarrativeQA is evaluated using only ROUGE F1 score. Due to GPU mem-ory constraints, we are limited to a batch size of 4, so we are unable replicate the Uniform Batches configuration of MRQA (requires a batch size of 8 to fit 1 instance from each of the 8 datasets).",Method,Tool,True,Extend（引用目的）,True,2020.acl-main.86_0_0,2020,Dynamic Sampling Strategies for Multi-Task Reading Comprehension,Footnote
2066,12070," https://leaderboard.allenai.org/orb/submissions/public"," ['3 Experiments']","ORB Evaluation Finally, Table 4 shows that our model trained with dynamic sampling and het-erogeneous batches significantly outperforms the previous ORB state-of-the-art NABERT baseline model (submitted on 11/12/2019 on the leader-board site [Cite_Footnote_4] ).",4 https://leaderboard.allenai.org/orb/submissions/ public Language Processing (EMNLP).,"ORB Evaluation Finally, Table 4 shows that our model trained with dynamic sampling and het-erogeneous batches significantly outperforms the previous ORB state-of-the-art NABERT baseline model (submitted on 11/12/2019 on the leader-board site [Cite_Footnote_4] ).",Method,Code,False,Produce（引用目的）,True,2020.acl-main.86_1_0,2020,Dynamic Sampling Strategies for Multi-Task Reading Comprehension,Footnote
2067,12071," http://nlp.csai.tsinghua.edu.cn/~ly/systems/TsinghuaAligner/TsinghuaAligner.html"," ['5 Experiments', '5.1 Setup']","For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. [Cite_Footnote_1]",1 http://nlp.csai.tsinghua.edu.cn/˜ly/systems/TsinghuaAligner/TsinghuaAligner.html,"The training corpus consists of 1.2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD cor-pus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. [Cite_Footnote_1] The evalu-ation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002).",Material,Dataset,True,Use（引用目的）,True,D15-1210_0_0,2015,"Generalized Agreement for Bidirectional Word Alignment Chunyang Liu † , Yang Liu †∗ , Huanbo Luan † , Maosong Sun † , and Heng Yu ‡",Footnote
2068,12072," https://github.com/Adaxry/ss_on_decoding_steps"," ['1 Introduction']",The main contributions of this paper can be sum-marized as follows [Cite_Footnote_3] :,3 Codes are available at https://github.com/Adaxry/ss_on_decoding_steps.,The main contributions of this paper can be sum-marized as follows [Cite_Footnote_3] :,Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.264_0_0,2021,Scheduled Sampling Based on Decoding Steps for Neural Machine Translation,Footnote
2069,12073," https://catalog.ldc.upenn.edu/LDC2010S01"," ['4 Experiments', '4.1 Experimental Setup']","We used Fisher Spanish corpus (Graff et al., 2010) [Cite_Ref] to perform Spanish speech to English text transla-tion.","David Graff, Shudong Huang, Ingrid Carta-gena, Kevin Walker, , and Christopher Cieri. 2010. Fisher spanish speech (ldc2010s01). https://catalog.ldc.upenn.edu/LDC2010S01.","We used Fisher Spanish corpus (Graff et al., 2010) [Cite_Ref] to perform Spanish speech to English text transla-tion. And we followed previous works (Inaguma et al., 2019) for pre-processing steps, and 40/160 hours of train set, standard dev-test are used for the experiments. Byte-pair-encoding (BPE) (Kudo and Richardson, 2018) was applied to the target transcriptions to form 10K subwords as the tar-get of the translation part. Spanish word embed-dings were obtained from FastText pre-trained on Wikipedia (Bojanowski et al., 2016), and 8000 Spanish words were used in the recognition part.",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.533_0_0,2020,"Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation",Reference
2070,12074," https://github.com/clarkkev/deep-coref"," ['2 Evaluation Setup']","The first resolver, the Stanford neural resolver (Clark and Manning, 2016) [Cite_Footnote_3] , takes as input a set of entity mentions identified for a given document by a rule-based MD system and trains using reinforce-ment learning a simple mention ranker consisting of three hidden layers of ReLU units and a final layer that is fully-connected.",3 https://github.com/clarkkev/deep-coref,"The first resolver, the Stanford neural resolver (Clark and Manning, 2016) [Cite_Footnote_3] , takes as input a set of entity mentions identified for a given document by a rule-based MD system and trains using reinforce-ment learning a simple mention ranker consisting of three hidden layers of ReLU units and a final layer that is fully-connected.",Method,Tool,True,Compare（引用目的）,False,2020.emnlp-main.536_0_0,2020,Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art,Footnote
2071,12076," https://github.com/FilippoC/disc-span-parser-release"," ['1 Introduction']",We release the C++/Python implementation of the parser. [Cite_Footnote_4],4 https://github.com/FilippoC/ disc-span-parser-release,We release the C++/Python implementation of the parser. [Cite_Footnote_4],Method,Tool,True,Produce（引用目的）,True,2020.emnlp-main.219_0_0,2020,Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n 6 ) down to O(n 3 ),Footnote
2072,12077," https://github.com/andreasvc/disco-dop"," ['4 Experiments', '4.3 Evaluation']",We report F-measure and discontinuous F-measure as computed using the disco-dop tool [Cite_Footnote_11] with standard parameters in Table 2.,11 https://github.com/andreasvc/ disco-dop,"We evaluate our parser on the test sets of the three treebanks. We report F-measure and discontinuous F-measure as computed using the disco-dop tool [Cite_Footnote_11] with standard parameters in Table 2. ants of our parsers produced exactly the same re-sults in all settings. This may be expected as their cover of the original treebanks are almost similar. Second, surprisingly, the O(n 3 ) parser produced better results in terms of F-measure than other vari-ants in all cases. We report labeled discontinuous constituent recall and precision measures for the fully supervised model in Table 3. We observe that while the O(n 5 ) and O(n 6 ) have an better recall than the O(n 3 ) parser, their precision is drastically lower. This highlights a benefit of restricting the search space: the parser can retrieve less erroneous constituents leading to an improved overall preci-sion.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.219_1_0,2020,Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n 6 ) down to O(n 3 ),Footnote
2073,12078," http://jgibblda.sourceforge.net/"," ['3 Methodology', '3.1 Encoding distributional semantics']","We employ the publicly available implementa-tion of LDA, JGibbLDA2 [Cite_Footnote_1] (Phan et al., 2008), which has two main execution methods: param-eter estimation (model building) and inference for new data (classification of a new document).",1 http://jgibblda.sourceforge.net/,"We employ the publicly available implementa-tion of LDA, JGibbLDA2 [Cite_Footnote_1] (Phan et al., 2008), which has two main execution methods: param-eter estimation (model building) and inference for new data (classification of a new document).",Method,Tool,True,Use（引用目的）,True,P15-1051_0_0,2015,Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment,Footnote
2074,12079," http://www.sogou.com/labs/dl/c.html"," ['5 Experiments', '5.1 Evaluation as a ranking problem']","For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news [Cite_Footnote_2] including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Mil-itary.",2 http://www.sogou.com/labs/dl/c.html,"Data preparation The data is composed of two parts: source documents and background knowl-edge. For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news [Cite_Footnote_2] including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Mil-itary. We then randomly but equally select 600 articles as the set of source documents from 9 top-ics without data bias. We use all the other 16,599 documents of the same corpus as the source of background knowledge, and then introduce a well-known Chinese open source tool (Che et al., 2010) to extract the triples of background knowledge from the raw text automatically. So the back-ground knowledge also distributes evenly across the same 9 topics. We use the same tool to extract the triples of source documents too.",Material,DataSource,True,Use（引用目的）,True,P15-1051_1_0,2015,Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment,Footnote
2075,12080," https://code.google.com/p/word2vec/"," ['5 Experiments', '5.1 Evaluation as a ranking problem']","Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with the publicly available tool word2vec [Cite_Footnote_3] .",3 https://code.google.com/p/word2vec/,"Baseline systems As Zhang et al. (2014) argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval (Man-ning et al., 2008) and entity linking (Han et al., 2011; Sen, 2012), as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link struc-ture. For better comparison, we implement three methods as baselines, which have been proved ef-fective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with the publicly available tool word2vec [Cite_Footnote_3] .",Method,Tool,True,Use（引用目的）,True,P15-1051_2_0,2015,Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment,Footnote
2076,12081," http://aclweb.org/aclwiki"," ['1 Introduction']","Overall, our rule base contains about 8 million candidate lexical ref-erence rules. [Cite_Footnote_1]",1 For download see Textual Entailment Resource Pool at the ACL-wiki (http://aclweb.org/aclwiki),"In addition, we extract LR rules from Wikipedia redirect and hyperlink relations. As a guide-line, we focused on developing simple extrac-tion methods that may be applicable for other Web knowledge resources, rather than focusing on Wikipedia-specific attributes. Overall, our rule base contains about 8 million candidate lexical ref-erence rules. [Cite_Footnote_1]",補足資料,Document,True,Introduce（引用目的）,True,P09-1051_0_0,2009,Extracting Lexical Reference Rules from Wikipedia,Footnote
2077,12082," http://ai.stanford.edu/"," ['4 Extraction Methods Analysis', '4.1 Judging Rule Correctness']","For comparison, Snow’s pub-lished extension to WordNet [Cite_Footnote_3] , which covers simi-lar types of terms but is restricted to synonyms and hyponyms, includes 400,000 relations.",3 http://ai.stanford.edu/ ∼ rion/swn/,"The middle columns of Table 2 present, for each extraction method, the obtained percentage of cor-rect rules (precision) and their estimated absolute number. This number is estimated by multiplying the number of annotated correct rules for the ex-traction method by the sampling proportion. In to-tal, we estimate that our resource contains 5.6 mil-lion correct rules. For comparison, Snow’s pub-lished extension to WordNet [Cite_Footnote_3] , which covers simi-lar types of terms but is restricted to synonyms and hyponyms, includes 400,000 relations.",Material,Knowledge,True,Compare（引用目的）,True,P09-1051_1_0,2009,Extracting Lexical Reference Rules from Wikipedia,Footnote
2078,12083," https://github.com/OnlpLab/morphodetection"," ['table, given a few forms of the same lexeme. 2']","We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [Cite_Footnote_3]",3 Our code is available on https://github.com/OnlpLab/morphodetection.,"We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [Cite_Footnote_3]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.159_0_0,2021,Minimal Supervision for Morphological Inflection,Footnote
2079,12084," https://en.wiktionary.org/wiki/Appendix:Finnish_conjugation"," ['4 Experimental Evaluation', '4.3 Analysis']","This difficulty is magnified with a large amount of classes, as in the Finnish verbal paradigm that includes 27 classes of which about a dozen seem to include more than a few lexemes according to the statistics over Wiktionary entries. [Cite_Footnote_11]",11 https://en.wiktionary.org/wiki/ Appendix:Finnish_conjugation,"This difficulty is magnified with a large amount of classes, as in the Finnish verbal paradigm that includes 27 classes of which about a dozen seem to include more than a few lexemes according to the statistics over Wiktionary entries. [Cite_Footnote_11]",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.159_1_0,2021,Minimal Supervision for Morphological Inflection,Footnote
2080,12085," https://spacy.io/"," ['6 BERT-based Trees VS Parser-provided Trees']","We denote the original PWCN with rel-ative position information as PWCN-Pos, and that utilizes dependency trees constructed by SpaCy [Cite_Footnote_5] as PWCN-Dep.",5 https://spacy.io/,"Setup. We experimented on two datasets from SemEval 2014 (Pontiki et al., 2014), which con-sist of reviews and comments from two categories: L APTOP and R ESTAURANT . We adopt the stan-dard evaluation metrics: Accuracy and Macro-Averaged F1. We follow the instructions of Zhang et al. (2019) to run the experiments 5 times with random initialization and report the averaged per-formance. We denote the original PWCN with rel-ative position information as PWCN-Pos, and that utilizes dependency trees constructed by SpaCy [Cite_Footnote_5] as PWCN-Dep. SpaCy has reported an UAS of 94.5 on English PTB and so it can serve as a good reference for human-designed dependency schema. We also compare our model against two trivial trees (left-chain and right-chain trees). For our model, we feed the corpus into BERT and ex-tract dependency trees with the best performing setting: Eisner+Dist. For parsing, we introduce an inductive bias to favor short dependencies (Eisner and Smith, 2010). To ensure a fair comparison, we induce the root word from the impact matrix F instead of using the gold root. Specifically, we se-lect the root word x k based on the simple heuristic arg max i P Tj=1 f(x i , x j ).",Method,Tool,True,Use（引用目的）,True,2020.acl-main.383_0_0,2020,Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT,Footnote
2081,12086," http://www.inoh.org:8083/ontology-viewer/"," ['3 Architecture', '3.1 Text View']","This is the model for several existing tools, including the VisDic viewer for WordNet (Horák and Smrž, 2004), the INOH ontology viewer (INOH, 2004) [Cite_Ref] , and the Gene Ontology viewer presented by Koike and Takagi (2004), among others.","INOH, 2004. INOH Ontology Viewer Website. http://www.inoh.org:8083/ontology-viewer/.","In a traditional ontology browser, the user starts looking for concepts of interest by typing words and phrases into a search field. This is the model for several existing tools, including the VisDic viewer for WordNet (Horák and Smrž, 2004), the INOH ontology viewer (INOH, 2004) [Cite_Ref] , and the Gene Ontology viewer presented by Koike and Takagi (2004), among others.",Method,Tool,True,Introduce（引用目的）,True,N06-4009_2_0,2006,SconeEdit: A Text-guided Domain Knowledge Editor,Reference
2082,12087," http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm"," ['3 Architecture', '3.1 Text View']","We take an article from Xinhuanet News Service (Xinhuanet, 2006) [Cite_Ref] as an example.",Xinhuanet. 2006. US accused of blocking approval of new UN human rights body. http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm.,"SconeEdit improves on this browsing paradigm by giving a user who is unfamiliar with the knowl-edge base an easy way to start exploring. Rather than generating a series of guesses at what may be Figure 2. Excerpt from Text View, with Search and Text Tabs covered by the KB, the user can load natural lan-guage text into SconeEdit from a file or the system clipboard. We take an article from Xinhuanet News Service (Xinhuanet, 2006) [Cite_Ref] as an example. Figure 2 shows an excerpt of this text after it has been loaded.",Material,DataSource,True,Use（引用目的）,True,N06-4009_3_0,2006,SconeEdit: A Text-guided Domain Knowledge Editor,Reference
2083,12088," https://github.com/thunlp/CodRED"," ['References']",We make CodRED and the code for our baselines publicly available at [Cite] https://github.com/thunlp/CodRED.,,"Existing relation extraction (RE) methods typ-ically focus on extracting relational facts be-tween entity pairs within single sentences or documents. However, a large quantity of re-lational facts in knowledge bases can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards knowledge acquisition in the wild. To fa-cilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges: Given two entities, (1) it requires finding the relevant documents that can provide clues for identi-fying their relations; (2) it requires reasoning over multiple documents to extract the rela-tional facts. We conduct comprehensive ex-periments to show that CodRED is challeng-ing to existing RE methods including strong BERT-based models. We make CodRED and the code for our baselines publicly available at [Cite] https://github.com/thunlp/CodRED.",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.366_0_0,2021,CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild,Body
2084,12089," https://en.wikipedia.org/wiki/"," ['1 Introduction']","[Cite] https://en.wikipedia.org/wiki/ versarial NA instance generation strategy at entity- Wikipedia:Lists_of_popular_pages_by_ WikiProject level, which requires RE models to pay more atten-Path-Level Adversarial NA Instance.",,"To overcome this problem, we employ a novel ad- 1 [Cite] https://en.wikipedia.org/wiki/ versarial NA instance generation strategy at entity- Wikipedia:Lists_of_popular_pages_by_ WikiProject level, which requires RE models to pay more atten-Path-Level Adversarial NA Instance. To test the each reasoning text path. The second challenge is model ability of cross-document reasoning in the that RE models need to synthesize all information presence of noise in closed setting (see Sec. 3), we in multiple text paths to obtain the final relation. generate NA reasoning text paths for both human-",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.366_1_0,2021,CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild,Body
2085,12090," https://tac.nist.gov/2019/SM-KBP/"," ['1 Introduction']",Sec. 5.2.2) and report the results in Table 6. [Cite_Footnote_5],"5 The results do not include the pipeline model, since both 6 https://tac.nist.gov/2019/SM-KBP/ supervisions are necessary for the model to infer the relation. index.html","Table 7: Ablation results on entity names (Ent.) and we ablate the corresponding supervision (see context(Ctx.). Sec. 5.2.2) and report the results in Table 6. [Cite_Footnote_5] We observe dramatic drops in performance when re- Han et al., 2018; Mesquita et al., 2019) or dis-moving either intra- or cross-document supervision. tant supervision (Riedel et al., 2010; Zhang et al., This shows that cross-doc RE requires deep text 2017; Elsahar et al., 2018). (2) Cross-sentence understanding both within and across documents. RE datasets focus on extracting cross-sentence Entity Names v.s. Context. Previous works have relations from documents (Li et al., 2016; Peng shown that RE systems tend to exploit shallow et al., 2017; Quirk and Poon, 2017; Yao et al., clues in existing datasets, i.e., predict relations 2019) or dialogues (Yu et al., 2020). Notably, NIST based on entity names, instead of inferring from TAC SM-KBP 2019 Track 6 aims to extract and link contexts (Peng et al., 2020). To investigate the con- document-level KBs from different languages and tribution of each information source in CodRED, modalities. However, these datasets are still limited we ablate each information source and report the re- at sentence-level or document-level without con-sults in Table 7: (1) Entity Only. Models are given sidering cross-document reasoning, which restricts only names of the entity pair to predict their rela- the coverage of knowledge acquisition. Hence, we tion. (2) Context Only. The mentions of head and extend RE to cross-document level, and construct tail entities in documents are replaced by special a large-scale human-annotated dataset CodRED to mask tokens. Experimental results show that mod- facilitate further research. els struggle to predict relations only from entity Cross-document natural language understand-names, and masking entity names does not dramati- ing has received increasing interest in recent years. cally hurt the performance. This indicates that there Several datasets have been constructed including are no obvious correlations between relations and cross-document question answering (Yang et al., entity names in CodRED, due to the existence of 2018; Welbl et al., 2018) and cross-document sum-adversarial NA entity pairs (see Sec. 2.3). In sum- marization (Over and Yen, 2004; Owczarzak and mary, although entity names can provide useful Dang, 2011; Fabbri et al., 2019). In comparison information in many cases, CodRED encourages with existing datasets, our dataset is tailored for RE models to infer relations by reasoning in rich the task of RE with fine-grained path and evidence context, instead of relying on shallow correlation annotations, and investigates the more open and between relations and entity names. In this sense, challenging scenario of knowledge acquisition. for knowledge acquisition systems.",補足資料,Paper,True,Produce（引用目的）,True,2021.emnlp-main.366_2_0,2021,CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild,Footnote
2086,12091," https://github.com/PlusLabNLP/ee-wiki-bias"," ['1 Introduction']","To facilitate the study, we collect a corpus that contains demographic information, personal life de-scription, and career description from Wikipedia. [Cite_Footnote_2]",2 https://github.com/PlusLabNLP/ ee-wiki-bias,"To facilitate the study, we collect a corpus that contains demographic information, personal life de-scription, and career description from Wikipedia. [Cite_Footnote_2] We first detect events in the collected corpus using a state-of-the-art event extraction model (Han et al., 2019). Then, we extract gender-distinct events with a higher chance to occur for one group than the other. Next, we propose a calibration technique to offset the potential confounding of gender biases in the event extraction model, enabling us to fo-cus on the gender biases at the corpus level. Our contributions are three-fold:",Material,DataSource,True,Use（引用目的）,True,2021.acl-short.45_0_0,2021,"Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia",Footnote
2087,12092," https://github.com/rujunhan/EMNLP-2019"," ['2 Experimental Setup']","For broader coverage, we choose a state-of-the-art event detec-tion model that focuses on detecting event trigger words by Han et al. (2019). [Cite_Footnote_3]",3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.,"Dataset. Our collected corpus contains demo-graphics information and description sections of celebrities from Wikipedia. Table 2 shows the statistics of the number of celebrities with Career or Personal Life sections in our corpora, together with all celebrities we collected. In this work, we only explored celebrities with Career or Personal Life sections, but there are more sections (e.g., Pol-itics and Background and Family) in our collected other defines an event as a complex structure includ-ing a trigger, arguments, time, and location (Ahn, 2006). The corpus following the former definition usually has much broader coverage, while the lat-ter can provide richer information. For broader coverage, we choose a state-of-the-art event detec-tion model that focuses on detecting event trigger words by Han et al. (2019). [Cite_Footnote_3] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|. Then, we separately construct dic-tionaries E m = {e m1 : |e m1 |,...,e mM : |e mM |} and E f = {e f1 : |e f1 |, ..., e fF : |e fF |} mapping events to their frequency for male and female respectively.",Method,Code,True,Use（引用目的）,True,2021.acl-short.45_1_0,2021,"Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia",Footnote
2088,12093," https://spacy.io/"," ['3 Detecting Gender Biases in Events']","For the former, we take all tokens excluding stop words. [Cite_Footnote_8]",8 We use spaCy (https://spacy.io/) to tokenize the corpus and remove stop words.,"To show the effectiveness of using events as a lens for gender bias analysis, we compute WEAT scores on the raw texts and detected events sepa-rately. For the former, we take all tokens excluding stop words. [Cite_Footnote_8] Together with gender attributes from Caliskan et al. (2017), we calculate and show the",Method,Tool,True,Use（引用目的）,True,2021.acl-short.45_3_0,2021,"Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia",Footnote
2089,12094," https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf"," ['A.3 Top Ten Extracted Events']","During annotation, we follow the defini-tion of events from the ACE annotation guideline. [Cite_Footnote_11]",11 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf,"Our model is trained on TB-Dense, a public dataset coming from news articles. These do not contain any explicit detail that leaks information about a user’s name, health, negative financial sta-tus, racial or ethnic origin, religious or philosophi-cal affiliation or beliefs, trade union membership, alleged or actual crime commission. A Appendix A.1 Quality Check: Event Detection Model To test the performance of the event extraction model in our collected corpus from Wikipedia. We manually annotated events in 10,508 (female: 5,543, male: 4,965) sampled sentences from the Career section in our corpus. Our annotators are two volunteers who are not in the current project but have experience with event detection tasks. We asked annotators to annotate all event trigger words in the text. During annotation, we follow the defini-tion of events from the ACE annotation guideline. [Cite_Footnote_11] We use the manual annotation as the ground truth and compare it with the event detection model out-put to calculate the metrics (i.e., precision, recall and F1) in Table 3.",補足資料,Document,True,Use（引用目的）,True,2021.acl-short.45_5_0,2021,"Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia",Footnote
2090,12095," http://peixianc.me/amfcodes.zip"," ['2 Approach', '2.2 Nonparametric Bayesian Modeling', '2.2.1 Nonparametric Bayesian Formulation']","After learning [Cite_Footnote_1] , we use the expec-tation E(P (y i,j )) in Eq.(9) to complete the entries in Y test .","1 We implement the system for relation extraction, based on the code at http://peixianc.me/amfcodes.zip.","Prediction. After learning [Cite_Footnote_1] , we use the expec-tation E(P (y i,j )) in Eq.(9) to complete the entries in Y test . Finally, we can acquire Top-N predicted relations via ranking the values E(P (y i,j )), given entity pair i, for different relations j.",Method,Code,True,Use（引用目的）,True,D17-1192_0_0,2017,Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective,Footnote
2091,12096," http://www/eml-research.de/nlp/download/wikirelations.php"," ['4 Representation', '4.2 Selectional preference features']","To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [Cite_Footnote_1] (Nastase and Strube, 2008).",1 http://www/eml-research.de/nlp/download/wikirelations.php,"To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [Cite_Footnote_1] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links.",Material,DataSource,True,Use（引用目的）,True,D09-1095_0_0,2009,"Combining Collocations, Lexical and Encyclopedic Knowledge for Metonymy Resolution",Footnote
2092,12097," https://github.com/PKUnlp-icler/GAIN"," ['References']",Our code is available at [Cite] https://github.com/PKUnlp-icler/GAIN.,,"Document-level relation extraction aims to ex-tract relations among entities within a docu-ment. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN con-structs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interac-tion among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer re-lations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at [Cite] https://github.com/PKUnlp-icler/GAIN.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.127_0_0,2020,Double Graph Based Reasoning for Document-level Relation Extraction,Body
2093,12098," https://github.com/headacheboy/data-of-multimodal-sarcasm-detection"," ['1 Introduction']",• We create a new dataset for multi-modal Twitter sarcasm detection and release it [Cite_Footnote_1] .,1 https://github.com/headacheboy/data-of-multimodal- sarcasm-detection,• We create a new dataset for multi-modal Twitter sarcasm detection and release it [Cite_Footnote_1] .,Material,Dataset,True,Produce（引用目的）,True,P19-1239_0_0,2019,Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model,Footnote
2094,12099," https://github.com/rycolab/info-theoretic-probing"," ['References']",Our implementa-tion is available in [Cite] https://github.com/rycolab/info-theoretic-probing.,,"The success of neural networks on a diverse set of NLP tasks has led researchers to ques-tion how much these networks actually “know” about natural language. Probes are a nat-ural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annota-tions in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that us-ing simpler models as probes is better; the logic is that simpler models will identify lin-guistic structure, but not learn the task it-self. We propose an information-theoretic op-erationalization of probing as estimating mu-tual information that contradicts this received wisdom: one should always select the high-est performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the lin-guistic information inherent in the represen-tation. The experimental portion of our pa-per focuses on empirically estimating the mu-tual information between a linguistic property and BERT, comparing these estimates to sev-eral baselines. We evaluate on a set of ten typologically diverse languages often under-represented in NLP research—plus English— totalling eleven languages. Our implementa-tion is available in [Cite] https://github.com/rycolab/info-theoretic-probing.",Method,Tool,True,Produce（引用目的）,True,2020.acl-main.420_0_0,2020,Information-Theoretic Probing for Linguistic Structure,Body
2095,12100," https://github.com/OceanskySun/"," ['4 Experiments and Results', '4.3 Baselines']","For GRAFT-Net (Sun et al., 2018), we use the implementation published by the author; [Cite_Footnote_6] how-ever, we retrieve data with a simpler process, as described in Section 4.4.",6 https://github.com/OceanskySun/,"For GRAFT-Net (Sun et al., 2018), we use the implementation published by the author; [Cite_Footnote_6] how-ever, we retrieve data with a simpler process, as described in Section 4.4.",Method,Tool,True,Use（引用目的）,True,D19-1242_0_0,2019,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,Footnote
2096,12101," https://github.com/PravallikaRao/SpellChecker"," ['1 Introduction']",• We create synthetic datasets [Cite_Footnote_1] of noisy and correct word mappings for Hindi and Telugu by collecting highly probable spelling errors and inducing noise in clean corpus.,1 https://github.com/PravallikaRao/SpellChecker,• We create synthetic datasets [Cite_Footnote_1] of noisy and correct word mappings for Hindi and Telugu by collecting highly probable spelling errors and inducing noise in clean corpus.,Material,Dataset,True,Produce（引用目的）,True,P18-3021_0_0,2018,Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning,Footnote
2097,12102," https://ltrc.iiit.ac.in/download.php"," ['4 Experiments and Results', '4.1 Dataset details']","For this, we have extracted a corpus of most frequent Hindi words [Cite_Footnote_2] and most frequent Telugu words .",2 https://ltrc.iiit.ac.in/download.php,"Due to lack of data with error patterns in Indic languages, we have built a synthetic dataset that SCMIL is trained on. Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words [Cite_Footnote_2] and most frequent Telugu words . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists. Thus, the Hindi and Telugu data lists consist of words and phrases consisting maximum of five words.",Material,Knowledge,True,Produce（引用目的）,True,P18-3021_1_0,2018,Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning,Footnote
2098,12103," https://en.wiktionary.org/Frequencylists/Telugu"," ['4 Experiments and Results', '4.1 Dataset details']","For this, we have extracted a corpus of most frequent Hindi words and most frequent Telugu words [Cite_Footnote_3] .",3 https://en.wiktionary.org/Frequencylists/Telugu,"Due to lack of data with error patterns in Indic languages, we have built a synthetic dataset that SCMIL is trained on. Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words and most frequent Telugu words [Cite_Footnote_3] . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists. Thus, the Hindi and Telugu data lists consist of words and phrases consisting maximum of five words.",Material,Knowledge,True,Produce（引用目的）,True,P18-3021_2_0,2018,Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning,Footnote
2099,12104," https://ltrc.iiit.ac.in/download.php"," ['4 Experiments and Results', '4.3 Results and Analysis']","Hence, we implemented HINSPELL using Shabdanjali dic-tionary [Cite_Footnote_4] consisting of 32952 Hindi word.",4 https://ltrc.iiit.ac.in/download.php,"The rule-based spell-checker for Hindi, HIN-SPELL (Singh et al., 2015) reported an accuracy of 77.9% on a data of 870 misspelled words ran-domly collected from books, newspapers and peo-ple etc. Te data used in Singh et al. (2015) and the HINSPELL system are not available. Hence, we implemented HINSPELL using Shabdanjali dic-tionary [Cite_Footnote_4] consisting of 32952 Hindi word. This system when tested on our Hindi synthetic dataset, gave an accuracy is 72.3%. This accuracy being lower than the original HINSPELL accuracy can be accounted to larger size of testing data and in-clusion of out-of-vocabulary words. Thus, SCMIL outperforms HINSPELL by reporting an accuracy of 85.4%.",Material,Knowledge,True,Use（引用目的）,True,P18-3021_3_0,2018,Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning,Footnote
2100,12105," https://git.io/JU0JJ"," ['References']","It also sub-stantially outperforms the previous grounded model, with largest improvements on more ‘abstract’ categories (e.g., +55.1% recall on VPs). [Cite_Footnote_1]",1 Our code is available at https://git.io/JU0JJ.,"Exploiting visual groundings for language un-derstanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a con-stituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via R EINFORCE and derives the learning signal only from the alignment of images and sen-tences. While their model is relatively accu-rate overall, its error distribution is very un-even, with low performance on certain con-stituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely in-sufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model estab-lishes a new state of the art, outperforming its non-grounded version and, thus, confirm-ing the effectiveness of visual groundings in constituency grammar induction. It also sub-stantially outperforms the previous grounded model, with largest improvements on more ‘abstract’ categories (e.g., +55.1% recall on VPs). [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.354_0_0,2020,Visually Grounded Compound PCFGs,Footnote
2101,12106," https://git.io/JfV6J"," ['4 Experiments', '4.1 Datasets and evaluation']","We use the same data preprocess-ing [Cite_Footnote_4] as in Shen et al. (2019) and Kim et al. (2019a), where punctuation is removed from all data, and the top 10,000 frequent words in training sentences are kept as the vocabulary.",4 https://git.io/JfV6J.,"Datasets: We use MSCOCO (Lin et al., 2014). It consists of 82,783 training images, 1,000 valida-tion images, and 1,000 test images. Each image is associated with 5 caption sentences. We encode images into 2048-dimensional vectors using the pre-trained ResNet-101 (He et al., 2016). At test time, only captions are used. We follow Shi et al. (2019) and parse test captions with Benepar (Kitaev and Klein, 2018). We use the same data preprocess-ing [Cite_Footnote_4] as in Shen et al. (2019) and Kim et al. (2019a), where punctuation is removed from all data, and the top 10,000 frequent words in training sentences are kept as the vocabulary.",補足資料,Website,False,Compare（引用目的）,False,2020.emnlp-main.354_1_0,2020,Visually Grounded Compound PCFGs,Footnote
2102,12107," https://git.io/Jf3nn"," ['4 Experiments', '4.2 Settings and hyperparameters']",For VG-NSL we run the authors’ code. [Cite_Footnote_5],5 https://git.io/Jf3nn.,"We adopt parameter settings suggested by the authors for the baseline models. For VG-NSL we run the authors’ code. [Cite_Footnote_5] We re-implement C-PCFG using automatic differentiation (Eisner, 2016) to speed up training. Our VC-PCFG com-prises a parsing model and an image-text match-ing model. The parsing model has the same pa-rameters as the baseline C-PCFG; the image-text matching model has the same parameters as the baseline VG-NSL. Concretely, the parsing model has 30 nonterminals and 60 preterminals. Each of them is represented by a 256-dimensional vec-tor. The inference model q φ (z|w) uses a single-layer BiLSTM. It has a 512-dimensitional hidden state and relies on 512-dimensitional word em-beddings. We apply a max-pooling layer over the hidden states of the BiLSTM and then ob-tain 64-dimensitional mean vectors µ φ (w) and log-variances log σ φ (w) by using an affine layer. The image-text matching model projects visual fea-tures into 512-dimensitional feature vectors and encodes spans as 512-dimensitional vectors. Our span representation model is another single-layer BiLSTM, with the same hyperparameters as in the inference model. α for visually grounded learning is set to 0.001. We implement VC-PCFG relying on Torch-Struct (Rush, 2020), and optimize it us-ing Adam (Kingma and Ba, 2015) with the learning rate set to 0.01, β 1 = 0.75, and β 2 = 0.999. All parameters are initialized with Xavier uniform ini-tializer (Glorot and Bengio, 2010).",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.354_2_0,2020,Visually Grounded Compound PCFGs,Footnote
2103,12108," http://code.google.com/p/egret-parser"," ['3 Parsing', '3.1 Parsing Overview']","Instead of the Berkeley Parser itself, we use a clone Egret, [Cite_Footnote_4] which achieves nearly iden-tical accuracy, and is able to output packed forests for use in MT, as mentioned below.",4 http://code.google.com/p/egret-parser,"For English, the two most widely referenced parsers are the Stanford Parser and Berkeley Parser. In this work, we compare the Stanford Parser’s CFG model, with the Berkeley Parser’s latent variable model. In previous reports, it has been noted (Kummerfeld et al., 2012) that the la-tent variable model of the Berkeley parser tends to have the higher accuracy of the two, so if the accu-racy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret, [Cite_Footnote_4] which achieves nearly iden-tical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other el-ement in the phrase.",Method,Tool,True,Use（引用目的）,True,P14-2024_0_0,2014,On the Elements of an Accurate Tree-to-String Machine Translation System,Footnote
2104,12109," http://plata.ar.media.kyoto-u.ac.jp/tool/EDA"," ['3 Parsing', '3.1 Parsing Overview']","For Japanese, our first method uses the MST-based pointwise dependency parser of Flannery et al. (2011), as implemented in the Eda toolkit. [Cite_Footnote_5]",5 http://plata.ar.media.kyoto-u.ac.jp/tool/EDA,"For Japanese, our first method uses the MST-based pointwise dependency parser of Flannery et al. (2011), as implemented in the Eda toolkit. [Cite_Footnote_5] In order to convert dependencies into phrase-structure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing. Both models are trained on the Japanese Word Dependency Treebank (Mori et al., 2014).",Method,Tool,True,Compare（引用目的）,False,P14-2024_1_0,2014,On the Elements of an Accurate Tree-to-String Machine Translation System,Footnote
2105,12110," http://code.google.com/p/nile"," ['4 Alignment', '4.1 Alignment Overview']","To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [Cite_Footnote_6]",6 http://code.google.com/p/nile,"As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [Cite_Footnote_6] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.",Method,Tool,True,Compare（引用目的）,False,P14-2024_2_0,2014,On the Elements of an Accurate Tree-to-String Machine Translation System,Footnote
2106,12111," http://www.phontron.com/kftt"," ['4 Alignment', '4.1 Alignment Overview']","We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) [Cite_Ref] (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences).",Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.,"We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) [Cite_Ref] (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences). As creating manual alignment data is costly, we also created two training sets that con-sisted of 1/4 and 1/16 of the total data to test if we can achieve an effect with smaller amounts of manually annotated data. The details of data size and alignment accuracy are shown in Table 3.",補足資料,Website,True,Introduce（引用目的）,True,P14-2024_3_0,2014,On the Elements of an Accurate Tree-to-String Machine Translation System,Reference
2107,12112," https://github.com/IntelLabs/academic-budget-bert"," ['References']","We demonstrate that through a combination of software optimiza-tions, design choices, and hyperparameter tun-ing, it is possible to produce models that are competitive with BERT BASE on GLUE tasks at a fraction of the original pretraining cost. [Cite_Footnote_1]",1 Our code is publicly available at: https://github.com/IntelLabs/academic-budget-bert,"While large language models à la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimiza-tions, design choices, and hyperparameter tun-ing, it is possible to produce models that are competitive with BERT BASE on GLUE tasks at a fraction of the original pretraining cost. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.831_0_0,2021,How to Train BERT with an Academic Budget,Footnote
2108,12113," https://github.com/tensorflow/models/tree/master/official/nlp/bert"," ['3 Combining Efficient Training Methods', '3.2 Combined Speedup']",We compare our optimized framework to the offi-cial implementation of Devlin et al. (2019). [Cite_Footnote_3],3 https://github.com/tensorflow/models/tree/master/official/nlp/bert,"We compare our optimized framework to the offi-cial implementation of Devlin et al. (2019). [Cite_Footnote_3] Ta-ble 1 shows that using the official code to train BERT BASE could take almost 6 days under our hard-ware assumptions (Section 2), and a large model might require close to a month of non-stop compu-tation. In contrast, our recipe significantly speeds up training, allowing one to train BERT LARGE with the original number of steps (1M) in a third of the time (8 days), or converge in 2-3 days by enlarging the batch size. While larger batch sizes do not guar-antee convergence to models of equal quality, they are generally recommended (Ott et al., 2018; Liu et al., 2019), and present a more realistic starting point for our next phase (hyperparameter tuning) given our 24-hour constraint.",Method,Tool,True,Compare（引用目的）,True,2021.emnlp-main.831_1_0,2021,How to Train BERT with an Academic Budget,Footnote
2109,12114," https://www.nationsreportcard.gov/reading_2017/nation/achievement?grade=4"," ['1 Introduction']","According to the results of the 2017 National As-sessment of Educational Progress (NAEP) [Cite_Footnote_1] , 32% of U.S. 4 th graders read below the Basic level.",1 https://www.nationsreportcard.gov/reading_2017/nation/achievement?grade=4,"According to the results of the 2017 National As-sessment of Educational Progress (NAEP) [Cite_Footnote_1] , 32% of U.S. 4 th graders read below the Basic level. Most such students lack foundational skills of oral reading fluency – accuracy, reading rate, and prosody. Furthermore, more than a million stu-dents at the Basic level are also relatively slow readers, have poor prosody, and make more errors than skilled readers (Sabatini et al., 2018).",補足資料,Document,True,Introduce（引用目的）,True,P19-3024_0_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Footnote
2110,12115," https://cordova.apache.org"," ['3 My Turn To Read App']",Mobile versions of MTTR are built using Apache Cordova [Cite_Footnote_3] – a cross-platform toolkit – with platform-specific modifications where nec-essary.,3 https://cordova.apache.org,"Mobile versions of MTTR are built using Apache Cordova [Cite_Footnote_3] – a cross-platform toolkit – with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.",Method,Tool,True,Use（引用目的）,True,P19-3024_1_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Footnote
2111,12116," https://readium.org"," ['3 My Turn To Read App']","The reading and listening components in all versions are built on top of Readium [Cite_Footnote_4] , a robust, standards-compliant, and open-source e-reader.",4 https://readium.org,"Mobile versions of MTTR are built using Apache Cordova – a cross-platform toolkit – with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium [Cite_Footnote_4] , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.",Method,Tool,True,Extend（引用目的）,True,P19-3024_2_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Footnote
2112,12117," https://www.youtube.com/watch?v=Efsl1ZMWFkE"," ['3 My Turn To Read App']",A video illustrating most of the user-facing features in action is cur-rently available at [Cite] https://www.youtube.com/watch?v=Efsl1ZMWFkE.,,"Next, we describe the salient MTTR features along with the underlying NLP & Speech tech-nologies, where appropriate. A video illustrating most of the user-facing features in action is cur-rently available at [Cite] https://www.youtube.com/watch?v=Efsl1ZMWFkE.",補足資料,Media,True,Produce（引用目的）,True,P19-3024_3_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Body
2113,12118,http://www.idpf.org/epub,"[\'3 My Turn To Read App\', \'3.1 Read Aloud eBooks\']","We use the EPUB format [Cite_Footnote_5] to create what we call a “Read Aloud eBook"" used by MTTR.",5 http://www.idpf.org/epub,"We use the EPUB format [Cite_Footnote_5] to create what we call a “Read Aloud eBook"" used by MTTR. To link the text in the book to the synchronized audio, we use SMIL (Synchronized Multimedia Integration Lan-guage), as defined in the EPUB Media Overlays specification. The complete process for generat-ing a Read Aloud eBook is as follows: book MP3 file for this chapter. The alignment is done using the Kaldi ASR toolkit (Povey et al., 2011) and the LibriSpeech acoustic models (Panayotov et al., 2015). The result-ing word-level alignment is used to compute the beginning and end timestamps for each sentence. We use Sequitur G2P (Bisani and Ney, 2008) to phonetically transcribe out-of-vocabulary words. The transcriptions are checked manually and added to the lexicon used for forced alignment.",Method,Code,True,Use（引用目的）,True,P19-3024_4_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Footnote
2114,12119," https://github.com/lxml/lxml"," ['3 My Turn To Read App', '3.1 Read Aloud eBooks']",We use lxml [Cite_Footnote_6] to extract the plain text from the original eBook EPUB.,6 https://github.com/lxml/lxml,1. We use lxml [Cite_Footnote_6] to extract the plain text from the original eBook EPUB. We then break up paragraphs into sentences and create a map-ping between sentence identifiers and token indices where sentences start and end.,Method,Code,True,Use（引用目的）,True,P19-3024_5_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Footnote
2115,12120," https://github.com/aerkalov/ebooklib"," ['3 My Turn To Read App', '3.1 Read Aloud eBooks']",We use ebooklib [Cite_Footnote_7] to generate a new EPUB file with sentences linked to time segments in the relevant MP3 file using SMIL.,7 https://github.com/aerkalov/ebooklib,3. We use ebooklib [Cite_Footnote_7] to generate a new EPUB file with sentences linked to time segments in the relevant MP3 file using SMIL.,Method,Code,True,Use（引用目的）,True,P19-3024_6_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Footnote
2116,12121," https://myturntoread.org"," ['5 Discussion & Future Work']",My Turn To Read is currently in beta and we plan to release freely-available web [Cite_Footnote_9] and mobile (iOS & Android) versions in August of 2019 with the public-domain book The Adventures of Pinocchio.,9 https://myturntoread.org.,My Turn To Read is currently in beta and we plan to release freely-available web [Cite_Footnote_9] and mobile (iOS & Android) versions in August of 2019 with the public-domain book The Adventures of Pinocchio. We plan to add more books in subsequent releases.,Method,Tool,True,Produce（引用目的）,True,P19-3024_7_0,2019,My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers,Footnote
2117,12122," https://code.google.com/p/word2vec/"," ['3 Representation of Words and Compounds']",We learn the embeddings of these compounds as single to-kens using the word2vec [Cite_Footnote_5] bag-of-word model.,5 https://code.google.com/p/word2vec/,"In this work we use word embeddings of Mikolov et al. (2013a) to represent the seman-tics of words and compounds. We chose an En-glish Wikipedia dump as our corpus. After fil-tering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [Cite_Footnote_5] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds’ component words. Compounds’ sizes are restricted to two (i.e. bigrams) for the sake of sim-plicity and to respect the evaluation set standards. The compounds and word embeddings are then used as supervised signals to learn a composition function.",Method,Tool,True,Use（引用目的）,True,D15-1201_0_0,2015,Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions,Footnote
2118,12123," https://www.elastic.co/products/elasticsearch"," ['3 Approach']","The query for each answer choice, E+C n , is sent to the retriever (e.g. Elastic Search [Cite_Footnote_2] ), and the top K retrieved sentences based on the scores returned by the retriever are then concatenated into the evidence passage P n = {w tP } tK=1 .",2 https://www.elastic.co/products/elasticsearch,"For a question with q words Q = {w tQ } qt=1 along with its N answer choices C = {C n } Nn=1 where C n = {w Ct } ct=1 , the essential-term selector chooses a subset of essential terms E ⊂ Q, which are then concatenated with each C n to formulate a query. The query for each answer choice, E+C n , is sent to the retriever (e.g. Elastic Search [Cite_Footnote_2] ), and the top K retrieved sentences based on the scores returned by the retriever are then concatenated into the evidence passage P n = {w tP } tK=1 .",Method,Tool,True,Use（引用目的）,True,N19-1030_0_0,2019,Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Open-domain Question Answering,Footnote
2119,12124," https://github.com/nijianmo/arc-etrr-code"," ['4 Experiments']",Our code is released at [Cite] https://github.com/nijianmo/arc-etrr-code.,,"In this section, we first discuss the performance of the essential term selector, ET-Net, on a public dataset. We then discuss the performance of the whole retriever-reader pipeline, ET-RR, on mul-tiple open-domain datasets.For both the ET-Net and ET-RR models, we use 96-dimensional hid-den states and 1-layer BiLSTMs in the sequence modeling layer. A dropout rate of 0.4 is applied for the embedding layer and the BiLSTMs’ out-put layer. We use adamax (Kingma and Ba, 2014) with a learning rate of 0.02 and batch size of 32. The model is trained for 100 epochs. Our code is released at [Cite] https://github.com/nijianmo/arc-etrr-code.",Method,Code,True,Produce（引用目的）,True,N19-1030_1_0,2019,Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Open-domain Question Answering,Body
2120,12125," http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2"," ['2 Data']","For the experiments in this paper, we used a full dump of Wikipedia from September 4, 2010. [Cite_Footnote_2]",2 http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2,"For the experiments in this paper, we used a full dump of Wikipedia from September 4, 2010. [Cite_Footnote_2] In-cluded in this dump is a total of 10,355,226 articles, of which 1,019,490 have been geotagged. Excluding various types of special-purpose articles used pri-marily for maintaining the site (specifically, redirect articles and articles outside the main namespace), the dump includes 3,431,722 content-bearing arti-cles, of which 488,269 are geotagged.",Material,DataSource,True,Use（引用目的）,True,P11-1096_0_0,2011,Simple Supervised Document Geolocation with Geodesic Grids,Footnote
2121,12126," http://download.freebase.com/wex/"," ['2 Data']","Automatically-processed versions of the English-language Wikipedia site are provided by Metaweb, [Cite_Footnote_3] which at first glance promised to signif-icantly simplify the preprocessing.",3 http://download.freebase.com/wex/,"It is necessary to process the raw dump to ob-tain the plain text, as well as metadata such as geo-tagged coordinates. Extracting the coordinates, for example, is not a trivial task, as coordinates can be specified using multiple templates and in mul-tiple formats. Automatically-processed versions of the English-language Wikipedia site are provided by Metaweb, [Cite_Footnote_3] which at first glance promised to signif-icantly simplify the preprocessing. Unfortunately, these versions still need significant processing and they incorrectly eliminate some of the important metadata. In the end, we wrote our own code to process the raw dump. It should be possible to ex-tend this code to handle other languages with little difficulty. See Lieberman and Lin (2009) for more discussion of a related effort to extract and use the geotagged articles in Wikipedia.",Material,DataSource,False,Use（引用目的）,False,P11-1096_1_0,2011,Simple Supervised Document Geolocation with Geodesic Grids,Footnote
2122,12127," http://www.ark.cs.cmu.edu/GeoText/"," ['2 Data']","As a second eval-uation corpus on a different domain, we use the corpus of geotagged tweets collected and used by Eisenstein et al. (2010). [Cite_Footnote_4]",4 http://www.ark.cs.cmu.edu/GeoText/,"Geo-tagged Microblog Corpus As a second eval-uation corpus on a different domain, we use the corpus of geotagged tweets collected and used by Eisenstein et al. (2010). [Cite_Footnote_4] It contains 380,000 mes-sages from 9,500 users tweeting within the 48 states of the continental USA.",Material,Dataset,True,Use（引用目的）,True,P11-1096_2_0,2011,Simple Supervised Document Geolocation with Geodesic Grids,Footnote
2123,12128," http://code.google.com/p/textgrounder/wiki/WingBaldridge2011"," ['2 Data']","Replication Our code (part of the TextGrounder system), our processed version of Wikipedia, and in-structions for replicating our experiments are avail-able on the TextGrounder website. [Cite_Footnote_5]",5 http://code.google.com/p/textgrounder/wiki/WingBaldridge2011,"Replication Our code (part of the TextGrounder system), our processed version of Wikipedia, and in-structions for replicating our experiments are avail-able on the TextGrounder website. [Cite_Footnote_5]",Mixed,Mixed,True,Produce（引用目的）,True,P11-1096_3_0,2011,Simple Supervised Document Geolocation with Geodesic Grids,Footnote
2124,12129," http://www.senseval.org/senseval3"," ['1 Introduction']","See also the common task of (CoNLL, 2004 2005; Senseval, 2004 [Cite_Ref] ).",Senseval. 2004. Third international workshop on the evalua-tion of systems for the semantic analysis of text (acl 2004). http://www.senseval.org/senseval3.,"Current statistical parsers do not use this richer information because performance of the parser usu-ally decreases considerably, since a more complex task is being solved. (Klein and Manning, 2003), for instance report a reduction in parsing accuracy of an unlexicalised PCFG from 77.8% to 72.9% if using function labels in training. (Blaheta, 2004) also reports a decrease in performance when at-tempting to integrate his function labelling system with a full parser. Conversely, researchers interested in producing richer semantic outputs have concen-trated on two-stage systems, where the semantic la-belling task is performed on the output of a parser, in a pipeline architecture divided in several stages (Gildea and Jurafsky, 2002). See also the common task of (CoNLL, 2004 2005; Senseval, 2004 [Cite_Ref] ).",補足資料,Website,True,Introduce（引用目的）,True,H05-1078_0_0,2005,Accurate Function Parsing,Reference
2125,12130," http://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz"," ['4 Experiments', '4.1 Dataset and Evaluation']","We report performance in F 0.5 -measure, as calculated by the m2scorer— the official implementation of the scoring metric in the shared task. [Cite_Footnote_1]",1 http://www.comp.nus.edu.sg/˜nlp/sw/m2scorer.tar.gz,"We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014). We report final performance on the CoNLL-14 test set without alternatives, and an-alyze model performance on the CoNLL-13 devel-opment set (Dahlmeier et al., 2013). We use the de-velopment and validation sets for model selection. The sizes of all datasets in number of sentences are shown in Table 1. We report performance in F 0.5 -measure, as calculated by the m2scorer— the official implementation of the scoring metric in the shared task. [Cite_Footnote_1] Given system outputs and gold-standard edits, m2scorer computes the F 0.5 measure of a set of system edits against a set of gold-standard edits.",Method,Tool,True,Use（引用目的）,False,P17-1070_0_0,2017,A Nested Attention Neural Hybrid Model for Grammatical Error Correction,Footnote
2126,12131," https://github.com/Pzoom522/L1-Refinement"," ['1 Introduction']",Our code is avail-able at [Cite] https://github.com/Pzoom522/L1-Refinement.,,"To demonstrate the effectiveness of our method, we select four state-of-the-art baselines and con-duct comprehensive evaluations in both supervised and unsupervised settings. Our experiments in-volve ten languages from diverse branches/families and embeddings trained on corpora of different domains. In addition to the standard Bilingual Lex-icon Induction (BLI) benchmark, we also investi-gate a downstream task, namely cross-lingual trans-fer for Natural Language Inference (NLI). In all setups tested, our algorithm significantly improves the performance of strong baselines. Finally, we provide an intuitive visualisation illustrating why ` 1 loss is more robust than it ` 2 counterpart when refining CLWEs (see Fig. 1). Our code is avail-able at [Cite] https://github.com/Pzoom522/L1-Refinement.",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.214_0_0,2021,Cross-Lingual Word Embedding Refinement by ` 1 Norm Optimisation,Body
2127,12132," http://www.netlib.org/ode/vode.f"," ['4 Experimental Setup', '4.3 Implementation Details of Algorithm 1']",A variable-coefficient ordinary differential equation (VODE) solver [Cite_Footnote_3] was implemented for the system described in Eq.,3 http://www.netlib.org/ode/vode.f,"The CSLS scheme with a neighbourhood size of 10 (CSLS-10) is adopted to build synthetic dictio-naries via the input CLWEs. A variable-coefficient ordinary differential equation (VODE) solver [Cite_Footnote_3] was implemented for the system described in Eq. (7). Suggested by Trendafilov (2003), we set the maxi-mum order at 15, the smoothness coefficient α in Eq. (5) at 1e8, the threshold in Eq. (8) at 1e-5, and performed the integration with a fixed time interval of 1e-6. An early-stopping design was adopted to ensure computation completed in a reasonable time: in addition to the two default stopping criteria in § 3, integration is terminated if R dt reaches 5e-3 (dt is the differentiation term in Eq. (7)).",Method,Tool,True,Produce（引用目的）,True,2021.naacl-main.214_1_0,2021,Cross-Lingual Word Embedding Refinement by ` 1 Norm Optimisation,Footnote
2128,12133," https://git.io/en-eo-dict-issue"," ['5 Results', '5.1 Bilingual Lexicon Induction']","1a fol-lows the main setup of Lample et al. (2018), who tested six language pairs using Wiki-Embs [Cite_Footnote_4] .","4 Note that we are unable to report the result of English to Esperanto as the corresponding dictionary is missing, see https://git.io/en-eo-dict-issue.","Refining unsupervised baselines. Tab. 1a fol-lows the main setup of Lample et al. (2018), who tested six language pairs using Wiki-Embs [Cite_Footnote_4] . Af-ter ` 1 refinement, M USE -` 1 , JA-M USE -` 1 , and V EC M AP -` 1 all significantly (p < 0.01) outper-form their corresponding base algorithms, with an average 1.1% performance gain over M USE , 1.1% over JA-M USE , and 0.5% over V EC M AP . To put these improvements in context, Heyman et al. (2019) reported an improvement of 0.4% for V EC M AP on same dataset and language pairs.",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-main.214_2_0,2021,Cross-Lingual Word Embedding Refinement by ` 1 Norm Optimisation,Footnote
2129,12134," https://stackoverflow.com/"," ['1 Introduction']","In this example, Answer [Cite_Footnote_1] is a good answer, be-cause it provides helpful information, e.g., “check it to the traffic dept”.",1 https://stackoverflow.com/,"A typical example for CQA is shown in Table 1. In this example, Answer [Cite_Footnote_1] is a good answer, be-cause it provides helpful information, e.g., “check it to the traffic dept”. Although Answer 2 is rele-vant to the question, it does not contain any useful information so that it should be regarded as a bad answer.",Material,Knowledge,False,Introduce（引用目的）,False,P18-1162_0_0,2018,Question Condensing Networks for Answer Selection in Community Question Answering,Footnote
2130,12136," https://github.com/pku-wuwei/QCN"," ['1 Introduction']","• Our proposed Question Condensing Net-works (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effec-tiveness of our model. [Cite_Footnote_3]",3 An implementation of our model is available at https: //github.com/pku-wuwei/QCN.,"• Our proposed Question Condensing Net-works (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effec-tiveness of our model. [Cite_Footnote_3]",Method,Tool,True,Produce（引用目的）,True,P18-1162_2_0,2018,Question Condensing Networks for Answer Selection in Community Question Answering,Footnote
2131,12137," http://alt.qcri.org/semeval2015/task3/index.php?id=data-and-tools"," ['3 Proposed Model', '3.1 Word-Level Embedding']","Word-level embeddings are composed of two components: GloVe (Pennington et al., 2014) word vectors trained on the domain-specific unan-notated corpus provided by the task [Cite_Footnote_4] , and con-volutional neural network-based character embed-dings which are similar to (Kim et al., 2016).",4 http://alt.qcri.org/semeval2015/task3/index.php?id=data-and-tools,"Word-level embeddings are composed of two components: GloVe (Pennington et al., 2014) word vectors trained on the domain-specific unan-notated corpus provided by the task [Cite_Footnote_4] , and con-volutional neural network-based character embed-dings which are similar to (Kim et al., 2016). Web text in CQA forums differs largely from normal-ized text in terms of spelling and grammar, so specifically trained GloVe vectors can model word interactions more precisely. Character embedding has proven to be very useful for out-of-vocabulary (OOV) words, so it is especially suitable for noisy web text in CQA.",Method,Tool,True,Produce（引用目的）,False,P18-1162_3_0,2018,Question Condensing Networks for Answer Selection in Community Question Answering,Footnote
2132,12138," http://www.qatarliving.com/forum"," ['4 Experimental Setup', '4.1 Datasets']","The corpora contain data from the QatarLiving forum [Cite_Footnote_5] , and are pub-licly available on the task website.",5 http://www.qatarliving.com/forum,"We use two community question answering datasets from SemEval (Nakov et al., 2015, 2017) to evaluate our model. The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [Cite_Footnote_5] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description. There are also some metadata associated with them, e.g., user ID, date of posting, and the question category. We do not use the metadata because they failed to boost performance in our model. Since the SemEval 2017 dataset is an updated version of SemEval 2016 6 , and shares the same evaluation metrics with SemEval 2016, we choose to use the SemEval 2017 dataset for evaluation.",Material,DataSource,True,Use（引用目的）,True,P18-1162_4_0,2018,Question Condensing Networks for Answer Selection in Community Question Answering,Footnote
2133,12139," http://ilk.uvt.nl/sabine/chunklink/"," ['2 Unsupervised Relation Extraction Problem', '2.1 Context Vector and Feature Design']",1 Software available at [Cite] http://ilk.uvt.nl/sabine/chunklink/,,1 Software available at [Cite] http://ilk.uvt.nl/sabine/chunklink/,Method,Tool,True,Produce（引用目的）,False,P06-2012_0_0,2006,Unsupervised Relation Disambiguation Using Spectral Clustering,Body
2134,12140," http://alpage.inria.fr/~sagot/lefff-en.html"," ['2 Methods']","Word labels are taken from the Lefff French morphological lexicon (Sagot, 2010) [Cite_Footnote_3] .",3 Lexique des Formes Fléchies du Français: http://alpage.inria.fr/˜sagot/lefff-en.html,"Morphological classification. The continuous word representations are used to train a logis-tic regression classifier 2 for each morphological feature: gender and number for noun and adjec-tives; tense for verbs (with labels: present, fu-ture, imperfect, or simple past). Word labels are taken from the Lefff French morphological lexicon (Sagot, 2010) [Cite_Footnote_3] . To ensure a fair comparison be-tween context-independent and context-dependent embedding classification, words that are ambigu-ous with respect to a given feature are excluded from the respective classifier’s training and test data.",Material,Knowledge,True,Use（引用目的）,True,D18-1313_0_0,2018,The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation,Footnote
2135,12141," https://www.github.com/tzshi/bubble-parser-acl21"," ['Appendix B Implementation Details']",Our implementation ( [Cite] https://www.github.com/tzshi/bubble-parser-acl21 ) is based on PyTorch.,,Our implementation ( [Cite] https://www.github.com/tzshi/bubble-parser-acl21 ) is based on PyTorch.,Method,Tool,True,Produce（引用目的）,True,2021.acl-long.557_0_0,2021,Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction,Body
2136,12142," http://www.nlm.nih.gov/mesh"," ['1. Introduction']",Subtree of MeSH [Cite_Footnote_1] tree.,1 MeSH is available at http://www.nlm.nih.gov/mesh. MeSH 2003 was used in this research.,Node Number Terms C18.452.297 diabetes mellitus insulin-dependent diabetes C18.452.297.267 mellitus C18.452.297.267.960 wolfram syndrome Table 1. Subtree of MeSH [Cite_Footnote_1] tree. Node numbers represent hierarchical structure of terms,Material,Knowledge,True,Produce（引用目的）,True,P04-2001_0_0,2004,Determining the Specificity of Terms using Compositional and Con-textual Information,Footnote
2137,12143," http://www.nlm.nih.gov"," ['4. Experiment and Evaluation', '4.1. Evaluation']",A set of journal abstracts was extracted from MEDLINE [Cite_Footnote_2] database using the disease names as quires.,"2 MEDLINE is a database of biomedical articles serviced by National Library of Medicine, USA. (http://www.nlm.nih.gov)","A sub-tree of MeSH thesaurus is selected for ex-periment. “metabolic diseases(C18.452)” node is root of the subtree, and the subtree consists of 436 disease names which are target terms of specificity measuring. A set of journal abstracts was extracted from MEDLINE [Cite_Footnote_2] database using the disease names as quires. Therefore, all the abstracts are related to some of the disease names. The set consists of about 170,000 abstracts (20,000,000 words). The abstracts are analyzed using Conexor parser, and various statistics are extracted: 1) frequency, tf.idf of the disease names, 2) distribution of modifiers of the disease names, 3) frequency, tf.idf of unit words of the disease names.",補足資料,Website,True,Introduce（引用目的）,True,P04-2001_1_0,2004,Determining the Specificity of Terms using Compositional and Con-textual Information,Footnote
2138,12144," http://www.conexor.com"," ['2. Information for Term Specificity', '2.2. Contextual Information']","We describe three estimat-ency parser (Conexor, 2004) [Cite_Ref] to analyze the struc- ing methods of p(x k ) in following sections.",Conexor. 2004. Conexor Functional Dependency Grammar Parser. http://www.conexor.com,"T = {t k |1≤ k ≤ n} (3) the distribution of predicates which have the terms as arguments, and the distribution of modi- where t k is a term and n is total number of terms. fiers of the terms are contextual information. In next step, a discrete random variable X is de-other words. Contrary, domain specific terms don’t tend to be modified by other words, be- X ={x k |1≤ k ≤ n} (4) cause they have sufficient information in them- p(x k ) = Prob(X = x k ) selves (Caraballo, 1999B). Under this assumption, we use probabilistic distribution of modifiers as where x k is an event of a term t k occurs in corpus, contextual information. Because domain specific p(x k ) is the probability of event x k . The informa-terms, unlike general words, are rarely modified tion quantity, I(x k ), gained after observing the in corpus, it is important to collect statistically event x k , is defined by the logarithmic function. sufficient modifiers from given corpus. Therefore Finally I(x k ) is used as specificity value of t k as accurate text processing, such as syntactic pars- equation (5). ing, is needed to extract modifiers. As Cara-Spec(t k ) ≈ I(x k ) = −log p(x k ) (5) ballo’s work was for general words, they extracted only rightmost prenominals as context In equation (5), we can measure specificity of information. We use Conexor functional depend- t k , by estimating p(x k ). We describe three estimat-ency parser (Conexor, 2004) [Cite_Ref] to analyze the struc- ing methods of p(x k ) in following sections. ture of sentences. Among many dependency 3.1. Compositional Information based functions defined in Conexor parser, “attr” and “mod” functions are used to extract from analyzed structures. If a term or Method (Method 1) modifiers modifiers In this section, we describe a method using com-of the term do not occur in corpus, specificity of positional information introduced in section 2.1. the term can not be measured using contextual This method is divided into two steps: In the first information step, specificity values of all words are measured independently. In the second step, the specificity",Method,Tool,True,Introduce（引用目的）,True,P04-2001_2_0,2004,Determining the Specificity of Terms using Compositional and Con-textual Information,Reference
2139,12145," http://www.uni-weimar.de/medien/webis/corpora"," ['1 Introduction']",It will be made freely avail-able to other researchers. [Cite_Footnote_1],1 http://www.uni-weimar.de/medien/webis/corpora,"The contribution of this paper is three-fold: First, through distant supervision we acquire a large cor-pus with 28,689 argumentative text segments from the online debate portal idebate.org. The corpus covers 14 separate domains with strongly varying feature distributions. It will be made freely avail-able to other researchers. [Cite_Footnote_1] Second, we obtain a ro-bust classifier for argumentativeness, providing ev-idence that distant supervision does not only save money and time, but also benefits the effectiveness of cross-domain and cross-register argumentation mining. Third, we evaluate—for the first time— the robustness of several features in classifying ar-gumentativeness across domains and registers.",Material,Dataset,True,Produce（引用目的）,True,N16-1165_0_0,2016,Cross-Domain Mining of Argumentative Text through Distant Supervision,Footnote
2140,12146," http://www.uni-weimar.de/medien/webis/corpora"," ['3 Mining Argumentative Text through Distant Supervision', '3.2 The Webis-Debate-16 Corpus']",The Webis-Debate-16 corpus will be made freely available online. [Cite_Footnote_2],2 http://www.uni-weimar.de/medien/webis/corpora,"As a result of applying the defined mapping func-tions, we obtained a large argumentation mining corpus, called Webis-Debate-16. The corpus con-tains 28,689 text segments from the 14 themes of idebate.org (23,880 argumentative, 4809 non-argumentative). Each theme is assumed to represent one domain. Table 2 lists the distribution of docu-ments over the domains in the corpus. Regarding the number of annotated text segments, Webis-Debate-16 is the largest dataset published so far for argu-mentation mining. While our review corpus from (Wachsmuth et al., 2014b) is even larger, its anno-tations are restricted to sentiment-related argumen-tation. Table 3 compares Webis-Debate-16 to other real argumentation mining corpora, namely, the Es-says corpus (Stab and Gurevych, 2014a), the Web discourse corpus (Habernal and Gurevych, 2015), the European Court of Human Rights (ECHR) cor-pus (Palau and Moens, 2009), and the Araucaria cor-pus (Reed and Rowe, 2004). The Webis-Debate-16 corpus will be made freely available online. [Cite_Footnote_2]",Material,Dataset,True,Produce（引用目的）,True,N16-1165_1_0,2016,Cross-Domain Mining of Argumentative Text through Distant Supervision,Footnote
2141,12147," http://www.yelp.com/datasetchallenge"," ['6 Experiment', '6.1 Dataset and Experimental Setup']","We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs [Cite_Footnote_4] epochs for the restaurant domain and 5 epochs for the laptop domain.",4 http://www.yelp.com/dataset challenge,"For word vector initialization, we train word em-beddings with word2vec (Mikolov et al., 2013) on the Yelp Challenge dataset 4 for the restaurant do-main and on the Amazon reviews dataset 5 (McAuley et al., 2015) for the laptop domain. The Yelp dataset contains 2.2M restaurant reviews with 54K vocab-ulary size. For the Amazon reviews, we only ex-tracted the electronic domain that contains 1M re-views with 590K vocabulary size. We vary differ-ent dimensions for word embeddings and chose 300 for both domains. Empirical sensitivity studies on different dimensions of word embeddings are also conducted. Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007). Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs [Cite_Footnote_4] epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02. We also try with varying context window size, and use 3 for the laptop domain and for the restaurant domain, re-spectively. All parameters are chosen by cross vali-dation.",補足資料,Website,False,Introduce（引用目的）,False,D16-1059_0_0,2016,Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis,Footnote
2142,12148," http://jmcauley.ucsd.edu/data/amazon/links.html"," ['6 Experiment', '6.1 Dataset and Experimental Setup']","We also try with varying context window size, and use 3 for the laptop domain and [Cite_Footnote_5] for the restaurant domain, re-spectively.",5 http://jmcauley.ucsd.edu/data/amazon/links.html,"For word vector initialization, we train word em-beddings with word2vec (Mikolov et al., 2013) on the Yelp Challenge dataset 4 for the restaurant do-main and on the Amazon reviews dataset 5 (McAuley et al., 2015) for the laptop domain. The Yelp dataset contains 2.2M restaurant reviews with 54K vocab-ulary size. For the Amazon reviews, we only ex-tracted the electronic domain that contains 1M re-views with 590K vocabulary size. We vary differ-ent dimensions for word embeddings and chose 300 for both domains. Empirical sensitivity studies on different dimensions of word embeddings are also conducted. Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007). Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02. We also try with varying context window size, and use 3 for the laptop domain and [Cite_Footnote_5] for the restaurant domain, re-spectively. All parameters are chosen by cross vali-dation.",Material,DataSource,True,Use（引用目的）,True,D16-1059_1_0,2016,Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis,Footnote
2143,12149," http://www.chokkan.org/software/crfsuite/"," ['6 Experiment', '6.1 Dataset and Experimental Setup']","Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007) [Cite_Ref] .",Naoaki Okazaki. 2007. CRFsuite: a fast im-plementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.,"For word vector initialization, we train word em-beddings with word2vec (Mikolov et al., 2013) on the Yelp Challenge dataset 4 for the restaurant do-main and on the Amazon reviews dataset 5 (McAuley et al., 2015) for the laptop domain. The Yelp dataset contains 2.2M restaurant reviews with 54K vocab-ulary size. For the Amazon reviews, we only ex-tracted the electronic domain that contains 1M re-views with 590K vocabulary size. We vary differ-ent dimensions for word embeddings and chose 300 for both domains. Empirical sensitivity studies on different dimensions of word embeddings are also conducted. Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007) [Cite_Ref] . Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02. We also try with varying context window size, and use 3 for the laptop domain and for the restaurant domain, re-spectively. All parameters are chosen by cross vali-dation.",Method,Tool,True,Use（引用目的）,True,D16-1059_2_0,2016,Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis,Reference
2144,12150," https://github.com/berlino/tensor2struct-public"," ['∗ Equal contribution.']",Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [Cite_Footnote_1],1 Our implementations are available at https://github.com/berlino/tensor2struct-public.,• We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [Cite_Footnote_1],Method,Tool,True,Produce（引用目的）,True,2021.acl-long.258_0_0,2021,Meta-Learning to Compositionally Generalize,Footnote
2145,12151," https://github.com/fxsjy/jieba"," ['4 Experiments']",The first two corpora were preprocessed by using the python jieba segmenter [Cite_Footnote_1] for word seg-mentation and filtering.,1 https://github.com/fxsjy/jieba,"The statistics of the three corpora are shown in Table 1. The first two corpora were preprocessed by using the python jieba segmenter [Cite_Footnote_1] for word seg-mentation and filtering. The third corpus SemEval is in English and can be tokenized by white spaces. Stop words and words appeared only once or in less than two documents were removed to allevi-ate data sparsity. Next, TF-IDF (term frequency-inverse document frequency) was used to extract the features from text. TF-IDF is a numerical s-tatistic method that is designed to reflect how im-portant a word is to a document in a corpus. In our experiments, we set the dimension of each text representation to 2,000 according to the ranking of the TF-IDF weights with each dimension of term-frequency(TF) features. After that, the text rep-resentations are fed into the proposed INN-RER method.",Method,Tool,True,Use（引用目的）,True,D18-1379_0_0,2018,An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking,Footnote
2146,12152," http://accent.gmu.edu"," ['Abstract']","STAT was initially designed to provide sets of phonological speech patterns in the compari-sons of various English accents found in the Speech Accent Archive [Cite] http://accent.gmu.edu, but its scope and utility expand to matters of language assessment, phonetic training, foren-sic linguistics, and speech recognition.",,"The Speech Transcription Analysis Tool (STAT) is an open source tool for aligning and comparing two phonetically transcribed texts of human speech. The output analysis is a parameterized set of phonological differ-ences. These differences are based upon a se-lectable set of binary phonetic features such as [voice], [continuant], [high], etc. STAT was initially designed to provide sets of phonological speech patterns in the compari-sons of various English accents found in the Speech Accent Archive [Cite] http://accent.gmu.edu, but its scope and utility expand to matters of language assessment, phonetic training, foren-sic linguistics, and speech recognition.",補足資料,Media,True,Use（引用目的）,True,N09-5003_0_0,2009,STAT: Speech Transcription Analysis Tool,Body
2147,12153," http://accent.gmu.edu"," ['1 Introduction']",It is also part of the research program behind the Speech Accent Archive ( [Cite] http://accent.gmu.edu) housed at George Mason University.,,"The theoretical and practical value of studying human accented speech is of interest to language teachers, linguists, and computational linguists. It is also part of the research program behind the Speech Accent Archive ( [Cite] http://accent.gmu.edu) housed at George Mason University. The Archive is a growing database of English speech varieties that contains more than 1,100 samples of native and non-native speakers reading from the same English paragraph. The non-native speakers of English come from more than 250 language back-grounds and include a variety of different levels of English speech abilities. The native samples dem-onstrate the various dialects of English speech from around the world. All samples include pho-netic transcriptions, phonological generalizations, demographic and geographic information. For comparison purposes, the Archive also includes phonetic sound inventories from more than 200 world languages so that researchers can perform various contrastive analyses and accented speech studies.",補足資料,Media,True,Introduce（引用目的）,True,N09-5003_1_0,2009,STAT: Speech Transcription Analysis Tool,Body
2148,12154," https://github.com/complementizer/news-tls"," ['References']",The dataset will be made available at [Cite] https://github.com/complementizer/news-tls.,,"Previous work on automatic news timeline summarization (TLS) leaves an unclear pic-ture about how this task can generally be ap-proached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection and date sum-marization, and to the previous lack of appro-priate evaluation metrics for the full TLS task. In this paper, we compare different TLS strate-gies using appropriate evaluation frameworks, and propose a simple and effective combina-tion of methods that improves over the state-of-the-art on all tested benchmarks. For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets. The dataset will be made available at [Cite] https://github.com/complementizer/news-tls.",Material,Dataset,True,Produce（引用目的）,True,2020.acl-main.122_0_0,2020,Examining the State-of-the-Art in News Timeline Summarization,Body
2149,12155," https://github.com/HeidelTime/heideltime"," ['3 Strategies for Timeline Summarization', 'Defining the Set of Dates']","We use the tool Hei-delTime [Cite_Footnote_2] (Strötgen and Gertz, 2013) to detect and resolve textual mentions of dates.",2 https://github.com/HeidelTime/ heideltime,"First, we identify the set of possible dates to in-clude in a timeline. We obtain these from (i) the publication dates of all articles in A and (ii) tex-tual references of dates in sentences in A, such as ’last Monday’, or ’12 April’. We use the tool Hei-delTime [Cite_Footnote_2] (Strötgen and Gertz, 2013) to detect and resolve textual mentions of dates. Date Selection Next, we select the l most important dates. We compare the following date selection methods in-troduced by Tran et al. (2013a):",Method,Tool,True,Use（引用目的）,True,2020.acl-main.122_1_0,2020,Examining the State-of-the-Art in News Timeline Summarization,Footnote
2150,12156," https://github.com/GuyAllard/markov_clustering"," ['3 Strategies for Timeline Summarization', 'Clustering']",We run MCL on this graph and obtain clusters by iden-tifying the connected components in the resulting connectivity matrix [Cite_Footnote_4] .,4 We use the implementation and default parameters from https://github.com/GuyAllard/markov_ clustering,"The edge weight is set to the cosine similarity be-tween the TF-IDF bag-of-words vectors of a 1 and a 2 . The constraint on the publication dates ensures that clusters do not have temporal gaps. Further-more, it reduces the number of similarity compu-tations between pairs of articles considerably. We run MCL on this graph and obtain clusters by iden-tifying the connected components in the resulting connectivity matrix [Cite_Footnote_4] .",Method,Code,True,Use（引用目的）,True,2020.acl-main.122_2_0,2020,Examining the State-of-the-Art in News Timeline Summarization,Footnote
2151,12157," http://edition.cnn.com/specials/world/fast-facts"," ['4 Dataset']","We obtain ground-truth timelines from CNN Fast Facts [Cite_Footnote_6] , which has a collection of several hundred timelines grouped in categories, e.g., ‘people’ or ‘disasters’.",6 http://edition.cnn.com/specials/world/fast-facts,"Ground-Truth Timelines: We obtain ground-truth timelines from CNN Fast Facts [Cite_Footnote_6] , which has a collection of several hundred timelines grouped in categories, e.g., ‘people’ or ‘disasters’. We pick all timelines of the ‘people’ category and a small number from other categories.",Material,Knowledge,True,Use（引用目的）,True,2020.acl-main.122_3_0,2020,Examining the State-of-the-Art in News Timeline Summarization,Footnote
2152,12158," http://open-platform.theguardian.com/"," ['4 Dataset']","Input Articles: For each entity from the ground-truth timelines, we search for news articles using TheGuardian API [Cite_Footnote_7] .",7 http://open-platform.theguardian.com/,"Input Articles: For each entity from the ground-truth timelines, we search for news articles using TheGuardian API [Cite_Footnote_7] . We use this source because it provides access to all published articles starting from 1999. We search for articles that have exact matches of the queries in the article body. The timespan for the article search is set so that it ex-tends the ground-truth timeline by 10% of its days before its first and after its last date.",補足資料,Website,True,Use（引用目的）,True,2020.acl-main.122_4_0,2020,Examining the State-of-the-Art in News Timeline Summarization,Footnote
2153,12159," https://github.com/jayelm/lsl"," ['Reproducibility']","Code, data, and experiments are available at [Cite] https: //github.com/jayelm/lsl and on CodaLab at https://bit.ly/lsl_acl20.",,"Code, data, and experiments are available at [Cite] https: //github.com/jayelm/lsl and on CodaLab at https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f θ . Like Andreas et al. (2018), f θ starts with fea-tures extracted from the last convolutional layer of a fixed ImageNet-pretrained VGG-19 network (Simonyan and Zisserman, 2015). These 4608-d embeddings are then fed into two fully connected layers ∈ R 4608×512 , R 512×512 with one ReLU non-linearity in between.",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.436_0_0,2020,Shaping Visual Representations with Language for Few-Shot Classification,Body
2154,12160," https://bit.ly/lsl_acl20"," ['Reproducibility']","Code, data, and experiments are available at https: //github.com/jayelm/lsl and on CodaLab at [Cite] https://bit.ly/lsl_acl20.",,"Code, data, and experiments are available at https: //github.com/jayelm/lsl and on CodaLab at [Cite] https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f θ . Like Andreas et al. (2018), f θ starts with fea-tures extracted from the last convolutional layer of a fixed ImageNet-pretrained VGG-19 network (Simonyan and Zisserman, 2015). These 4608-d embeddings are then fed into two fully connected layers ∈ R 4608×512 , R 512×512 with one ReLU non-linearity in between.",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.436_1_0,2020,Shaping Visual Representations with Language for Few-Shot Classification,Body
2155,12161," http://www.cis.upenn.edu/~treebank/tokenizer.sed"," ['5 Real World Stream: Twitter', '5.1 Setup']",Text was then tokenized according to a modified version of the Penn TreeBank tokenization standard [Cite_Footnote_6] that was less English-centric.,6 Such as codified in http://www.cis.upenn.edu/˜treebank/tokenizer.sed,"Content was lowercased, then processed by regu-lar expression to collapse the following into respec-tive single symbols: emoticons; URLs; usernames (@mentions); and hashtags. Any content deemed to be a retweet (following the characters RT) was removed. Text was then tokenized according to a modified version of the Penn TreeBank tokenization standard [Cite_Footnote_6] that was less English-centric.",補足資料,Document,True,Use（引用目的）,True,D12-1005_0_0,2012,Streaming Analysis of Discourse Participants,Footnote
2156,12162," https://github.com/thunlp/RSN"," ['References']",Our code is available at [Cite] https://github.com/thunlp/RSN.,,"Open relation extraction (OpenRE) aims to extract relational facts from the open-domain corpus. To this end, it discovers relation pat-terns between named entities and then clusters those semantically equivalent patterns into a united relation cluster. Most OpenRE meth-ods typically confine themselves to unsuper-vised paradigms, without taking advantage of existing relational facts in knowledge bases (KBs) and their high-quality labeled instances. To address this issue, we propose Relational Siamese Networks (RSNs) to learn similar-ity metrics of relations from labeled data of pre-defined relations, and then transfer the relational knowledge to identify novel rela-tions in unlabeled data. Experiment results on two real-world datasets show that our frame-work can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at [Cite] https://github.com/thunlp/RSN.",Method,Code,True,Produce（引用目的）,True,D19-1021_0_0,2019,Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervised Data,Body
2157,12163," http://www.wikipedia.org/"," ['2 The Structural Semantic Relatedness Measure', '2.1 Knowledge Sources']","Wikipedia [Cite_Footnote_1] , a large-scale online encyc-lopedia, its English version includes more than 3,000,000 concepts and new articles are added quickly and up-to-date.",1 http://www.wikipedia.org/,"1. Wikipedia [Cite_Footnote_1] , a large-scale online encyc-lopedia, its English version includes more than 3,000,000 concepts and new articles are added quickly and up-to-date. Wikipedia contains rich semantic knowledge in the form of hyperlinks between Wikipedia articles, such as Polysemy (disambiguation pages), Synonym (redirect pages) and Associative relation (hyperlinks between Wikipedia articles). In this paper, we extract the semantic relatedness sr between Wikipedia con-cepts using the method described in Milne and Witten(2008): where a and b are the two concepts of interest, A and B are the sets of all the concepts that are re-spectively linked to a and b, and W is the entire Wikipedia. For demonstration, we show the se-mantic relatedness between four selected con-cepts in Table 1.",Material,Dataset,True,Introduce（引用目的）,True,P10-1006_0_0,2010,Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation,Footnote
2158,12164," http://wordnet.princeton.edu/"," ['2 The Structural Semantic Relatedness Measure', '2.1 Knowledge Sources']","In this paper, given a NE Co-occurrence Corpus D, the social relatedness scr between two named entities ne 1 and ne 2 is measured using the Google Similarity Distance (Cilibrasi and Vitanyi, 2007): where D 1 and D 2 are the document sets corres-pondingly containing ne 1 and ne [Cite_Footnote_2] .",2 http://wordnet.princeton.edu/,"3. NE Co-occurrence Corpus, a corpus of documents for capturing the social relatedness between named entities. According to the fuzzy set theory (Baeza-Yates et al., 1999), the degree of named entities co-occurrence in a corpus is a measure of the relatedness between them. For example, in Google search results, the “Chicago Bulls” co-occurs with “NBA” in more than 7,900,000 web pages, while only co-occurs with “EMNLP” in less than 1,000 web pages. So the co-occurrence statistics can be used to measure the social relatedness between named entities. In this paper, given a NE Co-occurrence Corpus D, the social relatedness scr between two named entities ne 1 and ne 2 is measured using the Google Similarity Distance (Cilibrasi and Vitanyi, 2007): where D 1 and D 2 are the document sets corres-pondingly containing ne 1 and ne [Cite_Footnote_2] . An example of social relatedness is shown in Table 3, which is computed using the Web corpus through Google.",Material,DataSource,True,Use（引用目的）,True,P10-1006_1_0,2010,Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation,Footnote
2159,12165," http://www.ldc.org"," ['1 Introduction']",The creation of the Arabic Treebank (ATB) fa-cilitates corpus based studies of many interesting linguistic phenomena in Modern Standard Arabic (MSA). [Cite_Footnote_1],1 http://www.ldc.org,"The creation of the Arabic Treebank (ATB) fa-cilitates corpus based studies of many interesting linguistic phenomena in Modern Standard Arabic (MSA). [Cite_Footnote_1] The ATB comprises manually annotated morphological and syntactic analyses of newswire text from different Arabic sources. We exploit the ATB for the novel task of automatically creating lex-ical semantic verb classes for MSA. We are inter-ested in the problem of classifying verbs in MSA into groups that share semantic elements of mean-ing as they exhibit similar syntactic behavior. This manner of classifying verbs in a language is mainly advocated by Levin (1993). The Levin Hypothesis (LH) contends that verbs that exhibit similar syn-tactic behavior share element(s) of meaning. There exists a relatively extensive classification of English verbs according to different syntactic alternations, and numerous linguistic studies of other languages illustrate that LH holds cross linguistically, in spite of variations in the verb class assignment (Guerssel et al., 1985).",補足資料,Website,True,Introduce（引用目的）,False,N06-2039_0_0,2006,Unsupervised Induction of Modern Standard Arabic Verb Classes,Footnote
2160,12166," http://www.ldc.org"," ['4 Features']",Verb pattern The ATB includes morphological analyses for each verb resulting from the Buckwal-ter [Cite_Footnote_2] analyzer.,2 http://www.ldc.org,"Verb pattern The ATB includes morphological analyses for each verb resulting from the Buckwal-ter [Cite_Footnote_2] analyzer. Semitic languages such as Arabic have a rich templatic morphology, and this analy-sis includes the root and pattern information of each verb. This feature is of particular scientific interest because it is unique to the Semitic languages, and has an interesting potential correlation with argu-ment structure.",Method,Tool,True,Use（引用目的）,True,N06-2039_1_0,2006,Unsupervised Induction of Modern Standard Arabic Verb Classes,Footnote
2161,12167," https://github.com/clinc/oos-eval"," ['4 Experimental Settings', '4.1 Dataset: Multi-Domain Intent Detection']","We use a recently-released dataset, CLINC150, [Cite_Footnote_3] for multi-domain intent detection (Larson et al., 2019).",3 https://github.com/clinc/oos-eval.,"We use a recently-released dataset, CLINC150, [Cite_Footnote_3] for multi-domain intent detection (Larson et al., 2019). The CLINC150 dataset defines 150 types of intents in total (i.e., N = 150), where there are 10 different domains and 15 intents for each of them. Table 2 shows the dataset statistics.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.411_0_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2162,12168," https://github.com/huggingface/"," ['4 Experimental Settings', '4.3 Model Training and Configurations']","We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [Cite_Footnote_5] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments.",5 We use https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.,"We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [Cite_Footnote_5] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments. We combine three NLI datasets, SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque et al., 2011) from the GLUE benchmark (Wang et al., 2018) to pre-train our proposed model.",Method,Tool,False,Use（引用目的）,True,2020.emnlp-main.411_1_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2163,12169," https://github.com/UKPLab/sentence-transformers"," ['4 Experimental Settings', '4.3 Model Training and Configurations']","We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [Cite_Footnote_5] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments.",5 We use https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.,"We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [Cite_Footnote_5] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments. We combine three NLI datasets, SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque et al., 2011) from the GLUE benchmark (Wang et al., 2018) to pre-train our proposed model.",Method,Tool,False,Use（引用目的）,True,2020.emnlp-main.411_2_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2164,12170," https://github.com/connorbrinton/polyai-models/releases/tag/v1.0"," ['4 Experimental Settings', '4.3 Model Training and Configurations']",We modified their original code [Cite_Footnote_6] to apply the uncertainty-based OOS detection.,6 https://github.com/connorbrinton/polyai-models/releases/tag/v1.0.,"• Non-BERT classifier: We also test a state-of-the-art fast embedding-based classifier, “USE+ConveRT” (Henderson et al., 2019; Casanueva et al., 2020), in the “all domains” setting. Casanueva et al. (2020) showed that the “USE+ConveRT” outperformed a BERT classifier on the CLINC150 dataset, while it was not evaluated along with the OOS detec-tion task. We modified their original code [Cite_Footnote_6] to apply the uncertainty-based OOS detection.",Method,Code,True,Extend（引用目的）,True,2020.emnlp-main.411_3_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2165,12171," https://github.com/salesforce/DNNC-few-shot-intent"," ['4 Experimental Settings', '4.3 Model Training and Configurations']","• Proposed method: [Cite_Footnote_8] “DNNC” is our pro-posed method, and “DNNC-scratch” is with-out the NLI pre-training in Section 3.3.",8 Our code is available at https://github.com/salesforce/DNNC-few-shot-intent.,"• Proposed method: [Cite_Footnote_8] “DNNC” is our pro-posed method, and “DNNC-scratch” is with-out the NLI pre-training in Section 3.3. “DNNC-joint” is our joint approach on top of top-k retrieval by Emb-kNN (Section 3.4).",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.411_4_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2166,12172," https://github.com/clinc/oos-eval"," ['References']","Appendix A Training Details Dataset preparation To use the CLINC150 dataset (Larson et al., 2019) [Cite_Footnote_9] in our ways, espe-cially for the single-domain experiments, we pro-vide preprocessing scrips accompanied with our code.",9 https://github.com/clinc/oos-eval.,"This work is supported in part by NSF under grants III-1763325, III-1909323, and SaTC-1930941. We thank Huan Wang, Wenpeng Yin for their insightful discussions, and the anonymous reviewers for their helpful and thoughtful comments. We also thank Jin Qu, Tian Xie, Xinyi Yang, and Yingbo Zhou for their support in the deployment of DNNC into the internal system. Appendix A Training Details Dataset preparation To use the CLINC150 dataset (Larson et al., 2019) [Cite_Footnote_9] in our ways, espe-cially for the single-domain experiments, we pro-vide preprocessing scrips accompanied with our code.",Material,Dataset,True,Extend（引用目的）,False,2020.emnlp-main.411_5_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2167,12173," https://github.com/huggingface/"," ['References']","For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [Cite_Footnote_10]",10 https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.,"General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [Cite_Footnote_10] The example code to train the NLI-style models is also available. We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs.",Method,Tool,False,Use（引用目的）,False,2020.emnlp-main.411_6_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2168,12174," https://github.com/UKPLab/sentence-transformers"," ['References']","For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [Cite_Footnote_10]",10 https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.,"General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [Cite_Footnote_10] The example code to train the NLI-style models is also available. We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs.",Method,Tool,False,Use（引用目的）,False,2020.emnlp-main.411_7_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2169,12175," https://github.com/huggingface/transformers/tree/master/examples/text-classification"," ['References']",The example code to train the NLI-style models is also available. [Cite_Footnote_11],11 https://github.com/huggingface/transformers/tree/master/examples/text-classification.,"General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. The example code to train the NLI-style models is also available. [Cite_Footnote_11] We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs.",Method,Code,True,Produce（引用目的）,False,2020.emnlp-main.411_8_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2170,12176," https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json"," ['References']",We use the roberta-base configuration [Cite_Footnote_12] for all the RoBERTa/SRoBERTa-based models in our experiments.,12 https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json.,"General training This section describes the de-tails about the model training in Section 4.3. For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. The example code to train the NLI-style models is also available. We use the roberta-base configuration [Cite_Footnote_12] for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters. We use a gradient clipping technique (Pascanu et al., 2013) with a clipping value of 1.0, and also use a linear warmup learning-rate scheduling with a proportion of 0.1 with respect to the maximum number of training epochs.",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.411_9_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2171,12177," https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py"," ['References']","For all the RoBERTa-based models, we used the RoBERTa roberta-base’s tokenizer provided in the transformers library. [Cite_Footnote_13]",13 https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py.,"Text pre-processing For all the RoBERTa-based models, we used the RoBERTa roberta-base’s tokenizer provided in the transformers library. [Cite_Footnote_13] We did not perform any additional pre-processing in our experiments.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.411_10_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2172,12178," https://github.com/jasonwei20/eda_nlp"," ['B Data Augmentation']",We follow the publicly available code. [Cite_Footnote_14],14 https://github.com/jasonwei20/eda_nlp.,"EDA Classifier-EDA uses the following four data augmentation techniques in Wei and Zou (2019): synonym replacement, random insertion, random swap, and random deletion. We follow the publicly available code. [Cite_Footnote_14] For every training example, we empirically set one augmentation based on every technique. We apply each technique separately to the original sentence and therefore every training example will have four augmentations. The proba-bility of a word in an utterance being edited is set to 0.1 for all the techniques.",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.411_11_0,2020,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,Footnote
2173,12179," http://www.speech.bbn.com/ears"," ['4 Results', '4.2 Top Rules Learned']","All systems were evaluated using rteval (Rich Tran-scription Evaluation) version 2.3 (Kubala and Srivastava, 2003) [Cite_Ref] .",Francis Kubala and Amit Srivastava. 2003. A Framework for Evaluating Rich Transcription Technology. BBN Ears Web-site. http://www.speech.bbn.com/ears.,"All systems were evaluated using rteval (Rich Tran-scription Evaluation) version 2.3 (Kubala and Srivastava, 2003) [Cite_Ref] . Rteval aligns the system output to the annotated reference transcripts in such a way as to minimize the lex-eme error rate. The error rate is the number of disfluency errors (insertions and deletions) divided by the number of disfluent tokens in the reference transcript. Edit and filler errors are calculated separately. The results of the evalu-ation are shown in Table 1. Most of the small differences in the CTS results were not found to be significantly dif-ferent.",補足資料,Paper,True,Introduce（引用目的）,True,N04-4040_0_0,2004,A Lexically-Driven Algorithm for Disfluency Detection,Reference
2174,12180," http://www.darpa.muk/iao/EARS.htm"," ['2 EARS Disfluency Annotation']","One of the major goals of the DARPA program for Effective, Affordable, Reusable Speech-to-Text (EARS) (Wayne, 2003) [Cite_Ref] is to provide a rich transcription of speech recognition output, including speaker identification, sen-tence boundary detection and the annotation of disfluen-cies in the transcript (This collection of additional fea-tures is also known as Metadata).","Charles Wayne. 2003. Effective, Affordable, Reusable Speech-to-Text (EARS). Official web site for DARPA/EARS Pro-gram. http://www.darpa.muk/iao/EARS.htm.","One of the major goals of the DARPA program for Effective, Affordable, Reusable Speech-to-Text (EARS) (Wayne, 2003) [Cite_Ref] is to provide a rich transcription of speech recognition output, including speaker identification, sen-tence boundary detection and the annotation of disfluen-cies in the transcript (This collection of additional fea-tures is also known as Metadata). One result of this pro-gram has been production of an annotation specification for disfluencies in speech transcripts and the transcription of sizable amounts of speech data, both from conversa-tional telephone speech and broadcast news, according to this specification (Strassel, 2003).",補足資料,Paper,True,Introduce（引用目的）,True,N04-4040_1_0,2004,A Lexically-Driven Algorithm for Disfluency Detection,Reference
2175,12181," https://github.com/princeton-nlp/LM-BFF"," ['References']","Our approach makes minimal assumptions on task resources and do-main expertise, and hence constitutes a strong task-agnostic method for few-shot learning. [Cite_Footnote_2]",2 Our implementation is publicly available at https://github.com/princeton-nlp/LM-BFF.,"The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot perfor-mance solely by leveraging a natural-language prompt and a few task demonstrations as in-put context. Inspired by their findings, we study few-shot learning in a more practical sce-nario, where we use smaller language models for which fine-tuning is computationally effi-cient. We present LM-BFF—better few-shot fine-tuning of language models —a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt genera-tion; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a sys-tematic evaluation for analyzing few-shot per-formance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dra-matically outperform standard fine-tuning pro-cedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and do-main expertise, and hence constitutes a strong task-agnostic method for few-shot learning. [Cite_Footnote_2]",Method,Code,True,Produce（引用目的）,True,2021.acl-long.295_0_0,2021,Making Pre-trained Language Models Better Few-shot Learners,Footnote
2176,12182," https://www.quora.com/q/quoradata/"," ['B Datasets']","For SNLI (Bowman et al., 2015) and datasets from GLUE (Wang et al., 2019), including SST-2 (Socher et al., 2013), CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), QNLI (Ra-jpurkar et al., 2016), RTE (Dagan et al., 2005; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MRPC (Dolan and Brock-ett, 2005), QQP [Cite_Footnote_12] and STS-B (Wang et al., 2019)0, we follow (Wang et al., 2019)1 and use their original development sets for testing.",12 https://www.quora.com/q/quoradata/,"For SNLI (Bowman et al., 2015) and datasets from GLUE (Wang et al., 2019), including SST-2 (Socher et al., 2013), CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), QNLI (Ra-jpurkar et al., 2016), RTE (Dagan et al., 2005; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MRPC (Dolan and Brock-ett, 2005), QQP [Cite_Footnote_12] and STS-B (Cer et al., 2017), we follow Zhang et al. (2021) and use their original development sets for testing. For datasets which re-quire a cross-validation evaluation—MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), MPQA (Wiebe et al., 2005), Subj (Pang and Lee, 2004)—we sim-ply randomly sample 2,000 examples as the testing set and leave them out from training. For SST-5 (Socher et al., 2013) and TREC (Voorhees and Tice, 2000), we use their official test sets. We show dataset statistics in Table B.1.",Material,Dataset,True,Use（引用目的）,True,2021.acl-long.295_1_0,2021,Making Pre-trained Language Models Better Few-shot Learners,Footnote
2177,12183," https://github.com/UKPLab/sentence-transformers"," ['C Experimental Details', '-', 'C.3 Fine-tuning with demonstrations']","For selective demonstra-tions, we take roberta-large-nli-stsb mean-tokens [Cite_Footnote_14] from Reimers and Gurevych (2019) as our sentence embedding model.",14 https://github.com/UKPLab/ sentence-transformers,"When using demonstrations, we sample 16 dif-ferent sets of demonstrations for each input and average the predicted log probability for each class during inference. We find that further increasing the number of samples does not bring substantial improvement. Additional, we have tried different aggregation methods like taking the result with the maximum confidence and we did not find a meaningful improvement. For selective demonstra-tions, we take roberta-large-nli-stsb mean-tokens [Cite_Footnote_14] from Reimers and Gurevych (2019) as our sentence embedding model.",Material,Knowledge,False,Use（引用目的）,True,2021.acl-long.295_2_0,2021,Making Pre-trained Language Models Better Few-shot Learners,Footnote
2178,12184," http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html"," ['4 Experimental setup']","We also ran the Kurohashi-Nagao parser (KNP) 2.0 [Cite_Footnote_2] , a widely-used Japanese dependency parser to which Kurohashi and Nagao’s (1994) rule-based coordination analysis method is built in.",2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html,"We applied our improved model and Shimbo and Hara’s original model to the EDR corpus (EDR, 1995). We also ran the Kurohashi-Nagao parser (KNP) 2.0 [Cite_Footnote_2] , a widely-used Japanese dependency parser to which Kurohashi and Nagao’s (1994) rule-based coordination analysis method is built in. For comparison with KNP, we focus on bun-setsu-level coordinations. A bunsetsu is a chunk formed by a content word followed by zero or more non-content words like particles.",Method,Tool,True,Use（引用目的）,True,P09-2002_0_0,2009,Bypassed Alignment Graph for Learning Coordination in Japanese Sentences,Footnote
2179,12185," http://www2.nict.go.jp/r/r312/EDR/index.html"," ['4 Experimental setup']","We applied our improved model and Shimbo and Hara’s original model to the EDR corpus (EDR, 1995) [Cite_Ref] .","EDR, 1995. The EDR dictionary. NICT. http://www2.nict.go.jp/r/r312/EDR/index.html.","We applied our improved model and Shimbo and Hara’s original model to the EDR corpus (EDR, 1995) [Cite_Ref] . We also ran the Kurohashi-Nagao parser (KNP) 2.0 , a widely-used Japanese dependency parser to which Kurohashi and Nagao’s (1994) rule-based coordination analysis method is built in. For comparison with KNP, we focus on bun-setsu-level coordinations. A bunsetsu is a chunk formed by a content word followed by zero or more non-content words like particles.",Material,Knowledge,True,Use（引用目的）,True,P09-2002_1_0,2009,Bypassed Alignment Graph for Learning Coordination in Japanese Sentences,Reference
2180,12186," http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus/KyotoCorpus4.0/doc/synguideline.pdf"," ['4 Experimental setup', '4.2 Evaluation metrics']","KNP outputs dependency structures in Kyoto Cor-pus format (Kurohashi et al., 2000) [Cite_Ref] which spec-ifies the end of coordinating conjuncts (bunsetsu sequences) but not their beginning.","S. Kurohashi, Y. Igura, and M. Sakaguchi, 2000. An-notation manual for a morphologically and sytac-tically tagged corpus, Ver. 1.8. Kyoto Univ. In Japanese. http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus/KyotoCorpus4.0/doc/synguideline.pdf.","KNP outputs dependency structures in Kyoto Cor-pus format (Kurohashi et al., 2000) [Cite_Ref] which spec-ifies the end of coordinating conjuncts (bunsetsu sequences) but not their beginning.",補足資料,Document,True,Introduce（引用目的）,True,P09-2002_2_0,2009,Bypassed Alignment Graph for Learning Coordination in Japanese Sentences,Reference
2181,12187," http://isw3.naist.jp/IS/TechReport/report-list.html#2009"," ['2 Alignment-based coordinate structure analysis', '2.2 Features']","For the description of features used in our adap-tation of the Shimbo-Hara model to Japanese, see (Okuma et al., 2009) [Cite_Ref] .","H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.","For the description of features used in our adap-tation of the Shimbo-Hara model to Japanese, see (Okuma et al., 2009) [Cite_Ref] . In this model, all features are defined as indicator functions asking whether one or more attributes (e.g., surface form, part-of-speech) take specific values at the neighbor of an arc. One example of a feature assigned to a diag-onal arc at row i and column j of the alignment graph is where POS [i] denotes the part-of-speech of the ith word in a sentence.",補足資料,Paper,True,Introduce（引用目的）,True,P09-2002_3_0,2009,Bypassed Alignment Graph for Learning Coordination in Japanese Sentences,Reference
2182,12188," http://isw3.naist.jp/IS/TechReport/report-list.html#2009"," ['4 Experimental setup', '4.1 Dataset']","The detail of this procedure can be found in (Okuma et al., 2009) [Cite_Ref] .","H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.","A coordination is indicated by a specific relation of type “and” in the semantic frame. The scope of conjuncts (where a conjunct may be a word, or a series of words) can be obtained by combining this information with that of the syntactic tree. The detail of this procedure can be found in (Okuma et al., 2009) [Cite_Ref] .",補足資料,Paper,True,Introduce（引用目的）,True,P09-2002_3_1,2009,Bypassed Alignment Graph for Learning Coordination in Japanese Sentences,Reference
2183,12189," https://github.com/donnyslin/KCAT"," ['References']","In this paper, we in-troduce a Knowledge-Constraint Typing An-notation Tool (KCAT [Cite_Footnote_1] ), which is efficient for fine-grained entity typing annotation.",1 Code is available at https://github.com/donnyslin/KCAT,"Fine-grained Entity Typing is a tough task which suffers from noise samples extracted from distant supervision. Thousands of man-ually annotated samples can achieve greater performance than millions of samples gen-erated by the previous distant supervision method. Whereas, it’s hard for human be-ings to differentiate and memorize thousands of types, thus making large-scale human la-beling hardly possible. In this paper, we in-troduce a Knowledge-Constraint Typing An-notation Tool (KCAT [Cite_Footnote_1] ), which is efficient for fine-grained entity typing annotation. KCAT reduces the size of candidate types to an ac-ceptable range for human beings through en-tity linking and provides a Multi-step Typ-ing scheme to revise the entity linking result. Moreover, KCAT provides an efficient Anno-tator Client to accelerate the annotation pro-cess and a comprehensive Manager Module to analyse crowdsourcing annotations. Experi-ment shows that KCAT can significantly im-prove annotation efficiency, the time consump-tion increases slowly as the size of type set ex-pands.",Method,Tool,True,Introduce（引用目的）,True,P19-3017_0_0,2019,KCAT: A Knowledge-Constraint Typing Annotation Tool,Footnote
2184,12190," http://nlp.cs.rpi.edu/kbp/2018/EDL2018TaskSpec_V2.0.pdf"," ['5 Experiment']","Type hier-archy is extracted from following three datasets: (1) Conll 2003; (2) BBN(Ren et al., 2016a); (3) FIGER(Ling and Weld, 2012); and YAGO Knowl-edge Base(Heng et al., 2018) [Cite_Ref] .","Ji Heng, Sil Avirup, Trang Dang Hoa, J. Goldschen Alan, Duncan Jason, Getman Jeremy, Nothman Joel, Onyshkevych Boyan, Soboroff Ian, and Strassel Stephanie. 2018. Tac kbp2018 entity discovery and linking for 7,309 entity types. http://nlp.cs.rpi.edu/kbp/2018/EDL2018TaskSpec_V2.0.pdf.","In order to verify the efficiency of KCAT, we con-duct a mock annotation experiment. 100 sen-tences are extracted from the English dataset of Conll 2003 (Sang and Buchholz, 2000) as the cor-pus to be annotated. The entity mention spans in these sentences have been annotated. Type hier-archy is extracted from following three datasets: (1) Conll 2003; (2) BBN(Ren et al., 2016a); (3) FIGER(Ling and Weld, 2012); and YAGO Knowl-edge Base(Heng et al., 2018) [Cite_Ref] . The mappings be-tween entity and its related types are provided by these datasets. We have chosen two annotation modes: (a) without pre-linking, directly through top-down search or flatten search; (b) filtering out types that are inconsistent with entity types through entity linking.",補足資料,Paper,True,Introduce（引用目的）,True,P19-3017_1_0,2019,KCAT: A Knowledge-Constraint Typing Annotation Tool,Reference
2185,12191," http://nlp.cs.rpi.edu/kbp/2018/EDL2018TaskSpec_V2.0.pdf"," ['5 Experiment']","When there are thousands of types in Knowledge Base, such as YAGO Knowledge Base(Heng et al., 2018) [Cite_Ref] , which contains 7309 types, it’s impossible for human to annotate.","Ji Heng, Sil Avirup, Trang Dang Hoa, J. Goldschen Alan, Duncan Jason, Getman Jeremy, Nothman Joel, Onyshkevych Boyan, Soboroff Ian, and Strassel Stephanie. 2018. Tac kbp2018 entity discovery and linking for 7,309 entity types. http://nlp.cs.rpi.edu/kbp/2018/EDL2018TaskSpec_V2.0.pdf.","Annotation Efficiency. In Table 2, we compare the labeling time in aforementioned two modes and calculate the percentage of time saved with Entity Linking on different type set. It can be ob-served that with the number of types increases, time consumption increases slowly with entity linking, while without entity linking, time con-sumption increases exponentially. The percent-age of time saved also increases as the number of types expands on different datasets. When there are thousands of types in Knowledge Base, such as YAGO Knowledge Base(Heng et al., 2018) [Cite_Ref] , which contains 7309 types, it’s impossible for human to annotate.",補足資料,Paper,True,Introduce（引用目的）,True,P19-3017_1_1,2019,KCAT: A Knowledge-Constraint Typing Annotation Tool,Reference
2186,12192," https://translate.google.com/"," ['4 The Newsela Cross-Lingual Simplification Dataset', '4.1 Cross-Lingual Segment Alignment']","To leverage this align-ment flexibility, we apply MASSAlign to English articles and Spanish articles machine translated into English by Google translate. [Cite_Footnote_2]",2 https://translate.google.com/,"As a result, we adapt a monolingual text simpli-fication aligner for cross-lingual alignment. MAS-SAlign (Paetzold et al., 2017) is a Python li-brary designed to align segments of different length within comparable corpora of the same lan-guage. It employs a vicinity-driven search ap-proach, based on the assumption that the order in which information appears is roughly constant in simple and complex texts. A similarity ma-trix is created between the paragraphs/sentences of aligned documents/paragraphs using a standard bag-of-words TF-IDF model. It finds a starting point to begin the search for an alignment path, allowing long-distance alignment skips, capturing 1-N and N-1 alignments. To leverage this align-ment flexibility, we apply MASSAlign to English articles and Spanish articles machine translated into English by Google translate. [Cite_Footnote_2] An important property of Google translated articles is that they are aligned 1-1 at the sentence level. This lets us deterministically find the Spanish replacement for the aligned Google translated English version re-turned by MASSAlign. Translation quality is high for this language pair, and even noisy translated ar-ticles contain enough signal to construct the simi-larity matrix required by MASSAlign.",Method,Tool,True,Use（引用目的）,True,D19-1166_2_0,2019,Controlling Text Complexity in Neural Machine Translation,Footnote
2187,12193," https://github.com/cocoxu/simplification"," ['5 Experiment Settings', '5.1 Evaluation Metrics']","SARI (Xu et al., 2016) [Cite_Footnote_3] is designed to evalu-ate text simplification systems by comparing sys-tem output against references and against the input sentence.",3 https://github.com/cocoxu/ simplification,"SARI (Xu et al., 2016) [Cite_Footnote_3] is designed to evalu-ate text simplification systems by comparing sys-tem output against references and against the input sentence. It explicitly measures the goodness of words that are added, deleted and kept by the sys-tems. Xu et al. (2016) showed that BLEU shows high correlation with human scores for grammati-cality and meaning preservation and SARI shows high correlation with human scores for simplic-ity. In the cross-lingual setting, we cannot directly compare the Spanish input with English hypothe-ses and references, therefore we use the baseline machine translation of Spanish into English as a pseudo-source text. The resulting SARI score directly measures the improvement over baseline machine translation.",Method,Tool,True,Introduce（引用目的）,False,D19-1166_3_0,2019,Controlling Text Complexity in Neural Machine Translation,Footnote
2188,12194," https://github.com/mmautner/readability"," ['5 Experiment Settings', '5.1 Evaluation Metrics']","Here we estimate the reading grade level complexity of MT outputs and reference translations using the Automatic Read-ability Index (ARI) [Cite_Footnote_4] score, which combines evi-dence from the number of characters per word and number of words per sentence using hand-tuned weights (Senter and Smith, 1967):",4 https://github.com/mmautner/ readability,"In addition to BLEU and SARI, we report Pear-son’s correlation coefficient (PCC) to measure the strength of the linear relationship between the complexity of our system outputs and the com-plexity of reference translations. Heilman et al. (2008) use it to evaluate the performance of read-ing difficulty prediction. Here we estimate the reading grade level complexity of MT outputs and reference translations using the Automatic Read-ability Index (ARI) [Cite_Footnote_4] score, which combines evi-dence from the number of characters per word and number of words per sentence using hand-tuned weights (Senter and Smith, 1967):",Material,Knowledge,False,Use（引用目的）,True,D19-1166_4_0,2019,Controlling Text Complexity in Neural Machine Translation,Footnote
2189,12195," https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile"," ['1 Introduction']",Suggestion extraction can also be employed for the summarisation of dedicated suggestion fo-rums [Cite_Footnote_1] .,1 https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile,"Suggestion extraction can also be employed for the summarisation of dedicated suggestion fo-rums [Cite_Footnote_1] . People often provide the context in such posts, which gets repetitive over a large number of posts. Suggestion mining methods can identify the exact textual unit in the post where a suggestion is conveyed.",補足資料,Website,True,Introduce（引用目的）,True,P16-3018_0_0,2016,Suggestion Mining from Opinionated Text,Footnote
2190,12196," https://feedly.uservoice.com/forums/192636-suggestions"," ['3 Research Methodology', '3.2 RQ2: Suggestion Detection']","1) Customer posts from a publicly accessible suggestion forums for the products Feedly mobile app [Cite_Footnote_2] , and Windows App studio .",2 https://feedly.uservoice.com/forums/192636- suggestions,"1) Customer posts from a publicly accessible suggestion forums for the products Feedly mobile app [Cite_Footnote_2] , and Windows App studio . We crawled the suggestion for improvement posts for these products, and labeled only a subset of them due to the annotation costs. Although all the posts are about suggestions, they also comprise of explanatory and informative sentences around the suggestion sentences. With the availability of more annotation resources, this dataset can be easily extended. 2) We also prepared a new tweet dataset, where the tweets are first collected using the hashtags suggestion, advice, recommendation, warning, which appeared as top unigram features in our SVM based classification experiments. This sampling method increased the likelihood of the presence of suggestion tweets as compared to the Microsoft tweets dataset.",補足資料,Website,False,Introduce（引用目的）,False,P16-3018_1_0,2016,Suggestion Mining from Opinionated Text,Footnote
2191,12197," https://wpdev.uservoice.com/forums/110705-universal-windows-platform"," ['3 Research Methodology', '3.2 RQ2: Suggestion Detection']","1) Customer posts from a publicly accessible suggestion forums for the products Feedly mobile app , and Windows App studio [Cite_Footnote_3] .",3 https://wpdev.uservoice.com/forums/110705-universal- windows-platform,"1) Customer posts from a publicly accessible suggestion forums for the products Feedly mobile app , and Windows App studio [Cite_Footnote_3] . We crawled the suggestion for improvement posts for these products, and labeled only a subset of them due to the annotation costs. Although all the posts are about suggestions, they also comprise of explanatory and informative sentences around the suggestion sentences. With the availability of more annotation resources, this dataset can be easily extended. 2) We also prepared a new tweet dataset, where the tweets are first collected using the hashtags suggestion, advice, recommendation, warning, which appeared as top unigram features in our SVM based classification experiments. This sampling method increased the likelihood of the presence of suggestion tweets as compared to the Microsoft tweets dataset.",補足資料,Website,False,Introduce（引用目的）,False,P16-3018_2_0,2016,Suggestion Mining from Opinionated Text,Footnote
2192,12198," https://github.com/manuyavuz/TurkishMDSDataSet_alpha"," ['4 Experimental Setup', '4.1 Data Set']","In this study, we have collected and manually annotated a Turkish MDS data set, which is publicly available for future studies [Cite_Footnote_1] .",1 The data set can be retrieved from the following github repository: https://github.com/manuyavuz/ TurkishMDSDataSet_alpha,"One of the greatest challenges for MDS studies in Turk-ish is that there does not exist a manually annotated data set. In this study, we have collected and manually annotated a Turkish MDS data set, which is publicly available for future studies [Cite_Footnote_1] .",Material,Dataset,True,Produce（引用目的）,True,D14-1077_0_0,2014,Analyzing Stemming Approaches for Turkish Multi-Document Summarization,Footnote
2193,12199," https://github.com/chrishokamp/constrained_decoding"," ['3 Grid Beam Search']",The highest scoring hypothesis in the set of fin-ished hypotheses is the best sequence which cov-ers all constraints. [Cite_Footnote_1],1 Our implementation of GBS is available at https: //github.com/chrishokamp/constrained_ decoding,"The beams at the top level of the grid (beams where c = numConstraints) contain hypothe-ses which cover all of the constraints. Once a hy-pothesis on the top level generates the EOS token, it can be added to the set of finished hypotheses. The highest scoring hypothesis in the set of fin-ished hypotheses is the best sequence which cov-ers all constraints. [Cite_Footnote_1]",Method,Tool,False,Produce（引用目的）,True,P17-1141_0_0,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Footnote
2194,12200," http://aclweb.org/anthology/N/N16/N16-1148.pdf"," ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']",Pick-Revise is an interaction cycle for MT Post-Editing proposed by Cheng et al. (2016) [Cite_Ref] .,"Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240–1249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.","Pick-Revise is an interaction cycle for MT Post-Editing proposed by Cheng et al. (2016) [Cite_Ref] . Starting with the original translation hypothesis, a (sim-ulated) user first picks a part of the hypothesis which is incorrect, and then provides the correct translation for that portion of the output. The user-provided correction is then used as a constraint for the next decoding cycle. The Pick-Revise process can be repeated as many times as necessary, with a new constraint being added at each cycle.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1141_3_0,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Reference
2195,12201," http://aclweb.org/anthology/N/N16/N16-1148.pdf"," ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']","We modify the experiments of Cheng et al. (2016) [Cite_Ref] slightly, and assume that the user only pro-vides sequences of up to three words which are missing from the hypothesis.","Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240–1249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.","We modify the experiments of Cheng et al. (2016) [Cite_Ref] slightly, and assume that the user only pro-vides sequences of up to three words which are missing from the hypothesis. To simulate user interaction, at each iteration we chose a phrase of up to three tokens from the reference transla-tion which does not appear in the current MT hy-potheses. In the strict setting, the complete phrase must be missing from the hypothesis. In the re-laxed setting, only the first word must be missing. Table 1 shows results for a simulated editing ses-sion with four cycles. When a three-token phrase cannot be found, we backoff to two-token phrases, then to single tokens as constraints. If a hypoth-esis already matches the reference, no constraints are added. By specifying a new constraint of up to three words at each cycle, an increase of over 20 BLEU points is achieved in all language pairs.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1141_3_1,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Reference
2196,12202," http://aclweb.org/anthology/N/N16/N16-1148.pdf"," ['5 Related Work']","The Pick-Revise (PRIMT) (Cheng et al., 2016) [Cite_Ref] framework for Interactive Post Editing introduces the concept of edit cycles.","Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240–1249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.","Recently, some attention has also been given to SMT decoding with multiple lexical constraints. The Pick-Revise (PRIMT) (Cheng et al., 2016) [Cite_Ref] framework for Interactive Post Editing introduces the concept of edit cycles. Translators specify con-straints by editing a part of the MT output that is incorrect, and then asking the system for a new hypothesis, which must contain the user-provided correction. This process is repeated, maintain-ing constraints from previous iterations and adding new ones as needed. Importantly, their approach relies upon the phrase segmentation provided by the SMT system. The decoding algorithm can only make use of constraints that match phrase boundaries, because constraints are implemented as “rules” enforcing that source phrases must be translated as the aligned target phrases that have been selected as constraints. In contrast, our ap-proach decodes at the token level, and is not de-pendent upon any explicit structure in the underly-ing model.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1141_3_2,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Reference
2197,12203," http://www.opensubtitles.org/"," ['A.3 English-Portuguese']","Our English-Portuguese training corpus consists of 28.5 Million segments from the Europarl, JRC-Aquis (Steinberger et al., 2006) and OpenSubti-tles [Cite_Footnote_5] corpora.",5 http://www.opensubtitles.org/,"Our English-Portuguese training corpus consists of 28.5 Million segments from the Europarl, JRC-Aquis (Steinberger et al., 2006) and OpenSubti-tles [Cite_Footnote_5] corpora.",Material,DataSource,True,Use（引用目的）,True,P17-1141_20_0,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,Footnote
2198,12204," http://nlp.stanford.edu/data/glove.840B.300d.zip"," ['4 Experiment', '4.1 Implementation Details']","For word embedding, we use pre-trained case-sensitive GloVe embeddings [Cite_Footnote_2] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words.",2 Downloaded from http://nlp.stanford.edu/data/glove.840B.300d.zip.,"We use the tokenizer from Stanford CoreNLP (Manning et al., 2014) to preprocess each passage and question. The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used through-out our model. For word embedding, we use pre-trained case-sensitive GloVe embeddings [Cite_Footnote_2] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com-pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas-sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment. The hidden vec-tor length is set to 75 for all layers. The hidden size used to compute attention scores is also 75. We also apply dropout (Srivastava et al., 2014) be-tween layers with a dropout rate of 0.2. The model is optimized with AdaDelta (Zeiler, 2012) with an initial learning rate of 1. The ρ and used in AdaDelta are 0.95 and 1e −6 respectively.",Method,Tool,True,Use（引用目的）,True,P17-1018_0_0,2017,Gated Self-Matching Networks for Reading Comprehension and Question Answering,Footnote
2199,12205," http://stanford-qa.com"," ['4 Experiment', '4.2 Main Results']",The scores on dev set are evaluated by the offi-cial script [Cite_Footnote_3] .,3 Downloaded from http://stanford-qa.com,"Two metrics are utilized to evaluate model perfor-mance: Exact Match (EM) and F1 score. EM measures the percentage of the prediction that matches one of the ground truth answers exactly. F1 measures the overlap between the prediction and ground truth answers which takes the max-imum F1 over all of the ground truth answers. The scores on dev set are evaluated by the offi-cial script [Cite_Footnote_3] . Since the test set is hidden, we are re-quired to submit the model to Stanford NLP group to obtain the test scores.",Material,Knowledge,False,Use（引用目的）,False,P17-1018_1_0,2017,Gated Self-Matching Networks for Reading Comprehension and Question Answering,Footnote
2200,12206," http://stanford-qa.com"," ['4 Experiment', '4.2 Main Results']",Table 2 shows exact match and F1 scores on the dev and test set of our model and competing ap-proaches [Cite_Footnote_4] .,"4 Extracted from SQuAD leaderboard http: //stanford-qa.com on Feb. 6, 2017.","Table 2 shows exact match and F1 scores on the dev and test set of our model and competing ap-proaches [Cite_Footnote_4] . The ensemble model consists of 20 training runs with the identical architecture and hyper-parameters. At test time, we choose the an-swer with the highest sum of confidence scores amongst the 20 runs for each question. As we can see, our method clearly outperforms the baseline and several strong state-of-the-art systems for both single model and ensembles.",Material,DataSource,False,Use（引用目的）,False,P17-1018_2_0,2017,Gated Self-Matching Networks for Reading Comprehension and Question Answering,Footnote
2201,12207," https://aka.ms/Splash_dataset"," ['References']",SPLASH is publicly available at [Cite] https://aka.ms/Splash_dataset.,,"We study the task of semantic parse correction with natural language feedback. Given a natu-ral language utterance, most semantic parsing systems pose the problem as one-shot transla-tion where the utterance is mapped to a cor-responding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feed-back to correct the system when it generates an inaccurate interpretation of an initial utter-ance. We focus on natural language to SQL systems and construct, SPLASH , a dataset of ut-terances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the cor-rection task and show that incorporating such a rich form of feedback can significantly im-prove the overall semantic parsing accuracy while retaining the flexibility of natural lan-guage interaction. While we estimated hu-man correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at [Cite] https://aka.ms/Splash_dataset.",Material,Dataset,True,Produce（引用目的）,True,2020.acl-main.187_0_0,2020,Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback,Body
2202,12208," https://github.com/rshin/seq2struct"," ['3 Dataset Construction', '3.1 Generating Questions and Incorrect SQL Pairs']","We use the Seq2Struct parser (Shin, 2019) [Cite_Footnote_1] to generate erroneous SQL interpretations.",1 https://github.com/rshin/seq2struct,"To generate erroneous SQL interpretations of questions in Spider, we opted for using the output of a text-to-SQL parser to ensure that our dataset reflect the actual distribution of errors that contem-porary parsers make. This is a more realistic setup than artificially infusing errors in the gold SQL. We use the Seq2Struct parser (Shin, 2019) [Cite_Footnote_1] to generate erroneous SQL interpretations. Seq2Struct com-bines grammar-based decoder of Yin and Neubig (2017) with a self-attention-based schema encod-ing and it reaches a parsing accuracy of 42.94% on the development set of Spider.",Method,Tool,True,Use（引用目的）,True,2020.acl-main.187_1_0,2020,Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback,Footnote
2203,12209," https://github.com/cosmozhang/Modular_Neural_CRF"," ['6 Experimental Evaluation', '6.2 Q1: Monolithic vs. Modular Learning']",The source code and experimental setup are avail-able online [Cite_Footnote_4] .,4 https://github.com/cosmozhang/ Modular_Neural_CRF,"Target Sentiment task The results are summa-rized in Tab. 1. We also compared our models with recently published state-of-the-art models on these datasets. To help ensure a fair comparison with Ma et al. which does not use inference, we also included the results of our model without the CRF layer (denoted LSTM-Ti(g)). All of our models beat the state-of-the-art results by a large margin. The source code and experimental setup are avail-able online [Cite_Footnote_4] .",Method,Code,True,Produce（引用目的）,True,P19-1055_0_0,2019,Sentiment Tagging with Partial Labels using Modular Architectures,Footnote
2204,12210," https://en.wikipedia.org/wiki/Yes,_and"," ['1 Introduction']","Looking at the entire conversation, utterance (4) turns out to be a bad action to take because it offers no way of continuing the conversation. [Cite_Footnote_1]","1 A similar rule is often suggested in improvisational comedy: https://en.wikipedia.org/wiki/Yes,_and...","Another common problem, illustrated in the two sample conversations on the left of Table 1, is that the system becomes stuck in an infinite loop of repet-itive responses. This is due to MLE-based S EQ 2S EQ models’ inability to account for repetition. In exam-ple 2 (bottom left), the dialogue falls into an infinite loop after three turns, with both agents generating dull, generic utterances like i don’t know what you are talking about and you don’t know what you are saying. Looking at the entire conversation, utterance (4) turns out to be a bad action to take because it offers no way of continuing the conversation. [Cite_Footnote_1] two agents using a 4-layer LSTM encoder-decoder trained on the OpenSubtitles dataset. The first turn (index 1) is input by the authors. Then the two agents take turns conversing, taking as input the other agent’s prior generated turn. The output is generated using the mutual information model (Li et al., 2015) in which an N-best list is first obtained using beam search based on p(t|s) and reranked by linearly combining the backward probability p(s|t), where t and s respectively denote targets and sources. Right Column: Dialogue simulated using the proposed reinforcement learning model. The new model has more forward-looking utterances (questions like “Why are you asking?” and offers like “I’ll come with you”) and lasts longer before it falls into conversational black holes.",補足資料,Document,True,Compare（引用目的）,True,D16-1127_0_0,2016,Deep Reinforcement Learning for Dialogue Generation,Footnote
2205,12211," http://mallet.cs.umass.edu/"," ['3 Models', '3.1 Latent variable context models']",Follow-ing the recommendations of Wallach et al. (2009) we use asymmetric α and symmetric β; rather than using fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method. [Cite_Footnote_2],"2 We use the estimation methods provided by the MAL-LET toolkit, available from http://mallet.cs.umass.edu/.","In this paper we train LDA models of P (w|c) and P (c|w). In the former case, the analogy to document modelling is that each context type plays the role of a “document” consisting of all the words observed in that context in a corpus; for P (c|w) the roles are reversed. The models are trained by Gibbs sampling using the efficient procedure of Yao et al. (2009). The empirical estimates for distributions over words and latent variables are derived from the assignment of topics over the training corpus in a single sam-pling state. For example, to model P(w|c) we cal-culate: where f zw is the number of words of type w as-signed topic z, f zc is the number of times z is associ-ated with context c, f z· and f ·c are the marginal topic and context counts respectively, N is the number of word types and α and β parameterise the Dirichlet prior distributions over P (z|c) and P (w|z). Follow-ing the recommendations of Wallach et al. (2009) we use asymmetric α and symmetric β; rather than using fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method. [Cite_Footnote_2] We use standard set-tings for the number of training iterations (1000), the length of the burnin period before hyperparameter estimation begins (200 iterations) and the frequency of hyperparameter estimation (50 iterations).",Method,Code,True,Use（引用目的）,True,D11-1097_0_0,2011,Probabilistic models of similarity in syntactic context,Footnote
2206,12212," http://www.nlpado.de/~sebastian/sigf.html"," ['5 Experiment 2: Lexical substitution', '5.1 Data']","We calculate statistical significance of performance differences using strati-fied shuffling (Yeh, 2000). [Cite_Footnote_5]",5 We use the software package available at http://www.nlpado.de/˜sebastian/sigf.html.,"Previous authors have partitioned the dataset in various ways. Erk and Padó (2008) use only a sub-set of the data where the target is a noun headed by a verb or a verb heading a noun. Thater et al. (2010) discard sentences which their parser cannot parse and paraphrases absent from their training cor-pus and then optimise the parameters of their model through four-fold cross-validation. Here we aim for complete coverage on the dataset and do not perform any parameter tuning. We use two measures to eval-uate performance: Generalised Averaged Precision (Kishida, 2005) and Kendall’s τ b rank correlation coefficient, which were used for this task by Thater et al. (2010) and Dinu and Lapata (2010), respec-tively. Generalised Averaged Precision (GAP) is a precision-like measure for evaluating ranked pre-dictions against a gold standard. τ b is a variant of Kendall’s τ that is appropriate for data containing tied ranks. We do not use the “precision out of ten” measure that was used in the original Lexical Substi-tution Task; this measure assigns credit for the pro-portion of the first 10 proposed paraphrases that are present in the gold standard and in the context of ranking attested substitutes it is unclear how to ob-tain non-trivial results for target words with 10 or fewer possible substitutes. We calculate statistical significance of performance differences using strati-fied shuffling (Yeh, 2000). [Cite_Footnote_5]",Method,Tool,True,Use（引用目的）,True,D11-1097_1_0,2011,Probabilistic models of similarity in syntactic context,Footnote
2207,12213," http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html"," ['3 Features in CRFs']","We use an array of features in CRFs which are ei-ther derived or borrowed from the taxonomy that a Japanese tokenizer called JUMAN and KNP , [Cite_Footnote_6] a Japanese dependency parser (aka Kurohashi-Nagao Parser), make use of in characterizing the output they produce: both JUMAN and KNP are part of the compression model we build.",6 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html,"We use an array of features in CRFs which are ei-ther derived or borrowed from the taxonomy that a Japanese tokenizer called JUMAN and KNP , [Cite_Footnote_6] a Japanese dependency parser (aka Kurohashi-Nagao Parser), make use of in characterizing the output they produce: both JUMAN and KNP are part of the compression model we build.",Method,Tool,True,Use（引用目的）,True,P08-1035_0_0,2008,A Generic Sentence Trimmer with CRFs,Footnote
2208,12214," http://www.nikkei.co.jp"," ['5 Evaluation Setup']","We created a corpus of sentence summaries based on email news bulletins we had received over five to six months from an on-line news provider called Nikkei Net, which mostly deals with finance and politics. [Cite_Footnote_9]",9 http://www.nikkei.co.jp,"We created a corpus of sentence summaries based on email news bulletins we had received over five to six months from an on-line news provider called Nikkei Net, which mostly deals with finance and politics. [Cite_Footnote_9] Each bulletin consists of six to seven news briefs, each with a few sentences. Since a news brief contains nothing to indicate what its longer version might look like, we manually searched the news site for a full-length article that might reasonably be con-sidered a long version of that brief.",補足資料,Website,True,Introduce（引用目的）,True,P08-1035_1_0,2008,A Generic Sentence Trimmer with CRFs,Footnote
2209,12215," http://mallet.cs.umass.edu"," ['5 Evaluation Setup']","A part of our sys-tem makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006 [Cite_Ref] ).",Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.,"We extracted lead sentences both from the brief and from its source article, and aligned them, us-ing what is known as the Smith-Waterman algorithm (Smith and Waterman, 1981), which produced 1,401 pairs of summary and source sentence. For the ease of reference, we call the corpus so produced ‘NICOM’ for the rest of the paper. A part of our sys-tem makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006 [Cite_Ref] ). Throughout the experiments, we call our approach ‘Generic Sen-tence Trimmer’ or GST.",Method,Tool,True,Use（引用目的）,True,P08-1035_2_0,2008,A Generic Sentence Trimmer with CRFs,Reference
2210,12216," http://www.isi.edu/~och/YASMET.html"," ['3 Adjacency Pairs', '3.4 Results']","To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET. [Cite_Footnote_5]",5 http://www.isi.edu/˜och/YASMET.html,"We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training. To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET. [Cite_Footnote_5]",Method,Tool,False,Use（引用目的）,True,P04-1085_0_0,2004,Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies,Footnote
2211,12217," http://www.isi.edu/~ravichan/YASMET.html"," ['3 Adjacency Pairs', '3.4 Results']","Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smooth-ing and using incremental feature selection as de-scribed in (Berger et al., 1996) and implemented in the yasmetFS package. [Cite_Footnote_6]",6 http://www.isi.edu/˜ravichan/YASMET.html,"Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smooth-ing and using incremental feature selection as de-scribed in (Berger et al., 1996) and implemented in the yasmetFS package. [Cite_Footnote_6] Note that restricting our-selves to only backward looking features decreases the performance significantly, as we can see in Ta-ble 2.",Method,Tool,False,Use（引用目的）,True,P04-1085_1_0,2004,Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies,Footnote
2212,12218," https://github.com/serenayj/ABCD-ACL2021"," ['1 Introduction']",Experimental results show that ABCD achieves comparable or better performance than baselines. [Cite_Footnote_1],1 ABCD is available at https://github.com/serenayj/ABCD-ACL2021.,"The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. [Cite_Footnote_1]",Material,Knowledge,False,Produce（引用目的）,True,2021.acl-long.303_0_0,2021,ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences,Footnote
2213,12219," https://github.com/serenayj/DeSSE"," ['3 Datasets', '3.1 DeSSE']","DeSSE is also used for connec-tive prediction tasks, as in (Gao et al., 2021). [Cite_Footnote_2]",2 DeSSE and MinWiki are available at https://github.com/serenayj/DeSSE.,"DeSSE is collected in an undergraduate social science class, where students watched video clips about race relations, and wrote essays in a blog environment to share their opinions with the class. It was created to support analysis of student writ-ing, so that different kinds of feedback mechanisms can be developed regarding sentence organization. Students have difficulty with revision to address lack of clarity in their writing (Kuhn et al., 2016), such as non-specific uses of connectives, run on sentences, repetitive statements and the like. These make DeSSE different from corpus with expert written text, such as Wikipedia and newspaper. The annotation process is unique in that it involves iden-tifying where to split a source complex sentence into distinct clauses, and how to rephrase each re-sulting segment as a semantically complete simple sentence, omitting any discourse connectives. It differs from corpora that identify discourse units within sentences, such as RST-DT (Carlson et al., 2003) and PTDB (Prasad et al., 2008), because clauses are explicitly rewritten as simple sentences. It differs from split-and-rephrase corpora such as MinWikiSplit, because of the focus in DeSSE on rephrased simple sentences that have a one-to-one correspondence to tensed clauses in the original complex sentence. DeSSE is also used for connec-tive prediction tasks, as in (Gao et al., 2021). [Cite_Footnote_2]",Material,Dataset,True,Introduce（引用目的）,True,2021.acl-long.303_1_0,2021,ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences,Footnote
2214,12220," http://www.informatik.uni-trier.de/∼ley/db/"," ['1 Introduction']","The input to our training algorithm is a set of matching DBLP [Cite_Footnote_2] -record/citation-text pairs and global GE criteria of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word.",2 http://www.informatik.uni-trier.de/∼ley/db/,"We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP [Cite_Footnote_2] -record/citation-text pairs and global GE criteria of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008). Such sys-tems achieve very high accuracy close to 90% F1 on semi-structured domains similar to ours. Our trained alignment model can be used to directly align new record-text pairs to create a labeling of the texts. Empirical results demonstrate a 20.6% error reduction in token labeling accuracy com-pared to a strong baseline method that employs a set of high-precision alignments. Furthermore, we provide a 63.8% error reduction compared to IBM Model 4 (Brown et al., 1993). Alignments learned by our model are used to train a linear-chain CRF extractor. We obtain an error reduction of 35.1% over a previous state-of-the-art extraction method that uses heuristically generated alignments.",Material,Knowledge,False,Use（引用目的）,True,D09-1014_0_0,2009,Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment,Footnote
2215,12221," http://www.cs.umass.edu/∼kedarb/dbieexpts.txt"," ['1 Introduction']","The input to our training algorithm is a set of matching DBLP -record/citation-text pairs and global GE criteria [Cite_Footnote_3] of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word.",3 Expectation criteria used in our experiments are listed at http://www.cs.umass.edu/∼kedarb/dbieexpts.txt.,"We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP -record/citation-text pairs and global GE criteria [Cite_Footnote_3] of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008). Such sys-tems achieve very high accuracy close to 90% F1 on semi-structured domains similar to ours. Our trained alignment model can be used to directly align new record-text pairs to create a labeling of the texts. Empirical results demonstrate a 20.6% error reduction in token labeling accuracy com-pared to a strong baseline method that employs a set of high-precision alignments. Furthermore, we provide a 63.8% error reduction compared to IBM Model 4 (Brown et al., 1993). Alignments learned by our model are used to train a linear-chain CRF extractor. We obtain an error reduction of 35.1% over a previous state-of-the-art extraction method that uses heuristically generated alignments.",Material,Knowledge,False,Use（引用目的）,True,D09-1014_1_0,2009,Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment,Footnote
2216,12222," http://www.cs.umass.edu/∼kedarb/dbie"," ['4 Experiments']",Our data set [Cite_Footnote_6] con-tains 522 record-text pairs for 260 DBLP entries.,6 The data set can be found at http://www.cs.umass.edu/∼kedarb/dbie cite data.sgml.,For each DBLP record we searched on the web for matching citation texts using the first author’s last name and words in the title. Each citation text found is manually labeled for evaluation purposes. An example of a matching DBLP record-citation text pair is shown in Table 3. Our data set [Cite_Footnote_6] con-tains 522 record-text pairs for 260 DBLP entries.,Material,Dataset,True,Produce（引用目的）,True,D09-1014_2_0,2009,Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment,Footnote
2217,12223," http://www.cs.umass.edu/∼kedarb/dbieexpts.txt"," ['4 Experiments']",Table 4 shows a sample of GE criteria used in our experiments. [Cite_Footnote_8],8 A complete list of expectation criteria is available at http://www.cs.umass.edu/∼kedarb/dbieexpts.txt.,"As is common practice (Haghighi and Klein, 2006; Mann and McCallum, 2008), we simulate user-specified expectation criteria through statis-tics on manually labeled citation texts. For ex-traction criteria, we select for each label, the top N extraction features ordered by mutual informa-tion (MI) with that label. Also, we aggregate the alignment features of record tokens whose align-ment with a target text token results in a correct label assignment. The top N alignment features that have maximum MI with this correct label-ing are selected as alignment criteria. We bin tar-get expectations of these criteria into 11 bins as [0.05, 0.1, 0.2, 0.3, . . . , 0.9, 0.95]. In our exper-iments, we set N = 10 and use a fixed weight w = 10.0 for all expectation criteria (no tuning of parameters was performed). Table 4 shows a sample of GE criteria used in our experiments. [Cite_Footnote_8]",Material,Knowledge,True,Produce（引用目的）,True,D09-1014_3_0,2009,Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment,Footnote
2218,12224," http://www.wordgumbo.com/ie/cmp/iedata.txt"," ['1 Introduction', '1.1 Related Work']","Using modified versions of Swadesh’s lists [Cite_Footnote_1] , Dyen et al. (1992) investigate the classification of Indo-European languages by applying a lexicosta-tistical method.",1 http://www.wordgumbo.com/ie/cmp/iedata.txt,"According to Campbell (2003), the methods based on comparisons of cognate lists and sound corre-spondences are the most popular approaches em-ployed for establishing relationships between lan-guages. Barbançon et al. (2013) emphasize the va-riety of computational methods used in this field, and state that the differences in datasets and ap-proaches cause difficulties in the evaluation of the results regarding the reconstruction of the phylo-genetic tree of languages. Linguistic phylogeny reconstruction proves especially useful in histor-ical and comparative linguistics, as it enables the analysis of language evolution. Ringe et al. (2002) propose a computational method for evolutionary tree reconstruction based on a “perfect phylogeny” algorithm; using a Bayesian phylogeographic ap-proach, Alekseyenko et al. (2012), continuing the work of Atkinson et al. (2005), model the expan-sion of the Indo-European language family and find support for the hypothesis which places its homeland in Anatolia; Atkinson and Gray (2006) analyze language divergence dates and argue for the usage of computational phylogenetic meth-ods in the question of Indo-European age and ori-gins. Using modified versions of Swadesh’s lists [Cite_Footnote_1] , Dyen et al. (1992) investigate the classification of Indo-European languages by applying a lexicosta-tistical method.",Material,Dataset,True,Extend（引用目的）,False,D14-1112_0_0,2014,An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian,Footnote
2219,12225," http://lucene.apache.org"," ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.1 Preprocessing']",We use the lists of stop words for Roma-nian provided by the Apache Lucene [Cite_Footnote_2] text search engine library.,2 http://lucene.apache.org,"Step 2. Stop Words Removal. We focus on analyzing word content and, in order to obtain relevant results, we remove stop words from the datasets. We use the lists of stop words for Roma-nian provided by the Apache Lucene [Cite_Footnote_2] text search engine library. In Table 1 we list the total number of stop words from each corpus.",補足資料,Website,True,Use（引用目的）,True,D14-1112_1_0,2014,An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian,Footnote
2220,12226," http://dexonline.ro"," ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.1 Preprocessing']",We use the Dex-online [Cite_Footnote_3] machine-readable dictionary to lemmatize Romanian words.,3 http://dexonline.ro,Step 3. Lemmatization. We use lemmas for identifying words’ definitions in dictionaries and for computing adequate distances between words and their cognates or etymons. We use the Dex-online [Cite_Footnote_3] machine-readable dictionary to lemmatize Romanian words.,Material,Knowledge,True,Use（引用目的）,True,D14-1112_2_0,2014,An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian,Footnote
2221,12227," http://www.sapere.it/sapere/dizionari"," ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.2 Relationships Identification']",We use electronic dictionaries [Cite_Footnote_4] to extract etymology-related information and Google Trans-late to translate Romanian words.,4 Italian: http://www.sapere.it/sapere/dizionari,"In our previous work (Ciobanu and Dinu, 2014a) we applied this method on a Romanian dic-tionary, while here we extract cognates from Ro-manian corpora. We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries [Cite_Footnote_4] to extract etymology-related information and Google Trans-late to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well. We selected these six languages for the following reason: the first four in our list are Romance languages, and our intuition is that there are numerous words in these languages which share a common ancestor with Romanian words. We investigate the cog-nate pairs for Turkish because many French words were imported in both Romanian and Turkish in the 19 th century, and we believe that accounting for Romanian-Turkish cognates would provide a more accurate result for the similarity of these lan-guages. As for English, we decided to investigate the cognate pairs for this language in order to ana-lyze to what extent the influence of English on Ro-manian increases across time. In Table 2 we report the number of Romanian words having an etymon or a cognate pair in the six related languages.",Material,Knowledge,True,Use（引用目的）,True,D14-1112_3_0,2014,An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian,Footnote
2222,12228," http://translate.google.com"," ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.2 Relationships Identification']",We use electronic dictionaries to extract etymology-related information and Google Trans-late [Cite_Footnote_5] to translate Romanian words.,5 http://translate.google.com,"In our previous work (Ciobanu and Dinu, 2014a) we applied this method on a Romanian dic-tionary, while here we extract cognates from Ro-manian corpora. We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries to extract etymology-related information and Google Trans-late [Cite_Footnote_5] to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well. We selected these six languages for the following reason: the first four in our list are Romance languages, and our intuition is that there are numerous words in these languages which share a common ancestor with Romanian words. We investigate the cog-nate pairs for Turkish because many French words were imported in both Romanian and Turkish in the 19 th century, and we believe that accounting for Romanian-Turkish cognates would provide a more accurate result for the similarity of these lan-guages. As for English, we decided to investigate the cognate pairs for this language in order to ana-lyze to what extent the influence of English on Ro-manian increases across time. In Table 2 we report the number of Romanian words having an etymon or a cognate pair in the six related languages.",Method,Tool,True,Use（引用目的）,True,D14-1112_9_0,2014,An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian,Footnote
2223,12229," http://nlp.unibuc.ro/resources/rosim.pdf"," ['3 Experiments and Results', '3.1 Romanian Evolution', '3.1.2 Results']","In Table 3 we list the output of our method for each corpus, with and without diacritics [Cite_Footnote_6] .",6 The complete ranking of similarity is available online at http://nlp.unibuc.ro/resources/rosim.pdf.,"In this subsection we present and analyze the main results drawn from our research. In Table 3 we list the output of our method for each corpus, with and without diacritics [Cite_Footnote_6] . We report the similarity between Romanian and related languages, provid-ing the average value of the three metrics used. In the %words column we provide, for each corpus, the percentage of words having an etymon or a cognate pair in a given language (the typical mea-sure used in lexicostatistical comparison, i.e., the 0 distance function). The results for the Romanian datasets are plotted in Figure 4 and Figure 5.",Material,Knowledge,True,Produce（引用目的）,False,D14-1112_10_0,2014,An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian,Footnote
2224,12230," http://www.usc.edu/schools/college/vhi"," ['2 The Problem']","The USC Shoah Foundation Institute for Vis-ual History and Education manages what is pres-ently the world's largest archive of videotaped oral histories (USC, 2006) [Cite_Ref] .","USC. (2006) USC Shoah Foundation Institute for Visual History and Education, [online] http://www.usc.edu/schools/college/vhi","The USC Shoah Foundation Institute for Vis-ual History and Education manages what is pres-ently the world's largest archive of videotaped oral histories (USC, 2006) [Cite_Ref] . The archive contains 116,000 hours of video from the testimonies of over 52,000 survivors, liberators, rescuers and witnesses of the Holocaust. If viewed end to end, the collection amounts to 13 years of con-tinuous video. The Shoah Foundation uses a hi-erarchically arranged thesaurus of 56,000 key-word phrases representing domain-specific con-cepts. These are assigned to time-points in the video testimonies as a means of indexing the video content. Although the testimonies in the collection represent 32 different languages, the thesaurus used to catalog them is currently avail-able only in English. Our task was to translate this resource to facilitate multilingual access, with Czech as the first target language.",補足資料,Website,False,Introduce（引用目的）,True,P06-1119_0_0,2006,Leveraging Reusability: Cost-effective Lexical Acquisition for Large-scale Ontology Translation,Reference
2225,12231," http://cnts.uia.ac.be/conll2000/"," ['5 Classifier Features', '5.4 Word class features']",These are assigned using a word-chunking SVM-based system trained on the CoNLL-2000 data [Cite_Footnote_2] (which uses the lowest nodes of the Penn TreeBank syntactic trees to break sentences into base phrases).,2 http://cnts.uia.ac.be/conll2000/,"Syntactic-chunk label: The value of this feature is a B-I-O style label indicating what kind of syntactic chunk the word is contained in, e.g. noun phrase, verb phrase, or prepositional phrase. These are assigned using a word-chunking SVM-based system trained on the CoNLL-2000 data [Cite_Footnote_2] (which uses the lowest nodes of the Penn TreeBank syntactic trees to break sentences into base phrases).",Material,Knowledge,False,Use（引用目的）,True,W06-1618_0_0,2006,Identification of Event Mentions and their Semantic Class,Footnote
2226,12232," http://www.cs.ualberta.ca/~lindek/minipar.htm"," ['5 Classifier Features', '5.5 Governing features']","These features attempt to include some simple dependency information from the surrounding words, using the dependency parses produced by Minipar [Cite_Footnote_3] .",3 http://www.cs.ualberta.ca/~lindek/minipar.htm,"These features attempt to include some simple dependency information from the surrounding words, using the dependency parses produced by Minipar [Cite_Footnote_3] . These features aim to identify events that are expressed as phrases or that require knowledge of the surrounding phrase to be iden-tified.",Method,Tool,True,Use（引用目的）,True,W06-1618_1_0,2006,Identification of Event Mentions and their Semantic Class,Footnote
2227,12233," http://timex2.mitre.org/tern.html"," ['5 Classifier Features', '5.6 Temporal features']",The temporal annotations are produced by a word-chunking SVM-based system trained on the temporal ex-pressions (TIMEX2 annotations) in the TERN 2004 data [Cite_Footnote_4] .,4 http://timex2.mitre.org/tern.html,"Time chunk label: The value of this feature is a B-I-O label indicating whether or not this word is contained in a temporal annotation. The temporal annotations are produced by a word-chunking SVM-based system trained on the temporal ex-pressions (TIMEX2 annotations) in the TERN 2004 data [Cite_Footnote_4] . In addition to identifying expres-sions like Monday and this year, the TERN data identifies event-containing expressions like the time she arrived at her doctor's office.",Material,DataSource,True,Use（引用目的）,True,W06-1618_2_0,2006,Identification of Event Mentions and their Semantic Class,Footnote
2228,12234," http://chasen.org/~taku/software/TinySVM/"," ['6 Classifier Parameters']","For the learning task, we use the TinySVM [Cite_Footnote_6] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.",6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/,"The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [Cite_Footnote_6] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.",Method,Tool,True,Use（引用目的）,True,W06-1618_3_0,2006,Identification of Event Mentions and their Semantic Class,Footnote
2229,12235," http://chasen.org/~taku/software/yamcha/"," ['6 Classifier Parameters']","For the learning task, we use the TinySVM [Cite_Footnote_6] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.",6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/,"The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [Cite_Footnote_6] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.",Method,Tool,True,Use（引用目的）,True,W06-1618_4_0,2006,Identification of Event Mentions and their Semantic Class,Footnote
2230,12236," https://github.com/ybz79/AMR2text"," ['4 Experiments', '4.1 Settings']","As for batch size, we use 80 for LDC2015E86 and 120 for LDC2017T10. [Cite_Footnote_3]",3 Our code is available at https://github.com/ybz79/AMR2text,"Data and preprocessing We conduct our experi-ments with two benchmark datasets: LDC2015E85 and LDC2017T10. The two datasets contain 16833 and 36521 training samples, and they use a common development set with 1368 samples and a common test set with 1371 samples. We segment natural words in both AMR graphs and references into sub-words. As a result, a word node in AMR graphs may be divided into several sub-word nodes. We use a special edge subword to link the corresponding sub-word nodes. Then, for each AMR graph, we find its correspond-ing line graph and generate G c and G e respectively. Training details For model parameters, the number of graph encoding layers is fixed to 6, and the representation dimension d is set to 512. We set the graph neighborhood order K = 1,2 and 4 for both G c and G e . The Transformer decoder is based on Open-NMT (Klein et al., 2018), with 6 layers, 512 dimensions and 8 heads. We use Adam (Kingma and Ba, 2015) as our optimizer and β = (0.9,0.98). The learning rate is varied over the course of training, similar with Vaswani et al. (2017): where t denotes the accumulative training steps, and w indicates the warmup steps. We use w = 16000 and the coefficient γ is set to 0.75. As for batch size, we use 80 for LDC2015E86 and 120 for LDC2017T10. [Cite_Footnote_3]",Method,Code,True,Produce（引用目的）,True,2020.acl-main.67_0_0,2020,Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks,Footnote
2231,12237," https://www.perspectiveapi.com"," ['2 Related Work']","Hosseini et al. (2017) show that simple modifications, such as adding spaces or dots between characters, can drasti-cally change the toxicity score from Google’s perspective API [Cite_Footnote_1] .",1 https://www.perspectiveapi.com,"Adversarial examples are powerful tools to in-vestigate the vulnerabilities of a deep learning model (Szegedy et al., 2014). While this line of research has recently received a lot of attention in the deep learning community, it has a long history in machine learning, going back to adversarial at-tacks on linear spam classifiers (Dalvi et al., 2004; Lowd and Meek, 2005). Hosseini et al. (2017) show that simple modifications, such as adding spaces or dots between characters, can drasti-cally change the toxicity score from Google’s perspective API [Cite_Footnote_1] . Belinkov and Bisk (2018) show that character-level machine translation sys-tems are overly sensitive to random character ma-nipulations, such as keyboard typos. They manipu-late every word in a sentence with synthetic or nat-ural noise. However, throughout our experiments, we care about the degree of distortion in a sen-tence, and look for stronger adversaries which can increase the loss within a limited budget. Instead of randomly perturbing text, we propose an effi-cient method, which can generate adversarial text using the gradients of the model with respect to the input.",Method,Code,True,Introduce（引用目的）,False,P18-2006_0_0,2018,HotFlip: White-Box Adversarial Examples for Text Classification,Footnote
2232,12238," https://www.di.unipi.it/~gulli/"," ['4 Experiments']","We use the AG’s news dataset [Cite_Footnote_3] , which consists of 120,000 training and 7,600 test instances from four equal-sized classes: World, Sports, Business, and Science/Technology.",3 https://www.di.unipi.it/˜gulli/,"In principle, HotFlip could be applied to any dif-ferentiable character-based classifier. Here, we fo-cus on the CharCNN-LSTM architecture (Kim et al., 2016), which can be adapted for classifica-tion via a single dense layer after the last recur-rent hidden unit. We use the AG’s news dataset [Cite_Footnote_3] , which consists of 120,000 training and 7,600 test instances from four equal-sized classes: World, Sports, Business, and Science/Technology. The ar-chitecture consists of a 2-layer stacked LSTM with 500 hidden units, a character embedding size of 25, and 1000 kernels of width 6 for temporal convolutions. This classifier was able to outper-form (Conneau et al., 2017), which has achieved the state-of-the-art result on some benchmarks, on AG’s news. The model is trained with SGD and gradient clipping, and the batch size was set to 64. We used 10% of the training data as the develop-ment set, and trained for a maximum of 25 epochs. We only allow character changes if the new word does not exist in the vocabulary, to avoid changes that are more likely to change the meaning of text. The adversary uses a beam size of 10, and has a budget of maximum of 10% of characters in the document. In Figure 1, we plot the success rate of the adversary against an acceptable confidence score for the misclassification. That is, we con-sider the adversary successful only if the classifier misclassifies the instance with a given confidence score. For this experiment, we create adversarial examples for 10% of the test set.",Material,Dataset,True,Use（引用目的）,True,P18-2006_1_0,2018,HotFlip: White-Box Adversarial Examples for Text Classification,Footnote
2233,12239," https://github.com/teapot123/JASen"," ['References']",Our com-prehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets [Cite_Footnote_1] .,1 Our code and data are available at https://github.com/teapot123/JASen.,"Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this pa-per, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each as-pect/sentiment without using any labeled ex-amples. Existing methods are either designed only for one of the sub-tasks, neglecting the benefit of coupling both, or are based on topic models that may contain overlapping concepts. We propose to first learn hsentiment, aspecti joint topic embeddings in the word embedding space by imposing regularizations to encour-age topic distinctiveness, and then use neural models to generalize the word-level discrim-inative information by pre-training the clas-sifiers with embedding-based predictions and self-training them on unlabeled data. Our com-prehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets [Cite_Footnote_1] .",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.568_0_0,2020,Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding,Footnote
2234,12240," https://www.yelp.com/dataset/challenge"," ['5 Evaluation', '5.1 Experimental Setup']","For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge [Cite_Footnote_2] .",2 https://www.yelp.com/dataset/challenge,"• Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge [Cite_Footnote_2] . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment po-larity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it).",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.568_1_0,2020,Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding,Footnote
2235,12241," https://www.nltk.org/"," ['5 Evaluation', '5.1 Experimental Setup']","To preprocess the training corpus D, we use the word tokenizer provided by NLTK [Cite_Footnote_3] .",3 https://www.nltk.org/,"Preprocessing and Hyperparameter Setting. To preprocess the training corpus D, we use the word tokenizer provided by NLTK [Cite_Footnote_3] . We also use a phrase mining tool, AutoPhrase (Shang et al., 2017), to extract meaningful phrases such as “great wine” and “numeric keypad” such that they can capture complicated semantics in a single text unit. We use the benchmark validation set to fine-tune the hy-perparameters: embedding dimension = 100, local context window size h = 5, λ g = 2.5, λ r = 1, training epoch = 5. For neural model pre-training, we set T = 20. A CNN model is trained for each sub-task: aspect extraction and sentiment classifi-cation. Each model uses 20 feature maps for filters with window sizes of 2, 3, and 4. SGD is used with 1e − 3 as the learning rate in both pre-training and self-training and the batch size is set to 16.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.568_2_0,2020,Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding,Footnote
2236,12242," https://github.com/ictnlp/GS4NMT"," ['4 Experiment', '4.1 Settings']",We carry out experiments on Chinese-to-English translation. [Cite_Footnote_1],1 Experiment code: https://github.com/ictnlp/GS4NMT,"We carry out experiments on Chinese-to-English translation. [Cite_Footnote_1] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped. We use NIST 2002 (MT 02) as the vali-dation set and NIST 2003-2006 (MT 03-08) as the test sets. We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the transla-tion task.",Method,Code,True,Produce（引用目的）,True,D18-1510_0_0,2018,Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation,Footnote
2237,12243," http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip"," ['4 Experiments', '4.1 Session Based Task Oriented Parsing']",We open source SB-TOP in the follow-ing link: [Cite] http://www.dl.fbaipublicfiles.,,We open source SB-TOP in the follow-ing link: [Cite] http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip. More information about the dataset can be found in the Table ?? in the Appendix.,Material,Dataset,False,Produce（引用目的）,True,2020.emnlp-main.408_0_0,2020,Conversational Semantic Parsing,Body
2238,12244," http://lcl.uniroma1.it/nasari/"," ['References']",We are releasing our vector representations at [Cite] http://lcl.uniroma1.it/nasari/.,,"The semantic representation of individual word senses and concepts is of fundamental importance to several applications in Natu-ral Language Processing. To date, concept modeling techniques have in the main based their representation either on lexicographic re-sources, such as WordNet, or on encyclope-dic resources, such as Wikipedia. We pro-pose a vector representation technique that combines the complementary knowledge of both these types of resource. Thanks to its use of explicit semantics combined with a novel cluster-based dimensionality reduction and an effective weighting scheme, our repre-sentation attains state-of-the-art performance on multiple datasets in two standard bench-marks: word similarity and sense clustering. We are releasing our vector representations at [Cite] http://lcl.uniroma1.it/nasari/.",Mixed,Mixed,True,Produce（引用目的）,True,N15-1059_0_0,2015,N ASARI : a Novel Approach to a Semantically-Aware Representation of Items,Body
2239,12245," http://www.babelnet.org/"," ['2 Semantic Representation of Concepts']","As a bridge between the two resources we use the synset-to-article mappings provided by BabelNet [Cite_Footnote_1] (Navigli and Ponzetto, 2012), a high coverage mul-tilingual encyclopedic dictionary and semantic net-work that merges, among other resources, Wikipedia and WordNet.",1 http://www.babelnet.org/,"Lexical resources and concepts. The gist of our approach lies in its combination of knowledge from two different lexical resources: (1) the expert-based lexicographic WordNet, whose basic con-stituents are synsets, i.e., concepts expressed by sets of synonymous words (Miller et al., 1990), and (2) the collaboratively-constructed encyclopedic Wikipedia, whose articles can be considered as indi-vidual concepts. Throughout the paper, by a concept we mean a tuple b = (p, s) where p is a Wikipedia page and s is the corresponding WordNet synset. As a bridge between the two resources we use the synset-to-article mappings provided by BabelNet [Cite_Footnote_1] (Navigli and Ponzetto, 2012), a high coverage mul-tilingual encyclopedic dictionary and semantic net-work that merges, among other resources, Wikipedia and WordNet. Note that the concept b can also con-tain a Wikipedia page or a WordNet synset only, if a mapping is not provided by BabelNet.",Mixed,Mixed,True,Use（引用目的）,True,N15-1059_1_0,2015,N ASARI : a Novel Approach to a Semantically-Aware Representation of Items,Footnote
2240,12246," http://wordnet.princeton.edu/glosstag.shtml"," ['2 Semantic Representation of Concepts', '2.1 Collecting contextual information']",We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [Cite_Footnote_2] .,2 http://wordnet.princeton.edu/glosstag.shtml,"Figure 1 illustrates the process of obtaining a set of relevant Wikipedia pages T b as contextual informa-tion for a given concept b = (p,s). Let L p be the set containing p and all the Wikipedia pages hav-ing an outgoing link to p, and R s be the set con-sisting of s and all other synsets that are in its di-rect neighbourhood. We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [Cite_Footnote_2] . Let B be a function mapping each WordNet synset s 0 to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise. Hence, B(R s ) = ∪ s 0 ∈R s B(s 0 ). Then, our con-textual information is the set of Wikipedia pages T b = L p ∪ B(R s ). In the case either p or s is not present in the concept b, we take the contextual in-formation as T b = B(R s ) or T b = L p , respectively.",Material,Knowledge,False,Use（引用目的）,True,N15-1059_2_0,2015,N ASARI : a Novel Approach to a Semantically-Aware Representation of Items,Footnote
2241,12247," http://lcl.uniroma1.it/nasari/"," ['6 Conclusions']",We release the representations obtained for all the Wikipedia pages and WordNet synsets in [Cite] http://lcl.uniroma1.it/nasari/.,,"In this paper we presented a novel semantic approach, called N ASARI , for effective vector representation of arbitrary WordNet synsets and Wikipedia pages. The strength of our approach lies in its combination of complementary knowl-edge from different types of resource, while at the same time it also benefits from an effective vec-tor representation with two novel features: lexi-cal specificity for the calculation of vector weights and a semantically-aware dimensionality reduc-tion. N ASARI attains state-of-the-art performance on multiple standard benchmarks in word similarity as well as Wikipedia sense clustering. We release the representations obtained for all the Wikipedia pages and WordNet synsets in [Cite] http://lcl.uniroma1.it/nasari/. As future work we plan to integrate N ASARI into BabelNet and apply our representation to a mul-tilingual setting, enabling the comparison of pairs of concepts across languages. We also intend to use our approach on the task of multilingual Word Sense Disambiguation.",Material,DataSource,False,Extend（引用目的）,False,N15-1059_3_0,2015,N ASARI : a Novel Approach to a Semantically-Aware Representation of Items,Body
2242,12248," http://www.globalWordnet.org/gwa/Wordnet_table.html"," ['3 Experiments', '3.1 Publicly available Wordnets']",Wordnets in many languages are being constructed and developed [Cite_Footnote_2] .,2 http://www.globalWordnet.org/gwa/Wordnet_table.html,"The PWN is the oldest and the biggest available Wordnet. It is also free. Wordnets in many languages are being constructed and developed [Cite_Footnote_2] . However, only a few of these Wordnets are of high quality and free for downloading. The EuroWord-net (Vossen, 1998) is a multilingual database with Wordnets in European languages (e.g., Dutch, Ital-ian and Spanish). The AsianWordnet provides a platform for building and sharing Wordnets for Asian languages (e.g., Mongolian, Thai and Viet-namese). Unfortunately, the progress in building most of these Wordnets is slow and they are far from being finished.",Material,Knowledge,True,Introduce（引用目的）,True,P14-2018_0_0,2014,Automatically constructing Wordnet synsets,Footnote
2243,12249," http://www.asianWordnet.org/progress"," ['3 Experiments', '3.1 Publicly available Wordnets']","The AsianWordnet [Cite_Footnote_3] provides a platform for building and sharing Wordnets for Asian languages (e.g., Mongolian, Thai and Viet-namese).",3 http://www.asianWordnet.org/progress,"The PWN is the oldest and the biggest available Wordnet. It is also free. Wordnets in many languages are being constructed and developed . However, only a few of these Wordnets are of high quality and free for downloading. The EuroWord-net (Vossen, 1998) is a multilingual database with Wordnets in European languages (e.g., Dutch, Ital-ian and Spanish). The AsianWordnet [Cite_Footnote_3] provides a platform for building and sharing Wordnets for Asian languages (e.g., Mongolian, Thai and Viet-namese). Unfortunately, the progress in building most of these Wordnets is slow and they are far from being finished.",Material,Knowledge,True,Introduce（引用目的）,True,P14-2018_1_0,2014,Automatically constructing Wordnet synsets,Footnote
2244,12250," http://compling.hss.ntu.edu.sg/omw/"," ['3 Experiments', '3.2 Experimental results and discussion']","For the IWND approach, we use all [Cite_Footnote_4] Wordnets as intermediate resources.",4 http://compling.hss.ntu.edu.sg/omw/,"For the IWND approach, we use all [Cite_Footnote_4] Wordnets as intermediate resources. The number of Wordnet synsets we create using the IWND approach are presented in Table 4. We only construct Wordnet synsets for ajz, asm and dis using the IWND ap-proach because these languages are not supported by MT.",Material,Knowledge,True,Use（引用目的）,True,P14-2018_2_0,2014,Automatically constructing Wordnet synsets,Footnote
2245,12251," http://www.xobdo.org/"," ['3 Experiments', '3.1 Publicly available Wordnets']","For languages not supported by MT, we use three additional bilingual dictionaries: two dictio-naries Dict(eng,ajz) and Dict(eng,dis) provided by Xobdo [Cite_Footnote_5] ; one Dict(eng,asm) created by integrat-ing two dictionaries Dict(eng,asm) provided by Xobdo and Panlex .",5 http://www.xobdo.org/,"For languages not supported by MT, we use three additional bilingual dictionaries: two dictio-naries Dict(eng,ajz) and Dict(eng,dis) provided by Xobdo [Cite_Footnote_5] ; one Dict(eng,asm) created by integrat-ing two dictionaries Dict(eng,asm) provided by Xobdo and Panlex . The dictionaries are of vary-ing qualities and sizes. The total number of entries in Dict(eng,ajz), Dict(eng,asm) and Dict(eng,dis) are 4682, 76634 and 6628, respectively.",補足資料,Website,True,Introduce（引用目的）,True,P14-2018_3_0,2014,Automatically constructing Wordnet synsets,Footnote
2246,12252," http://panlex.org/"," ['3 Experiments', '3.1 Publicly available Wordnets']","For languages not supported by MT, we use three additional bilingual dictionaries: two dictio-naries Dict(eng,ajz) and Dict(eng,dis) provided by Xobdo ; one Dict(eng,asm) created by integrat-ing two dictionaries Dict(eng,asm) provided by Xobdo and Panlex [Cite_Footnote_6] .",6 http://panlex.org/,"For languages not supported by MT, we use three additional bilingual dictionaries: two dictio-naries Dict(eng,ajz) and Dict(eng,dis) provided by Xobdo ; one Dict(eng,asm) created by integrat-ing two dictionaries Dict(eng,asm) provided by Xobdo and Panlex [Cite_Footnote_6] . The dictionaries are of vary-ing qualities and sizes. The total number of entries in Dict(eng,ajz), Dict(eng,asm) and Dict(eng,dis) are 4682, 76634 and 6628, respectively.",補足資料,Website,True,Introduce（引用目的）,True,P14-2018_4_0,2014,Automatically constructing Wordnet synsets,Footnote
2247,12253," http://cs.uccs.edu/∼linclab/projects.html"," ['4 Conclusion and future work']",Some of Word-net synsets we created can be downloaded from [Cite] http://cs.uccs.edu/∼linclab/projects.html.,,"We present approaches to create Wordnet synsets for languages using available Wordnets, a public MT and a single bilingual dictionary. We create Wordnet synsets with good accuracy and high cov-erage for languages with low resources (arb and vie), resource-poor (asm) and endangered (ajz and dis). We believe that our work has the potential to construct full Wordnets for languages which do not have many existing resources. We are in the process of creating a Website where all Wordnet synsets we create will be available, along with a user friendly interface to give feedback on individ-ual entries. We will solicit feedback from commu-nities that use these languages as mother-tongue. Our goal is to use this feedback to improve the quality of the Wordnet synsets. Some of Word-net synsets we created can be downloaded from [Cite] http://cs.uccs.edu/∼linclab/projects.html.",補足資料,Website,True,Introduce（引用目的）,True,P14-2018_5_0,2014,Automatically constructing Wordnet synsets,Body
2248,12254," https://www.python.org/"," ['1 Introduction', '1.2 Design Goals']",• Minimal requirements on the underlying server structure: Sisyphus only requires Python 3 [Cite_Footnote_1] with a few basic packages and a Unix-type operating system.,1 https://www.python.org/,• Minimal requirements on the underlying server structure: Sisyphus only requires Python 3 [Cite_Footnote_1] with a few basic packages and a Unix-type operating system.,Method,Tool,True,Introduce（引用目的）,True,D18-2015_0_0,2018,"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition",Footnote
2249,12255," https://github.com/jhclark/ducttape"," ['2 Related Work']","The toolkit that seems to be most similar to our approach is Ducttape [Cite_Footnote_2] , the successor of LonnyBin (Clark and Lavie, 2010).",2 https://github.com/jhclark/ducttape,"The toolkit that seems to be most similar to our approach is Ducttape [Cite_Footnote_2] , the successor of LonnyBin (Clark and Lavie, 2010). It is well designed and covers many useful points. However, we miss a more flexible configuration of the workflow e.g. workflows that adjust to the outputs of finished jobs are not supported. This does not allow to trig-ger parts of the workflow only if current computa-tions show that they are required.",Method,Tool,True,Compare（引用目的）,True,D18-2015_1_0,2018,"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition",Footnote
2250,12256," http://www.apptek.com/"," ['6 Real-World Usage']",AppTek [Cite_Footnote_3] also uses Sisyphus internally.,3 http://www.apptek.com/,"Sisyphus is extensively used by the machine trans-lation and the automatic speech recognition teams at the RWTH Aachen University. All WMT and IWSLT submissions by the RWTH Aachen Uni-versity since 2015 until now have been created using Sisyphus (Peter et al., 2015b,a). It was used for speech recognition in Zeyer et al. (2017). AppTek [Cite_Footnote_3] also uses Sisyphus internally.",補足資料,Website,True,Introduce（引用目的）,True,D18-2015_2_0,2018,"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition",Footnote
2251,12257," https://github.com/rwth-i6/sisyphus"," ['7 Conclusion']",It is freely available online [Cite_Footnote_4] under the Mozilla License v2.0 to encourage the adoption by other groups.,4 https://github.com/rwth-i6/sisyphus,"We presented overview of our novel workflow manager Sisyphus. Features like automatic er-ror detection, efficient usage of computational re-sources, scalability, easy of reproducibility, abil-ity to share work with others have been proven to be extremely helpful for our research. The large collection of tools for Python can be used with-out modification for editing, debugging, and docu-menting the workflow, since it is written in Python. It is freely available online [Cite_Footnote_4] under the Mozilla License v2.0 to encourage the adoption by other groups.",Method,Tool,True,Produce（引用目的）,True,D18-2015_3_0,2018,"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition",Footnote
2252,12258," https://github.com/zhanglu-cst/ClassKG"," ['4 Experiments', '4.3 Experimental Settings']",Our code has already been released. [Cite_Footnote_1],1 https://github.com/zhanglu-cst/ ClassKG,"The training and evaluation are performed on NVIDIA RTX 2080Ti. In the subgraph annota-tor, we use a three-layer GIN (Xu et al., 2019). We first train it with our self-supervised task 10 6 iter-ations and then finetune it 10 epochs. We set the batch size of self-supervision/finetuning to 50/256. In classifier training, we set the batch size to 4/8 for long/short texts. Both the subgraph annotator and the text classifier use AdamW (Loshchilov and Hutter, 2019) as optimizer. Their learning rates are 1e-4 and 2e-6, respectively. The classifier uses bert-base-uncased for short texts and longformer-base-4096 for long texts. For keywords extraction, we select top 100 keywords per class in each it-eration. The hyperparameter M is set to 4. The keywords set change threshold is set to 0.1. Our code has already been released. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.222_0_0,2021,Weakly-supervised Text Classification Based on Keyword Graph,Footnote
2253,12259," http://www.statmt.org/wmt15/quality-estimation-task.html"," ['4 Experiments']",[Cite_Footnote_9] at sen-tence level of English-Spanish.,9 http://www.statmt.org/wmt15/quality-estimation-task.html,The proposed RNNs approach was evaluated on the WMT15 Quality Estimation Shared Task [Cite_Footnote_9] at sen-tence level of English-Spanish.,補足資料,Website,True,Introduce（引用目的）,True,N16-1059_0_0,2016,A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output,Footnote
2254,12260," http://fever.ai/"," ['5 Related Work']","More re-cently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking [Cite_Footnote_4] .",4 http://fever.ai/,"The majority of previous works deal with tex-tual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direc-tion, where evidence sentences come from 5.4 mil-lion Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. docu-ment retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More re-cently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking [Cite_Footnote_4] . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study abstractive summarization (Zhang et al., 2019).",Material,Dataset,False,Introduce（引用目的）,True,2020.acl-main.539_0_0,2020,LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network,Footnote
2255,12261," https://www.kaggle.com/c/fake-news-pair-classification-challenge/"," ['5 Related Work']","There is a fake news detection challenge [Cite_Footnote_5] hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published.",5 https://www.kaggle.com/c/fake-news-pair-classification-challenge/,"Statement In 2004, the score is less than 270. Label REFUTED Program 𝑙𝑒𝑠𝑠( ℎ𝑜𝑝( 𝑓𝑖𝑙𝑡𝑒𝑟_𝑒𝑞( 𝑌𝑒𝑎𝑟; 2004);𝑆𝑐𝑜𝑟𝑒); 270) Figure 1: An example of table-based fact checking. Given a statement and a table as the input, the task is Row 0 2005 Arlandastad 272David Patrick Row 1 Matthew King2004 270Arlandastad Row 2 2003 Falsterbo Titch Moore 273 fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six fine-grained labels and further uses meta-data features. There is a fake news detection challenge [Cite_Footnote_5] hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the fac-tual accuracy of the generated summary in neural abstractive summarization systems (Goodrich et al., 2019; Kryściński et al., 2019), as well as the use of this factual accuracy as a reward to improve",補足資料,Website,True,Introduce（引用目的）,True,2020.acl-main.539_1_0,2020,LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network,Footnote
2256,12262," http://goo.gl/7KG2U"," ['3 A Resource for German Derivation']","We use version 1.3 of DE RIV B ASE (Zeller et al., 2013), [Cite_Footnote_1] a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 non-singleton derivational families.",1 Downloadable from: http://goo.gl/7KG2U,"For German, there are several resources with derivational information. We use version 1.3 of DE RIV B ASE (Zeller et al., 2013), [Cite_Footnote_1] a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 non-singleton derivational families. It has a precision of 84% and a recall of 71%. Its higher coverage com-pared to C ELEX (Baayen et al., 1996) and IMSL EX (Fitschen, 2004) makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas.",Material,Knowledge,True,Use（引用目的）,True,P13-2128_0_0,2013,Derivational Smoothing for Syntactic Distributional Semantics,Footnote
2257,12263," http://goo.gl/bFokI"," ['5 Experimental Evaluation']","We lemmatized and POS-tagged the German G UR 350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analo-gously to the well-known Rubenstein and Good-enough (1965) dataset for English. [Cite_Footnote_2]",2 Downloadable from: http://goo.gl/bFokI,"Experiments. We evaluate the impact of smooth-ing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German G UR 350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analo-gously to the well-known Rubenstein and Good-enough (1965) dataset for English. [Cite_Footnote_2] We predict semantic similarity as cosine similarity. We make a prediction for a word pair if both words are repre-sented in the semantic space and their vectors have a non-zero similarity.",Material,Knowledge,True,Produce（引用目的）,True,P13-2128_1_0,2013,Derivational Smoothing for Syntactic Distributional Semantics,Footnote
2258,12264," http://www.nist.gov/itl/iad/mig/openmt12.cfm"," ['4 Experiments', '4.1 Data setting']","We carried out experiments in two different settings, both involving data from NIST Open MT 2012. [Cite_Footnote_2]",2 http://www.nist.gov/itl/iad/mig/openmt12.cfm,"We carried out experiments in two different settings, both involving data from NIST Open MT 2012. [Cite_Footnote_2] The first setting uses data from the Chinese to En-glish constrained track, comprising 283M English tokens. We manually identified 14 sub-corpora on the basis of genres and origins. Table 1 summarizes the statistics and genres of all the training corpora and the development and test sets; for the training corpora, we show their size in number of words as a percentage of all training data. Most training cor-pora consist of parallel sentence pairs. The isi and lex&ne corpora are exceptions: the former is ex-tracted from comparable data, while the latter is a lexicon that includes many named entities. The de-velopment set (tune) was taken from the NIST 2005 evaluation set, augmented with some web-genre ma-terial reserved from other NIST corpora.",補足資料,Website,True,Introduce（引用目的）,True,N13-1114_0_0,2013,Adaptation of Reordering Models for Statistical Machine Translation,Footnote
2259,12265," http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmt"," ['6 Related work']",The 2012 JHU workshop on Domain Adapta-tion for MT [Cite_Footnote_4] proposed phrase sense disambiguation (PSD) for translation model adaptation.,4 http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmt,"The 2012 JHU workshop on Domain Adapta-tion for MT [Cite_Footnote_4] proposed phrase sense disambiguation (PSD) for translation model adaptation. In this ap-proach, the context of a phrase helps the system to find the appropriate translation.",補足資料,Website,True,Introduce（引用目的）,True,N13-1114_1_0,2013,Adaptation of Reordering Models for Statistical Machine Translation,Footnote
2260,12266," http://ssl-acl08.wikidot.com/"," ['1 Introduction']","For more background on graph-based and general SSL and their applica-tions, see (Zhu, 2005a; Zhu and Ghahra-mani, 2002)7; Zhu and Ghahra-mani, 2002)8 [Cite_Ref] ).","Blitzer, J. and Zhu, J. (2008). ACL 2008 tutorial on Semi-Supervised learning. http://ssl-acl08.wikidot.com/.","Most graph-based SSL algorithms fall under one of two categories – those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahra-mani, 2002) and those that optimize a loss function based on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005). Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003). In general graph-based SSL algorithms are non-parametric and transductive. A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training. In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005). A common drawback of many graph-based SSL algorithms (e.g. (Blum and Chawla, 2001; Joachims, 2003; Belkin et al., 2005)) is that they assume binary classification tasks and thus require the use of sub-optimal (and often com-putationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone struc-tured domains such as strings and trees. There are also issues related to degenerate solutions (all un-labeled samples classified as belonging to a single class) (Blum and Chawla, 2001; Joachims, 2003; Zhu and Ghahramani, 2002). For more background on graph-based and general SSL and their applica-tions, see (Zhu, 2005a; Chapelle et al., 2007; Blitzer and Zhu, 2008 [Cite_Ref] ).",補足資料,Paper,True,Introduce（引用目的）,True,D08-1114_0_0,2008,Soft-Supervised Learning for Text Classification,Reference
2261,12267," http://www.daviddlewis.com/resources/"," ['5 Results', '5.1 Reuters-21578']","We used the “ModApte” split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987) [Cite_Ref] .","Lewis, D. et al. (1987). Reuters-21578. http: //www.daviddlewis.com/resources/","We used the “ModApte” split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987) [Cite_Ref] . The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents D u ). Of the 135 poten-tial topic categories only the 10 most frequent cate-gories are used (Joachims, 1999). Categories outside the 10 most frequent were collapsed into one class and assigned a label “other”. For each document i in the training and test sets, we extract features x i in the following manner: stop-words are removed fol-lowed by the removal of case and information about inflection (i.e., stemming) (Porter, 1980). We then compute TFIDF features for each document (Salton and Buckley, 1987). All graphs were constructed us-ing cosine similarity with TFIDF features.",Material,Dataset,True,Use（引用目的）,True,D08-1114_3_0,2008,Soft-Supervised Learning for Text Classification,Reference
2262,12268," https://github.com/hao-cheng/ds_doc_qa"," ['1 Introduction']","Results are further strengthened by transfer learning from fully labeled short-answer extrac-tion data in SQuAD 2.0 (Rajpurkar et al., 2018), leading to a final state-of-the-art performance of 76.3 F1 on TriviaQA-Wiki and 62.9 on the Narra-tiveQA summaries task. [Cite_Footnote_2]",2 The code is available at https://github.com/hao-cheng/ds_doc_qa,"We show that the choice of probability space puts constraints on the distant supervision as-sumptions that can be captured, and that all three choices interact, leading to large differences in performance. Specifically, we provide a frame-work for understanding different distant supervi-sion assumptions and the corresponding trade-off among the coverage, quality and strength of dis-tant supervision signal. The best configuration de-pends on the properties of the possible annotations A and is thus data-dependent. Compared with re-cent work also using BERT representations, our study show that the model with most suitable prob-abilistic treatment achieves large improvements of 4.6 F1 on TriviaQA and 1.7 Rouge-L on Narra-tiveQA respectively. Additionally, we design an efficient multi-loss objective that can combine the benefits of different formulations, leading to sig-nificant improvements in accuracy, surpassing the best previously reported results on the two studied tasks. Results are further strengthened by transfer learning from fully labeled short-answer extrac-tion data in SQuAD 2.0 (Rajpurkar et al., 2018), leading to a final state-of-the-art performance of 76.3 F1 on TriviaQA-Wiki and 62.9 on the Narra-tiveQA summaries task. [Cite_Footnote_2]",補足資料,Website,False,Introduce（引用目的）,True,2020.acl-main.501_0_0,2020,Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering,Footnote
2263,12269," https://github.com/google-research/bert"," ['2 Probability Space']","In the case that p k does not support answering the question q, special NULL positions are selected (follow-ing the SQuAD 2.0 BERT implementation [Cite_Footnote_3] ).",3 https://github.com/google-research/bert,"Paragraph-level model In paragraph level models, we assume that for a given question against a document d, each of its paragraphs p 1 , . . . , p K independently selects a pair of answer positions (i k ,j k ), which are the begin and end of the answer from paragraph p k . In the case that p k does not support answering the question q, special NULL positions are selected (follow-ing the SQuAD 2.0 BERT implementation [Cite_Footnote_3] ). Thus, the set of possible outcomes Ω in the paragraph-level probability space is the set of lists of begin/end position pairs, one from each paragraph: {[(i 1 , j 1 ), . . . , (i K , j K )]}, where i k and j k range over positions in the respective paragraphs.",Method,Code,False,Use（引用目的）,False,2020.acl-main.501_1_0,2020,Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering,Footnote
2264,12270," https://github.com/allenai/document-qa"," ['5 Experiments', '5.1 Data and Implementation']","Using the same preprocessing as Clark and Gardner (2018) for TriviaQA-Wiki [Cite_Footnote_6] , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation.",6 https://github.com/allenai/document-qa,"Two datasets are used in this paper: TriviaQA (Joshi et al., 2017) in its Wikipedia formulation, and NarrativeQA (summaries setting) (Kočiský et al., 2018). Using the same preprocessing as Clark and Gardner (2018) for TriviaQA-Wiki [Cite_Footnote_6] , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al. (2019), for NarrativeQA we define the possible answer string sets A using Rouge-L (Lin, 2004) similarity with crouwdsourced abstractive answer strings. We use identical data preprocessing and the evaluation script provided by the authors.",Method,Code,True,Produce（引用目的）,True,2020.acl-main.501_2_0,2020,Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering,Footnote
2265,12271," http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt"," ['5 Applying V-measure', '5.1 Document Clustering']","Using a subset of the TDT-4 cor-pus (Strassel and Glenn, 2003) [Cite_Ref] (1884 English news wire and broadcast news documents manually la-beled with one of 12 topics), we ran clustering experiments using k-means clustering (McQueen, 1967) and evaluated the results using V-Measure, VI and Q 0 – those measures that satisfied the de-sirable properties defined in section 4.",S. Strassel and M. Glenn. 2003. Creating the annotated tdt-4 y2003 evaluation corpus. http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt.,"Clustering techniques have been used widely to sort documents into topic clusters. We reproduce such an experiment here to demonstrate the usefulness of V-measure. Using a subset of the TDT-4 cor-pus (Strassel and Glenn, 2003) [Cite_Ref] (1884 English news wire and broadcast news documents manually la-beled with one of 12 topics), we ran clustering experiments using k-means clustering (McQueen, 1967) and evaluated the results using V-Measure, VI and Q 0 – those measures that satisfied the de-sirable properties defined in section 4. The top-ics and relative distributions are as follows: Acts of Violence/War (22.3%), Elections (14.4%), Diplo-matic Meetings (12.9%), Accidents (8.75%), Natu-ral Disasters (7.4%), Human Interest (6.7%), Scan-dals (6.5%), Legal Cases (6.4%), Miscellaneous (5.3%), Sports (4.7), New Laws (3.2%), Science and Discovery (1.4%).",Material,DataSource,True,Use（引用目的）,True,D07-1043_0_0,2007,V-Measure: A conditional entropy-based external cluster evaluation measure,Reference
2266,12272," http://turing.iimas.unam.mx/wix/MexSeg"," ['3 Morphological Segmentation Datasets']","In order to make follow-up work on minimal-resource settings for morphological segmentation easily comparable, we provide pre-defined splits of our datasets [Cite_Footnote_2] .",2 Our datasets can be found to-gether with the code of our models at http://turing.iimas.unam.mx/wix/MexSeg.,"Final splits. In order to make follow-up work on minimal-resource settings for morphological segmentation easily comparable, we provide pre-defined splits of our datasets [Cite_Footnote_2] . 40% of the data constitute the test sets. Of the remaining data, we use 20% for development and the rest for training. The final numbers of words per dataset and lan-guage are shown in Table 2.",Material,Dataset,True,Produce（引用目的）,True,N18-1005_0_0,2018,Fortification of Neural Morphological Segmentation Models for Polysynthetic Minimal-Resource Languages,Footnote
2267,12273," https://github.com/nyu-mll/jiant/tree/bert-friends-exps"," ['5 Models and Experimental Details']","We implement our models using the jiant toolkit, [Cite_Footnote_3] which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT.",3 https://github.com/nyu-mll/jiant/tree/bert-friends-exps,"We implement our models using the jiant toolkit, [Cite_Footnote_3] which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. Appendix A presents addi-tional details.",Method,Tool,True,Use（引用目的）,True,P19-1439_0_0,2019,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,Footnote
2268,12274," https://github.com/huggingface/pytorch-pretrained-BERT"," ['5 Models and Experimental Details']","We implement our models using the jiant toolkit, which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. [Cite_Footnote_4]",4 https://github.com/huggingface/ pytorch-pretrained-BERT,"We implement our models using the jiant toolkit, which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. [Cite_Footnote_4] Appendix A presents addi-tional details.",Method,Tool,False,Introduce（引用目的）,True,P19-1439_1_0,2019,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,Footnote
2269,12275," http://lucene.apache.org/java/"," ['5 Experiments']","Using the search en-gine [Cite_Footnote_2] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs.",2 http://lucene.apache.org/java/,"We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [Cite_Footnote_2] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set. We also used a set of 340 QA-type sentence pairs from RTE02-03 and 195 pairs from RTE04 by convert-ing the hypothesis sentences into question form to create additional set of q/a pairs. In total, we cre-ated labeled training dataset X L of around 7600 q/a pairs . We evaluated the performance of graph-based QA system using a set of 202 questions from the TREC04 as testing dataset (Voorhees, 2003), (Prager et al., 2000). We retrieved around 20 can-didate sentences for each of the 202 test questions and manually labeled each q/a pair as true/false en-tailment to compile 4037 test data.",Method,Tool,True,Use（引用目的）,True,P09-1081_0_0,2009,A Graph-based Semi-Supervised Learning for Question-Answering Asli Celikyilmaz Marcus Thint Zhiheng Huang,Footnote
2270,12276," https://github.com/subhadarship/learning-to-unjumble"," ['References']",Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations. [Cite_Footnote_1],1 The code is available at https://github.com/subhadarship/learning-to-unjumble.,"State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train models that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our approach. Applying random shuffling strategy on the word-level, we found that our approach enables the RoBERTa model achieve better performance on 4 out of 7 GLUE tasks. Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.naacl-srw.12_0_0,2021,Shuffled-token Detection for Refining Pre-trained RoBERTa,Footnote
2271,12277," https://github.com/scripts"," ['4 Experiments', '4.2 Dataset for Shuffled-Token Detection']",We extracted 133K articles from Wikidump. [Cite_Footnote_2],"2 Timestamp May 9th, 2020. We used the https://github.com/scripts from NVIDIA/DeepLearningExamples/tree/ master/PyTorch/LanguageModeling/BERT# getting-the-data to extract the data.",We extracted 133K articles from Wikidump. [Cite_Footnote_2] We used each paragraph in the extracted text as a data sample for our model. We filtered out samples that were either spaces-only or had more than 512 tokens after tokenizing with the pretrained RobertaTokenizer of the roberta-base model. We finally randomly split the samples into 1.3M for training and 14K for validation.,Method,Code,False,Use（引用目的）,True,2021.naacl-srw.12_1_0,2021,Shuffled-token Detection for Refining Pre-trained RoBERTa,Footnote
2272,12278," https://github.com/huggingface/transformers/blob/v2.8.0/examples/run_glue.py"," ['4 Experiments', '4.4 Downstream Evaluation']",The rest of the hyperparameters are set to default values. [Cite_Footnote_4],4 The default hyperparameters are as in https: //github.com/huggingface/transformers/ blob/v2.8.0/examples/run_glue.py.,"We evaluate our approach on 7 GLUE tasks us-ing the metrics outlined in Table 1. We use the same set of hyperparameters for fine-tuning for downstream tasks for each approach for a fair com-parison. Methods for comparison to our approach include (a) the baseline approach where the training objective is detecting masked tokens, and (b) the plain pre-trained RoBERTa base model. The values of hyperparameters used for GLUE fine-tuning are outlined in Table 2. The rest of the hyperparameters are set to default values. [Cite_Footnote_4]",Method,Code,True,Produce（引用目的）,False,2021.naacl-srw.12_2_0,2021,Shuffled-token Detection for Refining Pre-trained RoBERTa,Footnote
2273,12279," http://www.cs.ualberta.ca/~ab31/langid/"," ['5 Application to machine transliteration', '5.1 Data']","We manually classified these names as being of ei-ther Indian or non-Indian origin, occasionally resort-ing to web searches to help disambiguate them. [Cite_Footnote_1]",1 Our tagged data are available online at http://www.cs.ualberta.ca/˜ab31/langid/.,"The English-Hindi corpus of names (Li et al., 2009; MSRI, 2009) contains a test set of 1000 names rep-resented in both the Latin and Devanagari scripts. We manually classified these names as being of ei-ther Indian or non-Indian origin, occasionally resort-ing to web searches to help disambiguate them. [Cite_Footnote_1] We discarded those names that fell into both categories (e.g. “Maya”) as well as those that we could not confidently classify. In total, we discarded 95 of these names, and randomly selected 95 names from the training set that we could confidently classify to complete our corpus of 1000 names. Of the 1000 names, 546 were classified as being of Indian origin and the remaining 454 were classified as being of non-Indian origin; the names have an average length of 7.0 characters.",Material,Knowledge,True,Produce（引用目的）,True,N10-1102_0_0,2010,Language identification of names with SVMs,Footnote
2274,12280," http://www.csie.ntu.edu.tw/~cjlin/libsvm"," ['3 Language identification with SVMs']","In our experiments, we used the LIBLINEAR (Fan et al., 2008) package for the linear kernel and the LIBSVM (Chang and Lin, 2001) [Cite_Ref] package for the RBF and sigmoid kernels.","C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a li-brary for support vector machines. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.","In our experiments, we used the LIBLINEAR (Fan et al., 2008) package for the linear kernel and the LIBSVM (Chang and Lin, 2001) [Cite_Ref] package for the RBF and sigmoid kernels. We discarded any peri-ods and parentheses, but kept apostrophes and hy-phens, and we converted all letters to lower case. We removed very short names of length less than two. For all data sets, we held out 10% of the data as the test set. We then found optimal parameters for each kernel type using 10-fold cross-validation on the remaining training set. This yielded optimum maximum n-gram lengths of four for single names and five for full names. Using the optimal parame-ters, we constructed models from the entire training data and then tested the models on the held-out test set.",Method,Tool,True,Use（引用目的）,True,N10-1102_1_0,2010,Language identification of names with SVMs,Reference
2275,12281," http://research.microsoft.com/india"," ['5 Application to machine transliteration', '5.1 Data']","The English-Hindi corpus of names (Li et al., 2009; MSRI, 2009 [Cite_Ref] ) contains a test set of 1000 names rep-resented in both the Latin and Devanagari scripts.","MSRI, 2009. Microsoft Research India. http://research.microsoft.com/india.","The English-Hindi corpus of names (Li et al., 2009; MSRI, 2009 [Cite_Ref] ) contains a test set of 1000 names rep-resented in both the Latin and Devanagari scripts. We manually classified these names as being of ei-ther Indian or non-Indian origin, occasionally resort-ing to web searches to help disambiguate them. We discarded those names that fell into both categories (e.g. “Maya”) as well as those that we could not confidently classify. In total, we discarded 95 of these names, and randomly selected 95 names from the training set that we could confidently classify to complete our corpus of 1000 names. Of the 1000 names, 546 were classified as being of Indian origin and the remaining 454 were classified as being of non-Indian origin; the names have an average length of 7.0 characters.",補足資料,Website,True,Introduce（引用目的）,True,N10-1102_2_0,2010,Language identification of names with SVMs,Reference
2276,12282," http://statmt.org/wmt19/translation-task.html"," ['3 Experimental Setup', '3.1 Datasets']","We also collect additional monolingual data from WMT news-crawl, news-commentary, common-crawl, europarl-v9, news-discussions and wikidump datasets in all 16 lan-guages including English. [Cite_Footnote_2]","2 Followed the versions recommended by WMT’19 shared task, as in http://statmt.org/wmt19/translation-task.html","We use the parallel and monolingual training data provided with the WMT corpus, for 15 languages to and from English. The amount of parallel data available ranges from more than 60 million sen-tence pairs as in En-Cs to roughly 10k sentence pairs as in En-Gu. We also collect additional monolingual data from WMT news-crawl, news-commentary, common-crawl, europarl-v9, news-discussions and wikidump datasets in all 16 lan-guages including English. [Cite_Footnote_2] The amount of mono-lingual data varies from 2 million sentences in Zh to 270 million in De. The distribution of our paral-lel and monolingual data is depicted in Figure 1.",補足資料,Website,True,Introduce（引用目的）,True,2020.acl-main.252_0_0,2020,Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation,Footnote
2277,12283," https://github.com/google/sentencepiece"," ['3 Experimental Setup', '3.3 Architecture and Optimization']","Specifically, we use the Transformer Big model containing 375M parame-ters (6 layers, 16 heads, 8192 hidden dimension) (Chen et al., 2018) and a shared source-target Sen-tencePiece model (SPM) [Cite_Footnote_3] (Kudo and Richardson, 2018).",3 https://github.com/google/sentencepiece,"All experiments are performed with the Trans-former architecture (Vaswani et al., 2017) using the open-source Tensorflow-Lingvo implementa-tion (Shen et al., 2019). Specifically, we use the Transformer Big model containing 375M parame-ters (6 layers, 16 heads, 8192 hidden dimension) (Chen et al., 2018) and a shared source-target Sen-tencePiece model (SPM) [Cite_Footnote_3] (Kudo and Richardson, 2018). We use a vocabulary size of 32k for the bilingual models and 64k for the multilingual mod-els. Different SPMs are trained depending on the set of languages supported by the model.",Method,Tool,True,Use（引用目的）,True,2020.acl-main.252_1_0,2020,Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation,Footnote
2278,12284," https://github.com/lijuncen/Sentiment-and-Style-Transfer"," ['1 Introduction']","Our code and data, including newly collected human reference outputs for the Yelp and Amazon domains, can be found at [Cite] https://github.com/lijuncen/Sentiment-and-Style-Transfer.",,"We test our methods on three text attribute transfer datasets: altering sentiment of Yelp reviews, altering sentiment of Amazon reviews, and altering image captions to be more roman-tic or humorous. Averaged across these three datasets, our simple baseline generated gram-matical sentences with appropriate content and attribute 23% of the time, according to human raters; in contrast, the best adversarial method achieved only 12%. Our best neural system in turn outperformed our baseline, achieving an average success rate of 34%. Our code and data, including newly collected human reference outputs for the Yelp and Amazon domains, can be found at [Cite] https://github.com/lijuncen/Sentiment-and-Style-Transfer.",Mixed,Mixed,True,Produce（引用目的）,True,N18-1169_0_0,2018,"Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer",Body
2279,12285," https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948/"," ['5 Related Work and Discussion']","All code, data, and ex-periments for this paper are available on the CodaLab platform at [Cite] https://worksheets.",,"Reproducibility. All code, data, and ex-periments for this paper are available on the CodaLab platform at [Cite] https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948/.",Mixed,Mixed,True,Produce（引用目的）,True,N18-1169_1_0,2018,"Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer",Body
2280,12286," https://github.com/antonisa/embeddings"," ['References']","Second, we both expand a standard English-centered evaluation dictionary collection to in-clude all language pairs using triangulation, and create new dictionaries for under-represented languages. [Cite_Footnote_1]",1 Available at https://github.com/antonisa/embeddings.,"Most of recent work in cross-lingual word em-beddings is severely Anglocentric. The vast majority of lexicon induction evaluation dic-tionaries are between English and another lan-guage, and the English embedding space is se-lected by default as the hub when learning in a multilingual setting. With this work, how-ever, we challenge these practices. First, we show that the choice of hub language can sig-nificantly impact downstream lexicon induc-tion and zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to in-clude all language pairs using triangulation, and create new dictionaries for under-represented languages. [Cite_Footnote_1] Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we iden-tify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.",Material,Knowledge,True,Produce（引用目的）,True,2020.acl-main.766_0_0,2020,Should All Cross-Lingual Embeddings Speak English?,Footnote
2281,12287," https://github.com/antonisa/at"," ['3 New LI Evaluation Dictionaries']","In Following this simple triangulation approach, we cre-the case of feature mismatch (for instance, Greek uses 2 ate 4,704 new dictionaries over pairs between the 50 numbers, 4 cases and 3 genders while Italian has 2 num-languages of the MUSE dictionaries. [Cite_Footnote_9]","9 Available unable to filter dictionaries in the following 11 languages: aze,https://github.com/antonisa/atembeddings. bel, ben, bos, lit, mkd, msa, sqi, tam, tha, tel.","3.2 Dictionaries for all Language Pairs through matical gender is a more complicated matter: it is not Triangulation uncommon for word translations to be of different gram-matical gender across languages). Our second method for creating new dictionaries is Hence, we devise a filtering method for removing bla-inspired by phrase table triangulation ideas from the tant mistakes when triangulating morphologically rich pre-neural MT community (Wang et al., 2006; Levin-languages. We rely on automatic morphological tagging boim and Chiang, 2015). The concept can be easily which we can obtain for most of the MUSE languages, explained with an example, visualized in Figure 1. Con-using the StanfordNLP toolkit (Qi et al., 2020). 10 The sider the Portuguese (Pt) word trabalho which, ac-morphological tagging uses the Universal Dependen-cording to the MUSE Pt–En dictionary, has the words cies feature set (Nivre et al., 2016) making the tagging job and work as possible En translations. In turn, these comparable across almost all languages. Our filtering two En words can be translated to 4 and 5 Czech (Cs) technique iterates through the bridged dictionaries: for a words respectively. By utilizing the transitive property given source word, if we find a target word with the ex- (which translation should exhibit) we can identify the set act same morphological analysis, we filter out all other of 5 possible Cs translations for the Pt word trabalho. translations with the same lemma but different tags. In Following this simple triangulation approach, we cre-the case of feature mismatch (for instance, Greek uses 2 ate 4,704 new dictionaries over pairs between the 50 numbers, 4 cases and 3 genders while Italian has 2 num-languages of the MUSE dictionaries. [Cite_Footnote_9] For consistency, bers, 2 genders, and no cases) or if we only find a partial we keep the same train and test splits as with MUSE, so tag match over a feature subset, we filter out transla-that the source-side types are equal across all dictionar-tions with disagreeing tags. We ignore the grammatical ies with the same source language. gender and verb form features, as they are not directly Triangulating through English (which is unavoid-comparable cross-lingually. Coming back to our Greek– able, due to the relative paucity of non-English-centric Italian example, this means that for the form ειρηνικός dictionaries) is suboptimal – English is morphologi-we would only keep pacifico as a candidate transla-cally poor and lacks corresponding markings for gen-tion (we show more examples in Table 1). der, case, or other features that are explicitly marked in morphologically-rich languages map to the same En-ingly, we find that bridged dictionaries between mor-glish form. Similarly, gendered nouns or adjectives phologically rich languages require a lot more filter-in gendered languages map to English forms that lack ing. For instance more than 80% of the entries of the gender information. For example, the MUSE Greek– Urdu-Greek dictionary get filtered out. On average, the English dictionary lists the word peaceful as the trans-languages with more filtered entries are Urdu (62.4%), lation for all ειρηνικός, ειρηνική, ειρηνικό, ειρηνικά, Turkish (61.1%), and German (58.6%). On the other which are the male, female, and neutral (singular and hand, much fewer entries are removed from dictionaries plural) inflections of the same adjective. Equivalently, with languages like Dutch (36.2%) or English (38.1%). the English–Italian dictionary translates peaceful into sum of the GH distances between the source–hub and of the evaluation set. In our 10-languages experiments, target–hub spaces. On our distant languages experiment, a language different than the source and the target yields the correlation coefficient between P@1 and GH is 0.45, the best accuracy for over 93% of the evaluation sets, while it is slightly lower (0.34) for our 10-languages with the difference being statistically significant in more experiment. Figure 3 shows two high correlation exam-than half such cases. Similarly, in the distant-languages ples, namely Gl–En and En–Hi.",Material,Knowledge,True,Introduce（引用目的）,False,2020.acl-main.766_1_0,2020,Should All Cross-Lingual Embeddings Speak English?,Footnote
2282,12288," https://github.com/ccsasuke/umwe"," ['4 Lexicon Induction Experiments', '4.1 Methods and Setup']","We learn MWE with the MAT+MPSR method using the pub-licly available code, [Cite_Footnote_12] aligning several language subsets varying the hub language.",12 https://github.com/ccsasuke/umwe is English in only 17 instances (less than 20% of the,"We train and evaluate all models starting with pre-trained Wikipedia FastText embeddings for all lan-guages (Grave et al., 2018). We focus on the minimally supervised scenario which only uses similar character strings between any languages for supervision in order to mirror the hard, realistic scenario of not having anno-tated training dictionaries between the languages. We learn MWE with the MAT+MPSR method using the pub-licly available code, [Cite_Footnote_12] aligning several language subsets varying the hub language. We decided against compar-ing to the incremental hub (IHS) method of Heyman et al. (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. We also do not com-pare to UMH, as we consider it conceptually similar to MAT+MPSR and no code is publicly available. For BWE experiments, we use MUSEs 14 (MUSE, semisupervised) and VecMap 15 systems, and we additionally compare them to MAT+MPSR for completeness.",Method,Code,True,Use（引用目的）,True,2020.acl-main.766_2_0,2020,Should All Cross-Lingual Embeddings Speak English?,Footnote
2283,12289," https://github.com/facebookresearch/MUSE"," ['4 Lexicon Induction Experiments', '4.1 Methods and Setup']","We decided against compar-ing to the incremental hub (IHS) method of Heyman et al. (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. [Cite_Footnote_13]","13 We refer the reader to Table 2 from Heyman et al. (2019) which compares to MAT+MPSR, and to Table 7 of their appendix 14 https://github.com/facebookresearch/MUSE which shows the dramatic influence of language order. 15 https://github.com/artetxem/vecmap","We train and evaluate all models starting with pre-trained Wikipedia FastText embeddings for all lan-guages (Grave et al., 2018). We focus on the minimally supervised scenario which only uses similar character strings between any languages for supervision in order to mirror the hard, realistic scenario of not having anno-tated training dictionaries between the languages. We learn MWE with the MAT+MPSR method using the pub-licly available code, aligning several language subsets varying the hub language. We decided against compar-ing to the incremental hub (IHS) method of Heyman et al. (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. [Cite_Footnote_13] We also do not com-pare to UMH, as we consider it conceptually similar to MAT+MPSR and no code is publicly available. For BWE experiments, we use MUSEs 14 (MUSE, semisupervised) and VecMap 15 systems, and we additionally compare them to MAT+MPSR for completeness.",補足資料,Paper,False,Introduce（引用目的）,True,2020.acl-main.766_3_0,2020,Should All Cross-Lingual Embeddings Speak English?,Footnote
2284,12290," https://github.com/artetxem/vecmap"," ['4 Lexicon Induction Experiments', '4.1 Methods and Setup']","We decided against compar-ing to the incremental hub (IHS) method of Heyman et al. (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. [Cite_Footnote_13]","13 We refer the reader to Table 2 from Heyman et al. (2019) which compares to MAT+MPSR, and to Table 7 of their appendix 14 https://github.com/facebookresearch/MUSE which shows the dramatic influence of language order. 15 https://github.com/artetxem/vecmap","We train and evaluate all models starting with pre-trained Wikipedia FastText embeddings for all lan-guages (Grave et al., 2018). We focus on the minimally supervised scenario which only uses similar character strings between any languages for supervision in order to mirror the hard, realistic scenario of not having anno-tated training dictionaries between the languages. We learn MWE with the MAT+MPSR method using the pub-licly available code, aligning several language subsets varying the hub language. We decided against compar-ing to the incremental hub (IHS) method of Heyman et al. (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. [Cite_Footnote_13] We also do not com-pare to UMH, as we consider it conceptually similar to MAT+MPSR and no code is publicly available. For BWE experiments, we use MUSEs 14 (MUSE, semisupervised) and VecMap 15 systems, and we additionally compare them to MAT+MPSR for completeness.",Method,Tool,True,Introduce（引用目的）,False,2020.acl-main.766_4_0,2020,Should All Cross-Lingual Embeddings Speak English?,Footnote
2285,12291," https://github.com/xijiz/cfgen"," ['References']",The code is available at [Cite] https://github.com/xijiz/cfgen.,,"Past progress on neural models has proven that named entity recognition is no longer a prob-lem if we have enough labeled data. How-ever, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual ex-amples by the interventions on the existing observational examples to enhance the origi-nal dataset. Experiments across three datasets show that our method improves the generaliza-tion ability of models under limited observa-tional examples. Besides, we provide a theo-retical foundation by using a structural causal model to explore the spurious correlations be-tween input features and output labels. We in-vestigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Inter-estingly, we find that the non-spurious corre-lations are more located in entity representa-tion rather than context representation. As a result, our method eliminates part of the spu-rious correlations between context representa-tion and output labels. The code is available at [Cite] https://github.com/xijiz/cfgen.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.590_0_0,2020,Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,Body
2286,12292," http://www.ccks2019.cn/?pageid=62"," ['3 Experiments', '3.1 Dataset']","CNER [Cite_Footnote_1] CNER is a Chinese clinical NER dataset in the CCKS-2019 challenge, including anatomy, disease, imaging examination, laboratory examina-tion, drug, and operation.",1 http://www.ccks2019.cn/?page id=62,"CNER [Cite_Footnote_1] CNER is a Chinese clinical NER dataset in the CCKS-2019 challenge, including anatomy, disease, imaging examination, laboratory examina-tion, drug, and operation. We extract 1650 avail-able medical records from CNER, which contains entities of the disease type only.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.590_1_0,2020,Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,Footnote
2287,12293," https://labelstud.io/"," ['3 Experiments', '3.1 Dataset']","IDiag For guaranteeing the diversity of the ex-perimental data, we use Label Studio [Cite_Footnote_2] to create a new medical NER dataset.",2 https://labelstud.io/,"IDiag For guaranteeing the diversity of the ex-perimental data, we use Label Studio [Cite_Footnote_2] to create a new medical NER dataset. We collect 12127 health record images from the hospital, which are converted into text paragraphs by optical character recognition (OCR). We hire some people to anno-tate diagnoses in these text paragraphs. To ensure the high quality of the dataset, we removed 539 data examples in the final dataset. It is worth noting that the distribution of IDiag, compared to CNER, has a big difference due to error text recognition from OCR.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.590_2_0,2020,Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,Footnote
2288,12294," https://www.cluebenchmarks.com/"," ['3 Experiments', '3.1 Dataset']","In addition to the medical NER datasets, we also use a conven-tional NER dataset CLUENER released by CLUE organization [Cite_Footnote_3] , which is a well-defined and fine-grained dataset for named entity recognition in Chi-nese, including 10 categories like Person Name, Organzation, Book, etc.",3 https://www.cluebenchmarks.com/,"CLUENER (Xu et al., 2020) In addition to the medical NER datasets, we also use a conven-tional NER dataset CLUENER released by CLUE organization [Cite_Footnote_3] , which is a well-defined and fine-grained dataset for named entity recognition in Chi-nese, including 10 categories like Person Name, Organzation, Book, etc. We extract 12090 avail-able instances from this dataset.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.590_3_0,2020,Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,Footnote
2289,12295," http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html"," ['1 Introduction']",Consider the publicly available EHR note [Cite_Footnote_1] in Figure 1.,1 http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html,"Consider the publicly available EHR note [Cite_Footnote_1] in Figure 1. The note is divided into the 10 sections found in that EHR (Problems, Medications, His-tory, etc.). This example provides insight into why section prediction can be a difficult task. Although most of the headers appear to be bold, there is also plenty of bold text which is not the main header (e.g. see the History section). Additionally, in some cases (e.g. Allergies section) there is no text under the header at all. This makes it difficult to segment the data appropriately. Finally, although medications have their own section, they also ap-pear in the Plan section. Other issues that are not exposed in this example include: 1) Section order is not consistent across EHRs, 2) Headers may be missing, 3) Common features of headers (e.g. bold or colon) are not guaranteed to appear.",Material,DataSource,True,Use（引用目的）,True,D19-1492_0_0,2019,Leveraging Medical Literature for Section Prediction in Electronic Health Records,Footnote
2290,12296," https://en.wikipedia.org/wiki/Category:Clinical_medicine"," ['3 Data', '3.1 Medical Literature (MedLit)']","The medical literature dataset consists of passages from textbooks, guidelines, and a subset of med-ically relevant Wikipedia articles [Cite_Footnote_2] .",2 Articles under the ‘Clinical Medicine’ category (https://en.wikipedia.org/wiki/Category:Clinical_medicine).,"The medical literature dataset consists of passages from textbooks, guidelines, and a subset of med-ically relevant Wikipedia articles [Cite_Footnote_2] . The number of sentences per source type is shown in Table 1. In total there are four sources in this dataset: Wikipedia and licensed content from DynaMed, Elsevier, and Wiley publishers.",Material,DataSource,True,Use（引用目的）,True,D19-1492_1_0,2019,Leveraging Medical Literature for Section Prediction in Electronic Health Records,Footnote
2291,12297," https://github.com/huggingface/pytorch-pretrained-BERT"," ['4 Method and Results']",For our BERT ex-periments we use a PyTorch implementation [Cite_Footnote_5] with the bert-base-uncased model.,5 https://github.com/huggingface/ pytorch-pretrained-BERT,"For the GRU RNN, we use the Adam optimizer, a batch size of 32, dropout of 0.2, and embedding size 300. We experimented with other parameter values on the development set, but these worked best. We ran each model for 50 epochs—enough for the training loss to converge. For our BERT ex-periments we use a PyTorch implementation [Cite_Footnote_5] with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam op-timizer, a batch size of 32, dropout of 0.1, and em-bedding size 768. All text is cut off to the first 128 word-pieces. We experimented with differ-ent numbers of epochs, and chose the model that performed best on the dev set (usually one tuned at 10 epochs or fewer). Statistical significance was computed using McNemar’s test. We exper-imented with both section classification and sen-tence classification as described in the following subsections.",Method,Tool,True,Use（引用目的）,True,D19-1492_2_0,2019,Leveraging Medical Literature for Section Prediction in Electronic Health Records,Footnote
2292,12298," https://github.com/Priberam/exconsumm"," ['References']",We also make available a new dataset of oracle compressive summaries de-rived automatically from the CNN/DailyMail reference summaries. [Cite_Footnote_1],1 Our dataset and code is available at https://github.com/Priberam/exconsumm.,"We present a new neural model for text sum-marization that first extracts sentences from a document and then compresses them. The pro-posed model offers a balance that sidesteps the difficulties in abstractive methods while gener-ating more concise summaries than extractive methods. In addition, our model dynamically determines the length of the output summary based on the gold summaries it observes dur-ing training, and does not require length con-straints typical to extractive summarization. The model achieves state-of-the-art results on the CNN/DailyMail and Newsroom datasets, improving over current extractive and abstrac-tive methods. Human evaluations demonstrate that our model generates concise and informa-tive summaries. We also make available a new dataset of oracle compressive summaries de-rived automatically from the CNN/DailyMail reference summaries. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,N19-1397_0_0,2019,Jointly Extracting and Compressing Documents with Summary State Representations,Footnote
2293,12299," https://spacy.io"," ['1 Introduction']","There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y [Cite_Footnote_1] for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others.",1 https://spacy.io,"There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y [Cite_Footnote_1] for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines. Pre-processing and transforming MR datasets into a format that is us-able by a MR model as well as implementing com-mon architecture building blocks all require sub-stantial effort which is not specifically handled by any of the aforementioned solutions. This is due to the fact that they serve a different, typically much broader purpose.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_0_0,2018,Jack the Reader – A Machine Reading Framework,Footnote
2294,12300," https://lucene.apache.org"," ['1 Introduction']","There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE [Cite_Footnote_2] and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others.",2 https://lucene.apache.org,"There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE [Cite_Footnote_2] and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines. Pre-processing and transforming MR datasets into a format that is us-able by a MR model as well as implementing com-mon architecture building blocks all require sub-stantial effort which is not specifically handled by any of the aforementioned solutions. This is due to the fact that they serve a different, typically much broader purpose.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_1_0,2018,Jack the Reader – A Machine Reading Framework,Footnote
2295,12301," http://lucene.apache.org/solr/"," ['1 Introduction']","There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR [Cite_Footnote_3] for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others.",3 http://lucene.apache.org/solr/,"There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR [Cite_Footnote_3] for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines. Pre-processing and transforming MR datasets into a format that is us-able by a MR model as well as implementing com-mon architecture building blocks all require sub-stantial effort which is not specifically handled by any of the aforementioned solutions. This is due to the fact that they serve a different, typically much broader purpose.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_2_0,2018,Jack the Reader – A Machine Reading Framework,Footnote
2296,12302," http://scikit-learn.org"," ['1 Introduction']","There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN [Cite_Footnote_4] , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others.",4 http://scikit-learn.org,"There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN [Cite_Footnote_4] , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines. Pre-processing and transforming MR datasets into a format that is us-able by a MR model as well as implementing com-mon architecture building blocks all require sub-stantial effort which is not specifically handled by any of the aforementioned solutions. This is due to the fact that they serve a different, typically much broader purpose.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_3_0,2018,Jack the Reader – A Machine Reading Framework,Footnote
2297,12303," http://pytorch.org/"," ['1 Introduction']","There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH [Cite_Footnote_5] and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others.",5 http://pytorch.org/,"There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH [Cite_Footnote_5] and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines. Pre-processing and transforming MR datasets into a format that is us-able by a MR model as well as implementing com-mon architecture building blocks all require sub-stantial effort which is not specifically handled by any of the aforementioned solutions. This is due to the fact that they serve a different, typically much broader purpose.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_4_0,2018,Jack the Reader – A Machine Reading Framework,Footnote
2298,12304," https://opennlp.apache.org"," ['2 Related Work']","General NLP frameworks include C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), O PEN NLP [Cite_Footnote_6] and SPA C Y .",6 https://opennlp.apache.org,"Machine Reading requires a tight integration of Natural Language Processing and Machine Learn-ing models. General NLP frameworks include C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), O PEN NLP [Cite_Footnote_6] and SPA C Y . All these frameworks offer pre-built models for standard NLP preprocessing tasks, such as tokenisation, sentence splitting, named entity recognition and parsing.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_5_0,2018,Jack the Reader – A Machine Reading Framework,Footnote
2299,12305," https://github.com/uclmr/jack/blob/master/conf/nli/esim.yaml"," ['3 Overview', '3.2 Distinguishing Features']","For such a reason, J ACK allows to de-scribe these models at a high level, as a composi-tion of simpler building blocks [Cite_Footnote_7] , leaving concrete implementation details to the framework.","7 For instance, see https://github.com/uclmr/jack/blob/master/conf/nli/esim.yaml","Declarative Model Definition. Implementing different kinds of MR models can be repetitive, tedious, and error-prone. Most neural architec-tures are built using a finite set of basic building blocks for encoding sequences, and realising inter-action between sequences (e.g. via attention mech-anisms). For such a reason, J ACK allows to de-scribe these models at a high level, as a composi-tion of simpler building blocks [Cite_Footnote_7] , leaving concrete implementation details to the framework.",Method,Code,False,Introduce（引用目的）,True,P18-4005_6_0,2018,Jack the Reader – A Machine Reading Framework,Footnote
2300,12306," https://github.com/uclmr/jack/tree/master/notebooks"," ['6 Demo']",We created three tutorial Jupyter notebooks at [Cite] https://github.com/uclmr/jack/tree/master/notebooks to demo J ACK ’s use cases.,,"We created three tutorial Jupyter notebooks at [Cite] https://github.com/uclmr/jack/tree/master/notebooks to demo J ACK ’s use cases. The quick start notebook shows how to quickly set up, load and run the existing systems for QA and NLI. The model training notebook demonstrates training, testing, evaluating and saving QA and NLI models programmatically. However, normally the user will simply use the provided training script from command line. The model implementation notebook delves deeper into implementing new models from scratch by writing all modules for a custom model.",Method,Tool,False,Produce（引用目的）,True,P18-4005_7_0,2018,Jack the Reader – A Machine Reading Framework,Body
2301,12307," https://www.tensorflow.org/"," ['1 Introduction']","There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) [Cite_Ref] for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others.",Martín Abadi et al. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.,"There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) [Cite_Ref] for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines. Pre-processing and transforming MR datasets into a format that is us-able by a MR model as well as implementing com-mon architecture building blocks all require sub-stantial effort which is not specifically handled by any of the aforementioned solutions. This is due to the fact that they serve a different, typically much broader purpose.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_8_0,2018,Jack the Reader – A Machine Reading Framework,Reference
2302,12308," https://www.tensorflow.org/"," ['2 Related Work']","Multiple general machine learning frame-works, such as SCIKIT - LEARN (Pedregosa et al., 2011), P Y T ORCH , T HEANO (Theano Develop-ment Team, 2016) and T ENSOR F LOW (Abadi et al., 2015) [Cite_Ref] , among others, enable quick proto-typing and deployment of ML models.",Martín Abadi et al. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.,"Multiple general machine learning frame-works, such as SCIKIT - LEARN (Pedregosa et al., 2011), P Y T ORCH , T HEANO (Theano Develop-ment Team, 2016) and T ENSOR F LOW (Abadi et al., 2015) [Cite_Ref] , among others, enable quick proto-typing and deployment of ML models. However, unlike J ACK , they do not offer a simple framework for defining and evaluating MR models.",補足資料,Website,True,Introduce（引用目的）,True,P18-4005_8_1,2018,Jack the Reader – A Machine Reading Framework,Reference
2303,12309," http://ufal.mff.cuni.cz/treex"," ['2 Generator Architecture']","Deep-syntax annotation of sentences in the training set is needed to train the sentence plan-ner, but we assume automatic annotation and reuse an existing deep-syntactic analyzer from the Treex NLP framework (Popel and Žabokrtský, 2010). [Cite_Footnote_2]","2 See http://ufal.mff.cuni.cz/treex. Domain-independent deep syntax analysis for several languages is included in this framework; the English pipeline used here involves a statistical part-of-speech tagger (Spoustová et al., 2007) and a dependency parser (McDonald et al., 2005), fol-lowed by a rule-based conversion to deep syntax trees.","Deep-syntax annotation of sentences in the training set is needed to train the sentence plan-ner, but we assume automatic annotation and reuse an existing deep-syntactic analyzer from the Treex NLP framework (Popel and Žabokrtský, 2010). [Cite_Footnote_2]",補足資料,Website,True,Introduce（引用目的）,True,P15-1044_0_0,2015,Training a Natural Language Generator From Unaligned Data,Footnote
2304,12310," http://farm2.user.srcf.net/research/bagel/"," ['5 Experimental Setup', '5.1 Data set']","We performed our experiments on the BAGEL data set of Mairesse et al. (2010), which fits our usage scenario in a spoken dialogue sys-tem and is freely available. [Cite_Footnote_7]",7 Available for download at: http://farm2.user.srcf.net/research/bagel/.,"We performed our experiments on the BAGEL data set of Mairesse et al. (2010), which fits our usage scenario in a spoken dialogue sys-tem and is freely available. [Cite_Footnote_7] It contains a to-tal of 404 sentences from a restaurant informa-tion domain (describing the restaurant location, food type, etc.), which correspond to 202 dia-logue acts, i.e., each dialogue act has two para-phrases. Restaurant names, phone numbers, and other “non-enumerable” properties are abstracted – replaced by an “X” symbol – throughout the gen-eration process. Note that while the data set con-tains alignment of source SVPs to target phrases, we do not use it in our experiments.",Material,Dataset,True,Use（引用目的）,True,P15-1044_1_0,2015,Training a Natural Language Generator From Unaligned Data,Footnote
2305,12311," http://universaldependencies.github.io"," ['5 Experimental Setup', '5.1 Data set']",• We convert the representation of coordination structures into a format inspired by Universal Dependencies. [Cite_Footnote_8],8 http://universaldependencies.github.io,"• We convert the representation of coordination structures into a format inspired by Universal Dependencies. [Cite_Footnote_8] In the original Treex anno-tation style, the conjunction heads both con-juncts, whereas in our modification, the first conjunct is at the top, heading the coordina-tion and the second conjunct (see Figure 4).",補足資料,Website,True,Extend（引用目的）,True,P15-1044_2_0,2015,Training a Natural Language Generator From Unaligned Data,Footnote
2306,12312," https://github.com/UFAL-DSG/tgen"," ['8 Conclusions and Further Work']","The generator source code, along with config-uration files for experiments on the BAGEL data set, is available for download on Github. [Cite_Footnote_11]",11 https://github.com/UFAL-DSG/tgen,"The generator source code, along with config-uration files for experiments on the BAGEL data set, is available for download on Github. [Cite_Footnote_11]",Method,Code,True,Use（引用目的）,True,P15-1044_3_0,2015,Training a Natural Language Generator From Unaligned Data,Footnote
2307,12313," https://github.com/dykang/xslue"," ['References']",The preprocessed datasets and code are publicly available. [Cite_Footnote_1],1 https://github.com/dykang/xslue,"Every natural text is written in some style. Style is formed by a complex combination of different stylistic factors, including formality markers, emotions, metaphors, etc. One can-not form a complete understanding of a text without considering these factors. The fac-tors combine and co-vary in complex ways to form styles. Studying the nature of the co-varying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding. This paper provides the benchmark corpus ( X SLUE) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation. The bench-mark contains text in 15 different styles un-der the proposed four theoretical groupings: figurative, personal, affective, and interper-sonal groups. For valid evaluation, we col-lect an additional diagnostic set by annotat-ing all 15 styles on the same text. Using X SLUE, we propose three interesting cross-style applications in classification, correlation, and generation. First, our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers. Second, our study shows that some styles are highly dependent on each other in human-written text. Finally, we find that com-binations of some contradictive styles likely generate stylistically less appropriate text. We believe our benchmark and case studies help explore interesting future directions for cross-style research. The preprocessed datasets and code are publicly available. [Cite_Footnote_1]",Mixed,Mixed,True,Use（引用目的）,False,2021.acl-long.185_0_0,2021,Style is NOT a single variable: Case Studies for Cross-Style Language Understanding,Footnote
2308,12314," http://github.com/CrowdTruth/"," ['2 Related Work', '3.1 Style selection and groupings']","[Cite_Ref] 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017) 463k random 2 humor (50%), non-humor (50%)",CrowdTruth. 2016. Short Text Corpus For Hu-mor Detection. http://github.com/CrowdTruth/ Short-Text-Corpus-For-Humor-Detection . [On-line; accessed 1-Oct-2019].,"Style & dataset #S Split #L Label (distribution) B Domain Public Task . Formality NTERPERS GYAFC (Rao and Tetreault, 2018) 224k given 2 formal (50%), informal (50%) Y web N clsf. Politeness I StanfPolite (Danescu et al., 2013) 10k given 2 polite (49.6%), impolite (50.3%) Humor ShortHumor (CrowdTruth, 2016) [Cite_Ref] 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017) 463k random 2 humor (50%), non-humor (50%) IGURATIVE Sarcasm SarcGhosh (Ghosh and Veale, 2016) 43k given 2 sarcastic (45%), non-sarcastic (55%) EmoBank valence (Buechel and Hahn, 2017) 10k random 1 negative, positive EmoBank arousal (Buechel and Hahn, 2017) 10k random 1 calm, excited EmoBank dominance (Buechel and Hahn, 2017) 10k random 1 being_controlled, being_in_control FFECTIVE DailyDialog (Li et al., 2017) 102k given 7 noemotion(83%), happy(12%).. Offense HateOffensive (Davidson et al., 2017) 24k given 3 hate(6.8%), offensive(76.3%).. A Romance ShortRomance 2k random 2 romantic (50%), non-romantic (50%) Sentiment SentiBank (Socher et al., 2013) 239k given 2 positive (54.6%), negative (45.4%) Gender PASTEL (Kang et al., 2019) 41k given 3 Female (61.2%), Male (38.0%).. 41k given 8 35-44 (15.3%), 25-34 (42.1%).. ERSONAL Age PASTEL (Kang et al., 2019)",補足資料,Paper,True,Introduce（引用目的）,True,2021.acl-long.185_1_0,2021,Style is NOT a single variable: Case Studies for Cross-Style Language Understanding,Reference
2309,12315," https://github.com/amoudgl/short-jokes-dataset"," ['2 Related Work', '3.1 Style selection and groupings']","(CrowdTruth, 2016) 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017) [Cite_Ref]",Abhinav Moudgil. 2017. short jokes dataset. https://github.com/amoudgl/short-jokes-dataset. [On-line; accessed 1-Oct-2019].,"Style & dataset #S Split #L Label (distribution) B Domain Public Task . Formality NTERPERS GYAFC (Rao and Tetreault, 2018) 224k given 2 formal (50%), informal (50%) Y web N clsf. Politeness I StanfPolite (Danescu et al., 2013) 10k given 2 polite (49.6%), impolite (50.3%) Humor ShortHumor (CrowdTruth, 2016) 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017) [Cite_Ref] 463k random 2 humor (50%), non-humor (50%) IGURATIVE Sarcasm SarcGhosh (Ghosh and Veale, 2016) 43k given 2 sarcastic (45%), non-sarcastic (55%) EmoBank valence (Buechel and Hahn, 2017) 10k random 1 negative, positive EmoBank arousal (Buechel and Hahn, 2017) 10k random 1 calm, excited EmoBank dominance (Buechel and Hahn, 2017) 10k random 1 being_controlled, being_in_control FFECTIVE DailyDialog (Li et al., 2017) 102k given 7 noemotion(83%), happy(12%).. Offense HateOffensive (Davidson et al., 2017) 24k given 3 hate(6.8%), offensive(76.3%).. A Romance ShortRomance 2k random 2 romantic (50%), non-romantic (50%) Sentiment SentiBank (Socher et al., 2013) 239k given 2 positive (54.6%), negative (45.4%) Gender PASTEL (Kang et al., 2019) 41k given 3 Female (61.2%), Male (38.0%).. 41k given 8 35-44 (15.3%), 25-34 (42.1%).. ERSONAL Age PASTEL (Kang et al., 2019)",Material,Dataset,True,Use（引用目的）,True,2021.acl-long.185_2_0,2021,Style is NOT a single variable: Case Studies for Cross-Style Language Understanding,Reference
2310,12316," http://ufal.mff.cuni.cz/udpipe"," ['3 Discourse segmentation', '3.1 Binary task']",[Cite_Footnote_3],3 http://ufal.mff.cuni.cz/udpipe,(1) [But maintaining the key components (. . .)] 1 [– a stable exchange rate and high levels of imports –] 2 [will consume enormous amounts (. . .).] [Cite_Footnote_3],Method,Tool,True,Introduce（引用目的）,False,D17-1258_0_0,2017,Does syntax help discourse segmentation? Not so much,Footnote
2311,12317," https://www.sfu.ca/~mtaboada"," ['5 Corpora']","We also report perfor-mance on the SFU review corpus [Cite_Footnote_6] (En-SFU-DT) containing product reviews, and on the instruc-tional corpus (En-Instr-DT)",6 https://www.sfu.ca/˜mtaboada,"For English, we use three corpora, allowing us to evaluate how robust is our model across do-mains. First, we report results on the RST-DT (from now on called En-DT), the most widely used corpus for this task. This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report perfor-mance on the SFU review corpus [Cite_Footnote_6] (En-SFU-DT) containing product reviews, and on the instruc-tional corpus (En-Instr-DT) (Subba and Di Euge-nio, 2009) built on instruction manuals. 7",Material,Dataset,True,Use（引用目的）,True,D17-1258_1_0,2017,Does syntax help discourse segmentation? Not so much,Footnote
2312,12318," https://bitbucket.org/chloebt/discourse/"," ['9 Conclusion']",We make our code available at [Cite] https://bitbucket.org/chloebt/discourse/.,,"We proposed new discourse segmenters that make use of resources available for many languages and domains. We investigated the usefulness of syn-tactic information when derived from dependency parse trees, and showed that this information is not as useful as expected, and that gold POS tags give as high results as using predicted constituent trees. We also showed that scores are lowered when considering a realistic setting, relying on predicted tokenization and not assuming gold sen-tences. We make our code available at [Cite] https://bitbucket.org/chloebt/discourse/.",Method,Code,True,Produce（引用目的）,True,D17-1258_2_0,2017,Does syntax help discourse segmentation? Not so much,Body
2313,12319," https://github.com/asahi417/kex"," ['References']","Finally, based on our findings we discuss and devise some suggestions for practitioners. [Cite_Footnote_1]","1 Source code to reproduce our experimental results, includ-ing a keyword extraction library, are available in the following repository: https://github.com/asahi417/kex","Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we perform an exhaustive and large-scale empirical compar-ison of both statistical and graph-based term weighting methods in the context of keyword extraction. Our analysis reveals some interest-ing findings such as the advantages of the less-known lexical specificity with respect to tf-idf, or the qualitative differences between statisti-cal and graph-based methods. Finally, based on our findings we discuss and devise some suggestions for practitioners. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.638_0_0,2021,Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction,Footnote
2314,12320," https://github.com/asahi417/kex"," ['3 Experimental Setting']",All our experiments are run on a 16-core Ubuntu computer equipped with 3.8GHz i7 core and 64GiB memory. [Cite_Footnote_5],5 All the details to reproduce our experiments are available at https://github.com/asahi417/kex,"In this section, we explain our keyword extrac-tion experimental setting. All our experiments are run on a 16-core Ubuntu computer equipped with 3.8GHz i7 core and 64GiB memory. [Cite_Footnote_5]",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.638_1_0,2021,Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction,Footnote
2315,12321," https://github.com/LIAAD/KeywordExtractor-Datasets"," ['3 Experimental Setting']","To evaluate the keyword extraction methods, we consider 15 different public datasets in English. [Cite_Footnote_6]","6 All the datasets were fetched from a public data reposi-tory for keyword extraction data: https://github.com/LIAAD/KeywordExtractor-Datasets: KPCrowd (Marujo et al., 2013), Inspec (Hulth, 2003), Krapivin2009 (Krapivin et al., 2009), SemEval2017 (Augenstein et al., 2017), kdd (Gollapalli and Caragea, 2014), www (Gollapalli and Caragea, 2014), wiki20 (Medelyan and Witten, 2008), PubMed (Schutz et al., 2008), Schutz2008 (Schutz et al., 2008), citeulike180 (Medelyan et al., 2009), fao30 and fao780 (Medelyan and Witten, 2008), guyen2007 (Nguyen and Kan, 2007), and SemEval2010 (Kim et al., 2010).","Datasets. To evaluate the keyword extraction methods, we consider 15 different public datasets in English. [Cite_Footnote_6] Each entry in a dataset consists of a source document and a set of gold keyphrases, where the source document is processed through the pipeline described in Section 3 and the gold keyphrase set is filtered to include only phrases which appear in its candidate set. Table 1 pro-vides high-level statistics of each dataset, including length and number of keyphrases 7 (both average and standard deviation).",Material,DataSource,True,Use（引用目的）,True,2021.emnlp-main.638_2_0,2021,Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction,Footnote
2316,12322," https://pypi.org/project/segtok/"," ['3 Experimental Setting']","The documents are first tok-enized into words by segtok [Cite_Footnote_8] , a python library for tokenization and sentence splitting.",8 https://pypi.org/project/segtok/,"Preprocessing. Before running keyword extrac-tion on each dataset, we apply standard text pre-processing operations. The documents are first tok-enized into words by segtok [Cite_Footnote_8] , a python library for tokenization and sentence splitting. Then, each word is stemmed to reduce it to its base form for comparison purpose by Porter Stemmer from NLTK (Bird et al., 2009), a widely used python li-brary for text processing. Part-of-speech annotation is carried out using NLTK tagger. To select a can-didate phrase set P d , following the literature (Wan and Xiao, 2008b), we consider contiguous nouns in the document d that form a noun phrase satisfying the regular expression ( ADJECTIVE )*( NOUN )+. We then filter the candidates with a stopword list taken from the official YAKE implementation (Campos et al., 2020). Finally, for the statistical methods and the graph-based methods based on them (i.e., LexRank and TFIDFRank), we compute citeulike180 6.6 9.5 18.0 15.2 23.0 fao30 17.3 16.0 24.0 20.7 26.0 fao780 9.3 3.2 11.7 10.5 12.4 kdd 11.7 7.0 11.2 11.6 10.6 theses100 5.6 9.4 6.60.9 10.7 wiki20 13.0 13.0 17.0 21.0 13.0 www 12.2 8.1 11.9 12.2 10.6 prior statistics including term frequency (tf), tf-idf, and LDA by Gensim (Řehůřek and Sojka, 2010) within each dataset.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.638_3_0,2021,Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction,Footnote
2317,12323," https://github.com/LIAAD/yake"," ['3 Experimental Setting']","We then filter the candidates with a stopword list taken from the official YAKE implementation [Cite_Footnote_10] (Campos et al., 2020).",10 https://github.com/LIAAD/yake,"Preprocessing. Before running keyword extrac-tion on each dataset, we apply standard text pre-processing operations. The documents are first tok-enized into words by segtok , a python library for tokenization and sentence splitting. Then, each word is stemmed to reduce it to its base form for comparison purpose by Porter Stemmer from NLTK (Bird et al., 2009), a widely used python li-brary for text processing. Part-of-speech annotation is carried out using NLTK tagger. To select a can-didate phrase set P d , following the literature (Wan and Xiao, 2008b), we consider contiguous nouns in the document d that form a noun phrase satisfying the regular expression ( ADJECTIVE )*( NOUN )+. We then filter the candidates with a stopword list taken from the official YAKE implementation [Cite_Footnote_10] (Campos et al., 2020). Finally, for the statistical methods and the graph-based methods based on them (i.e., LexRank and TFIDFRank), we compute citeulike180 6.6 9.5 18.0 15.2 23.0 fao30 17.3 16.0 24.0 20.7 26.0 fao780 9.3 3.2 11.7 10.5 12.4 kdd 11.7 7.0 11.2 11.6 10.6 theses100 5.6 9.4 6.60.9 10.7 wiki20 13.0 13.0 17.0 21.0 13.0 www 12.2 8.1 11.9 12.2 10.6 prior statistics including term frequency (tf), tf-idf, and LDA by Gensim (Řehůřek and Sojka, 2010) within each dataset.",Material,Knowledge,True,Use（引用目的）,True,2021.emnlp-main.638_4_0,2021,Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction,Footnote
2318,12324," http://alt.qcri.org/semeval2016/task7/"," ['3 Creating a Sentiment Lexicon for Opposing Polarity Phrases']","Portions of the created lexicon have been used as development and evaluation sets in SemEval-2016 Task 7 ‘Determining Sentiment Intensity of English and Arabic Phrases’ (Kiritchenko et al., 2016). [Cite_Footnote_5]",5 http://alt.qcri.org/semeval2016/task7/,"Portions of the created lexicon have been used as development and evaluation sets in SemEval-2016 Task 7 ‘Determining Sentiment Intensity of English and Arabic Phrases’ (Kiritchenko et al., 2016). [Cite_Footnote_5] The objective of that task was to test different meth-ods of automatically predicting sentiment associa-tion scores for multi-word phrases.",補足資料,Website,True,Use（引用目的）,True,N16-1128_0_0,2016,Sentiment Composition of Words with Opposing Polarities,Footnote
2319,12325," http://www.acm.org/crossroads/xrds3-1/aied.html"," ['1 Introduction']","Among the tools, particular attention is given to Intelligent Tutoring Systems (ITSs), which are sophisticated software systems that can provide personalized instruction to students, in some respect similar to one-on-one tutoring (Beck et al., 1996) [Cite_Ref] .","Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996. Applications of AI in education. ACM crossroads. http://www.acm.org/crossroads/xrds3-1/aied.html.","One-on-one tutoring has been shown to be a very effective form of instruction (Bloom, 1984). The research community is working on discovering the characteristics of tutoring. One of the goals is to un-derstand the strategies tutors use, in order to design effective learning environments and tools to support learning. Among the tools, particular attention is given to Intelligent Tutoring Systems (ITSs), which are sophisticated software systems that can provide personalized instruction to students, in some respect similar to one-on-one tutoring (Beck et al., 1996) [Cite_Ref] . Many of these systems have been shown to be very effective (Evens and Michael, 2006; Van Lehn et al., 2005; Di Eugenio et al., 2005; Mitrović et al., 2004; Person et al., 2001). In many experiments, ITSs in-duced learning gains higher than those measured in a classroom environment, but lower than those ob-tained with one-on-one interactions with human tu-tors. The belief of the research community is that knowing more about human tutoring would help im-prove the design of ITSs. In particular, the effective use of natural language might be a key element. In most of the studies mentioned above, systems with more sophisticated language interfaces performed better than other experimental conditions.",補足資料,Paper,True,Introduce（引用目的）,True,P08-3006_0_0,2008,The role of positive feedback in Intelligent Tutoring Systems,Reference
2320,12326," http://www.acm.org/crossroads/xrds3-1/aied.html"," ['4 A tutoring system for linked lists']","Student model and pedagogical module, important components of a complete ITS (Beck et al., 1996) [Cite_Ref] , have not been implemented yet, and will be included in a future version.","Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996. Applications of AI in education. ACM crossroads. http://www.acm.org/crossroads/xrds3-1/aied.html.","The architecture of iList includes a problem model, a constraint evaluator, a feedback manager, and a graphical user interface. Student model and pedagogical module, important components of a complete ITS (Beck et al., 1996) [Cite_Ref] , have not been implemented yet, and will be included in a future version. Currently, the system provides only simple negative feedback in response to students’ mistakes, as customary in constraint-based ITSs.",補足資料,Paper,True,Introduce（引用目的）,True,P08-3006_0_1,2008,The role of positive feedback in Intelligent Tutoring Systems,Reference
2321,12327," https://github.com/declare-lab/MIME"," ['References']",The implementa-tion of MIME is publicly available at [Cite] https: //github.com/declare-lab/MIME .,,"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochas-ticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at [Cite] https: //github.com/declare-lab/MIME .",Method,Tool,True,Produce（引用目的）,True,2020.emnlp-main.721_0_0,2020,MIME: MIMicking Emotions for Empathetic Response Generation,Body
2322,12328," https://github.com/facebookresearch/EmpatheticDialogues"," ['4 Experimental Settings', '4.1 Dataset']","We evaluate our method on E MPATHETIC D IA - LOGUES [Cite_Footnote_1] (Rashkin et al., 2018), a dataset that con-tains 24,850 open-domain dyadic conversations be-tween two users, where one responds emphatically to the other.",1 https://github.com/facebookresearch/ EmpatheticDialogues,"We evaluate our method on E MPATHETIC D IA - LOGUES [Cite_Footnote_1] (Rashkin et al., 2018), a dataset that con-tains 24,850 open-domain dyadic conversations be-tween two users, where one responds emphatically to the other. For our experiments, we use the 8:1:1 train/validation/test split, defined by the authors of this dataset. Each sample consists of a context — defined by an excerpt of a full conversation and the emotion of the user — and the empathetic response to the last utterance in the context. There are a total of 32 different emotion categories roughly uniformly distributed across the dataset.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.721_1_0,2020,MIME: MIMicking Emotions for Empathetic Response Generation,Footnote
2323,12329," https://github.com/Oneplus/tamr"," ['1 Introduction']",Our code and the alignments for LDC2014T12 dataset are publicly available at [Cite] https://github.com/Oneplus/tamr,,Our code and the alignments for LDC2014T12 dataset are publicly available at [Cite] https://github.com/Oneplus/tamr,Method,Code,True,Produce（引用目的）,True,D18-1264_0_0,2018,An AMR Aligner Tuned by Transition-based Parser,Body
2324,12330," http://senseclusters.sourceforge.net/"," ['3 Vector Approaches']",SenseClusters [Cite_Footnote_1] is an unsupervised knowledge-lean word sense disambiguation package The pack-age uses clustering algorithms to group similar in-stances of target words and label them with the ap-propriate sense.,1 http://senseclusters.sourceforge.net/,"SenseClusters [Cite_Footnote_1] is an unsupervised knowledge-lean word sense disambiguation package The pack-age uses clustering algorithms to group similar in-stances of target words and label them with the ap-propriate sense. The clustering algorithms include Agglomerative, Graph partitional-based, Partitional biased agglomerative and Direct k-way clustering. The clustering can be done in either vector space where the vectors are clustered directly or similar-ity space where vectors are clustered by finding the pair-wise similarities among the contexts. The fea-ture options available are first and second-order co-occurrence, unigram and bigram vectors. First-order vectors are highly frequent words, unigrams or bi-grams that co-occur in the same window of context as the target word. Second-order vectors are highly frequent words that occur with the words in their re-spective first order vector.",Method,Tool,False,Produce（引用目的）,True,P08-3009_0_0,2008,An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline,Footnote
2325,12331," http://www.d.umn.edu/tpederse/namedata.html"," ['5 Data', '5.3 Conflate Test Dataset']",We create our dataset using name-conflate [Cite_Footnote_2] to extract instances containing the conflate words from the 2005 Medline Baseline.,2 http://www.d.umn.edu/tpederse/namedata.html,We create our dataset using name-conflate [Cite_Footnote_2] to extract instances containing the conflate words from the 2005 Medline Baseline. Table 4 shows our cur-rent set of conflated words with their corresponding number of test (test) and training (train) instances. We refer to the conflated words as their pseudowords throughout the paper.,Material,Dataset,True,Extend（引用目的）,True,P08-3009_1_0,2008,An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline,Footnote
2326,12332," http://cuitools.sourceforge.net"," ['References']","Our experiments were conducted using CuiTools v0.15, which is freely available from [Cite] http://cuitools.sourceforge.net.",,"Our experiments were conducted using CuiTools v0.15, which is freely available from [Cite] http://cuitools.sourceforge.net.",Method,Tool,True,Use（引用目的）,True,P08-3009_2_0,2008,An Unsupervised Vector Approach to Biomedical Term Disambiguation: Integrating UMLS and Medline,Body
2327,12333," http://www.di.unipi.it/~gulli/AG"," ['4 Experiments']","AG’s news corpus [Cite_Footnote_1] ,a news article corpus with categorized articles from more than 2,000 news sources.",1 http://www.di.unipi.it/~gulli/AG corpus of news articles.html,"1. AG’s news corpus [Cite_Footnote_1] ,a news article corpus with categorized articles from more than 2,000 news sources. We use the dataset with 4 largest classes constructed in (Zhang et al., 2015).",Material,Dataset,True,Use（引用目的）,False,D16-1093_0_0,2016,Recurrent Residual Learning for Sequence Classification,Footnote
2328,12334," http://ai.stanford.edu/~amaas/data/sentiment/"," ['4 Experiments']","IMDB movie review dataset [Cite_Footnote_2] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).",2 http://ai.stanford.edu/~amaas/data/sentiment/,"2. IMDB movie review dataset [Cite_Footnote_2] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).",Material,Knowledge,True,Introduce（引用目的）,True,D16-1093_1_0,2016,Recurrent Residual Learning for Sequence Classification,Footnote
2329,12335," http://ana.cachopo.org/datasets-for-single-label-text-categorization"," ['4 Experiments']","Simiar to (Dai and Le, 2015), we use the post-processed version [Cite_Footnote_3] , in which attach-ments, PGP keys and some duplicates are re-moved.",3 http://ana.cachopo.org/datasets-for-single-label-text- categorization,"3. 20 Newsgroups (20NG for short), an email collection dataset categorized into 20 news groups. Simiar to (Dai and Le, 2015), we use the post-processed version [Cite_Footnote_3] , in which attach-ments, PGP keys and some duplicates are re-moved.",Material,Dataset,True,Use（引用目的）,True,D16-1093_2_0,2016,Recurrent Residual Learning for Sequence Classification,Footnote
2330,12336," https://publish.illinois.edu/yirenwang/emnlp16source/"," ['4 Experiments']",All the source codes and datasets can be down-loaded at [Cite] https://publish.illinois.,,"Detailed statistics of each dataset are listed in Table 1. For all the text datasets, we take every word as input and feed word embedding vectors pre-trained by Word2Vec (Mikolov et al., 2013) on Wikipedia into the recurrent neural network. The top most frequent words with 95% total frequency coverage are kept, while others are replaced by the token “UNK”. We use the standard training/test split along with all these datasets and randomly pick 15% of training set as dev set, based on which we perform early stopping and for all models tune hyper-parameters such as dropout ratio (on non-recurrent layers) (Zaremba et al., 2014), gradient clipping value (Pascanu et al., 2013) and the skip connection length L for SC-LSTM (cf. equation (5)). The last hidden states of recurrent networks are put into logistic regression classifiers for label predictions. We use Adadelta (Zeiler, 2012) to perform parameter optimization. All our implementations are based on Theano (Theano De-velopment Team, 2016) and run on one K40 GPU. All the source codes and datasets can be down-loaded at [Cite] https://publish.illinois.edu/yirenwang/emnlp16source/.",Mixed,Mixed,True,Produce（引用目的）,True,D16-1093_3_0,2016,Recurrent Residual Learning for Sequence Classification,Body
2331,12337," https://github.com/martiansideofthemoon/hurdles-longform-qa"," ['References']","We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future. [Cite_Footnote_1]",1 Resources accompanying our paper can be found in https://github.com/martiansideofthemoon/ hurdles-longform-qa,"The task of long-form question answering (LFQA) involves retrieving documents rele-vant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental chal-lenges regarding evaluation and dataset cre-ation that currently preclude meaningful mod-eling progress. To demonstrate these chal-lenges, we first design a new system that relies on sparse attention and contrastive re-triever learning to achieve state-of-the-art per-formance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a de-tailed analysis reveals several troubling trends: (1) our system’s generated answers are not ac-tually grounded in the documents that it re-trieves; (2) ELI5 contains significant train / val-idation overlap, as at least 81% of ELI5 vali-dation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an infor-mative metric of generated answer quality and can be easily gamed; and (4) human evalua-tions used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.393_0_0,2021,Hurdles to Progress in Long-form Question Answering,Footnote
2332,12338," https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908"," ['1 Introduction']","In this paper, we build a state-of-the-art system [Cite_Footnote_2] for ELI5 by using a sparse Transformer variant (Roy et al., 2020) to condition over Wikipedia paragraphs returned by a REALM-style retriever (Guu et al., 2020).","2 State-of-the-art as of April 3, 2021 — the “Google Research & UMass Amherst” team entry on https: //evalai.cloudcv.org/web/challenges/ challenge-page/689/leaderboard/1908","The recently proposed KILT benchmark (Petroni et al., 2020), which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA models by the quality of both generated an-swers (ROUGE-L against reference answers) and retrieved documents (R-precision against human-annotated relevant documents). In this paper, we build a state-of-the-art system [Cite_Footnote_2] for ELI5 by using a sparse Transformer variant (Roy et al., 2020) to condition over Wikipedia paragraphs returned by a REALM-style retriever (Guu et al., 2020).",補足資料,Paper,False,Introduce（引用目的）,False,2021.naacl-main.393_1_0,2021,Hurdles to Progress in Long-form Question Answering,Footnote
2333,12339," https://huggingface.co/qa"," ['3 Analysis', '3.1 Are generations grounded in retrieval?']","Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by Jernite (2020). [Cite_Footnote_10]",10 https://huggingface.co/qa,"Other systems also have this issue, possibly due to source-reference divergence and train-validation overlap: We note that this issue is not unique to our system — other systems on the KILT leaderboard like BART + DPR and RAG actually perform worse than their no-retrieval counterpart (BART) in generation quality, as shown in Table 1. Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by Jernite (2020). [Cite_Footnote_10] A possible explanation for this issue is high source-reference divergence, a common problem in table-to-text generation (Wiseman et al., 2017; Tian et al., 2019). In Table 2 and Table 4, we measure the n-gram overlap of top-ranked gold validation answers (Gold Ans) with predicted retrievals. This overlap is low and similar to that of our generations, which we suspect encourages our model to ignore retrievals. A second explanation is the large amount of train-validation overlap (Section 3.2), which eliminates the need for retrieval.",Method,Tool,False,Introduce（引用目的）,True,2021.naacl-main.393_2_0,2021,Hurdles to Progress in Long-form Question Answering,Footnote
2334,12340," https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909"," ['3 Analysis', '3.1 Are generations grounded in retrieval?']","As seen on the public KILT leaderboard, [Cite_Footnote_12] our system has lower ROUGE-L scores than the BART / RAG baselines.",12 https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909,"Why does our model do well compared to other systems despite not using retrievals? While our model has similar capacity as the BART/RAG baselines (comparison in Appendix A.3), we hypothesize that our improvements in ROUGE-L are due to a different pretraining objective. BART is pretrained on a masked infilling task on short sequences. Instead, we pretrain our model to perform next-word prediction on long sequences from Project Gutenberg, which encourages long & fluent generations. To illustrate this length effect, in Appendix A.6 we show that truncated outputs from our model get lower ROUGE-L scores on ELI5. Prior summarization literature (Sun et al., 2019) has also shown that ROUGE scores vary heavily by length. To compare the same systems on shorter length outputs, we also tried finetuning the pretrained model on Wizard of Wikipedia (Dinan et al., 2019), an unconstrained dialogue generation task with single sentence dialogues (much shorter than ELI5). As seen on the public KILT leaderboard, [Cite_Footnote_12] our system has lower ROUGE-L scores than the BART / RAG baselines. Another possible explanation is issues with ROUGE-L itself, as discussed in Section 3.3. Takeaway (better evaluation of grounding): For evaluating LFQA, it is important to run control experiments with random retrievals & measure grounding of generations in retrieval. While the KILT benchmark does attempt to measure the com-bined retrieval + generation performance via KILT RL, it does not check whether the generations actu-ally used the retrievals. In other words, one can sub-mit independent retrieval & generation systems, but still perform well on the combined score. This may not be an issue for short-form QA tasks like Natural Questions, since the gold answer is often exactly contained as a span in the gold retrieval. Also, as retrieval might be less important for large language models with parametric knowledge (Roberts et al., 2020), the KILT-RL strategy of simply aggregat-ing top-1 retrieval score with ROUGE-L unfairly penalizes systems not relying on retrieval. 13",補足資料,Website,True,Introduce（引用目的）,True,2021.naacl-main.393_3_0,2021,Hurdles to Progress in Long-form Question Answering,Footnote
2335,12341," https://www.gstatic.com/gumdrop/sustainability/google-2019-environmental-report.pdf"," ['Ethical Considerations']","As men-tioned in the Google 2019 environment report, [Cite_Footnote_18] “TPUs are highly efficient chips which have been specifically designed for machine learning applica-tions”.",18 https://www.gstatic.com/gumdrop/sustainability/google-2019-environmental-report.pdf,"Our final models were trained using 64 Google Cloud TPUs for a total of 32 hours. As men-tioned in the Google 2019 environment report, [Cite_Footnote_18] “TPUs are highly efficient chips which have been specifically designed for machine learning applica-tions”. These accelerators run on Google Cloud, which has “matched 100% of its electricity con-sumption with renewable energy purchases, and has committed to fully decarbonize its electricity supply by 2030” (https://cloud.google.com/sustainability). More details on train-ing time are provided in Appendix A.1.",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-main.393_4_0,2021,Hurdles to Progress in Long-form Question Answering,Footnote
2336,12342," https://cloud.google.com/sustainability"," ['Ethical Considerations']",( [Cite] https://cloud.google.,,"Our final models were trained using 64 Google Cloud TPUs for a total of 32 hours. As men-tioned in the Google 2019 environment report, 18 “TPUs are highly efficient chips which have been specifically designed for machine learning applica-tions”. These accelerators run on Google Cloud, which has “matched 100% of its electricity con-sumption with renewable energy purchases, and has committed to fully decarbonize its electricity supply by 2030” ( [Cite] https://cloud.google.com/sustainability). More details on train-ing time are provided in Appendix A.1.",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-main.393_5_0,2021,Hurdles to Progress in Long-form Question Answering,Body
2337,12343," https://yjernite.github.io/lfqa.html"," ['2 A state-of-the-art LFQA system']",We use a dense retriever trained by scaling up a distantly supervised algorithm from Jernite (2020) [Cite_Ref] .,Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"In this section, we describe our proposed LFQA system, which conditions answer generation on Wikipedia articles identified by a pretrained re-triever. We use a dense retriever trained by scaling up a distantly supervised algorithm from Jernite (2020) [Cite_Ref] . Since retrieved articles can be quite long and often exceed the maximum sequence length of pretrained models like BERT (Devlin et al., 2019), we use a sparse-attention variant of the Transformer to allow modeling over longer sequences. While our system sets a new state-of-the-art on ELI5, we question the significance of this result in Section 3.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_0,2021,Hurdles to Progress in Long-form Question Answering,Reference
2338,12344," https://yjernite.github.io/lfqa.html"," ['2 A state-of-the-art LFQA system', '2.1 Retriever']","Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by Jernite (2020) [Cite_Ref] that uses gold answers for distant supervision.",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by Jernite (2020) [Cite_Ref] that uses gold answers for distant supervision. The key idea is to push the encoded vector for a ques-tion close to a vector representation of its ground-truth answer(s), but away from all other answer vectors in the mini-batch (negative examples). In-tuitively, this method works because both ELI5 answers and external documents are of paragraph length (documents are paragraph-length chunks from Wikipedia). Concretely, we optimize the loss,",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_1,2021,Hurdles to Progress in Long-form Question Answering,Reference
2339,12345," https://yjernite.github.io/lfqa.html"," ['2 A state-of-the-art LFQA system', '2.1 Retriever']","Scaling up from Jernite (2020) [Cite_Ref] , who used a mini-batch size of 512 and initialized their retriever with BERT, we use much large mini-batches of size 12,288 (and hence, many more negative examples) and initial-ize our retriever with a strong pretrained retriever, the REALM model (Guu et al., 2020) trained on the Common Crawl News (CC-News) corpus.",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"where B is the mini-batch and q i , a i are the encoded vector representations for (q i , a i ). This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning (Chen et al., 2020) and dense retriever training (Karpukhin et al., 2020). Scaling up from Jernite (2020) [Cite_Ref] , who used a mini-batch size of 512 and initialized their retriever with BERT, we use much large mini-batches of size 12,288 (and hence, many more negative examples) and initial-ize our retriever with a strong pretrained retriever, the REALM model (Guu et al., 2020) trained on the Common Crawl News (CC-News) corpus. These design decisions greatly improve retriever qual-ity, as we observe in an ablation study (see Ap-pendix A.2). During inference, we perform a maxi-mum inner-product search (MIPS) with the ScaNN library (Guo et al., 2020) to efficiently find the top K documents. In all our experiments we use K = 7, following the setup in Guu et al. (2020).",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_2,2021,Hurdles to Progress in Long-form Question Answering,Reference
2340,12346," https://yjernite.github.io/lfqa.html"," ['3 Analysis', '3.1 Are generations grounded in retrieval?']","Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by Jernite (2020) [Cite_Ref] .",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"Other systems also have this issue, possibly due to source-reference divergence and train-validation overlap: We note that this issue is not unique to our system — other systems on the KILT leaderboard like BART + DPR and RAG actually perform worse than their no-retrieval counterpart (BART) in generation quality, as shown in Table 1. Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by Jernite (2020) [Cite_Ref] . 10 A possible explanation for this issue is high source-reference divergence, a common problem in table-to-text generation (Wiseman et al., 2017; Tian et al., 2019). In Table 2 and Table 4, we measure the n-gram overlap of top-ranked gold validation answers (Gold Ans) with predicted retrievals. This overlap is low and similar to that of our generations, which we suspect encourages our model to ignore retrievals. A second explanation is the large amount of train-validation overlap (Section 3.2), which eliminates the need for retrieval.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_3,2021,Hurdles to Progress in Long-form Question Answering,Reference
2341,12347," https://yjernite.github.io/lfqa.html"," ['3 Analysis', '3.1 Are generations grounded in retrieval?']","11 While we do not have access to generations from base-lines on the KILT leaderboard, example generations from the demo of the BART model in Jernite (2020) [Cite_Ref] are significantly shorter (59 words avg.)",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"11 While we do not have access to generations from base-lines on the KILT leaderboard, example generations from the demo of the BART model in Jernite (2020) [Cite_Ref] are significantly shorter (59 words avg.) than our generations (187 words avg.).",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_4,2021,Hurdles to Progress in Long-form Question Answering,Reference
2342,12348," https://yjernite.github.io/lfqa.html"," ['3 Analysis', '3.2 Training / Validation Overlap']",14 The ELI5 demo from Jernite (2020) [Cite_Ref] also retrieves the top- 1 similar training set question.,Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"14 The ELI5 demo from Jernite (2020) [Cite_Ref] also retrieves the top- 1 similar training set question. Qualitatively, we found many validation examples had near-identical train paraphrases.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_5,2021,Hurdles to Progress in Long-form Question Answering,Reference
2343,12349," https://yjernite.github.io/lfqa.html"," ['7:453–466.', 'A.2 Ablation Study of C -REALM']","One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [Cite_Ref] .",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [Cite_Ref] . This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning. We perform controlled exper-iments initializing our retrievers with REALM-CCNews (Guu et al., 2020) and varying batch size and keeping all other hyperparameters consistent. In Table 8, we notice a steady increase in perfor-mance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) from 512 to 1024. Finally, in pre-liminary experiments we saw no benefit of more intelligent negative sampling schemes. performance of C -REALM. As a baseline, we also add the retrieval performance of the REALM pretrained model which is used as an initialization.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_6,2021,Hurdles to Progress in Long-form Question Answering,Reference
2344,12350," https://yjernite.github.io/lfqa.html"," ['7:453–466.', 'A.2 Ablation Study of C -REALM']","In Table 8, we notice a steady increase in perfor-mance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) [Cite_Ref] from 512 to 1024.",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020). This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning. We perform controlled exper-iments initializing our retrievers with REALM-CCNews (Guu et al., 2020) and varying batch size and keeping all other hyperparameters consistent. In Table 8, we notice a steady increase in perfor-mance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) [Cite_Ref] from 512 to 1024. Finally, in pre-liminary experiments we saw no benefit of more intelligent negative sampling schemes. performance of C -REALM. As a baseline, we also add the retrieval performance of the REALM pretrained model which is used as an initialization.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_7,2021,Hurdles to Progress in Long-form Question Answering,Reference
2345,12351," https://yjernite.github.io/lfqa.html"," ['7:453–466.', 'A.2 Ablation Study of C -REALM']","Batch size R-Prec Recall@5 REALM (pretrained) 6.6 14.9 256 6.2 11.0 512 (Jernite, 2020) [Cite_Ref] 6.8 12.6 1024 11.5 21.0 12288 (Ours) 13.3 21.2",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"Batch size R-Prec Recall@5 REALM (pretrained) 6.6 14.9 256 6.2 11.0 512 (Jernite, 2020) [Cite_Ref] 6.8 12.6 1024 11.5 21.0 12288 (Ours) 13.3 21.2",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_8,2021,Hurdles to Progress in Long-form Question Answering,Reference
2346,12352," https://yjernite.github.io/lfqa.html"," ['7:453–466.', 'A.2 Ablation Study of C -REALM']","Unlike Jernite (2020) [Cite_Ref] who initialize their model with BERT, before train-ing we initialize our retriever with a pretrained self-supervised retriever.",Yacine Jernite. 2020. Explain anything like i’m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.,"Next, we investigate the effect of initialization on the training of C -REALM. Unlike Jernite (2020) [Cite_Ref] who initialize their model with BERT, before train-ing we initialize our retriever with a pretrained self-supervised retriever. As a baseline, we initial-ize our model with ICT, a weaker self-supervised retriever introduced in Lee et al. (2019). Both mod-els are trained with minibatch sizes of 12228. In Table 9, we notice a large improvement in perfor-mance when using a better initialization, confirm-ing our design decisions.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.393_6_9,2021,Hurdles to Progress in Long-form Question Answering,Reference
2347,12353," https://github.com/google-research/language/tree/master/language/realm"," ['7:453–466.', 'A.1 Training & Model Details']",Our imple-mentations are based on the open-source codebases of REALM [Cite_Footnote_19] and the Routing Transformer.,19 https://github.com/google-research/language/tree/master/language/realm,"All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM [Cite_Footnote_19] and the Routing Transformer. Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search). Since our retriever is frozen, we do not use the document index refresher available in their codebase.",Method,Tool,True,Extend（引用目的）,True,2021.naacl-main.393_7_0,2021,Hurdles to Progress in Long-form Question Answering,Footnote
2348,12354," https://github.com/google-research/google-research/tree/master/routing_transformer"," ['7:453–466.', 'A.1 Training & Model Details']",Our imple-mentations are based on the open-source codebases of REALM and the Routing Transformer. [Cite_Footnote_20],"20 https://github.com/google-research/google-research/tree/master/routing_ transformer REALM predicted the correct retrieval. Notice very small differences in generation quality (R-L) as well as the fraction of n-grams (n-g) in the generation overlap-ping with retrievals predicted by C -REALM (vs pre-dicted retr.). To control for overlap due to stopwords, we also add n-gram overlaps with the randomly sam-pled retrievals. ble 13, notice very little difference with and without retrieval. no effect other than being a symbol of an economic entity or symbol of an era or location, and nothing more. For example look at Sears, Sears didn’t care what went on inside, it was all about the _appearance_ of its location, the prestige of the location, the facilities and so on. It didn’t care about how long it took it to operate, it was about how much people would pay to go see it. Sears was a landmark as a cultural movement and other big companies followed suit, so if you want to see a building you’ve never seen before, you have to go see Sears, just like you have to see a Toyota Camry for Toyota Camry. They used to be all about building new factories, some of them if I recall, but now that they’re bigger, that means that more factory jobs are coming to them. You’ve probably seen them in stores as stores where people buy and sell stuff, so there aren’t that many places for them to come from. Instead, it’s just for show, a symbol of rich people.","All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM and the Routing Transformer. [Cite_Footnote_20] Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search). Since our retriever is frozen, we do not use the document index refresher available in their codebase.",Method,Tool,True,Extend（引用目的）,True,2021.naacl-main.393_8_0,2021,Hurdles to Progress in Long-form Question Answering,Footnote
2349,12355," https://github.com/vikas95/AutoROCC"," ['1 Introduction']",The ROCC system and the codes for generat-ing all the analysis are provided here - [Cite] https: //github.com/vikas95/AutoROCC .,,The ROCC system and the codes for generat-ing all the analysis are provided here - [Cite] https: //github.com/vikas95/AutoROCC .,Mixed,Mixed,True,Produce（引用目的）,True,D19-1260_0_0,2019,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,Body
2350,12356," https://lucene.apache.org"," ['3 Approach']",We im-plemented this using the BM25 IR model with the default parameters in Lucene [Cite_Footnote_2] .,2 https://lucene.apache.org,"KBs (e.g., ARC), we retrieve the top n sentences from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We im-plemented this using the BM25 IR model with the default parameters in Lucene [Cite_Footnote_2] . For reading com-prehension datasets where the question is associ-ated with a text passage (e.g., MultiRC), all the sentences in this passage become candidates.",Method,Tool,False,Introduce（引用目的）,True,D19-1260_1_0,2019,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,Footnote
2351,12357," https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html"," ['3 Approach', '3.1 Ranking of Candidate Justification Sets']","We use the Lucene implementa-tion [Cite_Footnote_4] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer.",4 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html,"Relevance (R) We use the Lucene implementa-tion [Cite_Footnote_4] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer. In particular, we form a query that concate-nates the question and candidate answer, and use as underlying document collection (necessary to com-pute document statistics such as inverse document frequencies (IDF)) either: sentences in the entire KB (for ARC), or all sentences in the correspond-ing passage in the case of reading comprehension (MultiRC). The arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set.",Method,Tool,True,Use（引用目的）,True,D19-1260_2_0,2019,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,Footnote
2352,12358," https://github.com/thompsonb/prism"," ['1 Introduction']","Paraphrase tables were, in turn, used in MT segment-level WMT 2019 MT metrics task metrics to reward systems for paraphrasing words in every language pair; [Cite_Footnote_1] (Banerjee and Lavie, 2005) or phrases (Zhou et al.,","1 Except for Gujarati, where we had no training data. 2 https://github.com/thompsonb/prism","• Outperforms or ties with prior metrics and language pairs (Ganitkevitch and Callison-Burch, several contrastive neural methods on the 2014). Paraphrase tables were, in turn, used in MT segment-level WMT 2019 MT metrics task metrics to reward systems for paraphrasing words in every language pair; [Cite_Footnote_1] (Banerjee and Lavie, 2005) or phrases (Zhou et al.,",Method,Tool,True,Use（引用目的）,False,2020.emnlp-main.8_0_0,2020,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,Footnote
2353,12359," http://casmacat.eu/corpus/"," ['1 Introduction', '4.2 Model Training']","Our data comes primarily from WikiMatrix (Schwenk et al., 2019), Global Voices, [Cite_Footnote_5] EuroParl","5 http://casmacat.eu/corpus/ Bootstrap resampling (Koehn, 2004; Graham et al., global-voices.html 2014) is used to estimate confidence intervals for","Our data comes primarily from WikiMatrix (Schwenk et al., 2019), Global Voices, [Cite_Footnote_5] EuroParl",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.8_1_0,2020,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,Footnote
2354,12360," http://casmacat.eu/corpus/global-voices.html"," ['B Data Details for Replication']","Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, [Cite_Footnote_10] EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair).",10 http://casmacat.eu/corpus/global-voices.html,"Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, [Cite_Footnote_10] EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh–English and Kazakh–Russian data from WMT, to be able to evaluate on Kazakh.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.8_4_0,2020,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,Footnote
2355,12361," http://nlp.ffzg.hr/resources/corpora/setimes/"," ['B Data Details for Replication']","Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, [Cite_Footnote_11] United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair).",11 http://nlp.ffzg.hr/resources/corpora/setimes/,"Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, [Cite_Footnote_11] United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh–English and Kazakh–Russian data from WMT, to be able to evaluate on Kazakh.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.8_5_0,2020,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,Footnote
2356,12362," https://github.com/google-research/bleurt"," ['C.5 Baselines']","We compare to BLEURT (Sellam et al., 2020) using the authors’ recommended “BLEURT-Base 128” [Cite_Footnote_12] We compare to BERTscore F1",12 https://github.com/google-research/bleurt,"We compare to BLEURT (Sellam et al., 2020) using the authors’ recommended “BLEURT-Base 128” [Cite_Footnote_12] We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors.",Method,Code,False,Use（引用目的）,True,2020.emnlp-main.8_6_0,2020,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,Footnote
2357,12363," https://github.com/Tiiiger/bert_score"," ['C.5 Baselines']","(Zhang et al., 2020) using the model and code provided by the authors. [Cite_Footnote_13]",13 https://github.com/Tiiiger/bert_score,"We compare to BLEURT (Sellam et al., 2020) using the authors’ recommended “BLEURT-Base 128” We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors. [Cite_Footnote_13]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.8_7_0,2020,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,Footnote
2358,12364," http://data.statmt.org/wmt19/translation-task/wmt19-submitted-data-v3.tgz"," ['C.5 Baselines']","The remaining baseline results are computed using the metric scores as submitted to (Ma et al., 2019) [Cite_Footnote_14]",14 http://data.statmt.org/wmt19/translation-task/wmt19-submitted-data-v3.tgz,"The remaining baseline results are computed using the metric scores as submitted to (Ma et al., 2019) [Cite_Footnote_14]",Material,Knowledge,False,Use（引用目的）,True,2020.emnlp-main.8_8_0,2020,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,Footnote
2359,12365," https://doi.org/10.1109/ICDM.2010.75"," ['1 Introduction']","This problem can be cast within a supervised learning framework, and past work has demon-strated that it is possible to improve upon a naı̈ve classification-based approach, even without access to any labeled data from the target corpus (For-man, 2005, 2008; Bella et al., 2010 [Cite_Ref] ; Hopkins and King, 2010; Esuli and Sebastiani, 2015).","Antonio Bella, Maria Jose Ramirez-Quintana, Jose Hernandez-Orallo, and Cesar Ferri. 2010. Quan-tification via probability estimators. In IEEE In-ternational Conference on Data Mining. https: //doi.org/10.1109/ICDM.2010.75.","This problem can be cast within a supervised learning framework, and past work has demon-strated that it is possible to improve upon a naı̈ve classification-based approach, even without access to any labeled data from the target corpus (For-man, 2005, 2008; Bella et al., 2010 [Cite_Ref] ; Hopkins and King, 2010; Esuli and Sebastiani, 2015). How-ever, as we argue (§2), most of this work is based on a set of assumptions that we believe are in-valid in a significant portion of text-based research projects in the social sciences and humanities.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1148_2_0,2018,The Importance of Calibration for Estimating Proportions from Annotations,Reference
2360,12366," https://doi.org/10.1109/ICDM.2010.75"," ['3 Methods']","If using a probabilistic classifier, averaging the pre-dicted posterior probabilities rather than predicted labels will be referred to as probabilistic classify and count (PCC; Bella et al., 2010 [Cite_Ref] ).","Antonio Bella, Maria Jose Ramirez-Quintana, Jose Hernandez-Orallo, and Cesar Ferri. 2010. Quan-tification via probability estimators. In IEEE In-ternational Conference on Data Mining. https: //doi.org/10.1109/ICDM.2010.75.","Given a labeled training set and a target corpus, the naı̈ve approach is to train a classifier through any conventional means, predict labels on the tar-get corpus, and return the relative prevalence of predicted labels. Following Forman (2005), we re-fer to this approach as classify and count (CC). If using a probabilistic classifier, averaging the pre-dicted posterior probabilities rather than predicted labels will be referred to as probabilistic classify and count (PCC; Bella et al., 2010 [Cite_Ref] ).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1148_2_1,2018,The Importance of Calibration for Estimating Proportions from Annotations,Reference
2361,12367," https://doi.org/10.1145/2700406"," ['1 Introduction']","This problem can be cast within a supervised learning framework, and past work has demon-strated that it is possible to improve upon a naı̈ve classification-based approach, even without access to any labeled data from the target corpus (For-man, 2005, 2008; Bella et al., 2010; Hopkins and King, 2010; Esuli and Sebastiani, 2015 [Cite_Ref] ).",Andrea Esuli and Fabrizio Sebastiani. 2015. Optimiz-ing text quantifiers for multivariate loss functions. ACM Trans. Knowl. Discov. Data 9(4). https: //doi.org/10.1145/2700406.,"This problem can be cast within a supervised learning framework, and past work has demon-strated that it is possible to improve upon a naı̈ve classification-based approach, even without access to any labeled data from the target corpus (For-man, 2005, 2008; Bella et al., 2010; Hopkins and King, 2010; Esuli and Sebastiani, 2015 [Cite_Ref] ). How-ever, as we argue (§2), most of this work is based on a set of assumptions that we believe are in-valid in a significant portion of text-based research projects in the social sciences and humanities.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1148_6_0,2018,The Importance of Calibration for Estimating Proportions from Annotations,Reference
2362,12368," https://doi.org/10.1145/2700406"," ['5 Discussion']","An exception to this is Esuli and Sebastiani (2015) [Cite_Ref] , who test their method on the RCV1-v2 corpus, also splitting by time.",Andrea Esuli and Fabrizio Sebastiani. 2015. Optimiz-ing text quantifiers for multivariate loss functions. ACM Trans. Knowl. Discov. Data 9(4). https: //doi.org/10.1145/2700406.,"Additional related work. There is a small lit-erature on the problem of estimating proportions in a target dataset (see §1); as we have empha-sized, almost all of it makes the assumption that p(x | y) is the same for both source and tar-get. Moreover, most of the methods that have been proposed have been tested using relatively small datasets, or datasets where the target cor-pus has been artificially modified by altering the label proportions in the target corpus (as we did in the side experiment reported in Figure 3). It seems unclear that this is a good simulation of the kind of shift in distribution that one is likely to en-counter in practice. An exception to this is Esuli and Sebastiani (2015) [Cite_Ref] , who test their method on the RCV1-v2 corpus, also splitting by time. They perform a large number of experiments, but un-fortunately, nearly all of their experiments involve only a very small difference in label proportions between the source and target (with the vast ma-jority < 0.01), which limits the generalizability of their findings. Additional methods for calibration could also be considered, such as the isotonic re-gression approach of Zadrozny and Elkan (2002), but in practice we would expect the results to be very similar to Platt scaling.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1148_6_1,2018,The Importance of Calibration for Estimating Proportions from Annotations,Reference
2363,12369," https://doi.org/10.2200/S00429ED1V01Y201207AIM018"," ['5 Discussion']","Finally, this work also relates to the problem of active learning, where the goal is to interac-tively choose instances to be labeled, in a way that maximizes accuracy while minimizing the to-tal cost of annotation (Beygelzimer et al., 2009; Baldridge and Osborne, 2004; Rai et al., 2010; Settles, 2012 [Cite_Ref] ).",Burr Settles. 2012. Active Learning. Morgan & Claypool. https://doi.org/10.2200/S00429ED1V01Y201207AIM018.,"Finally, this work also relates to the problem of active learning, where the goal is to interac-tively choose instances to be labeled, in a way that maximizes accuracy while minimizing the to-tal cost of annotation (Beygelzimer et al., 2009; Baldridge and Osborne, 2004; Rai et al., 2010; Settles, 2012 [Cite_Ref] ). This is an interesting area that might be productively combined with the ideas in this paper. In general, however, the use of active learning involves additional logistical complica-tions and does not always work better than ran-dom sampling in practice (Attenberg and Provost, 2011).",補足資料,Paper,True,Introduce（引用目的）,True,N18-1148_17_0,2018,The Importance of Calibration for Estimating Proportions from Annotations,Reference
2364,12370," https://doi.org/10.1016/j.neunet.2010.10.005"," ['3 Methods', '3.2 Existing methods appropriate for extrinsic labels']","Although they are not typically thought of in the context of es-timating proportions, several methods have been proposed to deal directly with the problem of co-variate shift, including kernel mean matching and its extensions (Huang et al., 2006; Sugiyama et al., 2011 [Cite_Ref] ).","Masashi Sugiyama, Makoto Yamada, Paul von Bünau, Taiji Suzuki, Takafumi Kanamori, and Motoaki Kawanabe. 2011. Direct density-ratio estima-tion with dimensionality reduction via least-squares hetero-distributional subspace search. Neural Net-works 24(2). https://doi.org/10.1016/j.neunet.2010.10.005.","Reweighting for covariate shift. Although they are not typically thought of in the context of es-timating proportions, several methods have been proposed to deal directly with the problem of co-variate shift, including kernel mean matching and its extensions (Huang et al., 2006; Sugiyama et al., 2011 [Cite_Ref] ). Here, we consider the two-stage method from Bickel et al. (2009), which uses a logistic regression model to distinguish between source and target domains, and then uses the probabili-ties from this model to re-weight labeled training instances, to more heavily favor those that are rep-resentative of the target domain. The appeal of this method is that all unlabeled data can be used to es-timate this shift.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1148_18_0,2018,The Importance of Calibration for Estimating Proportions from Annotations,Reference
2365,12371," https://github.com/rycolab/homophony-as-renyi-entropy"," ['5 Results, Discussion and Conclusion 7']","5 Results, Discussion and Conclusion [Cite_Footnote_7]",7 Our code is available at https://github.com/rycolab/homophony-as-renyi-entropy.,"5 Results, Discussion and Conclusion [Cite_Footnote_7]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.653_0_0,2021,On Homophony and Rényi Entropy,Footnote
2366,12372," http://www.pascal-network.org/Challenges/RTE,March"," ['4 Experiments and Results']","These phrases were selected from pairs of questions and their correct answers from the set of factoid questions in TREC 2004 and also from the pairs of scenarios and hypotheses from first edition of PASCAL RTE Challenge (Dagan et al., 2005) [Cite_Ref] .","Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. Recognising Textual Entailment Challenge, http://www.pascal-network.org/Challenges/RTE,March.","The algorithm propagating verb arguments was tested on a set of 106 pairs of phrases with simi-lar meaning for which argument structures could be built. These phrases were selected from pairs of questions and their correct answers from the set of factoid questions in TREC 2004 and also from the pairs of scenarios and hypotheses from first edition of PASCAL RTE Challenge (Dagan et al., 2005) [Cite_Ref] . Table 6 shows algorithm performance. The columns in the table correspond to the follow-ing cases: a) how many cases the algorithm propagated all the arguments; b) how many cases the algorithm propagated one argument; c) home many cases the algorithm did not propa-gate any argument; using top 5, 20, 50 lexical chains.",補足資料,Website,True,Introduce（引用目的）,True,P06-1113_0_0,2006,Question Answering with Lexical Chains Propagating Verb Arguments,Reference
2367,12373," http://opennlp.sourceforge.net/"," ['6 Evaluation: Cloze', '6.2 Training and Test Data']",We use the OpenNLP [Cite_Footnote_1] coref-erence engine to resolve entity mentions.,1 http://opennlp.sourceforge.net/,"We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepo-sitional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and ar-gument head words. We use the OpenNLP [Cite_Footnote_1] coref-erence engine to resolve entity mentions.",Method,Tool,True,Use（引用目的）,True,P09-1068_0_0,2009,Unsupervised Learning of Narrative Schemas and their Participants,Footnote
2368,12374," http://www.ml4nlp.de/code-and-data/treex2conll"," ['4 Evaluation', '4.1 Datasets and Preprocessing']","The data for the second language pair is drawn from the Prague Czech-English Dependency Treebank 2.0 (Hajič et al., 2012), which we converted to a format simi-lar to that of CoNLL-ST [Cite_Footnote_1] .",1 see http://www.ml4nlp.de/code-and-data/treex2conll,"The evaluation datasets for English and Chi-nese are those from the CoNLL Shared Task 2009 (Hajič et al., 2009) (henceforth CoNLL-ST). Their annotation in the CoNLL-ST is not identi-cal, but the guidelines for “core” semantic roles are similar (Kingsbury et al., 2004), so we eval-uate only on core roles here. The data for the second language pair is drawn from the Prague Czech-English Dependency Treebank 2.0 (Hajič et al., 2012), which we converted to a format simi-lar to that of CoNLL-ST [Cite_Footnote_1] . The original annotation uses the tectogrammatical representation (Hajič, 2002) and an inventory of semantic roles (or func-tors), most of which are interpretable across vari-ous predicates. Also note that the syntactic anno-tation of English and Czech in PCEDT 2.0 is quite similar (to the extent permitted by the difference in the structure of the two languages) and we can use the dependency relations in our experiments.",Method,Tool,True,Compare（引用目的）,True,P13-1117_0_0,2013,Cross-lingual Transfer of Semantic Role Labeling Models,Footnote
2369,12375," http://ufal.mff.cuni.cz/"," ['5 Results', '5.2 Argument Classification']","Most of the labels [Cite_Footnote_2] are self-explanatory: Pa-tient (PAT), Actor (ACT), Time (TWHEN), Effect (EFF), Location (LOC), Manner (MANN), Ad-dressee (ADDR), Extent (EXT).",2 http://ufal.mff.cuni.cz/ ∼ toman/pcedt/en/functors.html,"Most of the labels [Cite_Footnote_2] are self-explanatory: Pa-tient (PAT), Actor (ACT), Time (TWHEN), Effect (EFF), Location (LOC), Manner (MANN), Ad-dressee (ADDR), Extent (EXT). CPHR marks the nominal part of a complex predicate, as in “to have [a plan] CPHR ”, and DIR3 indicates destination.",Material,Knowledge,True,Produce（引用目的）,False,P13-1117_1_0,2013,Cross-lingual Transfer of Semantic Role Labeling Models,Footnote
2370,12376," https://github.com/yuxingch/Implicature-Strength-Some"," ['1 Introduction']",We release data and code at [Cite] https://github.com/yuxingch/Implicature-Strength-Some.,,We release data and code at [Cite] https://github.com/yuxingch/Implicature-Strength-Some.,Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.479_0_0,2020,Harnessing the linguistic signal to predict scalar inferences,Body
2371,12377," http://psd.museum.upenn.edu/"," ['2 CDLI and the Annotations']","This format is the standard interchange for-mat for transliteration across many projects dealing in and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the Pennsylvania Sumerian Dictionary (PSD, 2006) [Cite_Ref] , and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014)).",PSD: The Pennsylvania Sumerian Dictionary. 2006. http://psd.museum.upenn.edu/,"In the study of the Ur III corpus, the most exhaus-tive infrastructure and documentation for lemmati-zation is that provided for “the Open Richly Anno-tated Cuneiform Corpus (Oracc)” (ORACC, 2014). The lemmatizer for the Oracc system is accessed via an Emacs interface designed to encourage si-multaneous transliteration and lemmatization by a human expert. The process begins with the human expert submitting an unlemmatized transliteration in a format called ATF (ASCII Transliteration For-mat). This format is the standard interchange for-mat for transliteration across many projects dealing in and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the Pennsylvania Sumerian Dictionary (PSD, 2006) [Cite_Ref] , and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014)). Via the Emacs interface, the transliteration is sub-mitted to the linguistic annotatation system, which identifies an existing project-specific glossary based on directives provided by the human expert in the transliteration, and returns a preliminary lemmatiza-tion whose completeness and content depends on the referenced project glossary. The transliterator may then modify any automatically-generated lemmata, or, in the case of new words or new senses in which existing words used, manually lemmatize the word to allow the lemmatizer to “harvest” the new lemma and add it to the glossary. Oracc’s lemmatizer also performs normalization and morphological analysis in order to automatically and consistently identify words in the text. The lemmatizer is not designed to “learn” new insights or induce new rules regard-ing Sumerian morphology on the basis of new lem-mata harvested from submissions, but rather serves as a mechanism to consistently apply rules that have been harvested.",Material,Knowledge,True,Extend（引用目的）,True,N15-1167_4_0,2015,Enhancing Sumerian Lemmatization by Unsupervised Named-Entity Recognition,Reference
2372,12378," http://oracc.museum.upenn.edu/dcclt/"," ['2 CDLI and the Annotations']","This format is the standard interchange for-mat for transliteration across many projects dealing in and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the Pennsylvania Sumerian Dictionary (PSD, 2006), and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014) [Cite_Ref] ).",DCCLT - Digital Corpus of Cuneiform Lexical Texts. 2014. http://oracc.museum.upenn.edu/dcclt/,"In the study of the Ur III corpus, the most exhaus-tive infrastructure and documentation for lemmati-zation is that provided for “the Open Richly Anno-tated Cuneiform Corpus (Oracc)” (ORACC, 2014). The lemmatizer for the Oracc system is accessed via an Emacs interface designed to encourage si-multaneous transliteration and lemmatization by a human expert. The process begins with the human expert submitting an unlemmatized transliteration in a format called ATF (ASCII Transliteration For-mat). This format is the standard interchange for-mat for transliteration across many projects dealing in and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the Pennsylvania Sumerian Dictionary (PSD, 2006), and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014) [Cite_Ref] ). Via the Emacs interface, the transliteration is sub-mitted to the linguistic annotatation system, which identifies an existing project-specific glossary based on directives provided by the human expert in the transliteration, and returns a preliminary lemmatiza-tion whose completeness and content depends on the referenced project glossary. The transliterator may then modify any automatically-generated lemmata, or, in the case of new words or new senses in which existing words used, manually lemmatize the word to allow the lemmatizer to “harvest” the new lemma and add it to the glossary. Oracc’s lemmatizer also performs normalization and morphological analysis in order to automatically and consistently identify words in the text. The lemmatizer is not designed to “learn” new insights or induce new rules regard-ing Sumerian morphology on the basis of new lem-mata harvested from submissions, but rather serves as a mechanism to consistently apply rules that have been harvested.",Material,Knowledge,True,Extend（引用目的）,True,N15-1167_5_0,2015,Enhancing Sumerian Lemmatization by Unsupervised Named-Entity Recognition,Reference
2373,12379," http://oracc.museum.upenn.edu/doc/about/aboutoracc/index.html"," ['3 Sumerian Personal Name Recognition', '3.2 Our System']","When the Sumerologists transliterate the tablets, they use metacharacters such as “[...]” and “#” to indicate damage to the text, and “!”, “?”, “*”, and “” to represent correction, querying or colla-tion (Tinney and Robson, 2014) [Cite_Ref] .",Steve Tinney and Eleanor Robson. 2014. Oracc: The Open Richly Annotated Cuneiform Corpus. http://oracc.museum.upenn.edu/doc/about/aboutoracc/index.html,"When the Sumerologists transliterate the tablets, they use metacharacters such as “[...]” and “#” to indicate damage to the text, and “!”, “?”, “*”, and “” to represent correction, querying or colla-tion (Tinney and Robson, 2014) [Cite_Ref] . For “[...]” and “” cases, the Sumerologists put their “best guess” within the brackets. For example, in the word “[nu]-su”, the first sign was originally damaged but restored by the Sumerologists as the “best guess”. Our system removes the metacharacters as noise, and treats the resulting text as if it were otherwise unannotated.",補足資料,Paper,True,Introduce（引用目的）,True,N15-1167_6_0,2015,Enhancing Sumerian Lemmatization by Unsupervised Named-Entity Recognition,Reference
2374,12380," https://github.com/clab/lstm-parser/tree/easy-to-use"," ['5 DeepCx neural network architecture', '5.3 Implementation details']",DeepCx is implemented using a refactored version of the LSTM parser codebase that performs iden-tically to the original. [Cite_Footnote_2],2 https://github.com/clab/lstm-parser/tree/easy-to-use.,"DeepCx is implemented using a refactored version of the LSTM parser codebase that performs iden-tically to the original. [Cite_Footnote_2] The neural network frame-work, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02–21; development: 22).",Method,Tool,True,Compare（引用目的）,True,D18-1196_0_0,2018,DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers,Footnote
2375,12381," https://github.com/duncanka/lstm-causality-tagger"," ['5 DeepCx neural network architecture', '5.3 Implementation details']",The code for DeepCx is available on GitHub. [Cite_Footnote_3],3 https://github.com/duncanka/lstm-causality-tagger.,The code for DeepCx is available on GitHub. [Cite_Footnote_3] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser.,Method,Code,True,Produce（引用目的）,True,D18-1196_1_0,2018,DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers,Footnote
2376,12382," http://www.ims.uni-stuttgart.de/schmid"," ['2 Feature Annotation']",The most important ones of these features [Cite_Footnote_1] will now be described in detail.,1 The complete annotation program is available from the author’s home page at http://www.ims.uni-stuttgart.de/ schmid,"Besides the slash features, we used other fea-tures in order to improve the parsing accuracy of the PCFG, inspired by the work of Klein and Man-ning (2003). The most important ones of these features [Cite_Footnote_1] will now be described in detail. Sec-tion 4.3 shows the impact of these features on labeled bracketing accuracy and empty category prediction.",Method,Code,True,Use（引用目的）,True,P06-1023_0_0,2006,Trace Prediction and Recovery With Unlexicalized PCFGs and Slash Features,Footnote
2377,12383," https://github.com/jzhou316/Unsupervised-Sentence-Summarization"," ['3 Experimental Setup']",We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [Cite_Footnote_1],1 Code available at https://github.com/jzhou316/Unsupervised-Sentence-Summarization.,"For the contextual matching model’s similarity function S, we adopt the forward language model of ELMo (Peters et al., 2018) to encode tokens to corresponding hidden states in the sequence, re-sulting in a three-layer representation each of di-mension 512. The bottom layer is a fixed char-acter embedding layer, and the above two layers are LSTMs associated with the generic unsuper-vised language model trained on a large amount of text data. We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [Cite_Footnote_1] The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024. It is trained using dropout rate 0.5 and SGD combined with gradient clipping.",Method,Code,True,Produce（引用目的）,True,P19-1503_0_0,2019,Simple Unsupervised Summarization by Contextual Matching,Footnote
2378,12384," http://opennlp.sourceforge.net"," ['2 Background', '2.2 Discourse in Textual Entailment']","A number of systems have tried to address the question of coreference in RTE as a preprocessing step prior to inference proper, with most systems using off-the-shelf coreference resolvers such as JavaRap (Qiu et al., 2004) or OpenNLP [Cite_Footnote_3] .",3 http://opennlp.sourceforge.net,"A number of systems have tried to address the question of coreference in RTE as a preprocessing step prior to inference proper, with most systems using off-the-shelf coreference resolvers such as JavaRap (Qiu et al., 2004) or OpenNLP [Cite_Footnote_3] . Gen-erally, anaphoric expressions were textually re-placed by their antecedents. Results were in-conclusive, however, with several reports about errors introduced by automatic coreference res-olution (Agichtein et al., 2008; Adams et al., 2007). Specific evaluations of the contribution of coreference resolution yielded both small nega-tive (Bar-Haim et al., 2008) and insignificant pos-itive (Chambers et al., 2007) results.",Method,Tool,True,Introduce（引用目的）,True,P10-1123_0_0,2010,Assessing the Role of Discourse References in Entailment Inference,Footnote
2379,12385," http://www.cs.biu.ac.il/~nlp/downloads/"," ['3 Motivation and Goals']","To our knowledge, this is the first such in-depth study. [Cite_Footnote_4]",4 The guidelines and the dataset are available at http://www.cs.biu.ac.il/˜nlp/downloads/,"In contrast to the numerous existing datasets annotated for discourse references (Hovy et al., 2006; Strassel et al., 2008), we do not annotate ex-haustively. Rather, we are interested specifically in those references instances that impact inference. Furthermore, we analyze each instance from an entailment perspective, characterizing the relevant factors that have an impact on inference. To our knowledge, this is the first such in-depth study. [Cite_Footnote_4]",Mixed,Mixed,True,Produce（引用目的）,True,P10-1123_1_0,2010,Assessing the Role of Discourse References in Entailment Inference,Footnote
2380,12386," https://github.com/cindyxinyiwang/TrDec_pytorch"," ['References']","Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [Cite_Footnote_1]",1 Our code is available at https://github.com/cindyxinyiwang/TrDec_pytorch.,"Recent advances in Neural Machine Transla-tion (NMT) show that adding syntactic infor-mation to NMT systems can improve the qual-ity of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like constituency and dependency parse trees. This is often done via a standard RNN decoder that operates on a lin-earized target tree structure. However, it is an open question of what specific linguistic for-malism, if any, is the best structural represen-tation for NMT. In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D18-1509_0_0,2018,"A Tree-based Decoder for Neural Machine Translation Xinyi Wang 1 , Hieu Pham 1,2 , Pengcheng Yin 1 , Graham Neubig 1",Footnote
2381,12387," http://www.phontron.com/kftt"," ['5 Experiments']","We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011) [Cite_Ref] , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible.",Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.,"Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011) [Cite_Ref] , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible. Details are in Tab. 1. English sentences are parsed using Ckylark (Oda et al., 2015) for the constituency parse trees, and Stanford Parser (de Marneffe et al., 2006; Chen and Manning, 2014) for the dependency parse trees. We use byte-pair encoding (Sennrich et al., 2016) with 8K merge operations on ja-en, 4K merge operations on or-en, and 24K merge operations on de-en.",Material,Dataset,True,Compare（引用目的）,True,D18-1509_1_0,2018,"A Tree-based Decoder for Neural Machine Translation Xinyi Wang 1 , Hieu Pham 1,2 , Pengcheng Yin 1 , Graham Neubig 1",Reference
2382,12388," https://github.com/sjcfr/CWVAE"," ['1 Introduction']",The code is released at [Cite] https://github.com/sjcfr/CWVAE.,,Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at [Cite] https://github.com/sjcfr/CWVAE.,Method,Code,True,Produce（引用目的）,True,D19-1270_0_0,2019,Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,Body
2383,12389," http://mstparser.sourceforge.net"," ['5 Evaluation Results', '5.1 Syntactic Dependency Parsers']","The parser is basically based on the MSTParser [Cite_Footnote_8] using all the features presented by (McDonald et al., 2006) with projective parsing.",8 It’s freely available at http://mstparser.sourceforge.net.,"The parser is basically based on the MSTParser [Cite_Footnote_8] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser. 1) Chen et al. (2008) used fea-tures derived from short dependency pairs based on large-scale auto-parsed data to enhance depen-dency parsing. Here, the same features are used, though all dependency pairs rather than short de-pendency pairs are extracted along with the de-pendency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large im-provement for English and Czech. Here, the same features are also used as word clusters are gen-erated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transition-based parser based on maximum entropy as in Zhao and Kit (2008). We still use the similar fea-ture notations of that work.",Method,Tool,True,Extend（引用目的）,True,D09-1004_0_0,2009,Semantic Dependency Parsing of NomBank and PropBank: An Efficient Integrated Approach via a Large-scale Feature Selection ∗,Footnote
2384,12390," http://www.ai.mit.edu/people/jrennie/20_newsgroups/"," ['6 Experiments', '6.1 Experiments Settings']","For simplicity, we follow the widely used cost model (Raghavan and Allan, 2007; Druck et al., 2008; Sindhwani et al., 2009) where features are roughly 5 times cheaper to label than examples, so we assume the cost is [Cite_Footnote_1] for a word query and is 5 for a document query.",1 http://www.ai.mit.edu/people/jrennie/20_newsgroups/,"Results are averaged over 10 random training-test splits. For each split, 30% examples are used for testing. All methods are initialized by a ran-dom choice of 10 document labels and 10 word la-bels. For simplicity, we follow the widely used cost model (Raghavan and Allan, 2007; Druck et al., 2008; Sindhwani et al., 2009) where features are roughly 5 times cheaper to label than examples, so we assume the cost is [Cite_Footnote_1] for a word query and is 5 for a document query. We set α = β = 5, γ = 1 for all the following experiments 2 .",補足資料,Document,False,Introduce（引用目的）,False,D11-1088_0_0,2011,A Non-negative Matrix Factorization Based Approach for Active Dual Supervision from Document and Word Labels,Footnote
2385,12391," http://svmlight.joachims.org/"," ['3 Exploiting Comparable Corpora', '3.1 Multilingual Domain Model', 'w il evaluated in the corpus T l .']","As Kernel Methods are the state-of-the-art su-pervised framework for learning and they have been successfully adopted to approach the TC task (Joachims, 2002), we chose this framework to per-form all our experiments, in particular Support Vector Machines [Cite_Footnote_3] .",3 We adopted the efficient implementation freely available at http://svmlight.joachims.org/.,"As Kernel Methods are the state-of-the-art su-pervised framework for learning and they have been successfully adopted to approach the TC task (Joachims, 2002), we chose this framework to per-form all our experiments, in particular Support Vector Machines [Cite_Footnote_3] . Taking into account the exter-nal knowledge provided by a MDM it is possible estimate the topic similarity among two texts ex-pressed in different languages, with the following kernel: where D is defined as in equation 1.",Method,Tool,True,Extend（引用目的）,True,P06-1070_0_0,2006,Exploiting Comparable Corpora and Bilingual Dictionaries for Cross-Language Text Categorization,Footnote
2386,12392," http://multiwordnet.itc.it"," ['4 Exploiting Bilingual Dictionaries']",MultiWordNet [Cite_Footnote_4] .,4 Available at http://multiwordnet.itc.it.,"MultiWordNet [Cite_Footnote_4] . It is a multilingual computa-tional lexicon, conceived to be strictly aligned with the Princeton WordNet. The available lan-guages are Italian, Spanish, Hebrew and Roma-nian. In our experiment we used the English and the Italian components. The last version of the Italian WordNet contains around 58,000 Italian word senses and 41,500 lemmas organized into 32,700 synsets aligned whenever possible with WordNet English synsets. The Italian synsets are created in correspondence with the Princeton WordNet synsets, whenever possible, and seman-tic relations are imported from the corresponding English synsets. This implies that the synset index structure is the same for the two languages.",Material,Knowledge,True,Introduce（引用目的）,False,P06-1070_1_0,2006,Exploiting Comparable Corpora and Bilingual Dictionaries for Cross-Language Text Categorization,Footnote
2387,12393," https://github.com/intersun/LightningDOT"," ['References']","In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000× mag-nitude of computational hours. [Cite_Footnote_1]",1 Code and pre-training checkpoints are available at https://github.com/intersun/LightningDOT.,"Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow infer-ence speed due to enormous computation cost mainly from cross-modal attention in Trans-former architecture. When applied to real-life applications, such latency and computa-tion demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most ma-ture scenario of V+L application, which has been widely studied even prior to the emer-gence of recent pre-trained models. We pro-pose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, with-out sacrificing accuracy. LightningDOT re-moves the time-consuming cross-modal atten-tion by pre-training on three novel learning objectives, extracting feature indexes offline, and employing instant dot-product matching with further re-ranking, which significantly speeds up retrieval process. In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000× mag-nitude of computational hours. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.77_0_0,2021,LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval,Footnote
2388,12394," https://github.com/yumeng5/LOTClass"," ['References']",We show that our model achieves around 90% ac-curacy on four benchmark datasets including topic and sentiment classification without us-ing any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name [Cite_Footnote_1] .,1 Source code can be found at https://github.com/yumeng5/LOTClass.,"Current text classification methods typically require a good number of human-labeled doc-uments as training data, which can be costly and difficult to obtain in real applications. Hu-mans can perform classification without see-ing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on un-labeled data, without using any labeled doc-uments. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as rep-resentation learning models for document clas-sification. Our method (1) associates semanti-cally related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% ac-curacy on four benchmark datasets including topic and sentiment classification without us-ing any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.724_0_0,2020,Text Classification Using Label Names Only: A Language Model Self-Training Approach,Footnote
2389,12396," http://open.weibo.com/wiki"," ['3 Experiment', '3.1 Data set']","We use the API provided by weibo.com [Cite_Footnote_1] to crawl 500,000 micro-blog texts of weibo.com, which contains 24,243,772 charac-ters.",1 http://open.weibo.com/wiki,"We evaluate our method using the data from weibo.com, which is the biggest micro-blog ser-vice in China. We use the API provided by weibo.com [Cite_Footnote_1] to crawl 500,000 micro-blog texts of weibo.com, which contains 24,243,772 charac-ters. To keep the experiment tractable, we first ran-domly choose 50,000 of all the texts as unlabeled data, which contain 2,420,037 characters. We manually segment 2038 randomly selected micro-blogs.We follow the segmentation standard as the PKU corpus.",補足資料,Website,True,Introduce（引用目的）,True,P13-2032_1_0,2013,Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations,Footnote
2390,12397," http://www.sighan.org/bakeoff2005/"," ['3 Experiment', '3.1 Data set']",We use the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff [Cite_Footnote_2] as the labeled data.,2 http://www.sighan.org/bakeoff2005/,We use the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff [Cite_Footnote_2] as the labeled data. We choose the PKU data in our experiment because our baseline meth-ods use the same segmentation standard.,補足資料,Website,True,Introduce（引用目的）,True,P13-2032_2_0,2013,Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations,Footnote
2391,12398," http://ictclas.org/"," ['3 Experiment', '3.1 Data set']","The first two are both famous Chinese word segmentation tools: ICTCLAS [Cite_Footnote_3] and Stan-ford Chinese word segmenter , which are widely used in NLP related to word segmentation.",3 http://ictclas.org/,"We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS [Cite_Footnote_3] and Stan-ford Chinese word segmenter , which are widely used in NLP related to word segmentation. Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours. ICTCLAS, on the other hand, is a HMM-based Chinese word segmenter. Another baseline is Li and Sun (2009), which also uses punctua-tion in their semi-supervised framework. F-score is used as the accuracy measure. The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to cor-rectly segment out of vocabulary words.",Method,Tool,True,Introduce（引用目的）,True,P13-2032_3_0,2013,Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations,Footnote
2392,12399,http://nlp.stanford.edu/projects/chinese-nlp.shtml\\#cws,"['3 Experiment', '3.1 Data set']","The first two are both famous Chinese word segmentation tools: ICTCLAS and Stan-ford Chinese word segmenter [Cite_Footnote_4] , which are widely used in NLP related to word segmentation.",4 http://nlp.stanford.edu/projects/chinese-nlp.shtml\\#cws,"We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS and Stan-ford Chinese word segmenter [Cite_Footnote_4] , which are widely used in NLP related to word segmentation. Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours. ICTCLAS, on the other hand, is a HMM-based Chinese word segmenter. Another baseline is Li and Sun (2009), which also uses punctua-tion in their semi-supervised framework. F-score is used as the accuracy measure. The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to cor-rectly segment out of vocabulary words.",Method,Tool,True,Introduce（引用目的）,True,P13-2032_4_0,2013,Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations,Footnote
2393,12400," https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss"," ['3 Model', '3.2 ReWE']","In the experiment, we have explored two cases for the ReWE loss : the minimum square error (MSE) [Cite_Footnote_1] and the cosine embedding loss (CEL) .",1 https://pytorch.org/docs/stable/nn.html#torch.nn. MSELoss,"In the experiment, we have explored two cases for the ReWE loss : the minimum square error (MSE) [Cite_Footnote_1] and the cosine embedding loss (CEL) . Finally, the NLL loss and the ReW E loss are com-bined to form the training objective using a posi-tive trade-off coefficient, λ:",Method,Code,False,Compare（引用目的）,False,N19-1041_0_0,2019,ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems,Footnote
2394,12401," https://pytorch.org/docs/stable/nn.html#torch.nn.CosineEmbeddingLoss"," ['3 Model', '3.2 ReWE']",[Cite_Footnote_2] .,2 https://pytorch.org/docs/stable/nn.html#torch.nn. CosineEmbeddingLoss,"In the experiment, we have explored two cases for the ReWE loss : the minimum square error (MSE) and the cosine embedding loss (CEL) [Cite_Footnote_2] . Finally, the NLL loss and the ReW E loss are com-bined to form the training objective using a posi-tive trade-off coefficient, λ:",Method,Code,False,Compare（引用目的）,False,N19-1041_1_0,2019,ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems,Footnote
2395,12402," https://github.com/ijauregiCMCRC/ReWENMT"," ['4 Experiments']","We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [Cite_Footnote_3] .",3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT,"We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [Cite_Footnote_3] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017). We have also explored the use of sub-word units learned with byte pair encoding (BPE) (Sennrich et al., 2016). All the preprocess-ing steps, hyperparameter values and training pa-rameters are described in detail in the supplemen-tary material to ease reproducibility of our results.",Method,Tool,True,Use（引用目的）,True,N19-1041_2_0,2019,ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems,Footnote
2396,12403," http://www.statmt.org/wmt16/"," ['4 Experiments']",We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) [Cite_Footnote_4] and the 2016 International Workshop on Spoken Language Translation (IWSLT16) .,4 WMT16: http://www.statmt.org/wmt16/,"We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) [Cite_Footnote_4] and the 2016 International Workshop on Spoken Language Translation (IWSLT16) . Table 1 lists the datasets and their main features. Despite hav-ing nearly 90,000 parallel sentences, the eu-en dataset only contains 2,000 human-translated sen-tences; the others are translations of Wikipedia page titles and localization files. Therefore, we regard the eu-en dataset as very low-resource.",補足資料,Website,True,Introduce（引用目的）,True,N19-1041_3_0,2019,ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems,Footnote
2397,12404," https://workshop2016.iwslt.org/"," ['4 Experiments']",[Cite_Footnote_5] .,5 IWSLT16: https://workshop2016.iwslt.org/,"We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) [Cite_Footnote_5] . Table 1 lists the datasets and their main features. Despite hav-ing nearly 90,000 parallel sentences, the eu-en dataset only contains 2,000 human-translated sen-tences; the others are translations of Wikipedia page titles and localization files. Therefore, we regard the eu-en dataset as very low-resource.",補足資料,Website,True,Introduce（引用目的）,True,N19-1041_4_0,2019,ReWE: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems,Footnote
2398,12405," https://github.com/cookielee77/CLARE"," ['1 Introduction']",We release our code and models at [Cite] https://github.com/cookielee77/CLARE.,,"We evaluate CLARE on text classification, nat-ural language inference, and sentence paraphrase tasks, by attacking finetuned BERT models (De-vlin et al., 2019). Extensive experiments and hu-man evaluation results show that CLARE outper-forms baselines in terms of attack success rate, tex-tual similarity, fluency, and grammaticality, and strikes a better balance between attack success rate and preserving input-output similarity. Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [Cite] https://github.com/cookielee77/CLARE.",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.400_0_0,2021,Contextualized Perturbation for Textual Adversarial Attack,Body
2399,12406," https://www.languagetool.org/"," ['3 Experiments', '3.2 Datasets and Evaluation']","Following (Zang et al., 2020; Morris et al., 2020b), we calculate this by the LanguageTool (Naber et al., 2003). [Cite_Footnote_7]",7 https://www.languagetool.org/,"• Grammar error (GErr): the absolute num-ber of increased grammatical errors in the suc-cessful adversarial example, compared to the original text. Following (Zang et al., 2020; Morris et al., 2020b), we calculate this by the LanguageTool (Naber et al., 2003). [Cite_Footnote_7]",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.400_1_0,2021,Contextualized Perturbation for Textual Adversarial Attack,Footnote
2400,12407," https://github.com/cookielee77/CLARE"," ['6 Conclusion']",We release our code and models at [Cite] https://github.com/cookielee77/CLARE.,,"We have presented CLARE, a contextualized ad-versarial example generation model for text. It uses contextualized knowledge from pretrained masked language models, and can generate ad-versarial examples that are natural, fluent and grammatical. With three contextualized perturba-tion patterns, Replace, Insert and Merge in our arsenal, CLARE can produce outputs of varied lengths and achieves a higher attack success rate than baselines and with fewer edits. Human eval-uation shows significant advantages of CLARE in terms of textual similarity, fluency and gram-maticality. We release our code and models at [Cite] https://github.com/cookielee77/CLARE.",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.400_3_0,2021,Contextualized Perturbation for Textual Adversarial Attack,Body
2401,12408," https://github.com/huggingface/transformers"," ['A Appendix', 'A.1 Additional Experiment Details']","All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers [Cite_Footnote_10] (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019).",10 https://github.com/huggingface/ transformers,"Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers [Cite_Footnote_10] (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively. We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler and BERTAt-tack are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set. In the adver-sarial training (§4.3), the small TextCNN victim model (Kim, 2014) has 128 embedding size and 100 filters for 3, 4, 5 window size with 0.5 dropout, resulting in 7M parameters.",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.400_4_0,2021,Contextualized Perturbation for Textual Adversarial Attack,Footnote
2402,12409," https://github.com/jind11/TextFooler"," ['A Appendix', 'A.1 Additional Experiment Details']",TextFooler [Cite_Footnote_11] and BERTAt-tack are built with their open source implemen-tation provided by the authors.,11 https://github.com/jind11/TextFooler,"Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively. We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler [Cite_Footnote_11] and BERTAt-tack are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set. In the adver-sarial training (§4.3), the small TextCNN victim model (Kim, 2014) has 128 embedding size and 100 filters for 3, 4, 5 window size with 0.5 dropout, resulting in 7M parameters.",Method,Tool,True,Introduce（引用目的）,True,2021.naacl-main.400_5_0,2021,Contextualized Perturbation for Textual Adversarial Attack,Footnote
2403,12410," https://github.com/LinyangLee/BERT-Attack"," ['A Appendix', 'A.1 Additional Experiment Details']",TextFooler and BERTAt-tack [Cite_Footnote_12] are built with their open source implemen-tation provided by the authors.,12 https://github.com/LinyangLee/ BERT-Attack,"Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively. We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler and BERTAt-tack [Cite_Footnote_12] are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set. In the adver-sarial training (§4.3), the small TextCNN victim model (Kim, 2014) has 128 embedding size and 100 filters for 3, 4, 5 window size with 0.5 dropout, resulting in 7M parameters.",Method,Tool,True,Introduce（引用目的）,True,2021.naacl-main.400_6_0,2021,Contextualized Perturbation for Textual Adversarial Attack,Footnote
2404,12411," https://github.com/UKPLab/acl2016-supersense-embeddings"," ['1 Introduction']","• We are the first to provide a joint word-and supersense-embedding model, which we make publicly available [Cite_Footnote_1] for the research com-munity.",1 https://github.com/UKPLab/ acl2016-supersense-embeddings,"• We are the first to provide a joint word-and supersense-embedding model, which we make publicly available [Cite_Footnote_1] for the research com-munity. This provides an insight into the word and supersense positions in the vector space",Method,Code,True,Produce（引用目的）,True,P16-1191_0_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2405,12412," https://github.com/kutschkem/SmithHeilmann_fork/tree/master/MIRATagger"," ['2 Related Work', '2.2 Supersense Tagging']","its reimple-mentation by Heilman [Cite_Footnote_2] , was widely used in applied tasks (Agirre et al., 2011; Surdeanu et al., 2011; Laparra and Rigau, 2013).",2 https://github.com/kutschkem/SmithHeilmann_fork/tree/master/ MIRATagger,"Supersenses, also known as lexicographer files or semantic fields, were originally used to organize lexical-semantic resources (Fellbaum, 1990). The supersense tagging task was introduced by Cia-ramita and Johnson (2003) for nouns and later expanded for verbs (Ciaramita and Altun, 2006). Their state-of-the-art system is trained and eval-uated on the SemCor data (Miller et al., 1994) with an F-score of 77.18%, using a hidden Markov model. Since then, the system, resp. its reimple-mentation by Heilman [Cite_Footnote_2] , was widely used in applied tasks (Agirre et al., 2011; Surdeanu et al., 2011; Laparra and Rigau, 2013). Supersense taggers have then been built also for Italian (Picca et al., 2008), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2013). Tsvetkov et al. (2015) proposes the us-age of SemCor supersense frequencies as a way to evaluate word embedding models, showing that a good alignment of embedding dimensions to super-senses correlates with performance of the vectors in word similarity and text classification tasks. Re-cently, Johannsen et al. (2014) introduced a task of multiword supersense tagging on Twitter. On their newly constructed dataset, they show poor do-main adaptation performance of previous systems, achieving a maximum performance with a search-based structured prediction model (Daumé III et al., 2009) trained on both Twitter and SemCor data. In parallel, Schneider and Smith (2015) expanded a multiword expression (MWE) annotated corpus of online reviews with supersense information, fol-lowing an alternative annotation scheme focused on MWE. Similarly to Johannsen et al. (2014), they find that SemCor may not be a sufficient re-source for supersense tagging adaption to different domains. Therefore, in our work, we explore the potential of using an automatically annotated Ba-belfied Wikipedia corpus (Scozzafava et al., 2015) for this task.",Method,Tool,True,Introduce（引用目的）,True,P16-1191_1_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2406,12414," https://github.com/coastalcph/supersense-data-twitter"," ['5 Building a Supersense Tagger', '5.1 Experimental Setup']",With a sliding window of size [Cite_Footnote_5] for the sequence learning setup we extract for each word the following seven feature vectors:,5 https://github.com/coastalcph/ supersense-data-twitter,"We implement a window-based approach with a multi-channel multi-layer perceptron model using the Theano framework (Bastien et al., 2012). With a sliding window of size [Cite_Footnote_5] for the sequence learning setup we extract for each word the following seven feature vectors:",Material,DataSource,False,Use（引用目的）,False,P16-1191_3_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2407,12415," https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py"," ['6 Using Supersense Embeddings in Document Classification Tasks', '6.1 Experimental Setup']","Keras demo [Cite_Footnote_7] , into which we incorporate the su-persense information.",7 https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py,"Keras demo [Cite_Footnote_7] , into which we incorporate the su-persense information. Figure 3 displays our net-work architecture. First, we use three channels of word embeddings on the plain textual input. The first channel are the 300-dimensional word em-beddings obtained from our enriched Wikipedia corpus. The second embedding channel consists of 41-dimensional vectors capturing the cosine simi-larity of the word to each supersense embedding. The third channel contains the vector of relative frequencies of the word occurring in the enriched Wikipedia together with its supersense, i.e. provid-ing the background supersense distribution for the word. Each of the document embeddings is then convoluted with the filter size of 3, followed by a pooling layer of length 2 and fed into a long-short-term-memory (LSTM) layer. In parallel, we feed as input a processed document text, where the words are replaced by their predicted super-senses. Given that we have the Wikipedia-based supersense embeddings in the same vector space as the word embeddings, we can now proceed to creating the 300-dimensional embedding channel also for the supersense text. As in the plain text channels, we feed also these embeddings into the convolutional and LSTM layers in a similar fashion. Afterwards, we concatenate all LSTM outputs and feed them into a standard fully connected neural network layer, followed by the sigmoid for the bi-nary output. The following subsections discuss our results on a range of classification tasks: subjectiv-ity prediction, sentiment polarity classification and metaphor detection.",Method,Tool,True,Produce（引用目的）,True,P16-1191_4_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2408,12416," http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html"," ['6 Using Supersense Embeddings in Document Classification Tasks', '6.2 Sentiment Polarity Classification']","The Movie Review dataset, published by Pang and Lee (2005) [Cite_Footnote_8] , has become a standard machine learning bench-mark task for binary sentence classification.",8 http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html,"Sentiment classification has been a widely explored task which received a lot of attention. The Movie Review dataset, published by Pang and Lee (2005) [Cite_Footnote_8] , has become a standard machine learning bench-mark task for binary sentence classification. Socher et al. (2011) address this task with recursive au-toencoders and Wikipedia word embeddings, later improving their score using recursive neural net-work with parse trees (Socher et al., 2012). Com-petitive results were achieved also by a sentiment-analysis-specific parser (Dong et al., 2015), with a fast dropout logistic regression (Wang and Man-ning, 2013), and with convolutional neural net-works (Kim, 2014). Table 7 compares these ap-proaches to our results for a 10-fold crossvalidation with 10% of the data withheld for parameter tuning. The line WORDS displays the performance using only the leftmost part of our architecture, i.e. only the text input with our word embeddings. The line SUPER shows the result of using the full super-sense architecture. As it can be seen from the table, the supersense features improve the accuracy by about 2%. Both systems are significantly different (p < 0.01), using the McNemar’s test.",補足資料,Document,False,Introduce（引用目的）,False,P16-1191_5_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2409,12417," https://www.cs.cornell.edu/people/pabo/movie-review-data/"," ['6 Using Supersense Embeddings in Document Classification Tasks', '6.3 Subjectivity Classification']","They compose a publicly avail-able dataset [Cite_Footnote_9] of 5000 subjective and 5000 objec-tive sentences, classifying them with a reported accuracy of 90-92% and further show",9 https://www.cs.cornell.edu/people/pabo/movie-review-data/,"Pang and Lee (2004) demonstrate that the subjec-tivity detection can be a useful input for a sen-timent classifier. They compose a publicly avail-able dataset [Cite_Footnote_9] of 5000 subjective and 5000 objec-tive sentences, classifying them with a reported accuracy of 90-92% and further show that predict-ing this information improves the end-level sen-timent classification on a movie review dataset. Kim (2014) and Wang and Manning (2013) fur-ther improve the performance through different machine learning methods. Supersenses are a nat-ural candidate for subjectivity prediction, as we hypothesize that the nouns and verbs in the sub-jective and objective sentences often come from different semantic classes (e.g. VERB . FEELING vs. VERB . COGNITION ). We employ the same archi-tecture as in previous task, automatically annotat-ing the words in the documents with their super-senses. Our results are reported in Table 9. The supersenses (SUPER) provide an additional infor-mation, improving the model performance by up to 2% over word embeddings (WORDS). The dif-ference between both systems is significant. Based on a manual error analysis, the supersense informa-tion contributes here in a similar manner as in the previous case. Subjective sentences contain more verbs of supersense PERCEPTION , while objective ones more frequently feature the supersenses POS - SESSION and SOCIAL . Nouns in the subjective cat-egory are characterized by supersenses COMMUNI - CATION and ATTRIBUTE , while in objective ones the PERSON and POSSESSION are more frequent.",Material,Dataset,True,Introduce（引用目的）,True,P16-1191_6_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2410,12418," http://www.cs.cmu.edu/~ytsvetko/metaphor/datasets.zip"," ['6 Using Supersense Embeddings in Document Classification Tasks', '6.4 Metaphor Identification']",Tsvetkov et al. (2013) pursue this work further by constructing and publishing a dataset of 985 literal and 985 methaphorical adjective-noun pairs [Cite_Footnote_10] and classify them.,10 http://www.cs.cmu.edu/˜ytsvetko/metaphor/datasets.zip,"Supersenses have recently been shown to provide improvements in metaphor prediction tasks (Ger-shman et al., 2014), as they hold the informa-tion of coarse semantic concepts. Turney et al. (2011) explore the task of discriminating literal and metaphoric adjective-noun expressions. They report an accuracy of 79% on a small dataset rated by five annotators. Tsvetkov et al. (2013) pursue this work further by constructing and publishing a dataset of 985 literal and 985 methaphorical adjective-noun pairs [Cite_Footnote_10] and classify them. Gersh-man et al. (2014) further expand on this work using 64-dimensional vector-space word representations constructed by Faruqui and Dyer (2014) for clas-sification. They report a state-of-the-art F-score of 85% with random decision forests, including also abstractness and imageability features (Wil-son, 1988) and supersenses from WordNet, aver-aged across senses.",Material,Knowledge,True,Introduce（引用目的）,True,P16-1191_7_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2411,12419," https://github.com/UKPLab/acl2016-supersense-embeddings"," ['8 Conclusions and Future Work']",The outcomes of this work are available to the research community. [Cite_Footnote_11] .,11 https://github.com/UKPLab/ acl2016-supersense-embeddings,"We have presented a novel joint embedding set of words and supersenses, which provides a new insight into the word and supersense positions in the vector space. We demonstrated the utility of these embeddings for predicting supersenses and manifested that the supersense enrichment can lead to a significant improvement in a range of down-stream classification tasks, using our embeddings in a neural network model. The outcomes of this work are available to the research community. [Cite_Footnote_11] . In follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as SemCor (Miller et al., 1994) and STREUSLE (Schneider and Smith, 2015) to examine the impact of the corpus choice in detail and extend the train-ing data beyond WordNet vocabulary. Moreover, the coarse semantic categorization contained in su-persenses was shown to be preserved in translation (Schneider et al., 2013), making them a perfect can-didate for a multilingual adaptation of the vector space, e.g. extending Faruqui and Dyer (2014).",Material,Knowledge,False,Produce（引用目的）,False,P16-1191_8_0,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization",Footnote
2412,12420," http://phontron.com/lader"," ['6 Experiments']","We test three types of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3-step), and the proposed model with latent derivations (lader). [Cite_Footnote_7]",7 Available open-source: http://phontron.com/lader,"For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3-step), and the proposed model with latent derivations (lader). [Cite_Footnote_7] Except when stated oth-erwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10 −3 (chosen through cross-validation).",Method,Code,True,Introduce（引用目的）,False,D12-1077_0_0,2012,Inducing a Discriminative Parser to Optimize Machine Translation Reordering,Footnote
2413,12421," http://www.phontron.com/kftt"," ['6 Experiments']","We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [Cite_Ref] .",Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.,"We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [Cite_Ref] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.",補足資料,Website,True,Introduce（引用目的）,True,D12-1077_1_0,2012,Inducing a Discriminative Parser to Optimize Machine Translation Reordering,Reference
2414,12422," http://www.phontron.com/kftt"," ['6 Experiments']","For Japanese, we use the KyTea tagger (Neu-big et al., 2011) [Cite_Ref] for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.",Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.,"As default features for lader and the mono-lingual parsing and reordering models in 3-step, we use all the features described in Section 5.2 except φ pos and φ cfg . In addition, we test sys-tems with φ pos and φ cfg added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neu-big et al., 2011) [Cite_Ref] for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.",補足資料,Paper,True,Introduce（引用目的）,True,D12-1077_1_1,2012,Inducing a Discriminative Parser to Optimize Machine Translation Reordering,Reference
2415,12423," http://www.casmacat.eu/"," ['3 Properties of Core Algorithm', '3.1 Experimental Setup']",Such data has been made available by the CAS - MACAT project [Cite_Footnote_1] .,1 http://www.casmacat.eu/,"Such data has been made available by the CAS - MACAT project [Cite_Footnote_1] . In the project’s first field trial , professional translators corrected machine transla-tions of news stories from a competitive English– Spanish machine translation system (Koehn and Haddow, 2012). This test set consists of 24,444 word predictions and 141,662 letter predictions.",補足資料,Website,True,Introduce（引用目的）,True,P14-2094_0_0,2014,Refinements to Interactive Translation Prediction Based on Search Graphs,Footnote
2416,12424," http://www.casmacat.eu/uploads/Deliverables/d6.1.pdf"," ['3 Properties of Core Algorithm', '3.1 Experimental Setup']","In the project’s first field trial [Cite_Footnote_2] , professional translators corrected machine transla-tions of news stories from a competitive English– Spanish machine translation system (Koehn and Haddow, 2012).",2 http://www.casmacat.eu/uploads/Deliverables/d6.1.pdf,"Such data has been made available by the CAS - MACAT project . In the project’s first field trial [Cite_Footnote_2] , professional translators corrected machine transla-tions of news stories from a competitive English– Spanish machine translation system (Koehn and Haddow, 2012). This test set consists of 24,444 word predictions and 141,662 letter predictions.",補足資料,Website,False,Introduce（引用目的）,False,P14-2094_1_0,2014,Refinements to Interactive Translation Prediction Based on Search Graphs,Footnote
2417,12425," http://www.d.umn.edu/~tpederse/data.html"," ['5 Word Translation Disambiguation', '6.1 Experiment 1: WSD Benchmark Data']","We first applied BB, MB-B, and MB-D to translation of the English words ‘line’ and ‘interest’ using a benchmark data [Cite_Footnote_2] .",2 http://www.d.umn.edu/~tpederse/data.html.,"We first applied BB, MB-B, and MB-D to translation of the English words ‘line’ and ‘interest’ using a benchmark data [Cite_Footnote_2] . The data mainly consists of articles in the Wall Street Journal and it is designed for conducting Word",Material,Dataset,True,Use（引用目的）,True,P02-1044_0_0,2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 343-351. Word Translation Disambiguation Using Bilingual Bootstrapping",Footnote
2418,12426," http://encarta.msn.com/default.asp"," ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky’s Words']","For each of the words, we extracted about 200 sentences containing the word from the Encarta [Cite_Footnote_4] English corpus and labeled those sentences with Chinese translations ourselves.",4 http://encarta.msn.com/default.asp,"For each of the words, we extracted about 200 sentences containing the word from the Encarta [Cite_Footnote_4] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great Encyclopedia Chinese corpus as unclassified data in Chinese. We defined, for each translation, a seed word in English as a classified example (cf., Table 5).",Material,DataSource,True,Use（引用目的）,True,P02-1044_1_0,2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 343-351. Word Translation Disambiguation Using Bilingual Bootstrapping",Footnote
2419,12427," http://www.whlib.ac.cn/sjk/bkqs.htm"," ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky’s Words']",We also used the sentences in the Great Encyclopedia [Cite_Footnote_5] Chinese corpus as unclassified data in Chinese.,5 http://www.whlib.ac.cn/sjk/bkqs.htm,"For each of the words, we extracted about 200 sentences containing the word from the Encarta English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great Encyclopedia [Cite_Footnote_5] Chinese corpus as unclassified data in Chinese. We defined, for each translation, a seed word in English as a classified example (cf., Table 5).",Material,DataSource,True,Use（引用目的）,True,P02-1044_2_0,2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 343-351. Word Translation Disambiguation Using Bilingual Bootstrapping",Footnote
2420,12428," http://www.everytrail.com/best/"," ['1 Introduction']","trails near Baltimore) and a web page (e.g., [Cite] http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.).",,"Figure 1: Entity extraction typically requires ad-ditional knowledge such as a small set of seed ex-amples or depends on multiple web pages. In our setting, we take as input a natural language query and extract entities from a single web page. trails near Baltimore) and a web page (e.g., [Cite] http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.). Figure 1 summarizes the task setup.",補足資料,Website,True,Use（引用目的）,True,P14-1037_0_0,2014,Zero-shot Entity Extraction from Web Pages,Body
2421,12429," http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014"," ['2 Problem statement', '2.1 Dataset']","To experiment with a diverse set of queries and web pages, we created a new dataset, O PEN W EB , using web pages from Google search results. [Cite_Footnote_1]",1 The O PEN W EB dataset and our code base are available for download at http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014.,"To experiment with a diverse set of queries and web pages, we created a new dataset, O PEN W EB , using web pages from Google search results. [Cite_Footnote_1] We use the method from Berant et al. (2013) to gen-erate search queries by performing a breadth-first search over the query space. Specifically, we use the Google Suggest API, which takes a par-tial query (e.g., “list of movies”) and out-puts several complete queries (e.g., “list of hor-ror movies”). We start with seed partial queries “list of • ” where • is one or two initial let-ters. In each step, we call the Google Suggest API on the partial queries to obtain complete queries, and then apply the transformation rules in Table 1 to generate more partial queries from complete queries. We run the procedure until we obtained 100K queries.",Mixed,Mixed,True,Produce（引用目的）,True,P14-1037_1_0,2014,Zero-shot Entity Extraction from Web Pages,Footnote
2422,12430," https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf"," ['4 Experiments', '4.1 Multimodal Translation']","The decoder is an RNN with 500 conditional GRU units (Firat and Cho, 2016) [Cite_Ref] in the recurrent layer.","Orhan Firat and Kyunghyun Cho. 2016. Con-ditional gated recurrent unit with attention mechanism. https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf. Published online, version adbaeea.","In our model, the visual input is processed with a pre-trained VGG 16 network (Simonyan and Zis-serman, 2014) without further fine-tuning. Atten-tion distribution over the visual input is computed from the last convolutional layer of the network. The decoder is an RNN with 500 conditional GRU units (Firat and Cho, 2016) [Cite_Ref] in the recurrent layer. We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.",補足資料,Paper,True,Introduce（引用目的）,True,P17-2031_8_0,2017,Attention Strategies for Multi-Source Sequence-to-Sequence Learning,Reference
2423,12431," http://hdl.handle.net/11372/LRT-1632"," ['4 Experiments', '4.2 Automatic MT Post-editing']","We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016 [Cite_Ref] ), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain.","Marco Turchi, Rajen Chatterjee, and Matteo Negri. 2016. WMT16 APE shared task data. LIN-DAT/CLARIN digital library at the Institute of For-mal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.","We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016 [Cite_Ref] ), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference. In case of this dataset, the MT outputs are almost perfect in and only little effort was required to post-edit the sen-tences. The results are evaluated using the human-targeted error rate (HTER) (Snover et al., 2006) and BLEU score (Papineni et al., 2002). of Multi30k dataset and the APE dataset. The col-umn ‘share’ denotes whether the projection matrix is shared for energies and context vector computa-tion, ‘sent.’ indicates whether the sentinel vector has been used or not.",補足資料,Website,True,Introduce（引用目的）,True,P17-2031_24_0,2017,Attention Strategies for Multi-Source Sequence-to-Sequence Learning,Reference
2424,12432," https://github.com/yfeng21/notaprediction"," ['References']",This paper discusses the importance of uncov-ering uncertainty in end-to-end dialog tasks and presents our experimental results on uncer-tainty classification on the processed Ubuntu Dialog Corpus [Cite_Footnote_1] .,1 Our datasets for the NOTA task are released at https://github.com/yfeng21/nota prediction,"This paper discusses the importance of uncov-ering uncertainty in end-to-end dialog tasks and presents our experimental results on uncer-tainty classification on the processed Ubuntu Dialog Corpus [Cite_Footnote_1] . We show that instead of re-training models for this specific purpose, we can capture the original retrieval model’s un-derlying confidence concerning the best pre-diction using trivial additional computation.",Material,Dataset,True,Produce（引用目的）,True,2020.acl-main.182_0_0,2020,“None of the Above”: Measure Uncertainty in Dialog Response Retrieval,Footnote
2425,12433," http://tedlab.mit.edu/∼dr/Tgrep2/"," ['2 Related Work']",TGrep2 [Cite_Footnote_1] is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts.,1 http://tedlab.mit.edu/∼dr/Tgrep2/,"There are several specialized tools for indexing and querying treebanks. (See Bird et al. (2005) for an overview and critical comparisons.) TGrep2 [Cite_Footnote_1] is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts. It allows Boolean expressions over nodes and regular expressions in-side nodes. Matching uses a binary index and is performed recursively starting at the top node in the query. TIGERSearch is associated with the German syntactic corpus TIGER. The tool is more typed than TGrep2 and allows search over discontinuous con-stituents that are common in German. TIGERSearch stores the corpus in a Prolog-like logical form and searches using unification matching. LPath is an extension of XPath with three features: immedi-ate precedence, subtree scoping and edge alignment. The queries are executed in an SQL database (Lai and Bird, 2004). Other tree query languages include CorpusSearch, Gsearch, Linguist’s Search Engine, Netgraph, TIQL, VIQTORYA etc.",Method,Tool,True,Introduce（引用目的）,True,P05-3017_0_0,2005,Supporting Annotation Layers for Natural Language Processing,Footnote
2426,12434," http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/"," ['2 Related Work']",TIGERSearch [Cite_Footnote_2] is associated with the German syntactic corpus TIGER.,2 http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/,"There are several specialized tools for indexing and querying treebanks. (See Bird et al. (2005) for an overview and critical comparisons.) TGrep2 is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts. It allows Boolean expressions over nodes and regular expressions in-side nodes. Matching uses a binary index and is performed recursively starting at the top node in the query. TIGERSearch [Cite_Footnote_2] is associated with the German syntactic corpus TIGER. The tool is more typed than TGrep2 and allows search over discontinuous con-stituents that are common in German. TIGERSearch stores the corpus in a Prolog-like logical form and searches using unification matching. LPath is an extension of XPath with three features: immedi-ate precedence, subtree scoping and edge alignment. The queries are executed in an SQL database (Lai and Bird, 2004). Other tree query languages include CorpusSearch, Gsearch, Linguist’s Search Engine, Netgraph, TIQL, VIQTORYA etc.",Method,Tool,True,Introduce（引用目的）,True,P05-3017_1_0,2005,Supporting Annotation Layers for Natural Language Processing,Footnote
2427,12435," http://agtk.sourceforge.net/"," ['2 Related Work']","Bird and Liberman (2001) introduce an abstract general annotation approach, based on annotation graphs. [Cite_Footnote_3]",3 http://agtk.sourceforge.net/,"Bird and Liberman (2001) introduce an abstract general annotation approach, based on annotation graphs. [Cite_Footnote_3] The model is best suited for speech data, where time constraints are limited within an inter-val, but it is unnecessarily complex for supporting annotations on written text.",Method,Code,False,Introduce（引用目的）,True,P05-3017_2_0,2005,Supporting Annotation Layers for Natural Language Processing,Footnote
2428,12436," http://www.nlm.nih.gov/pubs/factsheets/medline.html"," ['3 The Layered Query Language']",• Scalability to large collections such as MED-LINE (containing millions of documents). [Cite_Footnote_4],4 http://www.nlm.nih.gov/pubs/factsheets/medline.html,• Scalability to large collections such as MED-LINE (containing millions of documents). [Cite_Footnote_4],Material,Dataset,True,Introduce（引用目的）,True,P05-3017_3_0,2005,Supporting Annotation Layers for Natural Language Processing,Footnote
2429,12437," http://biotext.berkeley.edu/lql/"," ['3 The Layered Query Language']",Below we illustrate our Layered Query Language (LQL) using examples from bio-science NLP. [Cite_Footnote_5],5 See http://biotext.berkeley.edu/lql/ for a formal description of the language and additional examples.,"We assume that the underlying text is fairly static. While we support addition, removal and editing of annotations via a Java API, we do not optimize for efficient editing, but instead focus on compact rep-resentation, easy query formulation, easy addition and removal of layers, and straightforward trans-lation into SQL. Below we illustrate our Layered Query Language (LQL) using examples from bio-science NLP. [Cite_Footnote_5]",補足資料,Document,True,Introduce（引用目的）,True,P05-3017_4_0,2005,Supporting Annotation Layers for Natural Language Processing,Footnote
2430,12438," http://www.ncbi.nlm.nih.gov/LocusLink"," ['3 The Layered Query Language']",The gene/protein layer assigns IDs from the LocusLink database of gene names. [Cite_Footnote_7],7 http://www.ncbi.nlm.nih.gov/LocusLink,"Word, POS and shallow parse layers are sequen-tial (the latter can skip or span multiple words). The gene/protein layer assigns IDs from the LocusLink database of gene names. [Cite_Footnote_7] For a given gene there are as many LocusLink IDs as the number of organisms it is found in (e.g., 4 in the case of the gene Bcl-2).",Material,Knowledge,True,Introduce（引用目的）,False,P05-3017_5_0,2005,Supporting Annotation Layers for Natural Language Processing,Footnote
2431,12439," http://www.nlm.nih.gov/mesh/meshhome.html"," ['3 The Layered Query Language']",The MeSH layer contains entities from the hier-archical medical ontology MeSH (Medical Subject Headings). [Cite_Footnote_8],8 http://www.nlm.nih.gov/mesh/meshhome.html,"The MeSH layer contains entities from the hier-archical medical ontology MeSH (Medical Subject Headings). [Cite_Footnote_8] The MeSH annotations on Figure 1 are overlapping (share the word cell) and hierarchical both ways: spanning, since blood cell (with MeSH id D001773) orthographically spans the word cell (id A11), and ontologically, since blood cell is a kind of cell and cell death (id D016923) is a kind of Bio-logical Phenomena.",Material,Knowledge,True,Extend（引用目的）,False,P05-3017_6_0,2005,Supporting Annotation Layers for Natural Language Processing,Footnote
2432,12440," https://github.com/Leechikara/Dialogue-Based-Anti-Fraud"," ['References']","Furthermore, our learned dialogue strategies are interpretable and flexi-ble, which can help promote real-world appli-cations. [Cite_Footnote_1]",1 https://github.com/Leechikara/ Dialogue-Based-Anti-Fraud,"Identity fraud detection is of great importance in many real-world scenarios such as the finan-cial industry. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in loan applications and propose to solve this problem with a novel interactive dialogue system which consists of two modules. One is the knowledge graph (KG) constructor organizing the personal infor-mation for each loan applicant. The other is structured dialogue management that can dy-namically generate a series of questions based on the personal KG to ask the applicants and determine their identity states. We also present a heuristic user simulator based on problem analysis to evaluate our method. Experiments have shown that the trainable dialogue system can effectively detect fraudsters, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexi-ble, which can help promote real-world appli-cations. [Cite_Footnote_1]",Method,Code,True,Introduce（引用目的）,True,D19-1185_0_0,2019,Are You for Real? Detecting Identity Fraud via Dialogue Interactions,Footnote
2433,12441," https://www.ownthink.com"," ['2 Knowledge Graph Constructor']","To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG [Cite_Footnote_3] and crawl triplets that are directly related to them.",3 https://www.ownthink.com,"There are four types of personal information in a Chinese loan application form: “School”, “Com-pany”, “Residence” and “BirthPlace”. To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG [Cite_Footnote_3] and crawl triplets that are directly related to them. How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked. Thus we use wealthy geographic information about or-ganizations and locations in electronic maps (e.g., Amap ) to complete the KG.",Material,Knowledge,False,Use（引用目的）,False,D19-1185_1_0,2019,Are You for Real? Detecting Identity Fraud via Dialogue Interactions,Footnote
2434,12442," https://www.amap.com"," ['2 Knowledge Graph Constructor']","Thus we use wealthy geographic information about or-ganizations and locations in electronic maps (e.g., Amap [Cite_Footnote_5] ) to complete the KG.",5 https://www.amap.com,"There are four types of personal information in a Chinese loan application form: “School”, “Com-pany”, “Residence” and “BirthPlace”. To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG and crawl triplets that are directly related to them. How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked. Thus we use wealthy geographic information about or-ganizations and locations in electronic maps (e.g., Amap [Cite_Footnote_5] ) to complete the KG.",補足資料,Media,True,Introduce（引用目的）,True,D19-1185_2_0,2019,Are You for Real? Detecting Identity Fraud via Dialogue Interactions,Footnote
2435,12443," http://www.qatarliving.com/forum"," ['2 The Task']","The corpus contains data from the Qatar Living forum, [Cite_Footnote_1] and is publicly available on the task’s website.",1 http://www.qatarliving.com/forum,"We use the CQA-QL corpus from Subtask A of SemEval-2015 Task 3 on Answer Selection in CQA. The corpus contains data from the Qatar Living forum, [Cite_Footnote_1] and is publicly available on the task’s website. The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads. Each question, and each answer, consist of a short title and a more de-tailed description. There is also meta informa-tion associated with both, e.g., ID of the user ask-ing/answering the question, timestamp, category. The task asks participants to determine for each answer in the thread whether it is Good, Bad, or Potentially useful for the given question.",補足資料,Website,True,Introduce（引用目的）,True,D15-1068_0_0,2015,Global Thread-Level Inference for Comment Classification in Community Question Answering,Footnote
2436,12444," http://alt.qcri.org/semeval2015/task3/"," ['2 The Task']","The corpus contains data from the Qatar Living forum, and is publicly available on the task’s website. [Cite_Footnote_2]",2 http://alt.qcri.org/semeval2015/task3/,"We use the CQA-QL corpus from Subtask A of SemEval-2015 Task 3 on Answer Selection in CQA. The corpus contains data from the Qatar Living forum, and is publicly available on the task’s website. [Cite_Footnote_2] The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads. Each question, and each answer, consist of a short title and a more de-tailed description. There is also meta informa-tion associated with both, e.g., ID of the user ask-ing/answering the question, timestamp, category. The task asks participants to determine for each answer in the thread whether it is Good, Bad, or Potentially useful for the given question.",補足資料,Website,True,Introduce（引用目的）,True,D15-1068_1_0,2015,Global Thread-Level Inference for Comment Classification in Community Question Answering,Footnote
2437,12445," http://www.qatarliving.com/moving-qatar/posts/can-i-obtain-driving-license-my-qid-written-employee"," ['2 The Task']","A simplified example is shown in Figure 1, [Cite_Footnote_3] where answers 2 and 4 are good, answer 1 is po-tentially useful, and answer 3 is bad.",3 http://www.qatarliving.com/moving-qatar/posts/can-i- obtain-driving-license-my-qid-written-employee,"A simplified example is shown in Figure 1, [Cite_Footnote_3] where answers 2 and 4 are good, answer 1 is po-tentially useful, and answer 3 is bad. In this paper, we focus on a 2-class variant of the above Sub-task A, which is closer to a real CQA application. We merge Potential and Bad labels into Bad and we focus on the 2-class problem: Good-vs-Bad. Table 1 shows some statistics about the resulting dataset used for development, training and testing.",補足資料,Document,True,Introduce（引用目的）,True,D15-1068_2_0,2015,Global Thread-Level Inference for Comment Classification in Community Question Answering,Footnote
2438,12446," https://github.com/YerevaNN/WARP"," ['1 Introduction']",We call our method WARP: Word-level Adversarial RePrograming [Cite_Footnote_1] .,1 Our implementation is publicly available at: https: //github.com/YerevaNN/WARP,"In this paper, we introduce a novel technique to find optimal prompts. We call our method WARP: Word-level Adversarial RePrograming [Cite_Footnote_1] . The method is inspired by adversarial reprogramming (Elsayed et al., 2019) — a method of adding ad-versarial perturbations to an input image that re-programs a pretrained neural network to perform classification on a task other than the one it was originally trained for.",Method,Tool,True,Produce（引用目的）,True,2021.acl-long.381_0_0,2021,WARP: Word-level Adversarial ReProgramming,Footnote
2439,12447," https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs"," ['4 Experiments on GLUE', '4.1 Tasks']",Linear Classifier 64.2 78.1 74.9 59.2 88.4 WARP 0 70.9 78.8 77.1 72.2 89.8 WARP 1 83.9 87.6 81.6 72.6 93.8 WARP 2 85.4 88.0 81.5 69.7 94.3 WARP [Cite_Footnote_4] 86.9 92.4 83.1 68.2 95.9 WARP 8 87.6 93.0 83.8 72.9 95.4 WARP init 86.8 90.4 83.6 80.1 96.0 WARP 20 88.2 93.5 84.5,4 https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs,"Table 1: Test set results on GLUE Benchmark. The results are obtained from the GLUE Evaluation server. The subscript next to TinyBERT corresponds to the number of layers in the model. WARP for RTE, STS-B and MRPC are intialized from the MNLI parameters. Results for WNLI are not shown, although they are counted in the averaged GLUE score (AVG column). The last column # shows the number of trainable parameters. WARP’s average performance is higher than all models with up to three orders of magnitude more trainable parameters. Fully fine-tuned RoBERTa and the current state-of-the-art method (DeBERT) score higher by 6.5 and 9.2 points, respectively. Linear Classifier 64.2 78.1 74.9 59.2 88.4 WARP 0 70.9 78.8 77.1 72.2 89.8 WARP 1 83.9 87.6 81.6 72.6 93.8 WARP 2 85.4 88.0 81.5 69.7 94.3 WARP [Cite_Footnote_4] 86.9 92.4 83.1 68.2 95.9 WARP 8 87.6 93.0 83.8 72.9 95.4 WARP init 86.8 90.4 83.6 80.1 96.0 WARP 20 88.2 93.5 84.5 75.8 96.0 WARP MNLI 86.3",Material,Knowledge,False,Introduce（引用目的）,False,2021.acl-long.381_1_0,2021,WARP: Word-level Adversarial ReProgramming,Footnote
2440,12448," https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html"," ['References']",We enable mixed preci-sion and pad all sequence lengths to the multiples of 8 for the effective usage of TensorCores [Cite_Footnote_8] .,"8 https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.htmlels. [MASK] means the prompts are intialized with the word embedding of same token, and MNLI means the prompt is initialized with the prompts of out best MNLI run.","We disable all dropouts inside Transformer. We use huggingface implementation of AdamW op-timizer with weight decay disabled. The gradi-ent is normalized to the value 1.0. For the batch sampling we use bucketing with padding noise of 0.1. In order to use the device memory more ef-fectively, we also set maximum number of tokens per batch to 2048. The maximum sequence length is truncated to 512 tokens. We enable mixed preci-sion and pad all sequence lengths to the multiples of 8 for the effective usage of TensorCores [Cite_Footnote_8] .",Method,Tool,False,Introduce（引用目的）,False,2021.acl-long.381_2_0,2021,WARP: Word-level Adversarial ReProgramming,Footnote
2441,12449," https://linear-transformers.com"," ['4 Experimental Setup', '4.3 Implementation']",We re-implement the Transformer and use the orig-inal implementation of the LT. [Cite_Footnote_1],1 https://linear-transformers.com,"We re-implement the Transformer and use the orig-inal implementation of the LT. [Cite_Footnote_1] All models are trained to minimise cross-entropy with the AdamW optimiser (Loshchilov and Hutter, 2019). We use 300-D GloVe embeddings (Pennington et al., 2014) which are passed through a linear projection layer with size d model . All experiments were performed on a GPU GeForce GTX 1080 Ti. Details on the im-plementation, hyperparameters and reproducibility are available in the Appendix. Our implementation is publicly available.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.90_0_0,2021,Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU,Footnote
2442,12450," https://github.com/pkhdipraja/towards-incremental-transformers"," ['4 Experimental Setup', '4.3 Implementation']",Our implementation is publicly available. [Cite_Footnote_2],2 https://github.com/pkhdipraja/ towards-incremental-transformers,"We re-implement the Transformer and use the orig-inal implementation of the LT. All models are trained to minimise cross-entropy with the AdamW optimiser (Loshchilov and Hutter, 2019). We use 300-D GloVe embeddings (Pennington et al., 2014) which are passed through a linear projection layer with size d model . All experiments were performed on a GPU GeForce GTX 1080 Ti. Details on the im-plementation, hyperparameters and reproducibility are available in the Appendix. Our implementation is publicly available. [Cite_Footnote_2]",Method,Tool,True,Produce（引用目的）,True,2021.emnlp-main.90_1_0,2021,Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU,Footnote
2443,12451,https://www.comet.ml/docs/python-sdk/introduction-optimizer/,[\'Training details\'],"We perform hyperparameter search using Comet’s Bayesian search algorithm [Cite_Footnote_4] , maximising F1 score for sequence tagging and accuracy for sequence classification on the validation set.",4 https://www.comet.ml/docs/python-sdk/introduction-optimizer/,"We also apply label smoothing (Szegedy et al., 2016) with = 0.1 for sequence classification to make the model more robust for incremental pro-cessing. For OOV words, we randomly replace tokens by ""UNK"" token with p = 0.02 during training and use it for testing (Žilka and Jurčíček, 2015). We perform hyperparameter search using Comet’s Bayesian search algorithm [Cite_Footnote_4] , maximising F1 score for sequence tagging and accuracy for sequence classification on the validation set. The hyperparameter search trials are limited to 20 for all of our experiments. The hyperparameters for LT were also used for LT+R. We use similar hyper-parameters for LT+R+CM and LT+R+CM+D. We set the seed to 42119392 for all of our experiments.",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.90_2_0,2021,Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU,Footnote
2444,12452," https://github.com/asmadotgh/neural_chat_web"," ['4 Learning from talking to humans', '4.2 Hosting real-time conversations online']","The trained models were deployed to inter-act live with human users via a web server that hosts neural network dialog models on GPU for fast, real-time inference: [Cite] https: //github.com/asmadotgh/neural_chat_web .",,"The trained models were deployed to inter-act live with human users via a web server that hosts neural network dialog models on GPU for fast, real-time inference: [Cite] https: //github.com/asmadotgh/neural_chat_web .",補足資料,Website,True,Use（引用目的）,False,2020.emnlp-main.327_0_0,2020,Human-centric dialog training via offline reinforcement learning,Body
2445,12453," https://github.com/natashamjaques/neural_chat/tree/master/BatchRL"," ['4 Learning from talking to humans', '4.2 Hosting real-time conversations online']",The code for the RL models is available in open-source at [Cite] https://github.com/natashamjaques/neural_chat/tree/master/BatchRL.,,"Figure 2 shows a screenshot of the interface, which includes buttons that allow users to give manual feedback on responses they particularly liked or disliked. Users were encouraged to use these buttons, and we sum these manual votes to create an overall votes score. After chatting, users were asked to provide a Likert scale rating of the bot’s conversation quality, fluency, diversity, contingency/relatedness, and empathy. The code for the RL models is available in open-source at [Cite] https://github.com/natashamjaques/neural_chat/tree/master/BatchRL. Using the server, we collected a batch of human interaction data containing 46,061 pairs of user input and agent response. Because humans may use inappropriate language with bots online (see (Horton, 2016)), we filtered this data to remove 1 character responses, profanities, and invalid inputs for a remaining total of 45,179 response pairs. This filtering step is important to ensure undesirable human behavior is not learned by the RL algorithms. The offline data was used to train the RL models as described in Section 3.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.327_1_0,2020,Human-centric dialog training via offline reinforcement learning,Body
2446,12454," https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"," ['E Implicit Rewards Details', 'E.7 Toxicity']",Saleh et al. (2019) incor-porate a Toxicity Classifier trained with data from the Toxic Comment Classification Challenge [Cite_Footnote_3] as a reward in the training hierarchical RL dialog mod-els.,3 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge,"We also want to discourage our bot from malicious or offensive language. Saleh et al. (2019) incor-porate a Toxicity Classifier trained with data from the Toxic Comment Classification Challenge [Cite_Footnote_3] as a reward in the training hierarchical RL dialog mod-els. We compute Toxicity reward scores using this classifier as Bot Toxicity (e.g. lower toxicity score, higher Bot toxicity reward).",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.327_2_0,2020,Human-centric dialog training via offline reinforcement learning,Footnote
2447,12455," https://github.com/asmadotgh/neural_chat_web"," ['F Interactive bot platform details', 'F.1 Website server setup and configuration']","For further information and instructions on server configuration, please read the server doc-umentation available at [Cite] https://github.com/asmadotgh/neural_chat_web.",,"For further information and instructions on server configuration, please read the server doc-umentation available at [Cite] https://github.com/asmadotgh/neural_chat_web. We hope that this platform will allow others to host their own bots and evaluate them in an interactive setting.",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.327_3_0,2020,Human-centric dialog training via offline reinforcement learning,Body
2448,12456," http://ai.tencent.com/ailab/Encoding_Conversation_Context_"," ['References']",Experimental results on Twitter and Weibo datasets [Cite_Footnote_1] show that our framework with such encoders outper-forms state-of-the-art approaches.,1 Our datasets are released at: http://ai.tencent.com/ailab/Encoding_Conversation_Context_ for_Neural_Keyphrase_Extraction_from_ Microblog_Posts.html,"Existing keyphrase extraction methods suffer from data sparsity problem when they are con-ducted on short and informal texts, especially microblog messages. Enriching context is one way to alleviate this problem. Considering that conversations are formed by reposting and re-plying messages, they provide useful clues for recognizing essential content in target posts and are therefore helpful for keyphrase iden-tification. In this paper, we present a neural keyphrase extraction framework for microblog posts that takes their conversation context into account, where four types of neural encoders, namely, averaged embedding, RNN, attention, and memory networks, are proposed to repre-sent the conversation context. Experimental results on Twitter and Weibo datasets [Cite_Footnote_1] show that our framework with such encoders outper-forms state-of-the-art approaches.",Material,Dataset,True,Produce（引用目的）,True,N18-1151_0_0,2018,Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts,Footnote
2449,12457," http://trec.nist.gov/data/tweets/"," ['3 Experiment Setup', '3.1 Datasets']",The Twitter dataset is constructed based on TREC2011 microblog track [Cite_Footnote_4] .,4 http://trec.nist.gov/data/tweets/,"Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track [Cite_Footnote_4] . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its “in reply to status id” included. Recursively, we searched the “in re-ply to” tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.",Material,Dataset,True,Produce（引用目的）,True,N18-1151_1_0,2018,Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts,Footnote
2450,12458," http://developer.twitter.com/en/docs/tweets/search/api-reference/get-saved_searches-show-id"," ['3 Experiment Setup', '3.1 Datasets']","To recover conver-sations, we used Tweet Search API [Cite_Footnote_5] to retrieve full information of a tweet with its “in reply to status id” included.",5 http://developer.twitter.com/en/docs/tweets/search/api-reference/get-saved_ searches-show-id,"Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track . To recover conver-sations, we used Tweet Search API [Cite_Footnote_5] to retrieve full information of a tweet with its “in reply to status id” included. Recursively, we searched the “in re-ply to” tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.",Method,Tool,True,Use（引用目的）,True,N18-1151_2_0,2018,Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts,Footnote
2451,12459," http://open.weibo.com/wiki/Trends/hourly"," ['3 Experiment Setup', '3.1 Datasets']","To build the Weibo dataset, we tracked real-time trending hashtags [Cite_Footnote_6] on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries.",6 http://open.weibo.com/wiki/Trends/ hourly,"Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its “in reply to status id” included. Recursively, we searched the “in re-ply to” tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags [Cite_Footnote_6] on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.",Material,Knowledge,False,Use（引用目的）,True,N18-1151_3_0,2018,Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts,Footnote
2452,12460," http://www.open.weibo.com/wiki/2/search/topics"," ['3 Experiment Setup', '3.1 Datasets']","To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API [Cite_Footnote_7] to crawl the posts matching the given hash-tag queries.",7 http://www.open.weibo.com/wiki/2/search/topics,"Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its “in reply to status id” included. Recursively, we searched the “in re-ply to” tweet till the entire conversation is recov-ered. Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API [Cite_Footnote_7] to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.",Method,Tool,True,Use（引用目的）,True,N18-1151_4_0,2018,Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts,Footnote
2453,12461," http://www.cs.cmu.edu/~ark/TweetNLP/"," ['3 Experiment Setup', '3.1 Datasets']","We preprocessed Twitter dataset with Twitter NLP tool [Cite_Footnote_9] (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization.",9 http://www.cs.cmu.edu/˜ark/TweetNLP/,"We preprocessed Twitter dataset with Twitter NLP tool [Cite_Footnote_9] (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization. For Weibo dataset, we used NLPIR tool 10 (Zhang et al., 2003) for Chi-nese word segmentation. In particular, Weibo con-versations have an relatively wide range (from 3 to 8,846 words), e.g., one conversation could contain up to 447 messages. If use the maximum length of all conversations as the input length for en-coders, padding the inputs will lead to a sparse ma-trix. Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of messages and then encode the pro-duced summaries. In contrast, we do not sum-marize Twitter conversations because their length range is much narrower (from 4 to 1,035 words).",Method,Tool,True,Use（引用目的）,True,N18-1151_5_0,2018,Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts,Footnote
2454,12462," https://github.com/NLPIR-team/NLPIR"," ['3 Experiment Setup', '3.1 Datasets']","Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of [Cite_Footnote_10] messages and then encode the pro-duced summaries.",10 https://github.com/NLPIR-team/NLPIR,"We preprocessed Twitter dataset with Twitter NLP tool (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization. For Weibo dataset, we used NLPIR tool 10 (Zhang et al., 2003) for Chi-nese word segmentation. In particular, Weibo con-versations have an relatively wide range (from 3 to 8,846 words), e.g., one conversation could contain up to 447 messages. If use the maximum length of all conversations as the input length for en-coders, padding the inputs will lead to a sparse ma-trix. Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of [Cite_Footnote_10] messages and then encode the pro-duced summaries. In contrast, we do not sum-marize Twitter conversations because their length range is much narrower (from 4 to 1,035 words).",補足資料,Document,False,Introduce（引用目的）,False,N18-1151_6_0,2018,Encoding Conversation Context for Neural Keyphrase Extraction from Microblog Posts,Footnote
2455,12463," http://openie.allenai.org/"," ['2 Framework Description', '2.2 Answer-relevant Relation Extraction']",We utilize an off-the-shelf toolbox of OpenIE [Cite_Footnote_1] to the derive structured answer-relevant relations from sentences as to the point contexts.,1 http://openie.allenai.org/,"We utilize an off-the-shelf toolbox of OpenIE [Cite_Footnote_1] to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in dif-ferent granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure 2 shows n-ary relations extracted from OpenIE. As we can see, OpenIE extracts multiple relations for complex sentences. Here we select the most infor-mative relation according to three criteria in the order of descending importance: (1) having the maximal number of overlapped tokens between the answer and the relation; (2) being assigned the highest confidence score by OpenIE; (3) contain-ing maximum non-stop words. As shown in Fig-ure 2, our criteria can select answer-relevant rela-tions (waved in Figure 2), which is especially use-ful for sentences with extraneous information. In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context.",Method,Tool,True,Use（引用目的）,True,D19-1317_0_0,2019,Improving Question Generation With to the Point Context,Footnote
2456,12464," https://res.qyzhou.me/redistribute.zip"," ['3 Experimental Setting', '3.1 Dataset & Metrics']","We employ two different data splits by fol-lowing (Zhou et al., 2017) [Cite_Footnote_2] and (Du et al., 2017) .",2 https://res.qyzhou.me/redistribute.zip,"We conduct experiments on the SQuAD dataset (Rajpurkar et al., 2016). It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the arti-cles. We employ two different data splits by fol-lowing (Zhou et al., 2017) [Cite_Footnote_2] and (Du et al., 2017) . In (Zhou et al., 2017), the original SQuAD de-velopment set is evenly divided into dev and test sets, while (Du et al., 2017) treats SQuAD devel-opment set as its test set and splits SQuAD training set into a training set (90%) and a development set (10%). We also filter out questions which do not have any overlapped non-stop words with the cor-responding sentences and perform some prepro-cessing steps, such as tokenization and sentence splitting. The data statistics are given in Table 3.",Material,Knowledge,False,Use（引用目的）,True,D19-1317_1_0,2019,Improving Question Generation With to the Point Context,Footnote
2457,12465," https://github.com/xinyadu/nqg"," ['3 Experimental Setting', '3.1 Dataset & Metrics']","We employ two different data splits by fol-lowing (Zhou et al., 2017) and (Du et al., 2017) [Cite_Footnote_3] .",3 https://github.com/xinyadu/nqg,"We conduct experiments on the SQuAD dataset (Rajpurkar et al., 2016). It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the arti-cles. We employ two different data splits by fol-lowing (Zhou et al., 2017) and (Du et al., 2017) [Cite_Footnote_3] . In (Zhou et al., 2017), the original SQuAD de-velopment set is evenly divided into dev and test sets, while (Du et al., 2017) treats SQuAD devel-opment set as its test set and splits SQuAD training set into a training set (90%) and a development set (10%). We also filter out questions which do not have any overlapped non-stop words with the cor-responding sentences and perform some prepro-cessing steps, such as tokenization and sentence splitting. The data statistics are given in Table 3.",補足資料,Document,True,Use（引用目的）,True,D19-1317_2_0,2019,Improving Question Generation With to the Point Context,Footnote
2458,12466," https://github.com/ZurichNLP/understanding-mbr"," ['References']","We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift. [Cite_Footnote_1]",1 Code and documentation available at https://github.com/ZurichNLP/understanding-mbr,"Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Re-cent work has tied these shortcomings to beam search – the de facto standard inference algo-rithm in NMT – and Eikema and Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.22_0_0,2021,Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation,Footnote
2459,12467," https://tatoeba.org"," ['C Comments on the development sets distributed with the Tatoeba challenge']","What is peculiar about the challenge is that the training data is assembled from various sources through OPUS (Tiedemann, 2012), while the development and test data are contributed by users of Tatoeba [Cite_Footnote_3] .",3 https://tatoeba.org,"The Tatoeba Challenge (Tiedemann, 2020) distributes training, development and test data for a large number of language pairs. What is peculiar about the challenge is that the training data is assembled from various sources through OPUS (Tiedemann, 2012), while the development and test data are contributed by users of Tatoeba [Cite_Footnote_3] . This means that the development and test set can be considered out-of-domain material.",補足資料,Website,True,Introduce（引用目的）,True,2021.acl-long.22_1_0,2021,Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation,Footnote
2460,12468," https://github.com/UKPLab/acl2020-confidence-regularization"," ['References']","We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy. [Cite_Footnote_1]",1 The code is available at https://github.com/UKPLab/acl2020-confidence-regularization,"Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brit-tle against test cases outside the training dis-tribution. Recently, several proposed debias-ing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the ex-pense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the result-ing models on broader types of examples be-yond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debias-ing method, called confidence regularization, which discourage models from exploiting bi-ases while enabling them to receive enough incentive to learn from all the training ex-amples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.acl-main.770_0_0,2020,Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance,Footnote
2461,12469," https://github.com/TalSchuster/FeverSymmetric"," ['4 Experimental Setup', '4.2 Fact Verification']",We evaluate the models on the two versions (version 1 and 2) of their test sets. [Cite_Footnote_3],3 https://github.com/TalSchuster/ FeverSymmetric,"Fever-Symmetric Schuster et al. (2019) intro-duce this dataset to demonstrate that FEVER mod-els mostly rely on the claim-only bias, i.e., the occurrence of words and phrases in the claim that are biased toward certain labels. The dataset is manually constructed such that relying on cues of the claim can lead to incorrect predictions. We evaluate the models on the two versions (version 1 and 2) of their test sets. [Cite_Footnote_3]",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.770_1_0,2020,Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance,Footnote
2462,12470," http://people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html"," ['300']","We sampled [Cite_Footnote_1] 100 objects from each of the ten dis-tributions (clusters), and made a dataset of 1,000 ob-jects in total.","1 We used the random sampling code available at http: //people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html (Banerjee et al., 2005).","We sampled [Cite_Footnote_1] 100 objects from each of the ten dis-tributions (clusters), and made a dataset of 1,000 ob-jects in total.",Method,Code,True,Use（引用目的）,True,D13-1058_0_0,2013,Centering Similarity Measures to Reduce Hubs,Footnote
2463,12471," http://www.ofai.at/"," ['7 Experiments']","For comparison, we also tested two recently pro-posed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012) [Cite_Footnote_2] , and the one (Suzuki et al., 2012) based on graph Laplacian kernels.",2 We used the Matlab script downloaded from http://www.ofai.at/ ∼ dominik.schnitzer/mp/.,"For comparison, we also tested two recently pro-posed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012) [Cite_Footnote_2] , and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the co-sine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, we computed both the regularized Laplacian ker-nel (Chebotarev and Shamis, 1997; Smola and Kon-dor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels.",Method,Code,True,Use（引用目的）,True,D13-1058_1_0,2013,Centering Similarity Measures to Reduce Hubs,Footnote
2464,12472," http://archive.ics.uci.edu/ml/"," ['7 Experiments', '7.2 Document classification', '7.2.1 Task and dataset']","Two multiclass document classification datasets were used: Reuters Transcribed and Mini News-groups, distributed at [Cite] http://archive.ics.uci.edu/ml/.",,"Two multiclass document classification datasets were used: Reuters Transcribed and Mini News-groups, distributed at [Cite] http://archive.ics.uci.edu/ml/. The properties of the datasets are summarized in Ta-ble 2.",Material,Dataset,True,Use（引用目的）,True,D13-1058_2_0,2013,Centering Similarity Measures to Reduce Hubs,Body
2465,12473," http://sourceforge.net/projects/sserver/"," ['2 Application Domains']","In the R OBO C UP Coach Competi-tion, teams of agents compete on a simulated soccer field and receive coach advice written in a formal language called CL ANG (Chen et al., 2003) [Cite_Ref] .",M. Chen et al. 2003. Users manual: RoboCup soc-cer server manual for soccer server version 7.07 and later. Available at http://sourceforge.net/projects/sserver/.,"In this paper, we consider two domains. The first do-main is R OBO C UP . R OBO C UP ( www.robocup.org ) is an AI research initiative using robotic soccer as its primary domain. In the R OBO C UP Coach Competi-tion, teams of agents compete on a simulated soccer field and receive coach advice written in a formal language called CL ANG (Chen et al., 2003) [Cite_Ref] . Fig-ure 1 shows a sample MR in CL ANG .",補足資料,Document,True,Introduce（引用目的）,True,N06-1056_0_0,2006,Learning for Semantic Parsing with Statistical Machine Translation,Reference
2466,12474," https://github.com/huggingface/transformers"," ['References']",The li-brary is available at [Cite] https://github.com/huggingface/transformers.,,"Recent progress in natural language process-ing has been driven by advances in both model architecture and model pretraining. Trans-former architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this ca-pacity for a wide variety of tasks. Trans-formers is an open-source library with the goal of opening up these advances to the wider machine learning community. The li-brary consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a cu-rated collection of pretrained models made by and available for the community. Trans-formers is designed to be extensible by re-searchers, simple for practitioners, and fast and robust in industrial deployments. The li-brary is available at [Cite] https://github.com/huggingface/transformers.",Material,Knowledge,True,Introduce（引用目的）,True,2020.emnlp-demos.6_0_0,2020,Transformers: State-of-the-Art Natural Language Processing,Body
2467,12475," https://github.com/huggingface/transformers"," ['1 Introduction']",The library is re-leased under the Apache 2.0 license and is available on GitHub [Cite_Footnote_1] .,1 https://github.com/huggingface/ transformers,Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub [Cite_Footnote_1] . Detailed documentation and tutorials are available on Hugging Face’s website .,Material,Knowledge,True,Produce（引用目的）,True,2020.emnlp-demos.6_1_0,2020,Transformers: State-of-the-Art Natural Language Processing,Footnote
2468,12476," https://huggingface.co/transformers/"," ['1 Introduction']",Detailed documentation and tutorials are available on Hugging Face’s website [Cite_Footnote_2] .,"2 https://huggingface.co/transformers/ in small details, but need to be in sync with pretraining. (Left) Transformer architectures specialized for different tasks, e.g. understanding versus generation, or for specific use-cases, e.g. speed, image+text. (Top) heads allow a Transformer to be used for different tasks. Here we assume the input token sequence is x 1:N from a vocabulary V, and y represents different possible outputs, possibly from a class set C. Example datasets represent a small subset of example code distributed with the library.",Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub . Detailed documentation and tutorials are available on Hugging Face’s website [Cite_Footnote_2] .,補足資料,Document,True,Produce（引用目的）,True,2020.emnlp-demos.6_2_0,2020,Transformers: State-of-the-Art Natural Language Processing,Footnote
2469,12477," https://huggingface.co/transformers/"," ['3 Library Design']",For complete details about the features of the library refer to the documentation available on [Cite] https: //huggingface.co/transformers/ .,,"Transformers is designed to mirror the standard NLP machine learning model pipeline: process data, apply a model, and make predictions. Al-though the library includes tools facilitating train-ing and development, in this technical report we focus on the core modeling specifications. For complete details about the features of the library refer to the documentation available on [Cite] https: //huggingface.co/transformers/ .",補足資料,Document,True,Produce（引用目的）,True,2020.emnlp-demos.6_3_0,2020,Transformers: State-of-the-Art Natural Language Processing,Body
2470,12478," https://github.com/huggingface/"," ['3 Library Design']","This low-level library, available at [Cite] https://github.com/huggingface/ tokenizers , is written in Rust to speed up the tokenization procedure both during training and deployment.",,"For training on very large datasets, Python-based tokenization is often undesirably slow. In the most recent release, Transformers switched its im-plementation to use a highly-optimized tokeniza-tion library by default. This low-level library, available at [Cite] https://github.com/huggingface/ tokenizers , is written in Rust to speed up the tokenization procedure both during training and deployment.",補足資料,Document,True,Produce（引用目的）,True,2020.emnlp-demos.6_4_0,2020,Transformers: State-of-the-Art Natural Language Processing,Body
2471,12479," https://github.com/huggingface/swift-coreml-transformers"," ['5 Deployment']",Code is also made available [Cite_Footnote_3] .,3 https://github.com/huggingface/ swift-coreml-transformers,"Finally, as Transformers become more widely used in all NLP applications, it is increasingly im-portant to deploy to edge devices such as phones or home electronics. Models can use adapters to convert models to CoreML weights that are suit-able to be embedded inside a iOS application, to enable on-the-edge machine learning. Code is also made available [Cite_Footnote_3] . Similar methods can be used for Android devices.",Method,Code,True,Produce（引用目的）,True,2020.emnlp-demos.6_5_0,2020,Transformers: State-of-the-Art Natural Language Processing,Footnote
2472,12480," https://github.com/changzhisun/AntNRE"," ['1 Introduction']","To summarize, the main contributions of this work are [Cite_Footnote_1]",1 Our implementation is available at https://github.com/changzhisun/AntNRE.,"To further utilize the structure of the graph, we also propose assigning different weights on graph edges. In particular, we introduce a binary relation classification task, which is to determine whether the two entities form a valid relation. Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary rela-tion classification, which makes the proposed ad-jacency matrix more explanatory. To summarize, the main contributions of this work are [Cite_Footnote_1]",Method,Tool,True,Produce（引用目的）,True,P19-1131_0_0,2019,Joint Type Inference on Entities and Relations via Graph Convolutional Networks,Footnote
2473,12481," https://github.com/Tom556/OrthogonalTransformerProbing"," ['3 Method']",Our implementation is available at GitHub: [Cite] https://github.com/Tom556/OrthogonalTransformerProbing.,,"Scaling Vector d¯for a specific objective, allowing probing for multiple linguistic tasks simultaneously. In this work, an individual Orthogonal Transfor-mation V is trained for each language, facilitating multi-language probing. This approach assumes that the representations are isomorphic across lan-guages; we examine this claim in our experiments. Our implementation is available at GitHub: [Cite] https://github.com/Tom556/OrthogonalTransformerProbing.",Method,Tool,True,Produce（引用目的）,True,2021.emnlp-main.376_0_0,2021,Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes,Body
2474,12482," http://search.carrot2.org/stable/search"," ['4 Evaluation', '4.3 Comparative Evaluation']","With respect to implementation, we used the Carrot2 APIs [Cite_Footnote_4] which are freely available for STC, LINGO and the classical BIK.",4 http://search.carrot2.org/stable/search [Last access: 15/05/2013].,"With respect to implementation, we used the Carrot2 APIs [Cite_Footnote_4] which are freely available for STC, LINGO and the classical BIK. It is worth notic-ing that all implementations in Carrot2 are tuned to extract exactly 10 clusters. For OPTIMSRC, we reproduced the results presented in the paper of (Carpineto and Romano, 2010) as no imple-mentation is freely available. The results are il-lustrated in Table 2 including both F β -measure and F b 3 . They evidence clear improvements of our methodology when compared to state-of-the-art text-based PRC algorithms, over both datasets and all evaluation metrics. But more important, even when the p-context vector is small (p = 3), the adapted GK-means outperforms all other ex-isting text-based PRC which is particularly impor-tant as they need to perform in real-time.",Method,Code,True,Use（引用目的）,True,P13-2028_0_0,2013,Post-Retrieval Clustering Using Third-Order Similarity Measures,Footnote
2475,12483," https://github.com/airsplay/R2R-EnvDrop"," ['References']","Empirically, we show that our agent is substantially better at generalizabil-ity when fine-tuned with these triplets, outper-forming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard. [Cite_Footnote_1]","1 Our code, data, and models publicly available at: https://github.com/airsplay/R2R-EnvDrop","A grand goal in AI is to build a robot that can accurately navigate based on natural lan-guage instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environ-ment. One key challenge here is to learn to navigate in new environments that are un-seen during training. Most of the existing ap-proaches perform dramatically worse in un-seen environments as compared to seen ones. In this paper, we present a generalizable nav-igational agent. Our agent is trained in two stages. The first stage is training via mixed im-itation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization. The second stage is fine-tuning via newly-introduced ‘unseen’ triplets (envi-ronment, path, instruction). To generate these unseen triplets, we propose a simple but effec-tive ‘environmental dropout’ method to mimic unseen environments, which overcomes the problem of limited seen environment variabil-ity. Next, we apply semi-supervised learn-ing (via back-translation) on these dropped-out environments to generate new paths and instructions. Empirically, we show that our agent is substantially better at generalizabil-ity when fine-tuned with these triplets, outper-forming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,N19-1268_0_0,2019,Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout,Footnote
2476,12484," https://evalai.cloudcv.org/web/challenges/challenge-page/97/overview"," ['1 Introduction']","Overall, our fine-tuned model based on back-translation with environmental dropout substan-tially outperforms the previous state-of-the-art models, and achieves the most recent rank-1 on the Vision and Language Navigation (VLN) R2R challenge leaderboard’s private test data, outper-forming all other entries in success rate under all evaluation setups (single run, beam search, and pre-exploration). [Cite_Footnote_2]",2 https://evalai.cloudcv.org/web/challenges/challenge-page/97/overview,"Overall, our fine-tuned model based on back-translation with environmental dropout substan-tially outperforms the previous state-of-the-art models, and achieves the most recent rank-1 on the Vision and Language Navigation (VLN) R2R challenge leaderboard’s private test data, outper-forming all other entries in success rate under all evaluation setups (single run, beam search, and pre-exploration). [Cite_Footnote_2] We also present detailed abla-tion and analysis studies to explain the effective-ness of our generalization method.",補足資料,Document,True,Introduce（引用目的）,True,N19-1268_1_0,2019,Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout,Footnote
2477,12485," https://github.com/malllabiisc/pra-oda"," ['1 Introduction']",The code along with the results can be obtained at [Cite] https://github.com/malllabiisc/pra-oda.,,"We term this procedure as On-Demand Aug-mentation (ODA), because the search can be per-formed during test time in an on-demand man-ner. In contrast, the previous approaches of adding edges or embeddings to the KB (Gardner et al., 2013), and vector space random walk PRA (Gard-ner et al., 2014) are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; Gardner et al., 2014). Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments sug-gest that ODA provides better performance than (Gardner et al., 2013) and nearly the same pre-diction performance as provided by (Gardner et al., 2014), but in both cases with the added ad-vantage of faster running time and greater flex-ibility due to its online and on-demand nature. The code along with the results can be obtained at [Cite] https://github.com/malllabiisc/pra-oda.",Method,Code,True,Produce（引用目的）,False,D15-1241_0_0,2015,Knowledge Base Inference using Bridging Entities,Body
2478,12486," https://github.com/facebookresearch/mlqa"," ['References']","We present MLQA, a multi-way aligned ex-tractive QA evaluation benchmark intended to spur research in this area. [Cite_Footnote_1]",1 MLQA is publicly available at https://github.com/facebookresearch/mlqa,"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challeng-ing. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned ex-tractive QA evaluation benchmark intended to spur research in this area. [Cite_Footnote_1] MLQA contains QA instances in 7 languages, English, Ara-bic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K in-stances in English and 5K in each other lan-guage, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance.",Method,Tool,False,Introduce（引用目的）,True,2020.acl-main.653_0_0,2020,MLQA: Evaluating Cross-lingual Extractive Question Answering,Footnote
2479,12487," https://github.com/facebookresearch/LASER"," ['2 The MLQA corpus', '2.1 Parallel Sentence Mining']","To detect parallel sentences we use the LASER toolkit, [Cite_Footnote_3] which achieves state-of-the-art perfor-mance in mining parallel sentences (Artetxe and Schwenk, 2019).",3 https://github.com/facebookresearch/ LASER,"To detect parallel sentences we use the LASER toolkit, [Cite_Footnote_3] which achieves state-of-the-art perfor-mance in mining parallel sentences (Artetxe and Schwenk, 2019). LASER uses multilingual sen-tence embeddings and a distance or margin cri-terion in the embeddings space to detect parallel sentences. The reader is referred to Artetxe and Schwenk (2018) and Artetxe and Schwenk (2019) for a detailed description. See Appendix A.6 for further details and statistics on the number of par-allel sentences mined for all language pairs.",Method,Tool,True,Use（引用目的）,True,2020.acl-main.653_1_0,2020,MLQA: Evaluating Cross-lingual Extractive Question Answering,Footnote
2480,12488," http://www.unicode.org/reports/tr44/tr44-4.html#General_Category_Values"," ['4 Cross-lingual QA Experiments', '4.1 Evaluation Metrics for Multilingual QA']","We introduce the following modifica-tions for fairer multilingual evaluation: Instead of stripping ASCII punctuation, we strip all unicode characters with a punctuation General Category. [Cite_Footnote_6]",6 http://www.unicode.org/reports/tr44/tr44-4.html#General_Category_Values,"Most extractive QA tasks use Exact Match (EM) and mean token F1 score as performance metrics. The widely-used SQuAD evaluation also performs the following answer-preprocessing operations: i) lowercasing, ii) stripping (ASCII) punctuation iii) stripping (English) articles and iv) whitespace to-kenisation. We introduce the following modifica-tions for fairer multilingual evaluation: Instead of stripping ASCII punctuation, we strip all unicode characters with a punctuation General Category. [Cite_Footnote_6] When a language has stand-alone articles (English, Spanish, German and Vietnamese) we strip them. We use whitespace tokenization for all MLQA lan-guages other than Chinese, where we use the mixed segmentation method from Cui et al. (2019b).",Material,Knowledge,True,Use（引用目的）,True,2020.acl-main.653_2_0,2020,MLQA: Evaluating Cross-lingual Extractive Question Answering,Footnote
2481,12489," https://en.wikipedia.org/wiki/"," ['6 Discussion']","The use of machine translation is strongly discour-aged by the Wikipedia community, [Cite_Footnote_8] but from exam-ining edit histories of articles in MLQA, machine translation is occasionally used as an article seed, before being edited and added to by human authors.",8 https://en.wikipedia.org/wiki/ Wikipedia:Translation#Avoid_machine_ translations,"It is worth discussing the quality of context para-graphs in MLQA. Our parallel sentence mining approach can source independently-written docu-ments in different languages, but, in practice, arti-cles are often translated from English to the target languages by volunteers. Thus our method some-times acts as an efficient mechanism of sourcing existing human translations, rather than sourcing independently-written content on the same topic. The use of machine translation is strongly discour-aged by the Wikipedia community, [Cite_Footnote_8] but from exam-ining edit histories of articles in MLQA, machine translation is occasionally used as an article seed, before being edited and added to by human authors.",補足資料,Document,True,Introduce（引用目的）,True,2020.acl-main.653_3_0,2020,MLQA: Evaluating Cross-lingual Extractive Question Answering,Footnote
2482,12490," https://github.com/BYVoid/OpenCC"," ['A Appendices', 'A.5 Additional preprocessing Details']","OpenCC ( [Cite] https://github.com/BYVoid/OpenCC ) is used to convert all Chinese contexts to Simplified Chinese, as wikipedia dumps generally consist of a mixture of simplified and traditional Chinese text.",,"OpenCC ( [Cite] https://github.com/BYVoid/OpenCC ) is used to convert all Chinese contexts to Simplified Chinese, as wikipedia dumps generally consist of a mixture of simplified and traditional Chinese text.",Method,Tool,True,Introduce（引用目的）,True,2020.acl-main.653_4_0,2020,MLQA: Evaluating Cross-lingual Extractive Question Answering,Body
2483,12491," https://github.com/tingyaohsu/VIST-Edit"," ['References']","The dataset, VIST-Edit [Cite_Footnote_1] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories.",1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.,"We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [Cite_Footnote_1] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evalu-ation scores and human ratings, motivating the need for new automatic metrics.",Material,Dataset,True,Use（引用目的）,False,P19-1658_0_0,2019,Visual Story Post-Editing,Footnote
2484,12492," http://hdl.handle.net/11234/1-1983"," ['3 Experiments']","Data We experiment on twelve languages with varying morphological typologies (Table 1) in the Universal Dependencies (UD) treebanks version 2.0 (Nivre et al., 2017) [Cite_Ref] .","Joakim Nivre et al. 2017. Universal Dependencies 2.0. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983.","Data We experiment on twelve languages with varying morphological typologies (Table 1) in the Universal Dependencies (UD) treebanks version 2.0 (Nivre et al., 2017) [Cite_Ref] . Note that while Ara-bic and Hebrew follow a root & pattern typology, their datasets are unvocalized, which might reduce the observed effects of this typology. Following common practice, we remove language-specific dependency relations and multiword token anno-tations. We use gold sentence segmentation, to-kenization, universal POS (UPOS), and morpho-logical (XFEATS) annotations provided in UD.",補足資料,Paper,True,Introduce（引用目的）,True,D18-1278_0_0,2018,What do character-level models learn about morphology? The case of dependency parsing,Reference
2485,12493," http://www.ncbi.nlm.nih.gov/pubmed/"," ['3 Linguistic acceptability study', '3.1 Dataset creation']",We used equal numbers of sentences from three different genres [Cite_Footnote_1] :,"1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk",We used equal numbers of sentences from three different genres [Cite_Footnote_1] :,Material,DataSource,True,Use（引用目的）,True,N10-1144_0_0,2010,Reformulating Discourse Connectives for Non-Expert Readers,Footnote
2486,12494," http://www.natcorp.ox.ac.uk"," ['3 Linguistic acceptability study', '3.1 Dataset creation']",We used equal numbers of sentences from three different genres [Cite_Footnote_1] :,"1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk",We used equal numbers of sentences from three different genres [Cite_Footnote_1] :,Material,DataSource,True,Use（引用目的）,True,N10-1144_1_0,2010,Reformulating Discourse Connectives for Non-Expert Readers,Footnote
2487,12495," https://github.com/uhh-lt/Taxonomy_Refinement_Embeddings"," ['1 Introduction']","The source code has been published [Cite_Footnote_1] to recreate the employed embedding, to refine taxonomies as well as to enable further research of Poincaré em-beddings for other semantic tasks.",1 https://github.com/uhh-lt/Taxonomy_ Refinement_Embeddings,"We compare two types of dense vector em-beddings: the standard word2vec CBOW model (Mikolov et al., 2013a,b), that embeds terms in Euclidean space based on distributional similarity, and the more recent Poincaré embeddings (Nickel and Kiela, 2017), which capture similarity as well as hierarchical relationships in a hyperbolic space. The source code has been published [Cite_Footnote_1] to recreate the employed embedding, to refine taxonomies as well as to enable further research of Poincaré em-beddings for other semantic tasks.",Method,Code,True,Introduce（引用目的）,True,P19-1474_0_0,2019,Every child should have parents: a taxonomy refinement algorithm based on hyperbolic term embeddings,Footnote
2488,12496," http://jobimtext.org"," ['3 Taxonomy Refinement using Hyperbolic Word Embeddings', '3.1 Domain-specific Poincaré Embedding']","Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [Cite_Footnote_2] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).","2 http://jobimtext.org: The PattaMaika compo-nent is based on UIMA RUTA (Kluegl et al., 2016).","Training Dataset Construction To create domain-specific Poincaré embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gi-gaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [Cite_Footnote_2] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).",Method,Tool,True,Use（引用目的）,True,P19-1474_1_0,2019,Every child should have parents: a taxonomy refinement algorithm based on hyperbolic term embeddings,Footnote
2489,12497," http://alt.qcri.org/semeval2016/task13/index.php"," ['4 Evaluation']","Proposed methods are evaluated on the data of SemEval2016 TExEval (Bordea et al., 2016) for submitted systems that created taxonomies for all domains of the task [Cite_Footnote_4] , namely the task-winning system TAXI (Panchenko et al., 2016) as well as the systems USAAR (Tan et al., 2016) and JUNLP (Maitra and Das, 2016).",4 http://alt.qcri.org/semeval2016/task13/index.php,"Proposed methods are evaluated on the data of SemEval2016 TExEval (Bordea et al., 2016) for submitted systems that created taxonomies for all domains of the task [Cite_Footnote_4] , namely the task-winning system TAXI (Panchenko et al., 2016) as well as the systems USAAR (Tan et al., 2016) and JUNLP (Maitra and Das, 2016). TAXI harvests hypernyms with substring inclusion and lexical-syntactic patterns by obtaining domain-specific texts via focused web crawling. USAAR and JUNLP heavily rely on rule-based approaches. While USAAR exploits the endocentric nature of hyponyms, JUNLP combines two string inclusion heuristics with semantic relations from BabelNet. We use the taxonomies created by these systems as our baseline and additionally ensured that tax-onomies do neither have circles nor in-going edges to the taxonomy root by applying the Tarjan al-gorithm (Tarjan, 1972), removing a random link from detected cycles. This causes slight differ-ences between the baseline results in Figure 2 and (Bordea et al., 2016).",補足資料,Website,True,Introduce（引用目的）,True,P19-1474_2_0,2019,Every child should have parents: a taxonomy refinement algorithm based on hyperbolic term embeddings,Footnote
2490,12498," https://github.com/stanfordnlp/GloVe"," ['4 Experiments and Evaluation', '4.2 Implementation Details']","We use pre-trained word embeddings with 300 di-mensions [Cite_Footnote_9] , and keep them fixed during the train-ing process.","9 We download the pre-trained word embeddings from https://github.com/stanfordnlp/GloVe, and we select the smaller Wikipedia 2014 + Gigaword 5.","We use the validation set (SE7) to find the optimal settings of our framework: the hidden state size n, the number of passes |T M |, the optimizer, etc. We use pre-trained word embeddings with 300 di-mensions [Cite_Footnote_9] , and keep them fixed during the train-ing process. We employ 256 hidden units in both the gloss module and the context module, which means n=256. Orthogonal initialization is used for weights in LSTM and random uniform initializa-tion with range [-0.1, 0.1] is used for others. We assign gloss expansion depth K the value of 4. We also experiment with the number of passes |T M | from 1 to 5 in our framework, finding |T M | = 3 performs best. We use Adam optimizer (Kingma and Ba, 2014) in the training process with 0.001 initial learning rate. In order to avoid overfitting, we use dropout regularization and set drop rate to 0.5. Training runs for up to 100 epochs with early stopping if the validation loss doesn’t im-prove within the last 10 epochs.",Material,Knowledge,True,Use（引用目的）,True,P18-1230_0_0,2018,Incorporating Glosses into Neural Word Sense Disambiguation,Footnote
2491,12499," http://www.nlm.nih.gov/bsd/pmresources.html"," ['1 Introduction']","We observed that from a corpus of 74,000 randomly chosen Medline [Cite_Footnote_2] abstracts, of the first 150 most fre-quently occurring distinct demonstrative nouns (fre-quency > 30), 51.3% were abstract, 41.3% were concrete, and 7.3% were discourse deictic.",2 http://www.nlm.nih.gov/bsd/pmresources. html,"Demonstrative nouns, along with pronouns like both and either, are referred to as sortal anaphors (Castaño et al., 2002; Lin and Liang, 2004; Torii and Vijay-Shanker, 2007). Castaño et al. observed that sortal anaphors are prevalent in the biomedi-cal literature. They noted that among 100 distinct anaphors derived from a corpus of 70 Medline ab-stracts, 60% were sortal anaphors. But how often do demonstrative nouns refer to abstract objects? We observed that from a corpus of 74,000 randomly chosen Medline [Cite_Footnote_2] abstracts, of the first 150 most fre-quently occurring distinct demonstrative nouns (fre-quency > 30), 51.3% were abstract, 41.3% were concrete, and 7.3% were discourse deictic. This shows that abstract anaphora resolution is an impor-tant component of general anaphora resolution in the biomedical domain. However, automatic resolution of this type of anaphora has not attracted much atten-tion and the previous work for this task is limited.",Material,DataSource,True,Use（引用目的）,True,D12-1115_0_0,2012,Resolving “This-issue” Anaphora,Footnote
2492,12500," http://nlp.stanford.edu/software/lex-parser.shtml"," ['4 Resolution Algorithm', '4.1 Candidate Extraction']","Then, we parse every candidate sentence with the Stanford Parser [Cite_Footnote_7] .",7 http://nlp.stanford.edu/software/lex-parser.shtml,"For correct resolution, the set of extracted candidates must contain the correct antecedent in the first place. The problem of candidate extraction is non-trivial in abstract anaphora resolution because the antecedents are of many different types of syntactic constituents such as clauses, sentences, and nominalizations. Drawing on our observation that the mixed type an-tecedents are generally a combination of different well-defined syntactic constituents, we extract the set of candidate antecedents as follows. First, we create a set of candidate sentences which contains the sentence containing the this-issue anaphor and the two preceding sentences. Then, we parse every candidate sentence with the Stanford Parser [Cite_Footnote_7] . Ini-tially, the set of candidate constituents contains a list of well-defined syntactic constituents. We re-quire that the node type of these constituents be in the set {S, SBAR, NP, SQ, SBARQ, S+V}. This set was empirically derived from our data. To each constituent, there is associated a set of mixed type constituents. These are created by concatenating the original constituent with its sister constituents. For example, in (4), the set of well-defined eligible can-didate constituents is {NP, NP 1 } and so NP 1 PP 1 is a mixed type candidate.",Method,Tool,True,Use（引用目的）,True,D12-1115_1_0,2012,Resolving “This-issue” Anaphora,Footnote
2493,12501," http://cogcomp.cs.illinois.edu/page/software_view/SRL"," ['4 Resolution Algorithm', '4.2 Features']",We used the Illinois Semantic Role Labeler [Cite_Footnote_9] for SR features.,9 http://cogcomp.cs.illinois.edu/page/software_view/SRL,"We explored the effect of including 43 automati-cally extracted features (12 feature classes), which are summarized in Table 2. The features can also be broadly divided into two groups: issue-specific fea-tures and general abstract-anaphora features. Issue-specific features are based on our common-sense knowledge of the concept of issue and the different semantic forms it can take; e.g., controversy (X is controversial), hypothesis (It has been hypothesized X), or lack of knowledge (X is unknown), where X is the issue. In our data, we observed certain syn-tactic patterns of issues such as whether X or not and that X and the IP feature class encodes this in-formation. Other issue-specific features are IVERB and IHEAD. The feature IVERB checks whether the governing verb of the candidate is an issue verb (e.g., speculate, hypothesize, argue, debate), whereas IHEAD checks whether the candidate head in the dependency tree is an issue word (e.g., contro-versy, uncertain, unknown). The general abstract-anaphora resolution features do not make use of the semantic properties of the word issue. Some of these features are derived empirically from the training data (e.g., ST, L, D). The EL feature is bor-rowed from Müller (2008) and encodes the embed-ding level of the candidate within the candidate sen-tence. The MC feature tries to capture the idea of the THIS-NPs hypothesis (Gundel et al., 1993; Poesio and Modjeska, 2002) that the antecedents of this- NP anaphors are not the center of the previous utter-ance. The general abstract-anaphora features in the SR feature class capture the semantic role of the can-didate in the candidate sentence. We used the Illinois Semantic Role Labeler [Cite_Footnote_9] for SR features. The gen-eral abstract-anaphora features also contain a few lexical features (e.g., M, SC). But these features are independent of the semantic properties of the word issue. The general abstract-anaphora resolution fea-tures also contain dependency-tree features, lexical-overlap features, and context features.",Method,Tool,True,Use（引用目的）,True,D12-1115_2_0,2012,Resolving “This-issue” Anaphora,Footnote
2494,12502," http://www.lsi.upc.es/srlconll/home.html"," ['8 Experiments']",We evaluated our system using the setup of the Conll 2005 semantic role labeling task. [Cite_Footnote_2],2 http://www.lsi.upc.es/srlconll/home.html,"We evaluated our system using the setup of the Conll 2005 semantic role labeling task. [Cite_Footnote_2] Thus, we trained on Sections 2-21 of PropBank and used Section 24 as development data. Our test data includes both the selected portion of Section 23 of PropBank, plus the extra data on the Brown corpus. We used the Char-niak parses provided by the Conll distribution.",補足資料,Website,True,Introduce（引用目的）,True,P08-1040_0_0,2008,Sentence Simplification for Semantic Role Labeling,Footnote
2495,12503," http://www2.parc.com/isl/groups/nltt/xle/"," ['9 Related Work']","Both approaches attempt to abstract away from the surface level syntax of the sentence (e.g., the XLE system [Cite_Footnote_3] ).",3 http://www2.parc.com/isl/groups/nltt/xle/,"One area of current research which has similarities with this work is on Lexical Functional Grammars (LFGs). Both approaches attempt to abstract away from the surface level syntax of the sentence (e.g., the XLE system [Cite_Footnote_3] ). The most obvious difference be-tween the approaches is that we use SRL data to train our system, avoiding the need to have labeled data specific to our simplification scheme.",Method,Code,False,Introduce（引用目的）,False,P08-1040_1_0,2008,Sentence Simplification for Semantic Role Labeling,Footnote
2496,12504," https://github.com/yftah89/PBLM-Cross-language-Cross-domain"," ['References']",In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms pre-vious models even when it is trained in the lazy setup and previous models are trained in the full setup. [Cite_Footnote_1],1 Our code is publicly available at https://github.com/yftah89/ PBLM-Cross-language-Cross-domain,"While cross-domain and cross-language trans-fer have long been prominent topics in NLP re-search, their combination has hardly been ex-plored. In this work we consider this problem, and propose a framework that builds on pivot-based learning, structure-aware Deep Neu-ral Networks (particularly LSTMs and CNNs) and bilingual word embeddings, with the goal of training a model on labeled data from one (language, domain) pair so that it can be effec-tively applied to another (language, domain) pair. We consider two setups, differing with re-spect to the unlabeled data available for model training. In the full setup the model has ac-cess to unlabeled data from both pairs, while in the lazy setup, which is more realistic for truly resource-poor languages, unlabeled data is available for both domains but only for the source language. We design our model for the lazy setup so that for a given target domain, it can train once on the source language and then be applied to any target language without re-training. In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms pre-vious models even when it is trained in the lazy setup and previous models are trained in the full setup. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,D18-1022_0_0,2018,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,Footnote
2497,12505," https://github.com/quankiquanki/"," ['6 Experiments']","Following ZR18 we also consider a more chal-lenging setup where the English source domain consists of user airline (A) reviews (Nguyen, 2015) [Cite_Ref] .",Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.,"Following ZR18 we also consider a more chal-lenging setup where the English source domain consists of user airline (A) reviews (Nguyen, 2015) [Cite_Ref] . We use the dataset of ZR18, consisting of 1000 positive and 1000 negative reviews in the la-beled set, and 39396 reviews as the unlabeled set.",Material,Dataset,True,Introduce（引用目的）,False,D18-1022_1_0,2018,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,Reference
2498,12506," https://github.com/quankiquanki/"," ['6 Experiments']","69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015) [Cite_Ref] ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target",Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.,"Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), CLCD English-German S-T D-B M-B B-D M-D B-M D-M English-French All D-B M-B B-D M-D B-M D-M All PBLM Models P+BE 78.7 78.6 80.6 79.2 81.7 78.5 PBLM 70.9 62.9 74.5 66.5 75.0 75.5 Lazy 74.8 74.0 75.1 72.8 73.3 73.7 79.5 81.1 74.7 76.3 75.0 75.1 76.8 76.5 71.0 76.0 67.9 70.3 69.9 67.3 70.4 70.3 73.9 74.2 73.1 75.3 74.4 74.1 72.4 73.9 Autoencoder+pivot Models A-S-SR 68.3 62.5 69.4 69.9 70.2 69 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015) [Cite_Ref] ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target A-B A-D A-M All A-B A-D A-M All PBLM Models P+BE 64 6. 65.167.9 62.5 63.6 63.5 66.9 64.8 PBLM 60.9 59.6 60.1 .60 2 60.9 61.9 58.9 60.5 Lazy 66.3 65.0 66.6 66 0. 65.7 65.6 69.0 66.8 Autoencoder+pivot Models A-S-SR 55.8 57.5 60.8 58 55.9 56.2 58.2 55.8 52.9 56.3 55.7 A-SCL 56 8. 55.8 52.9 56.4 55.0 Pivot-based (no DNN) Models C-SCL 56.6 52.6 53.7 54 3. 52.7 54.5 53.1 53.4 DCI 55.9 52.1 54.5 54 1. 53.1 53.7 53.9 53.5 CLCD without CD Learning CNN 59.4 61.2 61.3 . 57.9 55.3 56.260 6 56.5 Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), Within Language German-German S-T D-B M-B B-D M-D B-M D-M French-French All D-B M-B B-D M-D B-M D-M All In-language cross-domain learning (no CD technique is employed) IL 81.5 78.9 77.8 76.7 77.6 79.8 78.7 80.2 78.2 79.2 79.7 78.5 79.7 79.3 In-language, In-domain learning S-T B-B – D-D – M-M – ILID 84.2 – 81.5 – 83.3 – All B-B – D-D – M-M – All 83 84.1 – 79.2 – 85.8 – 83",Material,Dataset,False,Introduce（引用目的）,False,D18-1022_1_1,2018,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,Reference
2499,12507," https://github.com/Babylonpartners/fastText_"," ['References']","• Bilingual word embeddings (Smith et al., 2017): [Cite] https://github.com/Babylonpartners/fastText_multilingual.",,"• Bilingual word embeddings (Smith et al., 2017): [Cite] https://github.com/Babylonpartners/fastText_multilingual. The authors employed their method to monolingual fastText em-beddings (Bojanowski et al., 2017) – the embeddings of 78 languages were aligned with the English embeddings.",補足資料,Document,True,Introduce（引用目的）,True,D18-1022_2_0,2018,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,Body
2500,12508," https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"," ['References']","• The bilingual embeddings are based on the fastText Facebook embeddings (Bojanowski et al., 2017): [Cite] https: //github.com/",,"• The bilingual embeddings are based on the fastText Facebook embeddings (Bojanowski et al., 2017): [Cite] https: //github.com/facebookresearch/ fastText/blob/master/ pretrained-vectors.md",Method,Code,True,Extend（引用目的）,True,D18-1022_3_0,2018,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,Body
2501,12509," http://scikit-learn.org/stable/"," ['References']",[Cite] http://scikit-learn.org/stable/,,• Logistic regression classifier: [Cite] http://scikit-learn.org/stable/,Method,Tool,False,Introduce（引用目的）,False,D18-1022_4_0,2018,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,Body
2502,12510," https://github.com/yftah89/Neural-SCLDomain-Adaptation"," ['References']",[Cite] https://github.com/yftah89/Neural-SCLDomain-Adaptation.,,• AE-SCL and AE-SCL-SR: We use the code from the author’s github: [Cite] https://github.com/yftah89/Neural-SCLDomain-Adaptation.,Method,Code,True,Use（引用目的）,True,D18-1022_5_0,2018,Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance,Body
2503,12511," https://github.com/huggingface/transformers/blob/master/examples/run"," ['3 Experiments and Discussion', '3.2 Language Model Setup']","Most of the hyperparmeters used for fine-tuning are the default ones from Wolf et al. (2019) [Cite_Footnote_1] , except for learning rate for BART, which we set to 1e − 5.",1 https://github.com/huggingface/transformers/blob/master/examples/run lm finetuning.py,"We use pretrained GPT2-base (12 layers, 117M parameters), GPT2-large (24 layers, 345M params), BART-base (6 layers encoder and 6 layers decoder, 139M params) and BART-large (12 layers encoder and 12 layers decoder, 406M params) models in our experiments. We adopted the implementation and pretrained models from Wolf et al. (2019). We fine-tune GPT2 and BART on each training dataset separately. We perform a maximum of 10 fine-tuning epochs and adopt early stopping using the validation sets. Most of the hyperparmeters used for fine-tuning are the default ones from Wolf et al. (2019) [Cite_Footnote_1] , except for learning rate for BART, which we set to 1e − 5.",Method,Code,True,Use（引用目的）,True,2020.emnlp-main.134_0_0,2020,Beyond [CLS] through Ranking by Generation,Footnote
2504,12512," https://github.com/Ryuto10/pzero-improves-zar"," ['1 Introduction']",• We design a new ZAR model [Cite_Footnote_2] that makes full use of pretrained MLMs with minimal architectural modifications;,2 Our code is publicly available: https://github.com/Ryuto10/pzero-improves-zar,• We design a new ZAR model [Cite_Footnote_2] that makes full use of pretrained MLMs with minimal architectural modifications;,Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.308_0_0,2021,Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution,Footnote
2505,12513," http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13"," ['1 Introduction']","Recently, Google has released the Web 1T dataset [Cite_Footnote_3] that pro-vides q(w) estimated on a text collection of one tril-lion tokens.",3 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13,"Obviously, the quality of this measure depends on the quality of estimating q(w), i.e. the general En-glish word distribution, which is usually estimated over a large text collection. The larger the collec-tion is, the better would be the estimation. Recently, Google has released the Web 1T dataset [Cite_Footnote_3] that pro-vides q(w) estimated on a text collection of one tril-lion tokens. We use it in our experimentation.",Material,Dataset,True,Introduce（引用目的）,True,D08-1005_0_0,2008,One-Class Clustering in the Text Domain,Footnote
2506,12514," http://www.cs.umass.edu/~ronb/name_disambiguation.html"," ['5 Experimentation', '5.1 Web appearance disambiguation']","We test our models on the WAD dataset, [Cite_Footnote_7] which consists of 1085 Web pages that mention 12 people names of AI researchers, such as Tom Mitchell and Leslie Kaelbling.",7 http://www.cs.umass.edu/˜ronb/name_disambiguation.html,"We test our models on the WAD dataset, [Cite_Footnote_7] which consists of 1085 Web pages that mention 12 people names of AI researchers, such as Tom Mitchell and Leslie Kaelbling. Out of the 1085 pages, 420 are on-topic, so we apply our algorithms with k = 420. At a preprocessing step, we binarize document vec-tors and remove low frequent words (both in terms of p(w) and q(w)). The results are summarized in the middle column of Table 1. We can see that both OCCC and LTB dramatically outperform their com-petitors, while showing practically indistinguishable results compared to each other. Note that when the size of the word cluster in OCCC is unfairly set to its optimal value, m r = 2200, the OCCC method is able to gain a 2% boost. However, for obvious reasons, the optimal value of m r may not always be obtained in practice.",Material,Dataset,True,Use（引用目的）,True,D08-1005_1_0,2008,One-Class Clustering in the Text Domain,Footnote
2507,12515," http://projects.ldc.upenn.edu/TDT5/"," ['5 Experimentation', '5.2 Detecting the topic of the week']","We evaluate the TW detection task on the bench-mark TDT-5 dataset [Cite_Footnote_8] , which consists of 250 news events spread over a time period of half a year, and 9,812 documents in English, Arabic and Chinese (translated to English), annotated by their relation-ship to those events.",8 http://projects.ldc.upenn.edu/TDT5/,"We evaluate the TW detection task on the bench-mark TDT-5 dataset [Cite_Footnote_8] , which consists of 250 news events spread over a time period of half a year, and 9,812 documents in English, Arabic and Chinese (translated to English), annotated by their relation-ship to those events. The largest event in TDT-5 dataset (#55106, titled “Bombing in Riyadh, Saudi Arabia”) has 1,144 documents, while 66 out of the 250 events have only one document each. We split the dataset to 26 weekly chunks (to have 26 full weeks, we delete all the documents dated with the last day in the dataset, which decreases the dataset’s size to 9,781 documents). Each chunk contains from 138 to 1292 documents.",Material,Dataset,True,Use（引用目的）,True,D08-1005_2_0,2008,One-Class Clustering in the Text Domain,Footnote
2508,12516," http://www.google.com/apis"," ['3 The U-SVM', '3.1 Clustering Web Search Results']","In submitting TREC 2004 test question 1.1 ”when was the first Crip gang started?” to Google ( [Cite] http://www.google.com/apis), we extract n(= 8) different candidates from the top m(= 30)",,"Here, we give an example illustrating the prin-ciple of clustering web search results in the FGS. In submitting TREC 2004 test question 1.1 ”when was the first Crip gang started?” to Google ( [Cite] http://www.google.com/apis), we extract n(= 8) different candidates from the top m(= 30) Google snippets. The Google snippets containing the same candidates are aligned snippets, and thus the 12 re-tained snippets are grouped into 8 clusters, as listed in Table 2. This table roughly indicates that the snip-pets with the same candidate answers contain the same sub-meanings, so these snippets are considered as aligned snippets. For example, all Google snip-pets that contain the candidate answer 1969 express the time of establishment of ”the first Crip gang”.",Method,Tool,False,Use（引用目的）,True,D07-1004_0_0,2007,Learning Unsupervised SVM Classifier for Answer Selection in Web Question Answering,Body
2509,12517," http://www.csie.ntu.edu.tw/cjlin/libsvm/"," ['3 The U-SVM', '3.2 Classifying Question']",This paper selects LIBSVM toolkit [Cite_Footnote_1] to implement the SVM classifier.,1 http://www.csie.ntu.edu.tw/cjlin/libsvm/,This paper selects LIBSVM toolkit [Cite_Footnote_1] to implement the SVM classifier. The kernel is the radical basis function with the parameter γ = 0.001 in the exper-iments.,Method,Tool,True,Use（引用目的）,True,D07-1004_1_0,2007,Learning Unsupervised SVM Classifier for Answer Selection in Web Question Answering,Footnote
2510,12518," http://nlp.stanford.edu/projects/glove/"," ['3 Deep Memory Network for Aspect Level Sentiment Classification', '3.6 Aspect Level Sentiment Classification']","We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.9M [Cite_Footnote_4] .",4 Available at: http://nlp.stanford.edu/projects/glove/.,"P c (s, a) is the probability of predicting (s, a) as cat-egory c produced by our system. P cg (s,a) is 1 or 0, indicating whether the correct answer is c. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent. We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.9M [Cite_Footnote_4] . We randomize other pa-rameters with uniform distribution U(−0.01, 0.01), and set the learning rate as 0.01.",Material,Knowledge,True,Produce（引用目的）,False,D16-1021_0_0,2016,Aspect Level Sentiment Classification with Deep Memory Network,Footnote
2511,12519," http://flowacldemo.appspot.com"," ['1. Introduction']","This paper presents FLOW [Cite_Footnote_1] (Figure 1), an interactive system for assisting EFL writers in composing and revising writing.",1 FLOW: http://flowacldemo.appspot.com,"This paper presents FLOW [Cite_Footnote_1] (Figure 1), an interactive system for assisting EFL writers in composing and revising writing. Different from existing tools, its context-sensitive and first-language-oriented features enable EFL writers to concentrate on their ideas and thoughts without being hampered by the limited lexical resources. Based on the studies that first language use can positively affect second language composing, FLOW attempts to meet such needs. Given any L1 input, FLOW displays appropriate suggestions including translation, paraphrases, and n-grams during composing and revising processes. We use the following example sentences to illustrate these two functionalities.",Method,Tool,True,Produce（引用目的）,True,P12-3027_0_0,2012,FLOW: A First-Language-Oriented Writing Assistant System,Footnote
2512,12520," http://ruscorpora.ru/"," ['5 Figurative Language Uses as Outliers']","For each Russian word shown in Table 3, we extracted from the Russian National Cor-pora ( [Cite] http://ruscorpora.ru/) several lit-eral and non-literal occurences.",,"For each Russian word shown in Table 3, we extracted from the Russian National Cor-pora ( [Cite] http://ruscorpora.ru/) several lit-eral and non-literal occurences. Some of these words have more than one meaning in Russian, e.g. ключ can be translated as a key or water spring and the word коса as a plait, scythe or spit.",Material,DataSource,True,Use（引用目的）,True,P10-3012_0_0,2010,A Framework for Figurative Language Detection Based on Sense Differentiation,Body
2513,12521," http://snowball.tartarus.org/"," ['5 Figurative Language Uses as Outliers']",( [Cite] http://snowball.tartarus.org/) for the Russian language.,,All the documents are stemmed and all stop-words are removed with the SnowBall Stem-mer ( [Cite] http://snowball.tartarus.org/) for the Russian language.,Method,Tool,True,Use（引用目的）,True,P10-3012_1_0,2010,A Framework for Figurative Language Detection Based on Sense Differentiation,Body
2514,12522," https://github.com/DancingSoul/NQ_BERT-DM"," ['1 Introduction']",We will release our code and models at [Cite] https: //github.com/DancingSoul/NQ_BERT-DM .,,"We will release our code and models at [Cite] https: //github.com/DancingSoul/NQ_BERT-DM . Figure 2: System overview. The document fragments of one document are fed into our model independently. The outputs of graph encoders are merged and sent into the answer selection module, which generates a long answer and a short answer.",Method,Code,True,Produce（引用目的）,True,2020.acl-main.599_0_0,2020,Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension,Body
2515,12523," https://bit.ly/2w7nUQK"," ['4 Experiments', '4.2 Experimental Settings']",We use three BERT encoders to initialize our token node representation: 1) BERT-base: a BERT-base-uncased model finetuned on SQuAD 2.0; 2) BERT-large: a BERT-large-uncased model finetuned on SQuAD 2.0; 3) BERT-syn: Google’s BERT-large-uncased model pre-trained on SQuAD2.0 with N-Gram Masking and Syn-thetic Self-Training. [Cite_Footnote_2],2 This model can be downloaded at https://bit.ly/2w7nUQK.,"We use three BERT encoders to initialize our token node representation: 1) BERT-base: a BERT-base-uncased model finetuned on SQuAD 2.0; 2) BERT-large: a BERT-large-uncased model finetuned on SQuAD 2.0; 3) BERT-syn: Google’s BERT-large-uncased model pre-trained on SQuAD2.0 with N-Gram Masking and Syn-thetic Self-Training. [Cite_Footnote_2] Since the Natural Question dataset does not provide sentence-level informa-tion, we additionally use spacy (Honnibal and Mon-tani, 2017) as the sentence segmentor to get the boundaries of sentences.",Method,Code,False,Produce（引用目的）,False,2020.acl-main.599_1_0,2020,Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension,Footnote
2516,12524," https://github.com/JasonForJoy/SPD"," ['5 Experiments', '5.2 Training Details']","All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [Cite_Footnote_1]",1 https://github.com/JasonForJoy/SPD,"All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.86_0_0,2021,Detecting Speaker Personas from Conversational Texts,Footnote
2517,12525," http://www.cs.cmu.edu/~nlao/"," ['5 Experiments', '5.1 Knowledge Base Completion']","We use the data generated from version 165 for training [Cite_Footnote_3] , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set .",3 http://www.cs.cmu.edu/˜nlao/,"We evaluate our approach on a knowledge base generated by the CMU Never Ending Language Learning (NELL) project (Carlson et al., 2010). NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training [Cite_Footnote_3] , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set . The data statistics of the training set are summarized in Table 1. The numbers of triples in the development and test sets are 19,665 and 117,889, respectively. Notice that this dataset is substantially larger than the datasets used in recent work. For example, the Freebase data used in (Socher et al., 2013) and (Bordes et al., 2013a) have 316k and 483k 5 triples, respec-tively, compared to 1.8M in this dataset.",Material,DataSource,True,Use（引用目的）,True,D14-1165_0_0,2014,Typed Tensor Decomposition of Knowledge Bases for Relation Extraction,Footnote
2518,12526," http://bit.ly/trescal"," ['5 Experiments', '5.1 Knowledge Base Completion']","We use the data generated from version 165 for training , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set [Cite_Footnote_4] .",4 http://bit.ly/trescal,"We evaluate our approach on a knowledge base generated by the CMU Never Ending Language Learning (NELL) project (Carlson et al., 2010). NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set [Cite_Footnote_4] . The data statistics of the training set are summarized in Table 1. The numbers of triples in the development and test sets are 19,665 and 117,889, respectively. Notice that this dataset is substantially larger than the datasets used in recent work. For example, the Freebase data used in (Socher et al., 2013) and (Bordes et al., 2013a) have 316k and 483k 5 triples, respec-tively, compared to 1.8M in this dataset.",Material,DataSource,True,Use（引用目的）,True,D14-1165_1_0,2014,Typed Tensor Decomposition of Knowledge Bases for Relation Extraction,Footnote
2519,12527," http://www.riedelcastro.org/uschema"," ['5 Experiments', '5.2 Relation Extraction']","We compare the proposed T RESCAL model to RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011), MI09 (Mintz et al., 2009) and SU12 (Surdeanu et al., 2012) [Cite_Footnote_8] .",8 The corpus and the system outputs are from http://www.riedelcastro.org/uschema,"We compare the proposed T RESCAL model to RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011), MI09 (Mintz et al., 2009) and SU12 (Surdeanu et al., 2012) [Cite_Footnote_8] . We follow the protocol used in (Riedel et al., 2013) to evaluate the results. Given a re-lation as query, the top 1,000 entity pairs output by each system are collected and the top 100 ones are judged manually. Besides comparing individ-ual models, we also report the results of combined models. To combine the scores from two models, we simply normalize the scores of entity-relation tuples to zero mean and unit variance and take the average. The results are summarized in Table 3.",Mixed,Mixed,True,Use（引用目的）,True,D14-1165_2_0,2014,Typed Tensor Decomposition of Knowledge Bases for Relation Extraction,Footnote
2520,12528," http://statnlp.org/statnlp-framework"," ['-']","In this tutorial, we will be discussing how such a wide spectrum of existing structured predic-tion models can all be implemented under a uni-fied framework [Cite_Footnote_1] that involves some basic building blocks.",1 http://statnlp.org/statnlp-framework,"In this tutorial, we will be discussing how such a wide spectrum of existing structured predic-tion models can all be implemented under a uni-fied framework [Cite_Footnote_1] that involves some basic building blocks. Based on such a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conve-niently and quickly. Furthermore, we also show that the framework can be used to solve certain structured prediction problems that otherwise can-not be easily handled by conventional structured prediction models. Specifically, we show how to use such a framework to construct models that are capable of predicting non-conventional structures, such as overlapping structures (Lu and Roth, 2015; Muis and Lu, 2016a). We will also discuss how to make use of the framework to build other related models such as topic models and highlight its po-tential applications in some recent popular tasks (e.g., AMR parsing (Flanigan et al., 2014)).",Method,Code,True,Introduce（引用目的）,True,D17-3006_0_0,2017,A Unified Framework for Structured Prediction: From Theory to Practice,Footnote
2521,12529," http://statnlp.org/tutorials/"," ['-']",This tutorial consists of the following 3 main sections. [Cite_Footnote_2],2 The material associated with this tutorial will be avail-able at http://statnlp.org/tutorials/.,This tutorial consists of the following 3 main sections. [Cite_Footnote_2],Mixed,Mixed,True,Produce（引用目的）,True,D17-3006_1_0,2017,A Unified Framework for Structured Prediction: From Theory to Practice,Footnote
2522,12530," http://statnlp.org/"," ['About the Instructor']","Wei Lu is an Assistant Professor at the Sin-gapore University of Technology and Design (SUTD), directing the StatNLP research group ( [Cite] http://statnlp.org/).",,"Wei Lu is an Assistant Professor at the Sin-gapore University of Technology and Design (SUTD), directing the StatNLP research group ( [Cite] http://statnlp.org/). He received his Ph.D. from the National University of Singapore (NUS) in 2009. He visited CSAIL, Massachusetts Institute of Technology (MIT) in 2007-2008, and worked as a postdoctoral research associate at the University of Illinois at Urbana-Champaign in 2011-2013. His research interests include devel-oping mathematical models and machine learning algorithms for solving natural language processing problems. He is particularly interested in semantic processing (in a broad sense). His papers appeared at venues such as ACL, EMNLP, NAACL, AAAI, and CIKM. He served as a program committee member for conferences such as ACL, EMNLP, NAACL, EACL, AAAI, IJCAI and NIPS, and is currently a member of the standing reviewer team for TACL. He served as an area co-chair for ACL 2016 and received the best paper award at EMNLP 2011.",補足資料,Website,True,Other（引用目的）,False,D17-3006_2_0,2017,A Unified Framework for Structured Prediction: From Theory to Practice,Body
2523,12531," http://research.microsoft.com/users/silviu/WebAssistant/TestData"," ['6 Evaluation']",The evalua-tion data can be downloaded from [Cite] http://research.,,"We evaluated the system in two ways: on a set of Wikipedia articles, by comparing the system out-put with the references created by human contribu-tors, and on a set of news stories, by doing a post-hoc evaluation of the system output. The evalua-tion data can be downloaded from [Cite] http://research.microsoft.com/users/silviu/WebAssistant/TestData. In both settings, we computed a disambiguation baseline in the following manner: for each surface form, if there was an entity page or redirect page whose title matches exactly the surface form then we chose the corresponding entity as the baseline disambiguation; otherwise, we chose the entity most frequently mentioned in Wikipedia using that surface form.",Material,Dataset,True,Introduce（引用目的）,True,D07-1074_0_0,2007,Large-Scale Named Entity Disambiguation Based on Wikipedia Data,Body
2524,12532," https://www.kaggle.com/c/asap-aes/"," ['1 Introduction']","For our experiments, we use the Automated Student As-sessment Prize (ASAP) dataset, [Cite_Footnote_3] which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4).",3 https://www.kaggle.com/c/asap-aes/,"At the outset, our goal is to develop a framework that strengthens the validity of state-of-the-art neu-ral AES approaches with respect to adversarial in-put related to local aspects of coherence. For our experiments, we use the Automated Student As-sessment Prize (ASAP) dataset, [Cite_Footnote_3] which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4).",Material,Dataset,True,Use（引用目的）,True,N18-1024_0_0,2018,Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input,Footnote
2525,12533," https://github.com/Youmna-H/Coherence_AES"," ['5 Model Parameters and Baselines']",The net-work is trained for 60 epochs and performance is monitored on the development sets – we select the model that yields the highest PRA value. [Cite_Footnote_12],12 Our implementation is available at https: //github.com/Youmna-H/Coherence_AES,"Coherence models We train and test the LC model described in Section 3.1 on the synthetic dataset and evaluate it using PRA and TPRA. Dur-ing pre-processing, words are lowercased and ini-tialized with pre-trained word embeddings (Zou et al., 2013). Words that occur only once in the training set are mapped to a special UNK embed-ding. All network weights are initialized to values drawn randomly from a uniform distribution with scale = 0.05, and biases are initialized to zeros. We apply a learning rate of 0.001 and RMSProp (Tieleman and Hinton, 2012) for optimization. A size of 100 is chosen for the hidden layers (d lstm and d cnn ), and the convolutional window size (m) is set to 3. Dropout (Srivastava et al., 2014) is ap-plied for regularization to the output of the convo-lutional operation with probability 0.3. The net-work is trained for 60 epochs and performance is monitored on the development sets – we select the model that yields the highest PRA value. [Cite_Footnote_12]",Method,Tool,False,Produce（引用目的）,True,N18-1024_1_0,2018,Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input,Footnote
2526,12534," https://bitbucket.org/melsner/browncoherence"," ['5 Model Parameters and Baselines']","Using the Brown Coherence Toolkit (Eisner and Charniak, 2011), [Cite_Footnote_13] we construct the entity transi-tion probabilities with length = 3 and salience = 2.",13 https://bitbucket.org/melsner/browncoherence,"We use as a baseline the LC model that is based on the multiplication of the clique scores (simi-larly to Li and Hovy (2014)), and compare the results (LC mul ) to our averaged approach. As another baseline, we use the entity grid (EGrid) (Barzilay and Lapata, 2008) that models transi-tions between sentences based on sequences of en-tity mentions labeled with their grammatical role. EGrid has been shown to give competitive re-sults on similar coherence tasks in other domains. Using the Brown Coherence Toolkit (Eisner and Charniak, 2011), [Cite_Footnote_13] we construct the entity transi-tion probabilities with length = 3 and salience = 2. The transition probabilities are then used as fea-tures that are fed as input to an SVM classifier with an RBF kernel and penalty parameter C = 1.5 to predict a coherence score.",Method,Tool,True,Use（引用目的）,True,N18-1024_2_0,2018,Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input,Footnote
2527,12535," https://github.com/nusnlp/nea"," ['5 Model Parameters and Baselines']",LSTM T&N model We replicate and evaluate the LSTM model of Taghipour and Ng (2016) [Cite_Footnote_14] on ASAP and our synthetic data.,14 https://github.com/nusnlp/nea,LSTM T&N model We replicate and evaluate the LSTM model of Taghipour and Ng (2016) [Cite_Footnote_14] on ASAP and our synthetic data.,Method,Tool,True,Use（引用目的）,True,N18-1024_3_0,2018,Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input,Footnote
2528,12536," http://bit.ly/mt-para"," ['9 Experimental Setup and Results', '9.3 Paraphrase Identification Task']","We evaluate paraphrase identification (PI) on the PAN corpus ( [Cite] http://bit.ly/mt-para, (Madnani et al., 2012)), consisting of training and test sets of 10,000 and 3000 sentence pairs, respectively.",,"We evaluate paraphrase identification (PI) on the PAN corpus ( [Cite] http://bit.ly/mt-para, (Madnani et al., 2012)), consisting of training and test sets of 10,000 and 3000 sentence pairs, respectively. Sen-tences are about 40 words long on average.",Material,Knowledge,False,Use（引用目的）,True,P15-1007_0_0,2015,MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity,Body
2529,12537," https://github.com/dguo98/seqmix"," ['3 Method']","To summarize, this results in a simple algorithm where we sample λ ∼ Beta(α,α) and train on these expected samples. [Cite_Footnote_1]","1 Our implementation can be found at https://github.com/dguo98/seqmix, and pseudocode can be found in supplementary materials.","To summarize, this results in a simple algorithm where we sample λ ∼ Beta(α,α) and train on these expected samples. [Cite_Footnote_1] Relationship to Existing Methods Table 1 shows that we can recover existing data augmenta-tion methods such as SwitchOut and word dropout under the above framework. In particular, these methods approximate a version of the “hard” latent variable objective in Eq. 2 by considering different swap distributions p(m) and sampling distributions D 0 . Compared to other approaches, SeqMix is es-sentially a relaxed variant of the same objective, similar to the difference between soft vs. hard at-tention (Xu et al., 2015; Deng et al., 2018; Wu et al., 2018; Shankar et al., 2018). SeqMix is also more efficient than more sophisticated augmenta-tion strategies such as GECA which requires a com-putationally expensive validation check for swaps.",Method,Tool,False,Produce（引用目的）,True,2020.emnlp-main.447_0_0,2020,Sequence-Level Mixed Sample Data Augmentation,Footnote
2530,12538," https://github.com/dolphin-zs/Doc2EDAG"," ['References']",Data and codes can be found at [Cite] https://github.com/dolphin-zs/Doc2EDAG.,,"Most existing event extraction (EE) methods merely extract event arguments within the sen-tence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sen-tences, and even multiple such event men-tions frequently co-exist in the same docu-ment. To address these challenges, we pro-pose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformal-ize a DEE task with the no-trigger-words de-sign to ease document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset con-sisting of Chinese financial announcements with the challenges mentioned above. Ex-tensive experiments with comprehensive anal-yses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at [Cite] https://github.com/dolphin-zs/Doc2EDAG.",Mixed,Mixed,True,Produce（引用目的）,True,D19-1032_0_0,2019,Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction,Body
2531,12539," https://www.ldc.upenn.edu/collaborations/past-projects/ace"," ['1 Introduction']","Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Ji and Grishman, 20080; Ji and Grishman, 20081; Ji and Grishman, 20082 have been put on EE, most of them are based on ACE 2005 [Cite_Footnote_2] , an expert-annotated benchmark, which only tagged event arguments within the sentence scope.",2 https://www.ldc.upenn.edu/collaborations/past-projects/ace,"Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 2005 [Cite_Footnote_2] , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously over-looks the arguments-scattering challenge. In con-trast, EE on financial documents, such as ChFi-nAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event.",補足資料,Website,True,Introduce（引用目的）,True,D19-1032_1_0,2019,Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction,Footnote
2532,12540," http://www.cninfo.com.cn/new/index"," ['6 Experiments', '6.1 Experimental Setup']",We uti-lize ten years (2008-2018) ChFinAnn [Cite_Footnote_4] documents and human-summarized event knowledge bases to conduct the DS-based event labeling.,4 Crawling from http://www.cninfo.com.cn/new/index,"Data Collection with Event Labeling. We uti-lize ten years (2008-2018) ChFinAnn [Cite_Footnote_4] documents and human-summarized event knowledge bases to conduct the DS-based event labeling. We focus on five event types: Equity Freeze (EF), Equity Re-purchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), which belong to major events required to be disclosed by the regulator and may have a huge impact on the company value. To ensure the labeling quality, we set constraints for matched document-record pairs as Section 4 describes. Moreover, we directly use the character tokenization to avoid error propaga-tions from Chinese word segmentation tools.",Material,DataSource,True,Use（引用目的）,True,D19-1032_2_0,2019,Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction,Footnote
2533,12541," https://github.com/nmrksic/attract-repel"," ['1 Introduction']","cialisation framework (Mrkšić et al., 2017). [Cite_Footnote_2]",2 https://github.com/nmrksic/attract-repel,"Figure 1 : An illustration of LEAR specialisation. LEAR controls the arrangement of vectors in the transformed vector space by: 1) emphasising sym-metric similarity of LE pairs through cosine dis-tance (by enforcing small angles between −−−−→terrier and −→dog or −→dog and −−−−→animal); and 2) by imposing an LE ordering using vector norms, adjusting them so that higher-level concepts have larger norms (e.g., |−−−−→animal| > |dog−→| > |−−−−→terrier|). cialisation framework (Mrkšić et al., 2017). [Cite_Footnote_2] The key idea of LEAR , illustrated by Figure 1, is to pull desirable ( ATTRACT ) examples described by the constraints closer together, while at the same time pushing undesirable ( REPEL ) word pairs away from each other. Concurrently, LEAR (re-)arranges vector norms so that norm values in the Euclidean space reflect the hierarchical organisation of con-cepts according to the given LE constraints: put simply, higher-level concepts are assigned larger norms. Therefore, LEAR simultaneously captures the hierarchy of concepts (through vector norms) and their similarity (through their cosine distance). The two pivotal pieces of information are combined into an asymmetric distance measure which quanti-fies the LE strength in the specialised space.",Mixed,Mixed,False,Produce（引用目的）,False,N18-1103_0_0,2018,Specialising Word Vectors for Lexical Entailment,Footnote
2534,12542," https://github.com/tticoin/AntonymDetection"," ['3 Experimental Setup']","In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on aver-age) and 380,873 antonymy pairs (6.5 per word). [Cite_Footnote_5]",5 https://github.com/tticoin/AntonymDetection,"Linguistic Constraints We use three groups of linguistic constraints in the LEAR specialisation model, covering three different relation types which are all beneficial to the specialisation process: di-rected 1) lexical entailment ( LE ) pairs; 2) syn-onymy pairs; and 3) antonymy pairs. Synonyms are included as symmetric ATTRACT pairs (i.e., the B A pairs) since they can be seen as defining a trivial symmetric IS - A relation (Rei and Briscoe, 2014; Vulić et al., 2017). For a similar reason, antonyms are clear REPEL constraints as they anti-correlate with the LE relation. Synonymy and antonymy constraints are taken from prior work (Zhang et al., 2014; Ono et al., 2015): they are ex-tracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009). In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on aver-age) and 380,873 antonymy pairs (6.5 per word). [Cite_Footnote_5] As in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (panga-sius, animal)). We retained only noun-noun and verb-verb pairs, while the rest were discarded: the final number of LE constraints is 1,545,630.",Material,Knowledge,True,Use（引用目的）,True,N18-1103_1_0,2018,Specialising Word Vectors for Lexical Entailment,Footnote
2535,12543," http://www.cl.cam.ac.uk/∼dk427/generality.html"," ['4 Results and Discussion', '4.1 LE Directionality and Detection']","The tasks are evaluated on three datasets used extensively in the LE literature (Roller et al., 2014; Santus et al., 2014; Weeds et al., 2014; Shwartz et al., 2017; Nguyen et al., 2017), compiled into an integrated evaluation set by Kiela et al. (2015b). [Cite_Footnote_7]",7 http://www.cl.cam.ac.uk/∼dk427/generality.html,"The first evaluation uses three classification-style tasks with increased levels of difficulty. The tasks are evaluated on three datasets used extensively in the LE literature (Roller et al., 2014; Santus et al., 2014; Weeds et al., 2014; Shwartz et al., 2017; Nguyen et al., 2017), compiled into an integrated evaluation set by Kiela et al. (2015b). [Cite_Footnote_7]",Material,Knowledge,True,Extend（引用目的）,False,N18-1103_2_0,2018,Specialising Word Vectors for Lexical Entailment,Footnote
2536,12544," http://people.ds.cam.ac.uk/iv250/hyperlex.html"," ['4 Results and Discussion', '4.2 Graded Lexical Entailment']","As shown by the high inter-annotator agreement on HyperLex (0.85), humans are able to consis-tently reason about graded LE . [Cite_Footnote_10]","10 For further details concerning HyperLex, we refer the reader to the resource paper (Vulić et al., 2017). The dataset is available at: http://people.ds.cam.ac.uk/iv250/hyperlex.html","As shown by the high inter-annotator agreement on HyperLex (0.85), humans are able to consis-tently reason about graded LE . [Cite_Footnote_10] However, current state-of-the-art representation architectures are far from this ceiling. For instance, Vulić et al. (2017) evaluate a plethora of architectures and report a high-score of only 0.320 (see the summary table in Figure 3). Two recent representation models (Nickel and Kiela, 2017; Nguyen et al., 2017) fo-cused on the LE relation in particular (and employ-ing the same set of WordNet-based constraints as LEAR ) report the highest score of 0.540 (on the entire dataset) and 0.512 (on the noun subset).",Material,Dataset,True,Produce（引用目的）,True,N18-1103_3_0,2018,Specialising Word Vectors for Lexical Entailment,Footnote
2537,12545," http://www.mturk.com"," ['5 Entrainment and dialogue quality', '5.1 Entrainment correlates of dialogue charac-teristics']","To look at more perceptual measures of dialogue quality, we used Amazon Mechanical Turk [Cite_Footnote_2] to an-notate each task (the sub-units of each game) in the Games Corpus for what we term social variables, the perceived social characteristics of an interaction and ints participants.",2 http://www.mturk.com,"To look at more perceptual measures of dialogue quality, we used Amazon Mechanical Turk [Cite_Footnote_2] to an-notate each task (the sub-units of each game) in the Games Corpus for what we term social variables, the perceived social characteristics of an interaction and ints participants. Details on the annotation pro-cess can be found in (Gravano et al., 2011). In this study, we focus on four social variables: trying to be liked, giving encouragement, trying to dominate, and conversation awkward. Based on Communica-tion Accommodation Theory (Giles et al., 1987), we expect the first two social variables, which represent the desire to minimize social distance, to be posi-tively correlated with entrainment. Someone who is trying to dominate, on the other hand, will try to in-crease social distance, and we therefore expect this variable to correlate negatively with entrainment, as should conversation awkward.",補足資料,Website,True,Use（引用目的）,True,N13-2012_0_0,2013,"Entrainment in Spoken Dialogue Systems: Adopting, Predicting and Influencing User Behavior",Footnote
2538,12546," https://github.com/baoguangsheng/g-transformer"," ['1 Introduction']",We release our code and model at [Cite] https://github.com/baoguangsheng/g-transformer.,,"We evaluate our model on three commonly used document-level MT datasets for English-German translation, covering domains of TED talks, News, and Europarl from small to large. Experiments show that G-Transformer converges faster and more stably than Transformer on dif-ferent settings, obtaining the state-of-the-art re-sults under both non-pretraining and pre-training settings. To our knowledge, we are the first to realize a truly document-by-document transla-tion model. We release our code and model at [Cite] https://github.com/baoguangsheng/g-transformer.",Method,Code,True,Produce（引用目的）,True,2021.acl-long.267_0_0,2021,G-Transformer for Document-level Machine Translation,Body
2539,12547," http://www.isi.edu"," ['1 Introduction']",We will make it available via [Cite] http://www.isi.edu.,,"In this paper, we present a large, fine-grained taxonomy of 43 noun compound relations, a dataset annotated according to this taxonomy, and a supervised, automatic classification method for determining the relation between the head and modifier words in a noun compound. We com-pare and map our relations to those in other tax-onomies and report the promising results of an inter-annotator agreement study as well as an au-tomatic classification experiment. We examine the various features used for classification and iden-tify one very useful, novel family of features. Our dataset is, to the best of our knowledge, the largest noun compound dataset yet produced. We will make it available via [Cite] http://www.isi.edu.",補足資料,Website,True,Introduce（引用目的）,True,P10-1070_0_0,2010,"A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation",Body
2540,12548," http://www.isi.edu"," ['8 Future Work']","We are hopeful that our current dataset and re-lation definitions, which will be made available via [Cite] http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.",,"Eventually, we would like to expand our data set and relations to cover proper nouns as well. We are hopeful that our current dataset and re-lation definitions, which will be made available via [Cite] http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.",補足資料,Website,True,Introduce（引用目的）,True,P10-1070_1_0,2010,"A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation",Body
2541,12549," http://mallet.cs.umass.edu.2002"," ['5 Automated Classification']","We use the implementation provided in the MALLET machine learning toolkit (McCallum, 2002) [Cite_Ref] .","McCallum, A. K. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.2002.","We use a Maximum Entropy (Berger et al., 1996) classifier with a large number of boolean features, some of which are novel (e.g., the inclusion of words from WordNet definitions). Maximum En-tropy classifiers have been effective on a variety of NLP problems including preposition sense disam-biguation (Ye and Baldwin, 2007), which is some-what similar to noun compound interpretation. We use the implementation provided in the MALLET machine learning toolkit (McCallum, 2002) [Cite_Ref] .",Method,Tool,True,Use（引用目的）,True,P10-1070_2_0,2010,"A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation",Reference
2542,12550," https://registry.opendata.aws/amazon-reviews-ml/"," ['1 Introduction']",The Multilingual Amazon Reviews Corpus (MARC) can be found at [Cite] https://registry.,,"The Multilingual Amazon Reviews Corpus (MARC) can be found at [Cite] https://registry.opendata.aws/amazon-reviews-ml/. The dataset description, code snippets, and li-cense agreement can be retrieved at https: //docs.opendata.aws/amazon-reviews-ml/ readme.html .",Material,Dataset,True,Introduce（引用目的）,True,2020.emnlp-main.369_0_0,2020,The Multilingual Amazon Reviews Corpus,Body
2543,12551," https://docs.opendata.aws/amazon-reviews-ml/readme.html"," ['1 Introduction']","The dataset description, code snippets, and li-cense agreement can be retrieved at [Cite] https: //docs.opendata.aws/amazon-reviews-ml/ readme.html .",,"The Multilingual Amazon Reviews Corpus (MARC) can be found at https://registry.opendata.aws/amazon-reviews-ml/. The dataset description, code snippets, and li-cense agreement can be retrieved at [Cite] https: //docs.opendata.aws/amazon-reviews-ml/ readme.html .",補足資料,Document,True,Produce（引用目的）,True,2020.emnlp-main.369_1_0,2020,The Multilingual Amazon Reviews Corpus,Body
2544,12552," https://github.com/fxsjy/jieba"," ['2 Data preparation', '2.2 Data Processing']",We used Jieba [Cite_Footnote_1] for Chi-nese and KyTea 2 for Japanese word segmentation.,1 https://github.com/fxsjy/jieba 2 http://www.phontron.com/kytea,"We also applied a vocabulary-based filter on the reviews. If a review contains a token that doesn’t occur in at least 20 other reviews, then the review is excluded from the dataset. We used Jieba [Cite_Footnote_1] for Chi-nese and KyTea 2 for Japanese word segmentation. The segmenters were only used during the filtering process, and the text provided in the dataset is not segmented or tokenized.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.369_2_0,2020,The Multilingual Amazon Reviews Corpus,Footnote
2545,12553," http://www.phontron.com/kytea"," ['2 Data preparation', '2.2 Data Processing']",We used Jieba [Cite_Footnote_1] for Chi-nese and KyTea 2 for Japanese word segmentation.,1 https://github.com/fxsjy/jieba 2 http://www.phontron.com/kytea,"We also applied a vocabulary-based filter on the reviews. If a review contains a token that doesn’t occur in at least 20 other reviews, then the review is excluded from the dataset. We used Jieba [Cite_Footnote_1] for Chi-nese and KyTea 2 for Japanese word segmentation. The segmenters were only used during the filtering process, and the text provided in the dataset is not segmented or tokenized.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.369_3_0,2020,The Multilingual Amazon Reviews Corpus,Footnote
2546,12554," https://lynx.invisible-island.net"," ['2 Data preparation', '2.2 Data Processing']",We used Lynx [Cite_Footnote_3] to render the reviews as UTF-8 plain-text.,3 https://lynx.invisible-island.net,Some Amazon reviews contain HTML markup. We used Lynx [Cite_Footnote_3] to render the reviews as UTF-8 plain-text.,Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.369_4_0,2020,The Multilingual Amazon Reviews Corpus,Footnote
2547,12555," https://github.com/Adaxry/GCDT"," ['References']","Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F 1 on NER and 97.30 F 1 on Chunking [Cite_Footnote_1] .",1 Code is available at: https://github.com/Adaxry/GCDT.,"Current state-of-the-art systems for the se-quence labeling tasks are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global informa-tion restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context en-hanced Deep Transition architecture for se-quence labeling named GCDT. We deepen the state transition path at each position in a sen-tence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard se-quence labeling tasks show that, given only training data and the ubiquitous word embed-dings (Glove), our GCDT achieves 91.96 F 1 on the CoNLL03 NER task and 95.43 F 1 on the CoNLL2000 Chunking task, which outper-forms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F 1 on NER and 97.30 F 1 on Chunking [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,False,P19-1233_0_0,2019,GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling,Footnote
2548,12556," https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt"," ['4 Experiments', '4.2 Implementation Details']",One layer CNN with a filter of size [Cite_Footnote_3] is utilized to generate 128-dimension word embeddings by max pooling.,3 https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt,"All trainable parameters in our model are initial-ized by the method described by Glorot and Ben-gio (2010). We apply dropout (Srivastava et al., 2014) to embeddings and hidden states with a rate of 0.5 and 0.3 respectively. All models are opti-mized by the Adam optimizer (Kingma and Ba, 2014) with gradient clipping of 5 (Pascanu et al., 2013). The initial learning rate α is set to 0.008, and decrease with the growth of training steps. We monitor the training process on the develop-ment set and report the final result on the test set. One layer CNN with a filter of size [Cite_Footnote_3] is utilized to generate 128-dimension word embeddings by max pooling. The cased, 300d Glove is adapted to initialize word embeddings, which is frozen in all models. In the auxiliary experiments, the out-put hidden states of BERT are taken as additional word embeddings and kept fixed all the time.",Method,Code,False,Use（引用目的）,False,P19-1233_2_0,2019,GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling,Footnote
2549,12557," https://github.com/JasperGuo/Unimer"," ['References']","Our benchmark, execution engines and implementation can be found on: [Cite] https: //github.com/JasperGuo/Unimer .",,"Meaning representation is an important com-ponent of semantic parsing. Although re-searchers have designed a lot of meaning rep-resentations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less un-derstood. Furthermore, existing work’s per-formance is often not comprehensively evalu-ated due to the lack of readily-available execu-tion engines. Upon identifying these gaps, we propose U NIMER , a new unified benchmark on meaning representations, by integrating ex-isting semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enu-meration of logical forms and execution en-gines over three datasets × four meaning rep-resentations. A thorough experimental study on U NIMER reveals that neural semantic pars-ing approaches exhibit notably different per-formance when they are trained to generate different meaning representations. Also, pro-gram alias and grammar rules heavily impact the performance of different meaning repre-sentations. Our benchmark, execution engines and implementation can be found on: [Cite] https: //github.com/JasperGuo/Unimer .",Method,Tool,True,Produce（引用目的）,True,2020.emnlp-main.118_0_0,2020,Benchmarking Meaning Representations in Neural Semantic Parsing,Body
2550,12558," https://github.com/JasperGuo/Unimer"," ['3 Benchmark']",We have made U NIMER along with the execution engines publicly available. [Cite_Footnote_3],"3 Our benchmark, execution engines and and our im-plementation can be found on: https://github.com/JasperGuo/Unimer",We plan to cover more domains and more MRs in U NIMER . We have made U NIMER along with the execution engines publicly available. [Cite_Footnote_3] We be-lieve that U NIMER can provide fertile soil for ex-ploring MRs and addressing challenges in semantic parsing.,Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.118_1_0,2020,Benchmarking Meaning Representations in Neural Semantic Parsing,Footnote
2551,12559," https://github.com/microsoft/nni"," ['4 Experimental Setup', '4.2 Implementations']","To make a fair compari-son, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform. [Cite_Footnote_6]",6 https://github.com/microsoft/nni,"We implement each approach with the Al-lenNLP (Gardner et al., 2018) and PyTorch (Paszke et al., 2019) frameworks. To make a fair compari-son, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform. [Cite_Footnote_6] Due to the limited number of test data in each domain, we run each approach five times and take the average number. Section A.2 in the supplementary material provides the search space of hyper-parameters for each approach and the pre-processing procedures of logical forms.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.118_2_0,2020,Benchmarking Meaning Representations in Neural Semantic Parsing,Footnote
2552,12560,http://www.cs.utexas.edu/ml/nldata/jobquery.html,"[\'-\', \'A.1 Details of Benchmark Construction\']",We directly use Prolog logical forms provided on the web-site. [Cite_Footnote_9],"9 http://www.cs.utexas.edu/ml/nldata/jobquery.html river_name := ""ohio"" | ""colorado” | … | ""red"" state_abbrev := ""dc"" | ""sd"" | … | ""me"" number := ""0"" | ""1.0""","Job. There are only annotated logical forms for Lambda Calculus and Prolog in Job. We directly use Prolog logical forms provided on the web-site. [Cite_Footnote_9] To provide annotations for FunQL, we semi-copying a source word to a logical form. In ATIS, following (Jia and Liang, 2016), we leverage an ex-ternal lexicon to identify potential copy candidates, e.g., slc:ap can be identified as a potential entity for description “salt lake city airport” in utterance. When we copy a source word that is part of a phrase in the lexicon, we write the entity associated with that lexicon entry to a logical form.",Material,Knowledge,False,Use（引用目的）,True,2020.emnlp-main.118_3_0,2020,Benchmarking Meaning Representations in Neural Semantic Parsing,Footnote
2553,12561," http://bitbucket.org/tclup/alto"," ['1 Introduction']","Our code is part of the Alto parser (Gontrum et al., 2017), available at [Cite] http://bitbucket.org/tclup/alto.",,"In this paper, we make two contributions. First, we generalize chart constraints to more expressive grammar formalisms by casting them in terms of allowable parse items that should be considered by the parser. The Roark chart constraints are the special case for PCFGs and CKY; our view applies to any grammar formalism for which a parser can be specified in terms of parsing schemata. Second, we present a neural tagger which predicts begin and end constraints with an accuracy around 98%. We show that these chart constraints speed up a PCFG parser by 18x and a TAG chart parser by 4x. Furthermore, chart constraints can be combined effectively with coarse-to-fine parsing for PCFGs (for an overall speedup of 70x) and supertagging for TAG (overall speedup of 124x), all while improving the accuracy over those of the baseline parsers. Our code is part of the Alto parser (Gontrum et al., 2017), available at [Cite] http://bitbucket.org/tclup/alto.",Method,Tool,True,Introduce（引用目的）,True,P18-2099_0_0,2018,Generalized chart constraints for efficient PCFG and TAG parsing,Body
2554,12562," http://www.cs.pitt.edu/mpqa/"," ['3 Experimental Methodology']","For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA) [Cite_Footnote_6] corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phrase-level subjectivity and intensity.",6 http://www.cs.pitt.edu/mpqa/,"For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA) [Cite_Footnote_6] corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phrase-level subjectivity and intensity. We use the expression-level boundary markings in MPQA to extract phrases. We evaluate on positive, negative and neutral opinion expressions that have intensities “medium”, “high” or “extreme”. 7 The schematic mapping of phrase polarity and intensity values on ordinal sentimental scale is shown in Table 1.",Material,Dataset,True,Use（引用目的）,True,D11-1016_0_0,2011,Compositional Matrix-Space Models for Sentiment Analysis,Footnote
2555,12563," http://www.ecdf.ed.ac.uk"," ['References']","We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility ( [Cite] http://www.ecdf.ed.ac.uk).",,"We would like to thank Phil Blunsom, Prachya Boonkwan, Christos Christodoulopoulos, Stephen Clark, Michael Collins, Chris Dyer, Timothy Fowler, Mark Granroth-Wilding, Philipp Koehn, Terry Koo, Tom Kwiatkowski, André Martins, Matt Post, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts, and the anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility ( [Cite] http://www.ecdf.ed.ac.uk). The ECDF is partially supported by the eDIKT initiative (http://www.edikt.org.uk).",補足資料,Website,True,Introduce（引用目的）,True,P11-1048_0_0,2011,A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing,Body
2556,12564," http://www.edikt.org.uk"," ['References']",The ECDF is partially supported by the eDIKT initiative ( [Cite] http://www.edikt.org.uk).,,"We would like to thank Phil Blunsom, Prachya Boonkwan, Christos Christodoulopoulos, Stephen Clark, Michael Collins, Chris Dyer, Timothy Fowler, Mark Granroth-Wilding, Philipp Koehn, Terry Koo, Tom Kwiatkowski, André Martins, Matt Post, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts, and the anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility (http://www.ecdf.ed.ac.uk). The ECDF is partially supported by the eDIKT initiative ( [Cite] http://www.edikt.org.uk).",補足資料,Website,True,Introduce（引用目的）,True,P11-1048_1_0,2011,A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing,Body
2557,12565," https://www.pwc.com/us/en/advisory-services/publications/consumer-intelligence-series/voice-assistants.pdf"," ['1 Introduction']",Music listening is among the top-5 reasons of daily usage of voice assistants in the US. [Cite_Footnote_1],1 Source: https://www.pwc.com/us/en/advisory-services/publications/consumer-intelligence-series/voice-assistants.pdf,"Music listening is among the top-5 reasons of daily usage of voice assistants in the US. [Cite_Footnote_1] Users can have different goals when formulating a music-related query to their home voice assistant or mo-bile phones. For instance, users may look for a spe-cific entity, which can be either explicit (e.g., “play Led Zeppelin”) or implicit (e.g., “play the latest al-bum by Foo Fighters”). They may also ask queries without having a specific entity in mind (e.g., “play some reggae music”), or make open-ended requests like “play something that I like” (Ostuni, 2019; Volokhin and Agichtein, 2018).",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-industry.7_0_0,2021,Bootstrapping a Music Voice Assistant with Weak Supervision,Footnote
2558,12566," http://github.com/wtimkey/rogue-dimensions"," ['1 Introduction']","Taken together, we argue that accounting for rogue dimensions is essential when evaluating representa-tional similarity in transformer language models. [Cite_Footnote_1]",1 Our code is publically released at: http://github.com/wtimkey/rogue-dimensions,"Finally, we show that these dimensions can be accounted for using a trivially simple transforma-tion of the embedding space: standardization. Once applied, cosine similarity more closely reflects hu-man word similarity judgments, and we see that representational quality is preserved across all lay-ers rather than degrading/becoming task-specific. Taken together, we argue that accounting for rogue dimensions is essential when evaluating representa-tional similarity in transformer language models. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.372_0_0,2021,All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality,Footnote
2559,12567," https://github.com/fromhuggingface/transformers"," ['3 Rogue Dimensions and', '3.1.2 Experiment']","We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. [Cite_Footnote_2]",2 All models https://github.com/fromhuggingface/transformers,"We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. [Cite_Footnote_2] We then normalize by the total expected cosine similarity Â(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia. All input sequences consisted of 128 tokens. From the resulting rep-resentations we take a random sample S of 500k token pairs. For each model, we report the three dimensions with the largest cosine contributions in the two most anisotropic layers, as well as the overall anisotropy Â( f ` ).",Method,Code,True,Introduce（引用目的）,True,2021.emnlp-main.372_1_0,2021,All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality,Footnote
2560,12568," https://zenodo.org/record/4421380"," ['3 Rogue Dimensions and', '3.1.2 Experiment']","[Cite_Footnote_3] and GloVe, for comparison.",3 https://zenodo.org/record/4421380,"We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. We then normalize by the total expected cosine similarity Â(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec [Cite_Footnote_3] and GloVe, for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia. All input sequences consisted of 128 tokens. From the resulting rep-resentations we take a random sample S of 500k token pairs. For each model, we report the three dimensions with the largest cosine contributions in the two most anisotropic layers, as well as the overall anisotropy Â( f ` ).",Method,Code,False,Compare（引用目的）,True,2021.emnlp-main.372_2_0,2021,All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality,Footnote
2561,12569," https://nlp.stanford.edu/projects/glove/"," ['3 Rogue Dimensions and', '3.1.2 Experiment']","We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, [Cite_Footnote_4] for comparison.","4 https://nlp.stanford.edu/projects/glove/ (Wikipedia+Gigaword 5, 300d)","We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. We then normalize by the total expected cosine similarity Â(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, [Cite_Footnote_4] for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia. All input sequences consisted of 128 tokens. From the resulting rep-resentations we take a random sample S of 500k token pairs. For each model, we report the three dimensions with the largest cosine contributions in the two most anisotropic layers, as well as the overall anisotropy Â( f ` ).",Method,Code,False,Compare（引用目的）,True,2021.emnlp-main.372_3_0,2021,All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality,Footnote
2562,12570," http://www.fjoch.com/YASMET.html"," ['3 Cascading Guidance Technique', 'SVM Classification']",We use the published package YASMET [Cite_Footnote_1] to conduct parameters training and classification.,1 http://www.fjoch.com/YASMET.html,We use the published package YASMET [Cite_Footnote_1] to conduct parameters training and classification. YASMET requires supervised learning for the training of maximum entropy model.,Method,Tool,True,Use（引用目的）,True,H05-1075_0_0,2005,Handling Biographical Questions with Implicature,Footnote
2563,12571," http://www.infoplease.com/people.html"," ['4 Experiments and Results', '4.1 Experimental Setup']","We download from infoplease.com [Cite_Footnote_2] and biogra-phy.com two corpora of people’s biographies, which include 24,975 and 24,345 bios respectively.",2 http://www.infoplease.com/people.html,"We download from infoplease.com [Cite_Footnote_2] and biogra-phy.com two corpora of people’s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information. To create the data set, we manually check and categorize each person as having multiple spouses, only one spouse, or no spouse. Similarly, we obtained another list of per-sons having multiple children, only one child, and no child. The sizes of data extracted are given in Table 1.",Material,Dataset,True,Use（引用目的）,True,H05-1075_1_0,2005,Handling Biographical Questions with Implicature,Footnote
2564,12572," http://www.biography.com/search/index.jsp"," ['4 Experiments and Results', '4.1 Experimental Setup']","We download from infoplease.com and biogra-phy.com [Cite_Footnote_3] two corpora of people’s biographies, which include 24,975 and 24,345 bios respectively.",3 http://www.biography.com/search/index.jsp,"We download from infoplease.com and biogra-phy.com [Cite_Footnote_3] two corpora of people’s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information. To create the data set, we manually check and categorize each person as having multiple spouses, only one spouse, or no spouse. Similarly, we obtained another list of per-sons having multiple children, only one child, and no child. The sizes of data extracted are given in Table 1.",Material,Dataset,True,Use（引用目的）,True,H05-1075_2_0,2005,Handling Biographical Questions with Implicature,Footnote
2565,12573," http://www.csie.ntu.edu.tw/~cjlin/libsvm/"," ['3 Cascading Guidance Technique', 'SVM Classification']","We use the SVM classification package LIBSVM (Chang and Lin, 2001) [Cite_Ref] in our problem.","Chang, C. and Lin, C. 2001. LIBSVM -- A library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/","We use the SVM classification package LIBSVM (Chang and Lin, 2001) [Cite_Ref] in our problem. ME Classification ME (Maximum Entropy) classification is used here to directly estimate the posterior probability for classification.",補足資料,Website,True,Introduce（引用目的）,True,H05-1075_3_0,2005,Handling Biographical Questions with Implicature,Reference
2566,12574," http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"," ['3 Cascading Guidance Technique', 'SVM Classification']","Then the classification task requires the solution of the following optimi-zation problem (Hsu et al., 2003) [Cite_Ref] :","Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-tical Guide to Support Vector Classification. Avail-able at: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.","Suppose p i , i = 1,..., n represent the training set of persons, and the classes for classifications are C ={c ,c } (for simplicity, we represent the 1 2 classes with C = { −1,1 } ). Then the classification task requires the solution of the following optimi-zation problem (Hsu et al., 2003) [Cite_Ref] :",補足資料,Paper,True,Introduce（引用目的）,True,H05-1075_4_0,2005,Handling Biographical Questions with Implicature,Reference
2567,12575," https://github.com/dykang/cgraph"," ['5 Results', '5.1 Data and Metrics']",The news dataset [Cite_Footnote_1] we used for stock price predic-tion contains news crawled from 2010 to 2013 us-ing Google News APIs and New York Times data from 1989 to 2007.,1 https://github.com/dykang/cgraph,"The news dataset [Cite_Footnote_1] we used for stock price predic-tion contains news crawled from 2010 to 2013 us-ing Google News APIs and New York Times data from 1989 to 2007. We construct PCG from the time series representation of its 12,804 unigrams and 25,909 bigrams over the entire news corpora of more than 23 years, as well as the 10 stock prices from 2010 to 2012 for training and 2013 as test data for prediction. The prediction is done with varying step sizes (1,3,5), which indicates the time lag between the news data and the day of the predicted stock price in days. The results shown in Table 1 is the root mean squared error (RMSE) in predicted stock value calculated on a 30 day win-dow averaged by moving it by 10 days over the pe-riod and directly comparable to our baseline (Kang et al., 2017). To evaluate the time-varying fac-tors over a larger time window, we present average monthly cross validation RMSE % sampled over a 4 year window of 2010-13 in Table 3. Please note that the results in Table 3 are not comparable with (Kang et al., 2017) as we report a cross validation error over a longer time window.",Material,Dataset,True,Use（引用目的）,True,D19-1238_0_0,2019,Identifying Predictive Causal Factors from News Streams,Footnote
2568,12576," https://finance.yahoo.com"," ['5 Results', '5.1 Data and Metrics']","We construct PCG from the time series representation of its 12,804 unigrams and 25,909 bigrams over the entire news corpora of more than 23 years, as well as the 10 stock prices [Cite_Footnote_2] from 2010 to 2012 for training and 2013 as test data for prediction.",2 https://finance.yahoo.com,"The news dataset we used for stock price predic-tion contains news crawled from 2010 to 2013 us-ing Google News APIs and New York Times data from 1989 to 2007. We construct PCG from the time series representation of its 12,804 unigrams and 25,909 bigrams over the entire news corpora of more than 23 years, as well as the 10 stock prices [Cite_Footnote_2] from 2010 to 2012 for training and 2013 as test data for prediction. The prediction is done with varying step sizes (1,3,5), which indicates the time lag between the news data and the day of the predicted stock price in days. The results shown in Table 1 is the root mean squared error (RMSE) in predicted stock value calculated on a 30 day win-dow averaged by moving it by 10 days over the pe-riod and directly comparable to our baseline (Kang et al., 2017). To evaluate the time-varying fac-tors over a larger time window, we present average monthly cross validation RMSE % sampled over a 4 year window of 2010-13 in Table 3. Please note that the results in Table 3 are not comparable with (Kang et al., 2017) as we report a cross validation error over a longer time window.",Material,Knowledge,True,Use（引用目的）,True,D19-1238_1_0,2019,Identifying Predictive Causal Factors from News Streams,Footnote
2569,12577," https://timesofindia.indiatimes.com/archive.cms"," ['6 Interpretation of Predictive Causal Factors']","[Cite_Footnote_3] , the most circulated Indian English newspaper.",3 https://timesofindia.indiatimes.com/archive.cms,"In order to qualitatively validate that the latent inter-topic edges learnt from the news stream is also humanly interpretable, we constructed PCG from the online archives of Times of India (TOI) [Cite_Footnote_3] , the most circulated Indian English newspaper. We used this dataset as, unlike the previous dataset which provided just the time series of words, we also have the raw text of the articles, which al-lowed us to perform manual causal signature ver-ification. This dataset contains all the articles published in their online edition between Jan-uary 1, 2006 and December 31, 2015 containing 1,538,932 articles.",Material,DataSource,True,Extend（引用目的）,True,D19-1238_2_0,2019,Identifying Predictive Causal Factors from News Streams,Footnote
2570,12578," https://doi.org/10.1145/2623330.2623709"," ['2 Related Work']","FBLG (Cheng et al., 2014) [Cite_Ref] focused on discovering temporal dependency from time series data and applied it to a Twitter dataset mention-ing the Haiti earthquake.","Dehua Cheng, Mohammad Taha Bahadori, and Yan Liu. 2014. FBLG: A Simple and Effective Ap-proach for Temporal Dependence Discovery from Time Series Data (KDD ’14). 382–391. https: //doi.org/10.1145/2623330.2623709","Apart from using causality, there are many other methods explored to extract information from news and are used in time series based forecasting. Amodeo et al. (Amodeo et al., 2011) proposed a hybrid model consisting of time-series analysis, to predict future events using the New York Times corpus. FBLG (Cheng et al., 2014) [Cite_Ref] focused on discovering temporal dependency from time series data and applied it to a Twitter dataset mention-ing the Haiti earthquake. Similar work by Luo et al. (Luo et al., 2014) showed correlations between real-world events and time-series data for inci-dent diagnosis in online services. Other similar works like, Trend Analysis Model (TAM) (Kawa-mae, 2011) and Temporal-LDA (TM-LDA) (Wang et al., 2012) model the temporal aspect of topics in social media streams like Twitter. Structured data extraction from news have also been used for stock price prediction using techniques of information retrieval in (Ding et al., 2014; Xie et al., 2013; Ding et al., 2015; Chang et al., 2016; Ding et al., 2016). Vaca et al. (Vaca et al., 2014) used a collec-tive matrix factorization method to track emerg-ing, fading and evolving topics in news streams. PCG is inspired by such time series models and leverages the Granger causality detection frame-work for the trend prediction task.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_7_0,2019,Identifying Predictive Causal Factors from News Streams,Reference
2571,12579," https://www.aclweb.org/anthology/N10-1138"," ['1 Introduction']","PCG differs from existing relationship extraction (Das et al., 2010) [Cite_Ref] and rep-resentational frameworks (Mikolov et al., 2013) across two dimensions.","Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Human Language Tech-nologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Com-putational Linguistics, Los Angeles, California, 948–956. https://www.aclweb.org/anthology/N10-1138","This paper aims to uncover latent relationships between words describing events in news streams, allowing us to unveil hidden links between events spread across time, and integrate them into a news-based predictive model for stock prices. We propose the Predictive Causal Graphs (PCG), a framework allowing us to detect latent relation-ships between words when such relationships are not directly observed. PCG differs from existing relationship extraction (Das et al., 2010) [Cite_Ref] and rep-resentational frameworks (Mikolov et al., 2013) across two dimensions. First, PCG identifies un-supervised causal relationships based on consis-tent time series prediction instead of association, allowing us to uncover paths of influence between news items. Second, PCG finds inter-topic in-fluence relationships outside the “context” or the confines of a single document. Construction of PCG naturally leads to news-dependent predictive models for numerous variables, like stock prices.",補足資料,Paper,True,Compare（引用目的）,True,D19-1238_9_0,2019,Identifying Predictive Causal Factors from News Streams,Reference
2572,12580," https://www.aclweb.org/anthology/N10-1138"," ['5 Results', '5.2 Prediction Performance of PCG']","Baseline: Kang et al. (2017) extract relevant variables based on a semantic parser - SEMAFOR (Das et al., 2010) [Cite_Ref] by filtering causation related frames from news corpora, topics and sentiments from tweets.","Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Human Language Tech-nologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Com-putational Linguistics, Los Angeles, California, 948–956. https://www.aclweb.org/anthology/N10-1138","Baseline: Kang et al. (2017) extract relevant variables based on a semantic parser - SEMAFOR (Das et al., 2010) [Cite_Ref] by filtering causation related frames from news corpora, topics and sentiments from tweets. To overcome the problem of low re-call, they adopt a topic-based knowledge base ex-pansion. This expanded dataset is then used to train a neural reasoning model which generates se-quence of cause-effect statements using an atten-tion model where the words are represented using word2vec vectors. (Kang et al., 2017)’s CGRAPH based forecasting model - C best model uses the top 10 such generated cause features, given the stock name as the effect and apply a vector auto-regressive model on the combined time series of text and historical stock values.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_9_1,2019,Identifying Predictive Causal Factors from News Streams,Reference
2573,12581," https://www.aclweb.org/anthology/N10-1138"," ['5 Results', '5.2 Prediction Performance of PCG']","The nodes of CGRAPH are tuples extracted from a seman-tic parser (SEMAFOR (Das et al., 2010) [Cite_Ref] ) based on evidence of causality in a sentence.","Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Human Language Tech-nologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Com-putational Linguistics, Los Angeles, California, 948–956. https://www.aclweb.org/anthology/N10-1138","Key PCG factors for 2013: The causal links in PCG are more generic (Table 2) than the ones described in CGRAPH, supporting the hypothesis that latent word relationships do exist that go be-yond the scope of a single news article. The nodes of CGRAPH are tuples extracted from a seman-tic parser (SEMAFOR (Das et al., 2010) [Cite_Ref] ) based on evidence of causality in a sentence. PCG poses no such restriction and derives topical (unfriended, FB) and inter-topical (healthcare, AMZN), sparse, latent and semantic relationships.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_9_2,2019,Identifying Predictive Causal Factors from News Streams,Reference
2574,12582," https://doi.org/10.1016/j.dss.2013.02.006"," ['1 Introduction']","Ex-isting work on stock price prediction using news have typically relied on extracting features from financial news (Falinouss, 2007; Hagenau et al., 2013 [Cite_Ref] ), or sentiments expressed on Twitter (Mao et al., 2011; Rao and Srivastava, 2012; Bernardo et al., 2018), or by focusing on features present in a single document (Kalyani et al., 2016; Shynke-vich et al., 2015).","Michael Hagenau, Michael Liebmann, and Dirk Neu-mann. 2013. Automated news reading: Stock price prediction based on financial news using context-capturing features. Decision Support Systems 55, 3 (2013), 685 – 697. https://doi.org/10.1016/j.dss.2013.02.006","Contextual embedding models (Devlin et al., 2018) have managed to produce effective repre-sentations of words, achieving state-of-the-art per-formance on a range of NLP tasks. In this pa-per, we consider a specific task of predicting vari-ations in stock prices based on word relationships extracted from news streams. Existing word em-bedding techniques are not suited to learn relation-ships between words appearing in different docu-ments and contexts (Le and Mikolov, 2014). Ex-isting work on stock price prediction using news have typically relied on extracting features from financial news (Falinouss, 2007; Hagenau et al., 2013 [Cite_Ref] ), or sentiments expressed on Twitter (Mao et al., 2011; Rao and Srivastava, 2012; Bernardo et al., 2018), or by focusing on features present in a single document (Kalyani et al., 2016; Shynke-vich et al., 2015). However, relationships between events affecting stock prices can be quite com-plex, and their mentions can be spread across mul-tiple documents. For instance, market volatility is known to be triggered by recessions; this relation-ship may be reflected with a spike in the frequency of the word ”recession” followed by a spike in the frequency of the word ”volatility” a few weeks later. Existing methods are not well-equipped to deal with these cases.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_12_0,2019,Identifying Predictive Causal Factors from News Streams,Reference
2575,12583," https://doi.org/10.1145/2623330.2623374"," ['2 Related Work']","(Luo et al., 2014) [Cite_Ref] showed correlations between real-world events and time-series data for inci-dent diagnosis in online services.","Chen Luo, Jian-Guang Lou, Qingwei Lin, Qiang Fu, Rui Ding, Dongmei Zhang, and Zhe Wang. 2014. Correlating Events with Time Series for Incident Di-agnosis (KDD ’14). 1583–1592. https://doi.org/10.1145/2623330.2623374","Apart from using causality, there are many other methods explored to extract information from news and are used in time series based forecasting. Amodeo et al. (Amodeo et al., 2011) proposed a hybrid model consisting of time-series analysis, to predict future events using the New York Times corpus. FBLG (Cheng et al., 2014) focused on discovering temporal dependency from time series data and applied it to a Twitter dataset mention-ing the Haiti earthquake. Similar work by Luo et al. (Luo et al., 2014) [Cite_Ref] showed correlations between real-world events and time-series data for inci-dent diagnosis in online services. Other similar works like, Trend Analysis Model (TAM) (Kawa-mae, 2011) and Temporal-LDA (TM-LDA) (Wang et al., 2012) model the temporal aspect of topics in social media streams like Twitter. Structured data extraction from news have also been used for stock price prediction using techniques of information retrieval in (Ding et al., 2014; Xie et al., 2013; Ding et al., 2015; Chang et al., 2016; Ding et al., 2016). Vaca et al. (Vaca et al., 2014) used a collec-tive matrix factorization method to track emerg-ing, fading and evolving topics in news streams. PCG is inspired by such time series models and leverages the Granger causality detection frame-work for the trend prediction task.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_18_0,2019,Identifying Predictive Causal Factors from News Streams,Reference
2576,12584," https://doi.org/10.1017/CBO9780511809071.007"," ['3 Predictive Causal Graph', '3.1 Selecting Informative Words:']","(Manning et al., 2008) [Cite_Ref] .","Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Scoring, term weighting, and the vector space model. Cambridge University Press, 100123. https://doi.org/10.1017/CBO9780511809071.007","Only a small percentage of the words appearing in news can be used for meaningful information extraction and analysis (Manning et al., 1999; Ho-vold, 2005). Specifically, we eliminated too fre-quent (at least once in more than 50% of the days) or too rare (appearing in less than 100 articles) (Manning et al., 2008) [Cite_Ref] . Many common English nouns, adjectives and verbs, whose contribution to semantics is minimal (Forman, 2003) were also removed from the vocabulary. However, named-entities were retained for their newsworthiness and a set of “trigger” words were retained that de-pict events (e.g. flood, election) using an existing “event trigger” detection algorithm (Ahn, 2006). The vocabulary set was enhanced by adding bi-grams that are significantly collocated in the cor-pus, such as, ‘fuel price’ and ‘prime minister’ etc.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_19_0,2019,Identifying Predictive Causal Factors from News Streams,Reference
2577,12585," https://doi.org/10.1109/IJCNN.2015.7280517"," ['1 Introduction']","Ex-isting work on stock price prediction using news have typically relied on extracting features from financial news (Falinouss, 2007; Hagenau et al., 2013), or sentiments expressed on Twitter (Mao et al., 2011; Rao and Srivastava, 2012; Bernardo et al., 2018), or by focusing on features present in a single document (Kalyani et al., 2016; Shynke-vich et al., 2015 [Cite_Ref] ).","Y. Shynkevich, T. M. McGinnity, S. Coleman, and A. Belatreche. 2015. Stock price prediction based on stock-specific and sub-industry-specific news ar-ticles. In 2015 International Joint Conference on Neural Networks (IJCNN). 1–8. https://doi.org/10.1109/IJCNN.2015.7280517","Contextual embedding models (Devlin et al., 2018) have managed to produce effective repre-sentations of words, achieving state-of-the-art per-formance on a range of NLP tasks. In this pa-per, we consider a specific task of predicting vari-ations in stock prices based on word relationships extracted from news streams. Existing word em-bedding techniques are not suited to learn relation-ships between words appearing in different docu-ments and contexts (Le and Mikolov, 2014). Ex-isting work on stock price prediction using news have typically relied on extracting features from financial news (Falinouss, 2007; Hagenau et al., 2013), or sentiments expressed on Twitter (Mao et al., 2011; Rao and Srivastava, 2012; Bernardo et al., 2018), or by focusing on features present in a single document (Kalyani et al., 2016; Shynke-vich et al., 2015 [Cite_Ref] ). However, relationships between events affecting stock prices can be quite com-plex, and their mentions can be spread across mul-tiple documents. For instance, market volatility is known to be triggered by recessions; this relation-ship may be reflected with a spike in the frequency of the word ”recession” followed by a spike in the frequency of the word ”volatility” a few weeks later. Existing methods are not well-equipped to deal with these cases.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_24_0,2019,Identifying Predictive Causal Factors from News Streams,Reference
2578,12586," http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-06826"," ['2 Related Work']","More recently, (Wang and Blei, 2018) [Cite_Ref] showed that with multiple causal factors, it is possible to lever-age the correlation of those multiple causal fac-tors and deconfound using a latent variable model.",Yixin Wang and David M. Blei. 2018. The Blessings of Multiple Causes. CoRR abs/1805.06826. http: //dblp.uni-trier.de/db/journals/ corr/corr1805.html#abs-1805-06826,"Deriving true causality from observational stud-ies has been studied extensively. One of the most widely used algorithm is to control for vari-ables which satisfy the backdoor criterion (Pearl, 2009). This however, requires a knowledge of the causal graph and the unconfoundedness assump-tion that there are no other unobserved confound-ing variables. While the unconfoundedness as-sumption is to some extent valid when we ana-lyze all news streams (under the assumption that all significant events are reported), it is still hard to get away from the causal graph requirement. Propensity score based matching aims to control for most confounding variables by using an ex-ternal method for estimating and controlling for the likelihood of outcomes (Olteanu et al., 2017). More recently, (Wang and Blei, 2018) [Cite_Ref] showed that with multiple causal factors, it is possible to lever-age the correlation of those multiple causal fac-tors and deconfound using a latent variable model. This setting is similar to the one we consider, and is guaranteed to be truly causal if there is no con-founder which links a single cause and the out-come. This assumption is less strict than the un-confoundedness assumption and makes the case for using predictive causality in such scenarios. Another approach taken by (Athey and Imbens, 2016) estimates heterogeneous treatment effects by honest estimation where the model selection and factor weight estimation is done on two sub-populations of data by extending regression trees.",補足資料,Paper,True,Introduce（引用目的）,True,D19-1238_26_0,2019,Identifying Predictive Causal Factors from News Streams,Reference
2579,12587," http://tac.nist.gov/2009/Summarization/"," ['3 Experiments']",Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [Cite_Footnote_1],"1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/","Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [Cite_Footnote_1] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words. We use only the so-called initial sum-maries (A summaries), but not the update part.",補足資料,Website,True,Introduce（引用目的）,True,P17-2005_0_0,2017,A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments,Footnote
2580,12588," http://tac.nist.gov/2008/Summarization/"," ['3 Experiments']",Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [Cite_Footnote_1],"1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/","Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [Cite_Footnote_1] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words. We use only the so-called initial sum-maries (A summaries), but not the update part.",補足資料,Website,True,Introduce（引用目的）,True,P17-2005_1_0,2017,A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments,Footnote
2581,12589," https://github.com/MiuLab/TC-Bot"," ['4 Experiments', '4.3 Task-Oriented Dialogue Learning']","We use the simulator [Cite_Footnote_2] as in Li et al. (2016c) to generate user utterances, and the threshold interval is set to a range between 0.45 and 0.55.",2 https://github.com/MiuLab/TC-Bot,"Dataset Following the previous work (Peng et al., 2018; Su et al., 2018), we use the same Movie-Ticket Booking dataset collected from Amazon Mechanical Turk for evaluation. The dataset is manually labeled based on a schema de-fined by domain experts consisting of 11 intents and 16 slots in the full domain setting. In total, the dataset has 280 annotated dialogues with an aver-age length of approximately 11 turns. In this sce-nario, the goal of dialogue systems is to help the user complete the tasks through the conversation. Baselines We compare our SSN-based dis-criminator within the state-of-the-art task-oriented dialogue policy learning approach, Discriminative Deep Dyna-Q (D3Q) (Su et al., 2018). At each turn, the D3Q agent takes S planning steps inter-acting with the simulator and store stimulated user experiences based on the scoring of the discrimi-nator. The stimulated user experiences are gener-ated by the world model, which can be viewed as the generator G in our case. We replace the con-ventional discriminator D of D3Q with our SSN . Implementation Details For a fair comparison, we remain most of the parameters in the D3Q al-gorithm the same as in Su et al. (2018). In the self-supervised network, the dimension of the ut-terance embeddings is 80. The hidden size is 128 for utterance encoding bi-LSTM and 512 for triple reasoning bi-LSTM. The MLP has a single hidden layer of size 128. We use the simulator [Cite_Footnote_2] as in Li et al. (2016c) to generate user utterances, and the threshold interval is set to a range between 0.45 and 0.55.",Method,Tool,True,Use（引用目的）,True,P19-1375_0_0,2019,Self-Supervised Dialogue Learning,Footnote
2582,12590," http://berouge.com/default.aspx"," ['2 Related Work']","Both ROUGE and BE have been implemented and included in the ROUGE/BE evaluation toolkit [Cite_Footnote_1] , which has been used as the default evaluation tool in the summarization track in the Document Un-derstanding Conference (DUC) and Text Analysis Conference (TAC).",1 http://berouge.com/default.aspx,"Both ROUGE and BE have been implemented and included in the ROUGE/BE evaluation toolkit [Cite_Footnote_1] , which has been used as the default evaluation tool in the summarization track in the Document Un-derstanding Conference (DUC) and Text Analysis Conference (TAC). DUC and TAC also manually evaluated machine generated summaries by adopt-ing the Pyramid method. Besides evaluating with ROUGE/BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall re-sponsiveness.",Method,Tool,True,Introduce（引用目的）,False,P12-1106_0_0,2012,Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation,Footnote
2583,12591," http://wing.comp.nus.edu.sg/~linzihen/parser/"," ['4 DICOMER: Evaluating Summary Readability', '4.3 Predicting Readability Scores']","To obtain the discourse relations of a summary, we use the discourse parser [Cite_Footnote_2] developed in Lin et al. (2010).",2 http://wing.comp.nus.edu.sg/˜linzihen/parser/,"We use the data from AESOP 2009 and 2010 as the training data, and test our metrics on AESOP 2011 data. To obtain the discourse relations of a summary, we use the discourse parser [Cite_Footnote_2] developed in Lin et al. (2010).",Method,Tool,True,Use（引用目的）,True,P12-1106_1_0,2012,Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation,Footnote
2584,12592," http://wing.comp.nus.edu.sg/~linzihen/summeval/"," ['7 Conclusion']","Experimental results on AESOP 2011 show that DICOMER significantly outperforms all sub-mitted metrics on both initial and update tasks with large gaps, while TESLA-S and CREMER signifi-cantly outperform all metrics on the initial task. [Cite_Footnote_3]",3 Our metrics are publicly available at http://wing.comp.nus.edu.sg/˜linzihen/summeval/.,"We proposed TESLA-S by adapting an MT eval-uation metric to measure summary content cover-age, and introduced DICOMER by applying a dis-course coherence model with newly introduced fea-tures to evaluate summary readability. We com-bined these two metrics in the CREMER metric – an SVM-trained regression model – for auto-matic summarization overall responsiveness evalu-ation. Experimental results on AESOP 2011 show that DICOMER significantly outperforms all sub-mitted metrics on both initial and update tasks with large gaps, while TESLA-S and CREMER signifi-cantly outperform all metrics on the initial task. [Cite_Footnote_3]",Method,Code,True,Produce（引用目的）,True,P12-1106_2_0,2012,Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation,Footnote
2585,12593," https://github.com/adalmia96/Cluster-Analysis"," ['4 Computational Complexity']",5 Experimental Setup Our implementation is freely available online. [Cite_Footnote_4],4 https://github.com/adalmia96/ Cluster-Analysis,"ter (?) without weighting, while the figure on the right shows that after weighting (larger points have higher weight) a hopefully more representative cluster center is found. Note that top words based on distance from the cluster center could still very well be low frequency word types, motivating reranking (§3.3). 3.1 Obtaining top-J words In traditional topic modeling (LDA), the top J words are those with highest probability under each topic-word distribution. For centroid based clus-tering algorithms, the top words of some cluster i are naturally those closest to the cluster center c (i) , or with highest probability under the cluster parameters. Formally, this means choosing the set of types J as 3.2 Weighting while clustering The intuition of weighted clustering is based on the formulation of classical LDA which models the probability of the word type t belonging to a topic i as PN t,i +β t , where N t,i refers to the number of t0 N t0i +β t0 times word type t has been assigned to topic i, and 4.1 Cost of obtaining Embeddings For readily available pretrained word embeddings such as word2vec, FastText, GloVe and Spherical, ments and therefore do not consider the runtime of training these models from scratch. 5 Experimental Setup Our implementation is freely available online. [Cite_Footnote_4] 5.1 Datasets We use the 20 newsgroup dataset (20NG) which contains around 18000 documents and 20 cate-gories, 5 and a subset of Reuters21578 6 which con-tains around 10000 documents. 5.2 Evaluation (Topic Coherence) We adopt a standard 60-40 train-test split for 20NG and 70-30 for Reuters. NPMI, for k-means (KM) before and after reranking (KM r ): reranking clearly improves NPMI for BERT and Spherical. (p < 0.05). 7 6.3 Reranking duced by term frequency on 20NG. Embeddings are more sensitive to noisy vocabulary (infrequent terms) than LDA, but reweighting ( w ) helps to alleviate this. A k-means (KM) vs k-medoids (KD) To further understand the effect of other centroid Top 10 Word for Each Topic NPMI dollar rate rates exchange currency market dealers central interest point year growth rise government economic economy expected domestic inflation report gold reserves year tons company production exploration ounces feet mine billion year rose dlrs fell marks earlier figures surplus rise year tonnes crop production week grain sugar estimated expected area dlrs company sale agreement unit acquisition assets agreed subsidiary sell bank billion banks money interest market funds credit debt loans tonnes wheat export sugar tonne exports sources shipment sales week plan bill industry farm proposed government administration told proposal change prices production price crude output barrels barrel increase demand industry group company investment stake firm told companies capital chairman president trade countries foreign officials told official world government imports agreement offer company shares share dlrs merger board stock tender shareholders shares stock share common dividend company split shareholders record outstanding dlrs year quarter earnings company share sales reported expects results market analysts time added long analyst term noted high back coffee meeting stock producers prices export buffer quotas market price loss dlrs profit shrs includes year gain share mths excludes",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.135_0_0,2020,Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!,Footnote
2586,12594," http://qwone.com/~jason/20Newsgroups/"," ['4 Computational Complexity']",The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [Cite_Footnote_5] random seeds.,5 http://qwone.com/˜jason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html,"For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [Cite_Footnote_5] random seeds.",Material,Dataset,True,Use（引用目的）,False,2020.emnlp-main.135_1_0,2020,Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!,Footnote
2587,12595," https://www.nltk.org/book/ch02.html"," ['4 Computational Complexity']",The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [Cite_Footnote_5] random seeds.,5 http://qwone.com/˜jason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html,"For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [Cite_Footnote_5] random seeds.",Material,Dataset,True,Use（引用目的）,False,2020.emnlp-main.135_2_0,2020,Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!,Footnote
2588,12596," https://github.com/IBM/twitter-customer-care-document-prediction"," ['References']",We also introduce a new pub-lic dataset [Cite_Footnote_1] which supports the aforementioned problem.,1 The Twitter dataset is available at: https://github.com/IBM/ twitter-customer-care-document-prediction,"A frequent pattern in customer care conversa-tions is the agents responding with appropri-ate webpage URLs that address users’ needs. We study the task of predicting the documents that customer care agents can use to facilitate users’ needs. We also introduce a new pub-lic dataset [Cite_Footnote_1] which supports the aforementioned problem. Using this dataset and two others, we investigate state-of-the-art deep learning (DL) and information retrieval (IR) models for the task. We also analyze the practicality of such systems in terms of inference time complex-ity. Our results show that an hybrid IR+DL approach provides the best of both worlds.",Material,Dataset,True,Introduce（引用目的）,False,2020.emnlp-main.25_0_0,2020,Conversational Document Prediction to Assist Customer Care Agents,Footnote
2589,12597," https://github.com/IBM/twitter-customer-care-document-prediction"," ['2 Data']","We also release a new Twitter dataset, containing conversations between users and CC agents in 25 organizations on the Twitter platform [Cite_Footnote_3] .","3 The Twitter dataset is available at: https://github.com/IBM/ twitter-customer-care-document-prediction based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, tion set of Telco-Support dataset for dialog context se-quence input handling.","We explore the CDP task using three datasets which contain human-to-human conversations between users and CC agents. Two of these datasets are internal: one from an internal customer support service on Mac devices (Mac-Support) and another from an external client in the telecommunication domain (Telco-Support). We also release a new Twitter dataset, containing conversations between users and CC agents in 25 organizations on the Twitter platform [Cite_Footnote_3] . We summarize the statistics of the three datasets in Table 2.",Material,Dataset,True,Extend（引用目的）,False,2020.emnlp-main.25_2_0,2020,Conversational Document Prediction to Assist Customer Care Agents,Footnote
2590,12598," https://github.com/IBM/MDfromHTML"," ['URL documents']",The tools for data prepro-cessing are available here: [Cite] https://github.com/IBM/MDfromHTML.,,"For the internal Mac-Support dataset, the docu-ment content for each URL was obtained by API calls to the customer service knowledge base. For the Telco-Support and Twitter datasets, we cap-ture the HTML content using a Selenium Chrome webdriver, which renders the URL document by loading all CSS styling and Javascript. The ex-tracted HTML was cleaned through a Markdown generation pipeline, where we manually identify and filter the DOM tags (using CSS id and/or class) which correspond to header(s), footer, navigation bars etc. This process is repeated for each URL domain in both datasets. The tools for data prepro-cessing are available here: [Cite] https://github.com/IBM/MDfromHTML.",Method,Tool,True,Introduce（引用目的）,True,2020.emnlp-main.25_3_0,2020,Conversational Document Prediction to Assist Customer Care Agents,Body
2591,12599," https://en.wikipedia.org/wiki/SinaWeibo"," ['2 Dataset Preparation', '2.1 Data Collection']","To study the profile consistency identification prob-lem, we use data from Weibo [Cite_Footnote_1] , a popular and plenti-ful Chinese social media, in which people routinely respond to different posts and have publicly avail-able profiles, such as gender and location.",1 https://en.wikipedia.org/wiki/Sina Weibo,"To study the profile consistency identification prob-lem, we use data from Weibo [Cite_Footnote_1] , a popular and plenti-ful Chinese social media, in which people routinely respond to different posts and have publicly avail-able profiles, such as gender and location. We fol-low the protocol of the previous profile-based dia-logue dataset (Qian et al., 2018; Zheng et al., 2019) to collect Weibo post-response pairs, together with users’ available profiles. Here we filter out overly long or short pairs and finally obtain a tuple pool that contains about 30 million tuples, which are in a {profile, post, response} format. Each pro-file includes three popular attributes: gender, lo-cation and constellation, and organized in a key-value format. For instance, {gender: female, loca-tion:Beijing, constellation: Aquarius}. This format is widely applied in real-world dialogue systems, such as Bowden et al. (2017), Shum et al. (2018), and Pichl et al. (2018).",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.539_0_0,2020,Profile Consistency Identification for Open-domain Dialogue Agents,Footnote
2592,12600," https://github.com/songhaoyu/KvPI"," ['Task I: Reranking Retrieved Responses', '5.5 Reproducibility']","The code, data, and trained model are available at [Cite] https://github.com/songhaoyu/KvPI.",,"The code, data, and trained model are available at [Cite] https://github.com/songhaoyu/KvPI.",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.539_1_0,2020,Profile Consistency Identification for Open-domain Dialogue Agents,Body
2593,12601," https://github.com/anjalief/unsupervised_gender_bias"," ['References']","Ulti-mately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. [Cite_Footnote_1]",1 Code and pre-trained models are available at https: //github.com/anjalief/unsupervised_ gender_bias,"Despite their prevalence in society, social bi-ases are difficult to identify, primarily because human judgements in this domain can be un-reliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other ar-tifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments di-rected towards other female public figures fo-cus on appearance and sexualization. Ulti-mately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,2020.emnlp-main.44_0_0,2020,Unsupervised Discovery of Implicit Gender Bias,Footnote
2594,12602," https://github.com/thunlp/SDLM-pytorch"," ['References']",Source code and data used in the experiments can be accessed at [Cite] https://github.com/thunlp/SDLM-pytorch.,,"Most language modeling methods rely on large-scale data to statistically learn the se-quential patterns of words. In this pa-per, we argue that words are atomic lan-guage units but not necessarily atomic seman-tic units. Inspired by HowNet, we use se-memes, the minimum semantic units in hu-man languages, to represent the implicit se-mantics behind words for language model-ing, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe dis-tribution given textual context. Afterwards, it regards each sememe as a distinct semantic ex-pert, and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language mod-els to work beyond word-level manipulation to fine-grained sememe-level semantics, and of-fers us more powerful tools to fine-tune lan-guage models and improve the interpretabil-ity as well as the robustness of language mod-els. Experiments on language modeling and the downstream application of headline gener-ation demonstrate the significant effectiveness of SDLM. Source code and data used in the experiments can be accessed at [Cite] https://github.com/thunlp/SDLM-pytorch.",Mixed,Mixed,True,Produce（引用目的）,True,D18-1493_0_0,2018,Language Modeling with Sparse Product of Sememe Experts,Body
2595,12603," https://github.com/google-research/bert"," ['4 Experiments', '4.1 Datasets and Experimental Settings']","We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [Cite_Footnote_8] .",8 We use Google’s official base models. See: https://github.com/google-research/bert and https://github.com/google-research/albert.,"We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [Cite_Footnote_8] . Three sets of NLP tasks are used for evaluation, with the statistics of all the seven public datasets reported in Table 1:",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.768_0_0,2021,Meta Distant Transfer Learning for Pre-trained Language Models,Footnote
2596,12604," https://github.com/google-research/albert"," ['4 Experiments', '4.1 Datasets and Experimental Settings']","We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [Cite_Footnote_8] .",8 We use Google’s official base models. See: https://github.com/google-research/bert and https://github.com/google-research/albert.,"We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [Cite_Footnote_8] . Three sets of NLP tasks are used for evaluation, with the statistics of all the seven public datasets reported in Table 1:",Method,Code,False,Use（引用目的）,True,2021.emnlp-main.768_1_0,2021,Meta Distant Transfer Learning for Pre-trained Language Models,Footnote
2597,12605," http://bit.ly/9rs6pa"," ['3 Problem Definition']",[Cite] http://bit.ly/9rs6pa #Hussein via @NewsBusters #tcot ..”,,(rumor) “RT @johnnyA99 Ann Coulter Tells Larry King Why People Think Obama Is A Muslim [Cite] http://bit.ly/9rs6pa #Hussein via @NewsBusters #tcot ..”,補足資料,Document,True,Introduce（引用目的）,True,D11-1147_0_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Body
2598,12606," http://bit.ly/bk42ZQ”"," ['3 Problem Definition']",[Cite] http://bit.ly/bk42ZQ”,,"(doubtful) “President Barack Obama’s Religion: Christian, Muslim, or Agnostic? - The News of Today (Google): Share With Friend... [Cite] http://bit.ly/bk42ZQ”",補足資料,Document,True,Introduce（引用目的）,True,D11-1147_1_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Body
2599,12607," http://twitter.com/about"," ['4 Data']","As September 2010, Twitter reports that its users publish nearly 95 million tweets per day [Cite_Footnote_1] .",1 http://twitter.com/about,"As September 2010, Twitter reports that its users publish nearly 95 million tweets per day [Cite_Footnote_1] . This makes Twitter an excellent case to analyze misin-formation in social media.",補足資料,Document,True,Introduce（引用目的）,True,D11-1147_2_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Footnote
2600,12608," http://ow.ly/iNxF”"," ['4 Data', '4.1 Annotation']",[Cite] http://ow.ly/iNxF”,,"(1) “Sarah and Todd Palin to divorce, according to local Alaska paper. [Cite] http://ow.ly/iNxF”",補足資料,Document,True,Introduce（引用目的）,True,D11-1147_3_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Body
2601,12609," http://ff.im/62Evd”"," ['4 Data', '4.1 Annotation']",(12) “Sarah Palin Divorce Rumor Debunked on Face-book [Cite] http://ff.im/62Evd”,,(12) “Sarah Palin Divorce Rumor Debunked on Face-book [Cite] http://ff.im/62Evd”,補足資料,Document,True,Introduce（引用目的）,True,D11-1147_4_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Body
2602,12610," http://bit.ly/15StNc”"," ['4 Data', '4.1 Annotation']",(11) “Todd and Sarah Palin to divorce [Cite] http://bit.ly/15StNc”,,(11) “Todd and Sarah Palin to divorce [Cite] http://bit.ly/15StNc”,補足資料,Document,True,Introduce（引用目的）,True,D11-1147_5_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Body
2603,12611," http://urbanlegends.about.com"," ['4 Data']",Each query represents a popular rumor that is listed as “false” or only “partly true” on About.com’s Urban Leg-ends reference site [Cite_Footnote_2] between 2009 and 2010.,2 http://urbanlegends.about.com,"To use the search API, we carefully designed reg-ular expression queries to be broad enough to match all the tweets that are about a rumor. Each query represents a popular rumor that is listed as “false” or only “partly true” on About.com’s Urban Leg-ends reference site [Cite_Footnote_2] between 2009 and 2010. Table 1 lists the rumor examples that we used to collect our dataset along with their corresponding regular ex-pression queries and the number of tweets collected.",補足資料,Website,True,Introduce（引用目的）,True,D11-1147_6_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Footnote
2604,12612," http://www.lemurproject.org/"," ['6 Experiments', '6.1 Rumor Retrieval', '6.1.1 Baselines']","Finally, using the Lemur Toolkit software [Cite_Footnote_3] , we employ a KL divergence retrieval model with Dirichlet smoothing (KL).",3 http://www.lemurproject.org/,"Finally, using the Lemur Toolkit software [Cite_Footnote_3] , we employ a KL divergence retrieval model with Dirichlet smoothing (KL). In this model, documents are ranked according to the negation of the diver-gence of query and document language models. More formally, given the query language model θ Q , and the document language model θ D , the docu-ments are ranked by −D(θ Q ||θ D ), where D is the KL-divergence between the two models.",Method,Tool,True,Use（引用目的）,True,D11-1147_7_0,2011,Rumor has it: Identifying Misinformation in Microblogs,Footnote
2605,12613," http://lcg-www.uia.ac.be/conll2000/chunking"," ['1 Introduction']","In order for us to rigorously com-pare our system with others, we use the CoNLL-2000 shared task dataset (Sang and Buchholz, 2000), which is publicly available from [Cite] http://lcg-www.uia.ac.be/conll2000/chunking.",,"In this paper, we compare regularized Winnow and Winnow algorithms on text chunking (Ab-ney, 1991). In order for us to rigorously com-pare our system with others, we use the CoNLL-2000 shared task dataset (Sang and Buchholz, 2000), which is publicly available from [Cite] http://lcg-www.uia.ac.be/conll2000/chunking. An advan-tage of using this dataset is that a large number of state of the art statistical natural language pro-cessing methods have already been applied to the data. Therefore we can readily compare our re-sults with other reported results.",Material,Dataset,True,Use（引用目的）,True,P01-1069_0_0,2001,Text Chunking using Regularized Winnow,Body
2606,12614," http://lcg-www.uia.ac.be/conll2000/chunking"," ['3 CoNLL-2000 chunking task']",A standard software program has been provided (which is available from [Cite] http://lcg-www.uia.ac.be/conll2000/chunking) to compute the performance of each algorithm.,,"A standard software program has been provided (which is available from [Cite] http://lcg-www.uia.ac.be/conll2000/chunking) to compute the performance of each algorithm. For each chunk, three figures of merit are computed: precision (the percentage of detected phrases that are correct), recall (the percentage of phrases in the data that are found), and the „L nL% metric the recall. The overall precision, recall and „L nL% which is the harmonic mean of the precision and overall „L nL% metric on all chunks are also computed. The metric gives a single number that can be used to compare different algorithms.",Method,Tool,True,Use（引用目的）,True,P01-1069_1_0,2001,Text Chunking using Regularized Winnow,Body
2607,12615," https://github.com/allenai/iterative-search-semparse"," ['6 Experiments', '6.2 Experimental setup']",The code and models are publicly available at [Cite] https://github.com/allenai/iterative-search-semparse.,,"Implementation We ourimplemented model and training algorithms within the AllenNLP (Gardner et al., 2018) toolkit. The code and models are publicly available at [Cite] https://github.com/allenai/iterative-search-semparse.",Method,Code,True,Introduce（引用目的）,True,N19-1273_0_0,2019,Iterative Search for Weakly Supervised Semantic Parsing,Body
2608,12616," http://cwc-story.isi.edu"," ['1 Introduction']","We develop a system [Cite_Footnote_1] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty.","1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.","Swanson and Gordon (2009) use an informa-tion retrieval based system to write by alternating turns between a human and their system. Clark and Smith (2018) use a similar turn-taking ap-proach to interactivity, but employ a neural model for generation and allow the user to edit the gen-erated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. Roem-mele and Swanson. (2017) experiment with a user-driven setup, where the machine doesn’t gener-ate until the user requests it to, and then the user can edit or delete at will. They leverage user-acceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful gen-eration. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user’s ability to knit a coherent overall story to-gether out of locally related sentences. They also do not allow a user to control the novelty or “un-expectedness” of the generations, which Clark and Smith (2018) find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [Cite_Footnote_1] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.",補足資料,Media,True,Produce（引用目的）,True,N19-4016_0_0,2019,"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",Footnote
2609,12617," https://youtu.be/-hGd2399dnA"," ['1 Introduction']","We develop a system [Cite_Footnote_1] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty.","1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.","Swanson and Gordon (2009) use an informa-tion retrieval based system to write by alternating turns between a human and their system. Clark and Smith (2018) use a similar turn-taking ap-proach to interactivity, but employ a neural model for generation and allow the user to edit the gen-erated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. Roem-mele and Swanson. (2017) experiment with a user-driven setup, where the machine doesn’t gener-ate until the user requests it to, and then the user can edit or delete at will. They leverage user-acceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful gen-eration. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user’s ability to knit a coherent overall story to-gether out of locally related sentences. They also do not allow a user to control the novelty or “un-expectedness” of the generations, which Clark and Smith (2018) find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [Cite_Footnote_1] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.",補足資料,Media,True,Produce（引用目的）,True,N19-4016_1_0,2019,"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",Footnote
2610,12618," https://github.com/seraphinatarrant/plan-write-revise"," ['1 Introduction']","We develop a system [Cite_Footnote_1] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty.","1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.","Swanson and Gordon (2009) use an informa-tion retrieval based system to write by alternating turns between a human and their system. Clark and Smith (2018) use a similar turn-taking ap-proach to interactivity, but employ a neural model for generation and allow the user to edit the gen-erated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. Roem-mele and Swanson. (2017) experiment with a user-driven setup, where the machine doesn’t gener-ate until the user requests it to, and then the user can edit or delete at will. They leverage user-acceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful gen-eration. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user’s ability to knit a coherent overall story to-gether out of locally related sentences. They also do not allow a user to control the novelty or “un-expectedness” of the generations, which Clark and Smith (2018) find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [Cite_Footnote_1] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.",Method,Code,True,Produce（引用目的）,True,N19-4016_2_0,2019,"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",Footnote
2611,12619," https://youtu.be/-hGd2399dnA"," ['References']",The three-minute video demonstrating the interac-tion capabilities of the system can be viewed at [Cite] https://youtu.be/-hGd2399dnA. (Same video as linked in the paper footnote).,,The three-minute video demonstrating the interac-tion capabilities of the system can be viewed at [Cite] https://youtu.be/-hGd2399dnA. (Same video as linked in the paper footnote).,補足資料,Media,True,Produce（引用目的）,True,N19-4016_3_0,2019,"Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",Body
2612,12620," http://www.nist.gov/speech/tests/mt"," ['4 Data Sets and Measures', '4.1 Data sets']","For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 [Cite_Footnote_2] .",2 http://www.nist.gov/speech/tests/mt,"In this paper, we provide empirical results for MT and AS. For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 [Cite_Footnote_2] . Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005). Thus, human as-sessments are in a 2-10 scale. For AS, we have used the AS test suites developed in the DUC 2005 and DUC 2006 evaluation campaigns . This AS task was to generate a question focused summary of 250 words from a set of 25-50 documents to a complex question. Summaries were evaluated according to several criteria. Here, we will consider the respon-siveness judgements, in which the quality score was an integer between 1 and 5. See Tables 1 and 2 for a brief quantitative description of these test beds.",Material,Dataset,True,Use（引用目的）,True,D11-1042_0_0,2011,Corroborating Text Evaluation Results with Heterogeneous Measures,Footnote
2613,12621," http://duc.nist.gov/"," ['4 Data Sets and Measures', '4.1 Data sets']","For AS, we have used the AS test suites developed in the DUC 2005 and DUC 2006 evaluation campaigns [Cite_Footnote_4] .",4 http://duc.nist.gov/,"In this paper, we provide empirical results for MT and AS. For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 . Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005). Thus, human as-sessments are in a 2-10 scale. For AS, we have used the AS test suites developed in the DUC 2005 and DUC 2006 evaluation campaigns [Cite_Footnote_4] . This AS task was to generate a question focused summary of 250 words from a set of 25-50 documents to a complex question. Summaries were evaluated according to several criteria. Here, we will consider the respon-siveness judgements, in which the quality score was an integer between 1 and 5. See Tables 1 and 2 for a brief quantitative description of these test beds.",補足資料,Website,True,Introduce（引用目的）,True,D11-1042_1_0,2011,Corroborating Text Evaluation Results with Heterogeneous Measures,Footnote
2614,12622," http://www.lsi.upc.edu/~nlp/Asiya"," ['4 Data Sets and Measures', '4.2 Measures']",[Cite_Footnote_5] .,5 http://www.lsi.upc.edu/˜nlp/Asiya,"As for evaluation measures, for MT we have used a rich set of 64 measures provided within the ASIYA Toolkit (Giménez and Màrquez, 2010) [Cite_Footnote_5] . This in-cludes measures operating at different linguistic lev-els: lexical, syntactic, and semantic. At the lexical level this set includes variants of 8 measures em-ployed in the state of the art: BLEU, NIST, GTM, METEOR, ROUGE, WER, PER and TER. In addi-tion, we have included a basic measure O l that com-putes the lexical overlap without considering word ordering. All these measures have similar granular-ity. They use n-grams of a varying length as the ba-sic unit with additional information provided by lin-guistic tools. The underlying similarity criteria in-clude precision, recall, overlap, or edit rate, and the decomposition functions include words, dependency tree nodes (DP HWC, DP-Or, etc.), constituency parsing (CP-STM), discourse roles (DR-Or), seman-tic roles (SR-Or), named entities, etc. Further details on the measure set may be found in the ASIYA tech-nical manual (Giménez and Màrquez, 2010).",Material,Dataset,True,Use（引用目的）,True,D11-1042_2_0,2011,Corroborating Text Evaluation Results with Heterogeneous Measures,Footnote
2615,12623," http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf"," ['4 Data Sets and Measures', '4.1 Data sets']","In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005) [Cite_Ref] .","LDC. 2005. Linguistic Data Annotation Spec-ification: Assessment of Adequacy and Flu-ency in Translations. Revision 1.5. Tech-nical report, Linguistic Data Consortium. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf.","In this paper, we provide empirical results for MT and AS. For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 2 . Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data 3 . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005) [Cite_Ref] . Thus, human as-sessments are in a 2-10 scale. For AS, we have used the AS test suites developed in the DUC 2005 and DUC 2006 evaluation campaigns 4 . This AS task was to generate a question focused summary of 250 words from a set of 25-50 documents to a complex question. Summaries were evaluated according to several criteria. Here, we will consider the respon-siveness judgements, in which the quality score was an integer between 1 and 5. See Tables 1 and 2 for a brief quantitative description of these test beds.",補足資料,Document,True,Introduce（引用目的）,False,D11-1042_3_0,2011,Corroborating Text Evaluation Results with Heterogeneous Measures,Reference
2616,12624," http://www.everyzing.com/"," ['6 Conclusion']","Some examples include EveryZing, [Cite_Footnote_1] the MIT Lec-ture Browser, 2 and Comcast’s video search.",1 http://www.everyzing.com/,"The question of how much a system should ex-pose its internal workings (e.g., its document rep-resentations) to external systems is a long standing problem in meta-search. We’ve taken the rather nar-row view that systems might only expose the list of scores they assigned to retrieved documents, a plau-sible scenario considering the many systems now emerging which are effectively doing this already. Some examples include EveryZing, [Cite_Footnote_1] the MIT Lec-ture Browser, 2 and Comcast’s video search. 3 This trend is likely to continue as the underlying repre-sentations of the content are themselves becoming increasingly complex (e.g., word and subword level lattices or confusion networks). The cost of expos-ing such a vast quantity of such complex data rapidly becomes difficult to justify.",補足資料,Website,True,Introduce（引用目的）,True,P08-1053_0_0,2008,Combining Speech Retrieval Results with Generalized Additive Models,Footnote
2617,12625," https://www.youtube.com/watch?v=zKnP5QEHVAE"," ['3 Design and Internals']","For more detailed demonstration, please refer to the video. [Cite_Footnote_2]",2 https://www.youtube.com/watch?v= zKnP5QEHVAE,"Figure 2(a) illustrates the interactive user interface (UI) of SODA where one can select a new target collection for the analysis task (i.e. sentiment cat-egorization). For a new target collection, it identi-fies relevant adaptable source collections based on their similarity. One can select any of the candidate source collections (selected collection highlighted in Figure 2(a)) and adapt. Figure 2(b) shows the performance report along with the predicted com-ments from the target collection. User evaluates the adaptation performance in unlabeled target collec-tion by analyzing the predicted comments and de-cides whether to annotate additional comments in target collection? If yes, Figure 2(c) lists a few in-formative comments selected using the active learn-ing module to seek annotations. One can mark these comments as positive, negative or neutral and subse-quently adapt using these labeled instances from the target collection. Figure 2(b) also shows the adap-tation performance with a few labeled instances in the target collection. One can continue annotating more instances in the target collection until satis-factory performance is achieved. For more detailed demonstration, please refer to the video. [Cite_Footnote_2]",補足資料,Media,True,Produce（引用目的）,True,N16-3016_0_0,2016,SODA : Service Oriented Domain Adaptation Architecture for Microblog Categorization,Footnote
2618,12626," https://pytorch.org/"," ['4 Experiments']","In all the experiments, we use PyTorch [Cite_Footnote_2] (Paszke et al., 2019) as the backend.",2 https://pytorch.org/,"In all the experiments, we use PyTorch [Cite_Footnote_2] (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package (Grefenstette et al., 2019) to implement the proposed algorithm.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.527_0_0,2021,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,Footnote
2619,12627," https://github.com/facebookresearch/higher"," ['4 Experiments']","We use the Higher package [Cite_Footnote_3] (Grefenstette et al., 2019) to implement the proposed algorithm.",3 https://github.com/facebookresearch/ higher,"In all the experiments, we use PyTorch (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package [Cite_Footnote_3] (Grefenstette et al., 2019) to implement the proposed algorithm.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.527_1_0,2021,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,Footnote
2620,12628," https://iwslt.org/"," ['4 Experiments', '4.2 Neural Machine Translation']","For the low-resource experiments, we use [Cite_Footnote_4] : English-Vietnamese from IWSLT’15, German-English from IWSLT’14, and French-English from IWSLT’16.",4 https://iwslt.org/,"Datasets. We adopt three low-resource datasets and a rich-resource dataset. Dataset statistics are summarized in Table 1. For the low-resource experiments, we use [Cite_Footnote_4] : English-Vietnamese from IWSLT’15, German-English from IWSLT’14, and French-English from IWSLT’16. For the rich-resource experiments, we use the English-German dataset from WMT’16, which contains about 4.5 million training samples.",補足資料,Website,True,Use（引用目的）,True,2021.emnlp-main.527_2_0,2021,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,Footnote
2621,12629," https://github.com/pytorch/fairseq"," ['4 Experiments', '4.2 Neural Machine Translation']","We use Fairseq [Cite_Footnote_5] (Ott et al., 2019) to imple-ment our algorithms.",5 https://github.com/pytorch/fairseq,"Implementation. Recall that to generate adversar-ial examples, we perturb the word embeddings. In NMT experiments, we perturb both the source-side and the target-side embeddings. This strategy is empirically demonstrated (Sato et al., 2019) to be more effective than perturbing only one side of the inputs. We use Fairseq [Cite_Footnote_5] (Ott et al., 2019) to imple-ment our algorithms. We adopt the Transformer-base (Vaswani et al., 2017) architecture in all the low-resource experiments, except IWSLT’14 De-En. In this dataset, we use a model smaller than Transformer-base by decreasing the hidden dimen-sion size from 2048 to 1024, and decreasing the number of heads from 8 to 4 (while dimension of each head doubles). For the rich-resource experi-ments, we use the Transformer-big (Vaswani et al., 2017) architecture. Training details are presented in Appendix B.1.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.527_3_0,2021,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,Footnote
2622,12630," https://github.com/microsoft/MT-DNN"," ['4 Experiments', '4.2 Neural Machine Translation']","Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [Cite_Footnote_6] .",6 https://github.com/microsoft/MT-DNN,"Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [Cite_Footnote_6] . Training details are presented in Appendix B.2.",Method,Code,True,Extend（引用目的）,True,2021.emnlp-main.527_4_0,2021,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,Footnote
2623,12631," https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt"," ['References']","B.1 Neural Machine Translation For the rich-resource WMT’16 En-De dataset, we use the pre-processed data from Ott et al. (2018) [Cite_Footnote_7] .",7 https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt,"Note that in this paper, we target for models’ generalization performance on the unperturbed test data, therefore we do not want a strong adversary that “traps” the model parameters to a bad local optima. Most of the existing algorithms achieve this goal by carefully tuning the hyper-parameters and K, i.e., a small usually generates weaker adversaries, so does a small K. However, these heuristics do not work well, and at times δ K is too strong. Consequently, conventional adversarial training results in undesirable underfitting on the clean data. B.1 Neural Machine Translation For the rich-resource WMT’16 En-De dataset, we use the pre-processed data from Ott et al. (2018) [Cite_Footnote_7] . For the low-resource datasets, we use byte-pair encoding (Sennrich et al., 2016) with 10,000 merge operations to build the vocabulary for the IWSLT (’14, ’15, ’16) datasets. We follow the scripts in Ott et al. (2019) for other pre-processing steps.",Material,Knowledge,False,Use（引用目的）,True,2021.emnlp-main.527_5_0,2021,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,Footnote
2624,12632," https://github.com/pytorch/fairseq/tree/master/examples/translation"," ['References']",We follow the scripts in Ott et al. (2019) [Cite_Footnote_8] for other pre-processing steps.,8 https://github.com/pytorch/fairseq/tree/master/examples/translation,"Note that in this paper, we target for models’ generalization performance on the unperturbed test data, therefore we do not want a strong adversary that “traps” the model parameters to a bad local optima. Most of the existing algorithms achieve this goal by carefully tuning the hyper-parameters and K, i.e., a small usually generates weaker adversaries, so does a small K. However, these heuristics do not work well, and at times δ K is too strong. Consequently, conventional adversarial training results in undesirable underfitting on the clean data. B.1 Neural Machine Translation For the rich-resource WMT’16 En-De dataset, we use the pre-processed data from Ott et al. (2018) . For the low-resource datasets, we use byte-pair encoding (Sennrich et al., 2016) with 10,000 merge operations to build the vocabulary for the IWSLT (’14, ’15, ’16) datasets. We follow the scripts in Ott et al. (2019) [Cite_Footnote_8] for other pre-processing steps.",補足資料,Document,True,Use（引用目的）,True,2021.emnlp-main.527_6_0,2021,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,Footnote
2625,12633," http://www.brenbarn.net/werewolf/"," ['1 Introduction']","(Barnwell, 2012) [Cite_Ref] provides a broader description of the game and roles.",Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].,"Players are assigned roles that define their goals and available actions. For our purpose, all roles are collapsed together as either werewolf or non-werewolf. There are other important roles in Were-wolf, such as seer, vigilante, etc, the goals and avail-able actions of which are not the focus of this paper. (Barnwell, 2012) [Cite_Ref] provides a broader description of the game and roles.",補足資料,Paper,True,Introduce（引用目的）,True,N16-1047_0_0,2016,Psycholinguistic Features for Deceptive Role Detection in Werewolf ∗,Reference
2626,12634," http://www.brenbarn.net/werewolf/"," ['3 Experiments', '3.1 Data']","The raw data consists of 86 game transcripts col-lected by Barnwell (Barnwell, 2012) [Cite_Ref] .",Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].,"The raw data consists of 86 game transcripts col-lected by Barnwell (Barnwell, 2012) [Cite_Ref] . The tran-scripts have an average length of 205 messages per transcript, including judge comments.",Material,Dataset,True,Use（引用目的）,True,N16-1047_0_1,2016,Psycholinguistic Features for Deceptive Role Detection in Werewolf ∗,Reference
2627,12635," http://crfpp.sourceforge.net/"," ['4 From MaxEnt to CRFs']","A CRF models the entire label sequence y as: where F(y, x) is a global feature vector for input sequence x and label sequence y and Z(x) is a nor-malization term. [Cite_Footnote_5]",5 CRF experiments used the CRF++ package http://crfpp.sourceforge.net/,"A CRF models the entire label sequence y as: where F(y, x) is a global feature vector for input sequence x and label sequence y and Z(x) is a nor-malization term. [Cite_Footnote_5]",Method,Tool,True,Use（引用目的）,False,N10-1025_0_0,2010,Contextual Information Improves OOV Detection in Speech,Footnote
2628,12636," http://search.cpan.org/~snowhare/Lingua-Stem-0.83"," ['6 Local Lexical Context']",• All above features and their stems. [Cite_Footnote_7],"7 To obtain stemmed words, we use the CPAN package: http://search.cpan.org/~snowhare/Lingua-Stem-0.83.",• All above features and their stems. [Cite_Footnote_7] (All-Words-Stemmed),Method,Tool,False,Use（引用目的）,True,N10-1025_1_0,2010,Contextual Information Improves OOV Detection in Speech,Footnote
2629,12637," http://mallet.cs.umass.edu"," ['2 Maximum Entropy OOV Detection']","us-ing Mallet’s MaxEnt classifier (McCallum, 2002) [Cite_Ref] .",Andrew McCallum. 2002. MALLET: A machine learn-ing for language toolkit. http://mallet.cs.umass.edu.,"We obtained confusion networks for a standard word based system and the hybrid system described above. We re-implemented the above features, ob-taining nearly identical results to Rastrow et al. us-ing Mallet’s MaxEnt classifier (McCallum, 2002) [Cite_Ref] . All real-valued features were normalized and quan-tized using the uniform-occupancy partitioning de-scribed in White et al. (2007). The MaxEnt model is regularized using a Gaussian prior (σ 2 = 100), but we found results generally insensitive to σ.",補足資料,Paper,True,Introduce（引用目的）,True,N10-1025_2_0,2010,Contextual Information Improves OOV Detection in Speech,Reference
2630,12638," https://2020.emnlp.org/tutorials"," ['1 Introduction']","The ACL 2020 and EMNLP 2020 [Cite_Footnote_1] featured tutorials on the topic ; Conneau et al., 20184.",1 https://2020.emnlp.org/tutorials,"Interpretation of neural network models is a broad area of research. Significant work has an-alyzed network at representation-level (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2016; Tenney et al., 2019), and at neuron-level (Bau et al., 2020; Mu and Andreas, 2020a; Bau et al., 2019; Dalvi et al., 2019a). Others have experimented with various behavioural studies to analyze mod-els (Gulordava et al., 2018; Linzen et al., 2016; Marvin and Linzen, 2018). Moreover, a number of studies cover the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018a; Lundberg and Lee, 2017; Tran et al., 2018). The topic of interpretation of neural mod-els has gained a lot of attention in a last couple of years. For example, it has been added as a regular track in major *CL conferences. There is an annual workshop, BlackboxNLP, dedicated for this pur-pose. The ACL 2020 and EMNLP 2020 [Cite_Footnote_1] featured tutorials on the topic (Belinkov et al., 2020). The ACL tutorial focused on two subareas of interpreta-tion which are the representation analysis and the behavioral studies. The EMNLP tutorial is solely focused on behavioral studies i.e. assess a model’s behavior using constructed examples. Both of these tutorials serves as a great starting point for the new researchers in this area.",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-tutorials.2_0_0,2021,Fine-grained Interpretation and Causation Analysis in Deep NLP Models Hassan Sajjad Narine Kokhlikyan * Fahim Dalvi Nadir Durrani,Footnote
2631,12639," https://www.eccox.io/"," ['2 Description']",3 [Cite] https://www.eccox.io/ interpretation of model predictions.,,3 [Cite] https://www.eccox.io/ interpretation of model predictions. (10 min-utes),補足資料,Media,False,Introduce（引用目的）,False,2021.naacl-tutorials.2_2_0,2021,Fine-grained Interpretation and Causation Analysis in Deep NLP Models Hassan Sajjad Narine Kokhlikyan * Fahim Dalvi Nadir Durrani,Body
2632,12640," https://fdalvi.github.io"," ['6 Instructor Information (Alphabetic order']",[Cite] https://fdalvi.github.io Fahim Dalvi is an experienced Software En-gineer with a demonstrated history of working in the research industry and is currently employed at the Qatar Computing Research Institute.,,"Fahim Dalvi, Software Engineer, Qatar Comput-ing Research Institute, Qatar Email: faimaduddin@hbku.edu.qa Website: [Cite] https://fdalvi.github.io Fahim Dalvi is an experienced Software En-gineer with a demonstrated history of working in the research industry and is currently employed at the Qatar Computing Research Institute. Fahim’s research is centered around the intersection of Natural Language Processing and Deep Learning, and he has worked on wide variety of problems in these fields including Machine Translation, Language Modelling and Explainability in Deep Neural Networks. He also spends his time converting research into practical applications, with a focus on scalable web applications. Fahim also spends some time every year mentoring and teaching Deep Learning at Fall and Summer schools.",補足資料,Document,True,Other（引用目的）,True,2021.naacl-tutorials.2_3_0,2021,Fine-grained Interpretation and Causation Analysis in Deep NLP Models Hassan Sajjad Narine Kokhlikyan * Fahim Dalvi Nadir Durrani,Body
2633,12641," https://www.linkedin.com/in/narine-k-88916721/"," ['6 Instructor Information (Alphabetic order']",narine@fb.com Website: [Cite] https://www.linkedin.com/in/narine-k-88916721/ Narine is a Research Scientist focusing on Model Interpretability as part of PyTorch team at Facebook.,,"Hassan Sajjad is a Senior Research Scientist at the Qatar Computing Research Institute (QCRI), HBKU. His research interests include the interpretation of deep neural models, machine translation, domain adaptation, and natural language processing involving low-resource and morphologically-rich languages. His research work has been published in several prestigious venues such as CL, CSL, ICLR, ACL, NAACL and EMNLP. His work in collaboration with MIT and Harvard on the interpretation of deep models has also been featured in several tech blogs including MIT News. Hassan co-organized BlackboxNLP 2020, and the WMT 2019/2020 machine translation robustness task. He served as an area chair for the analysis and interpretability, NLP Application, and machine translation tracks at various *CL conferences. In addition, Hassan has been regularly teaching courses on deep learning in-ternationally at various spring and summer schools. Narine Kokhlikyan, Research Scientist, Facebook AI Email: narine@fb.com Website: [Cite] https://www.linkedin.com/in/narine-k-88916721/ Narine is a Research Scientist focusing on Model Interpretability as part of PyTorch team at Facebook. Her research interests include the understanding of Deep Neural Network internals and their predictions across different applications such as Natural Language Processing, Computer Vision and Recommender Systems. In the recent years she gave talks and presented tutorials on Model Interpretability at KDD 2020 and NeurIPS 2019. Before joining Facebook Narine worked on Natural Language Processing, Time Series Analysis and numerical optimizations.",補足資料,Document,True,Other（引用目的）,True,2021.naacl-tutorials.2_5_0,2021,Fine-grained Interpretation and Causation Analysis in Deep NLP Models Hassan Sajjad Narine Kokhlikyan * Fahim Dalvi Nadir Durrani,Body
2634,12642," http://alt.qcri.org/~ndurrani/"," ['6 Instructor Information (Alphabetic order']","Nadir Durrani, Research Scientist, Qatar Computing Research Institute, Qatar Email: ndurrani@hbku.edu.qa Website: [Cite] http://alt.qcri.org/~ndurrani/ Nadir Durrani is a Research Scientist at the Arabic Language Technologies group at Qatar Computing Research Institute.",,"Nadir Durrani, Research Scientist, Qatar Computing Research Institute, Qatar Email: ndurrani@hbku.edu.qa Website: [Cite] http://alt.qcri.org/~ndurrani/ Nadir Durrani is a Research Scientist at the Arabic Language Technologies group at Qatar Computing Research Institute. His research interests include interpretation of neural networks, neural and statistical machine translation (with focus on reordering, domain adaptation, translitera-tion, dialectal translation, pivoting, closely related and morphologically rich languages), eye-tracking for MT evaluation, spoken language translation and speech synthesis. His recent work focuses on analyzing contextualized representations with the focus of linguistic interpretation, manipulation, feature selection and model distillation. His work on analyzing deep neural networks has been published at venues like Computational Linguistics, *ACL, AAAI and ICLR. Nadir has been involved in co-organizing workshops such as simultaneous machine translation and WMT 2019/2020 Machine translation robustness task. He regularly serves as program committee and has served as Area chair at ACL and AAAI this year.",補足資料,Document,True,Other（引用目的）,True,2021.naacl-tutorials.2_6_0,2021,Fine-grained Interpretation and Causation Analysis in Deep NLP Models Hassan Sajjad Narine Kokhlikyan * Fahim Dalvi Nadir Durrani,Body
2635,12643," http://www.ibi-square.jp/index.htm"," ['4 Experiments', '4.1 Experimental Settings']","We used the five-minute chart of Nikkei 225 from March 2013 to October 2016 as numerical time-series data, which were collected from IBI-Square Stocks [Cite_Footnote_1] , and 7,351 descriptions as market com-ments, which are written in Japanese and provided by Nikkei QUICK News.",1 http://www.ibi-square.jp/index.htm,"We used the five-minute chart of Nikkei 225 from March 2013 to October 2016 as numerical time-series data, which were collected from IBI-Square Stocks [Cite_Footnote_1] , and 7,351 descriptions as market com-ments, which are written in Japanese and provided by Nikkei QUICK News. We divided the dataset into three parts: 5,880 for training, 730 for val-idation, and 741 for testing. For a human eval-uation, we randomly selected 100 comments and their time-series data included in the test set.",Material,Knowledge,True,Use（引用目的）,True,P17-1126_0_0,2017,Learning to Generate Market Comments from Stock Prices,Footnote
2636,12644," https://doi.org/10.1109/SMC.2013.661"," ['3 Generating Market Comments', '3.1 Encoding Numerical Time-Series Data']","Data are commonly preprocessed to remove noise and enhance generalizability of a model (Zhang and Qi, 2005; Banaee et al., 2013a [Cite_Ref] ).","Hadi Banaee, Mobyen Uddin Ahmed, and Amy Loutfi. 2013a. A framework for automatic text generation of trends in physiological time series data. In Process-ing of IEEE International Conference on Systems, Man, and Cybernetics. pages 3876–3881. https: //doi.org/10.1109/SMC.2013.661.","Data are commonly preprocessed to remove noise and enhance generalizability of a model (Zhang and Qi, 2005; Banaee et al., 2013a [Cite_Ref] ). We use two preprocessing methods: standardization and moving reference. Standardization substitutes each element x i of input x by where µ and σ are the mean and standard devia-tion of the values in the training data, respectively. Standardized values are less affected by scale. The second method, moving reference (Freitas et al., 2009), substitutes each element x i of input x by where r i is the closing price of the previous trad-ing day of x. This is introduced to capture price fluctuations from the previous day.",補足資料,Paper,True,Introduce（引用目的）,True,P17-1126_3_0,2017,Learning to Generate Market Comments from Stock Prices,Reference
2637,12645," https://doi.org/10.1016/j.artint.2008.12.002"," ['2 Related Work']","The task of generating descriptions from time-series or structured data has been tackled in vari-ous domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009 [Cite_Ref] ; Banaee et al., 2013b), and sports (Liang et al., 2009).","François Portet, Ehud Reiter, Albert Gatt, Jim Hunter, Somayajulu Sripada, Yvonne Freer, and Cindy Sykes. 2009. Automatic generation of textual sum-maries from neonatal intensive care data. Artificial Intelligence 173(7-8):789–816. https://doi.org/10.1016/j.artint.2008.12.002.","The task of generating descriptions from time-series or structured data has been tackled in vari-ous domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009 [Cite_Ref] ; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used hand-crafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, in-terest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relation-ship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast genera-tion (Mei et al., 2016b).",補足資料,Paper,True,Introduce（引用目的）,True,P17-1126_29_0,2017,Learning to Generate Market Comments from Stock Prices,Reference
2638,12646," https://doi.org/10.1016/j.artint.2005.06.006"," ['2 Related Work']","Traditionally, many studies used hand-crafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005 [Cite_Ref] ).","Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu, and Ian Davy. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence 167(1-2):137–169. https://doi.org/10.1016/j.artint.2005.06.006.","The task of generating descriptions from time-series or structured data has been tackled in vari-ous domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used hand-crafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005 [Cite_Ref] ). On the other hand, in-terest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relation-ship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast genera-tion (Mei et al., 2016b).",補足資料,Paper,True,Introduce（引用目的）,True,P17-1126_30_0,2017,Learning to Generate Market Comments from Stock Prices,Reference
2639,12647," https://github.com/IBM/row-column-intersection"," ['References']","Also, the inter-action model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (T A P AS and T A B ERT ), achieving ∼3.4% and ∼18.86% additional pre-cision improvement on the standard WikiSQL benchmark [Cite_Footnote_1] .",1 The source code and the models we built are available at https://github.com/IBM/row-column-intersection.,"Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrat-ing that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that indepen-dently classifies rows and columns to iden-tify relevant cells. While this model yields extremely high accuracy at finding cell val-ues on recent benchmarks, a second model we propose, called RCI representation, pro-vides a significant efficiency advantage for on-line QA systems over tables by materializ-ing embeddings for existing tables. Experi-ments on recent benchmarks prove that the pro-posed methods can effectively locate cell val-ues on tables (up to ∼98% Hit@1 accuracy on WikiSQL lookup questions). Also, the inter-action model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (T A P AS and T A B ERT ), achieving ∼3.4% and ∼18.86% additional pre-cision improvement on the standard WikiSQL benchmark [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.96_0_0,2021,Capturing Row and Column Semantics in Transformer Based Question Answering over Tables,Footnote
2640,12648," https://github.com/IBM/row-column-intersection"," ['5 Evaluation']",We are releasing the processing and evaluation code for the datasets to support reproducibility [Cite_Footnote_3] .,3 https://github.com/IBM/row-column-intersection,"To evaluate these three approaches, we adapt three standard TableQA datasets: WikiSQL (Zhong et al., 2017), WikiTableQuestions (Pasupat and Liang, 2015) and TabMCQ (Jauhar et al., 2016). Wik-iSQL and WikiTableQuestions include both lookup questions as well as aggregation questions. As men-tioned in Section 1, our primary focus in this paper is on lookup questions that require selection and projection operations over tables (i.e., identifying the row and column of a table with very high pre-cision for a given natural language question). We are releasing the processing and evaluation code for the datasets to support reproducibility [Cite_Footnote_3] . Table 2 gives a summary of these datasets.",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.96_1_0,2021,Capturing Row and Column Semantics in Transformer Based Question Answering over Tables,Footnote
2641,12649," https://github.com/google-research/tapas"," ['5 Evaluation']",For T A P AS we used the trained BASE (reset) models [Cite_Footnote_4] for Wik-iSQL and applied to the lookup subsets of the dev and test sets.,4 https://github.com/google-research/tapas,"In order to evaluate our proposed approaches on these datasets we built three different systems and also used three existing models: IS-SP, pro-vided by (Dasigi et al., 2019), T A B ERT (Yin et al., 2020) and T A P AS (Herzig et al., 2020). IS-SP is a semantic parsing based model trained on WikiTa-blesQuestions (Pasupat and Liang, 2015) dataset (See Section 2 for the details of this work). For building their model we used the code provided in (Gardner et al., 2020). For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset. For T A P AS we used the trained BASE (reset) models [Cite_Footnote_4] for Wik-iSQL and applied to the lookup subsets of the dev and test sets.",Method,Code,True,Use（引用目的）,True,2021.naacl-main.96_2_0,2021,Capturing Row and Column Semantics in Transformer Based Question Answering over Tables,Footnote
2642,12650," https://github.com/allenai/allennlp-semparse"," ['5 Evaluation']","For building their model we used the code provided in (Gardner et al., 2020) [Cite_Ref] .","Matt Gardner, Dirk Groeneveld, Brendan Roof, Pradeep Dasigi, Michael Schmitz, and Nitish Gupta. 2020. allennlp-semparse. https://github.com/allenai/allennlp-semparse.","In order to evaluate our proposed approaches on these datasets we built three different systems and also used three existing models: IS-SP, pro-vided by (Dasigi et al., 2019), T A B ERT (Yin et al., 2020) and T A P AS (Herzig et al., 2020). IS-SP is a semantic parsing based model trained on WikiTa-blesQuestions (Pasupat and Liang, 2015) dataset (See Section 2 for the details of this work). For building their model we used the code provided in (Gardner et al., 2020) [Cite_Ref] . For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset. For T A P AS we used the trained BASE (reset) models 4 for Wik-iSQL and applied to the lookup subsets of the dev and test sets.",補足資料,Paper,True,Introduce（引用目的）,True,2021.naacl-main.96_3_0,2021,Capturing Row and Column Semantics in Transformer Based Question Answering over Tables,Reference
2643,12651," https://github.com/huggingface/transformers"," ['A Appendix']",Both the MRC and RCI training was carried out using the pytorch transformers toolkit made avail-able by Huggingface [Cite_Footnote_5] .,5 https://github.com/huggingface/ transformers,Both the MRC and RCI training was carried out using the pytorch transformers toolkit made avail-able by Huggingface [Cite_Footnote_5] . Table 9 gives the number of parameters for each introduced model.,Method,Tool,True,Use（引用目的）,True,2021.naacl-main.96_4_0,2021,Capturing Row and Column Semantics in Transformer Based Question Answering over Tables,Footnote
2644,12652," http://stackexchange.com/about"," ['2 Politeness data']",Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. [Cite_Footnote_1],1 http://stackexchange.com/about,"Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. [Cite_Footnote_1] On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.",補足資料,Website,True,Extend（引用目的）,True,P13-1025_0_0,2013,A computational approach to politeness with application to social factors,Footnote
2645,12653," http://en.wikipedia.org/wiki/Wikipedia:User_pages"," ['2 Politeness data']","On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; [Cite_Footnote_2] re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page.",2 http://en.wikipedia.org/wiki/ Wikipedia:User_pages,"Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; [Cite_Footnote_2] re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.",補足資料,Website,True,Introduce（引用目的）,True,P13-1025_1_0,2013,A computational approach to politeness with application to social factors,Footnote
2646,12654," http://www.mpi-sws.org/~cristian/Politeness.html"," ['2 Politeness data']","We there-fore label a large portion of our request data (over 10,000 utterances) using Amazon Mechan-ical Turk (AMT), creating the largest corpus with politeness annotations (see Table 1 for details). [Cite_Footnote_3]",3 Publicly available at http://www.mpi-sws.org/˜cristian/Politeness.html,"Politeness annotation Computational studies of politeness, or indeed any aspect of linguistic prag-matics, demand richly labeled data. We there-fore label a large portion of our request data (over 10,000 utterances) using Amazon Mechan-ical Turk (AMT), creating the largest corpus with politeness annotations (see Table 1 for details). [Cite_Footnote_3]",Material,Knowledge,True,Introduce（引用目的）,True,P13-1025_2_0,2013,A computational approach to politeness with application to social factors,Footnote
2647,12655," http://en.wikipedia.org/wiki/Wikipedia:Administrators"," ['5 Relation to social factors', '5.1 Relation to social outcome']","Administra-tors (admins) are editors who have been granted certain rights, including the ability to block other editors and to protect or delete articles. [Cite_Footnote_7]",7 http://en.wikipedia.org/wiki/ Wikipedia:Administrators,"Among Wikipedia editors, status is a salient so-cial variable (Anderson et al., 2012). Administra-tors (admins) are editors who have been granted certain rights, including the ability to block other editors and to protect or delete articles. [Cite_Footnote_7] Ad-mins have a higher status than common editors (non-admins), and this distinction seems to be widely acknowledged by the community (Burke and Kraut, 2008b; Leskovec et al., 2010; Danescu-Niculescu-Mizil et al., 2012). Aspiring editors become admins through public elections, so we know when the status change from non-admin to admins occurred and can study users’ language use in relation to that time.",補足資料,Document,True,Introduce（引用目的）,True,P13-1025_3_0,2013,A computational approach to politeness with application to social factors,Footnote
2648,12656," http://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship"," ['5 Relation to social factors', '5.1 Relation to social outcome']","Aspiring editors become admins through public elections, [Cite_Footnote_8] so we know when the status change from non-admin to admins occurred and can study users’ language use in relation to that time.",8 http://en.wikipedia.org/wiki/ Wikipedia:Requests_for_adminship,"Among Wikipedia editors, status is a salient so-cial variable (Anderson et al., 2012). Administra-tors (admins) are editors who have been granted certain rights, including the ability to block other editors and to protect or delete articles. Ad-mins have a higher status than common editors (non-admins), and this distinction seems to be widely acknowledged by the community (Burke and Kraut, 2008b; Leskovec et al., 2010; Danescu-Niculescu-Mizil et al., 2012). Aspiring editors become admins through public elections, [Cite_Footnote_8] so we know when the status change from non-admin to admins occurred and can study users’ language use in relation to that time.",補足資料,Document,True,Introduce（引用目的）,True,P13-1025_4_0,2013,A computational approach to politeness with application to social factors,Footnote
2649,12657," https://ifttt.com/"," ['2 Core competencies', '2.1 Learning Procedures']","Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [Cite_Footnote_1] .","1 https://ifttt.com/, https://zapier.com/","Here, the condition and the effect (action) are highlighted in green and red respectively. The condition in each example requires a check that has to be grounded in the perception sensors. If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [Cite_Footnote_1] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example). Another advantage of a conversational setting is that LIA can take initiative when certain things are left am-biguous by the user (e.g., ask the user what to do if there is a conflict on the calendar for the last rule in the list above) — an issue that cannot be coped with in traditional programming environments.",Method,Tool,True,Compare（引用目的）,True,D18-2025_0_0,2018,LIA: A Natural Language Programmable Personal Assistant,Footnote
2650,12658," https://zapier.com/"," ['2 Core competencies', '2.1 Learning Procedures']","Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [Cite_Footnote_1] .","1 https://ifttt.com/, https://zapier.com/","Here, the condition and the effect (action) are highlighted in green and red respectively. The condition in each example requires a check that has to be grounded in the perception sensors. If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [Cite_Footnote_1] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example). Another advantage of a conversational setting is that LIA can take initiative when certain things are left am-biguous by the user (e.g., ask the user what to do if there is a conflict on the calendar for the last rule in the list above) — an issue that cannot be coped with in traditional programming environments.",Method,Tool,True,Compare（引用目的）,True,D18-2025_1_0,2018,LIA: A Natural Language Programmable Personal Assistant,Footnote
2651,12659," http://y2u.be/YfKqpT0apQw"," ['2 Core competencies', '2.3 Grounding Knowledge to Perception']",A video demon-stration of the system can be seen at [Cite] http://y2u.,,"Example Interaction: Figure 2 shows an exam-ple interaction exemplifying these abilities in LIA, and also outlining its working. A video demon-stration of the system can be seen at [Cite] http://y2u.be/YfKqpT0apQw. Next, we describe how these abilities are implemented in LIA, and highlight salient features of its architecture.",補足資料,Media,True,Produce（引用目的）,True,D18-2025_2_0,2018,LIA: A Natural Language Programmable Personal Assistant,Body
2652,12660," http://wordnet.princeton.edu/"," ['1 Introduction']","Pre-compiled taxonomies such as WordNet [Cite_Footnote_1] and text corpora have been used in previous work on se-mantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b).",1 http://wordnet.princeton.edu/,"Pre-compiled taxonomies such as WordNet [Cite_Footnote_1] and text corpora have been used in previous work on se-mantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, seman-tic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, loca-tion names, product names) and the new uses of ex-isting words. For example, apple is frequently asso-ciated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible.",Material,Knowledge,True,Introduce（引用目的）,True,N07-1043_0_0,2007,An Integrated Approach to Measuring Semantic Similarity between Words Using Information available on the Web,Footnote
2653,12661," http://www.google.com"," ['3 Method']",We utilize page counts and snippets returned by the Google [Cite_Footnote_3] search engine for simple text queries to define various similarity scores.,3 http://www.google.com,In this section we will describe the various similarity features we use in our model. We utilize page counts and snippets returned by the Google [Cite_Footnote_3] search engine for simple text queries to define various similarity scores.,補足資料,Website,True,Introduce（引用目的）,True,N07-1043_1_0,2007,An Integrated Approach to Measuring Semantic Similarity between Words Using Information available on the Web,Footnote
2654,12662," http://dmoz.org"," ['4 Experiments', '4.4 Named Entity Clustering']","We selected 50 person names from 5 categories : tennis players, golfers, actors, politicians and scien-tists, (10 names from each category) from the dmoz directory [Cite_Footnote_7] .",7 http://dmoz.org,"Measuring semantic similarity between named en-tities is vital in many applications such as query expansion (Sahami and Heilman, 2006) and com-munity mining (Matsuo et al., 2006a). Since most named entities are not covered by WordNet, simi-larity measures based on WordNet alone cannot be used in such tasks. Unlike common English words, named entities are constantly being created. Manu-ally maintaining an up-to-date taxonomy of named entities is costly, if not impossible. The proposed semantic similarity measure is appealing as it does not require pre-compiled taxonomies. In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 categories : tennis players, golfers, actors, politicians and scien-tists, (10 names from each category) from the dmoz directory [Cite_Footnote_7] . For each pair of names in our dataset, we measure the association between the two names using the proposed method and baselines. We use group-average agglomerative hierarchical clustering to cluster the names in our dataset into five clusters. We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. As summarized in Table 5 the proposed method outper-forms all the baselines with a statistically significant (p ≤ 0.01 Tukey HSD) F score of 0.7897.",Material,Knowledge,True,Extend（引用目的）,False,N07-1043_2_0,2007,An Integrated Approach to Measuring Semantic Similarity between Words Using Information available on the Web,Footnote
2655,12663," https://developer.amazon.com/docs/custom-skills/create-the-interaction-model-for-your-skill.html"," ['1 Introduction']","Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018; Wit.ai, 2018; Amazon, 2018 [Cite_Ref] ; Microsoft, 2018).",Amazon. 2018. Create the interaction model for your skill. https://developer.amazon.com/docs/custom-skills/create-the-interaction-model-for-your-skill.html.,"Conversational interfaces are a prominent fea-ture of many consumer technology products to-day. Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018; Wit.ai, 2018; Amazon, 2018 [Cite_Ref] ; Microsoft, 2018).",補足資料,Paper,True,Introduce（引用目的）,True,D18-2027_0_0,2018,Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing,Reference
2656,12664," https://dialogflow.com/docs/getting-started/first-agent"," ['1 Introduction']","Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018 [Cite_Ref] ; Wit.ai, 2018; Amazon, 2018; Microsoft, 2018).",Dialogflow. 2018. Create and query your first agent. https://dialogflow.com/docs/getting-started/first-agent.,"Conversational interfaces are a prominent fea-ture of many consumer technology products to-day. Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018 [Cite_Ref] ; Wit.ai, 2018; Amazon, 2018; Microsoft, 2018).",補足資料,Document,True,Introduce（引用目的）,True,D18-2027_1_0,2018,Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing,Reference
2657,12665," https://docs.microsoft.com/en-us/cortana/skills/mva32-building-conversations"," ['1 Introduction']","Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018; Wit.ai, 2018; Amazon, 2018; Microsoft, 2018 [Cite_Ref] ).",Microsoft. 2018. Building conversations. https://docs.microsoft.com/en-us/cortana/skills/mva32-building-conversations.,"Conversational interfaces are a prominent fea-ture of many consumer technology products to-day. Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018; Wit.ai, 2018; Amazon, 2018; Microsoft, 2018 [Cite_Ref] ).",補足資料,Document,True,Introduce（引用目的）,True,D18-2027_2_0,2018,Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing,Reference
2658,12666," https://inklab.usc.edu/XCSR/"," ['References']","It significantly enhances sen-tence representations, yielding a large perfor-mance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R L ) [Cite_Footnote_1] .",1 We release our code and data at the project website: https://inklab.usc.edu/XCSR/.,"Commonsense reasoning research has so far been limited to English. We aim to evalu-ate and improve popular multilingual language models (ML-LMs) to help advance common-sense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a language-agnostic probing task for fairly evaluating the common sense of popular ML-LMs across dif-ferent languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 15 other languages, so that we can evaluate pop-ular ML-LMs for cross-lingual commonsense reasoning. To improve the performance be-yond English, we propose a simple yet effec-tive method — multilingual contrastive pre-training (MCP). It significantly enhances sen-tence representations, yielding a large perfor-mance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R L ) [Cite_Footnote_1] .",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.102_0_0,2021,Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning,Footnote
2659,12667," https://github.com/commonsense/conceptnet5/wiki/Downloads"," ['* Ethical Considerations']","Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [Cite_Footnote_3] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively.",3 https://github.com/commonsense/conceptnet5/wiki/Downloads,"Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [Cite_Footnote_3] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research. Cultural Bias Reduction Like most most mul-tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural dif-ferences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like “what do people usually drink in the morning? (cof-fee/tea/milk)” or “when does a wedding usually start? (morning/afternoon/evening)” might be an-swered very differently by people from different backgrounds and cultures, not to mention differ-ent languages. The prior English commonsense resources which our datasets are built on are al-ready possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either labeled as non-neutral by a pre-trained sentiment classifier, or contained any keywords that are relevant to social behavior (e.g., weddings). We manually inspect test examples in X-CSQA and X-CODAH in the English and Chi-nese versions and have a strong confidence there is few strongly controversial example. However, we admit that such reduction of cultural differences in common sense has not been systematically mea-sured in this work for other languages.",Material,DataSource,True,Introduce（引用目的）,True,2021.acl-long.102_1_0,2021,Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning,Footnote
2660,12668," https://www.tau-nlp.org/commonsenseqa"," ['* Ethical Considerations']","Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [Cite_Footnote_4] , and CODAH (Chen et al., 2019) 5 re-spectively.",4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH,"Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [Cite_Footnote_4] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research. Cultural Bias Reduction Like most most mul-tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural dif-ferences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like “what do people usually drink in the morning? (cof-fee/tea/milk)” or “when does a wedding usually start? (morning/afternoon/evening)” might be an-swered very differently by people from different backgrounds and cultures, not to mention differ-ent languages. The prior English commonsense resources which our datasets are built on are al-ready possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either labeled as non-neutral by a pre-trained sentiment classifier, or contained any keywords that are relevant to social behavior (e.g., weddings). We manually inspect test examples in X-CSQA and X-CODAH in the English and Chi-nese versions and have a strong confidence there is few strongly controversial example. However, we admit that such reduction of cultural differences in common sense has not been systematically mea-sured in this work for other languages.",Material,DataSource,True,Introduce（引用目的）,True,2021.acl-long.102_2_0,2021,Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning,Footnote
2661,12669," https://github.com/Websail-NU/CODAH"," ['* Ethical Considerations']","Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [Cite_Footnote_4] , and CODAH (Chen et al., 2019) 5 re-spectively.",4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH,"Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [Cite_Footnote_4] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research. Cultural Bias Reduction Like most most mul-tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural dif-ferences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like “what do people usually drink in the morning? (cof-fee/tea/milk)” or “when does a wedding usually start? (morning/afternoon/evening)” might be an-swered very differently by people from different backgrounds and cultures, not to mention differ-ent languages. The prior English commonsense resources which our datasets are built on are al-ready possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either labeled as non-neutral by a pre-trained sentiment classifier, or contained any keywords that are relevant to social behavior (e.g., weddings). We manually inspect test examples in X-CSQA and X-CODAH in the English and Chi-nese versions and have a strong confidence there is few strongly controversial example. However, we admit that such reduction of cultural differences in common sense has not been systematically mea-sured in this work for other languages.",Material,DataSource,True,Introduce（引用目的）,True,2021.acl-long.102_3_0,2021,Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning,Footnote
2662,12670," https://github.com/facebookresearch/unlu"," ['References']",Our code and data are available at [Cite] https://github.com/facebookresearch/unlu.,,"Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Under-standing (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at [Cite] https://github.com/facebookresearch/unlu.",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.569_0_0,2021,UnNatural Language Inference,Body
2663,12671," https://github.com/facebookresearch/unlu"," ['5 Results']","We drop examples from test sets where we are unable to compute all unique randomizations, typi-cally these are examples with sentences of length of less than 6 tokens. [Cite_Footnote_2]","2 Code, data, and model checkpoints will be available at https://github.com/facebookresearch/unlu. models are trained on OCNLI corpus (Hu et al., 2020a). Bold marks the highest value per metric (red shows the model is insensitive to permutation).","We present results for two types of models: (a) Transformer-based models and (b) Non-Transformer Models. In (a), we investigate the state-of-the-art pre-trained models such as RoBERTa-Large (Liu et al., 2019), BART-Large (Lewis et al., 2020) and DistilBERT (Sanh et al., 2020). For (b) we consider several recurrent and convolution based neural networks, such as In-ferSent (Conneau et al., 2017), Bidirectional LSTM (Collobert and Weston, 2008) and ConvNet (Zhao et al., 2015). We train all models on MNLI, and evaluate on in-distribution (SNLI and MNLI) and out-of-distribution datasets (ANLI). We in-dependently verify results of (a) using both our fine-tuned model using HuggingFace Transform-ers (Wolf et al., 2020) and pre-trained checkpoints from FairSeq (Ott et al., 2019) (using PyTorch Model Hub). For (b), we use the InferSent code-base. We sample q = 100 permutations for each example in D test , and use 100 seeds for each of those permutations to ensure full reproducibility. We drop examples from test sets where we are unable to compute all unique randomizations, typi-cally these are examples with sentences of length of less than 6 tokens. [Cite_Footnote_2] Models accept many permuted examples. We find Ω max is very high for models trained and evalu-ated on MNLI (in-domain generalization), reaching 98.7% on MNLI dev. and test sets (in RoBERTa, compared to A of 90.6% (Table 2). Recall, hu-man accuracy is approximately 92% on MNLI dev., Nangia and Bowman 2019). This shows that there exists at least one permutation (usually many more) trained on MNLI corpus (Williams et al., 2018b). The highest values are bolded (red indicates the model most insensitive to permutation) per metric and per model class (Transformers and non-Transformers). A1*, A2* and A3* refer to the ANLI dev. sets (Nie et al., 2020). predicts the gold label (or give up by exhausting our set of q permutations). For out-of-domain gen-eralization, Ω rand decreases considerably (36.4% Ω rand on A1), which means fewer permutations are accepted by the model. Next, recall that a classic bag-of-words model would have P c = 100 and P f = 0. No model performs strictly like a classic bag of words although they do perform somewhat BOW-like (P c >> P f for all test splits, Figure 5). We find this BOW-likeness to be higher for certain non-Transformer models, (InferSent) as they ex-hibit higher P c (84.2% for InferSent compared to 70.7% for RoBERTa on MNLI).",Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.569_1_0,2021,UnNatural Language Inference,Footnote
2664,12672," https://github.com/facebookresearch/unlu"," ['F Training with permuted examples']",• Link to downloadable version of data and code: We provide download-able version of our data and code at [Cite] https://github.com/facebookresearch/unlu.,,• Link to downloadable version of data and code: We provide download-able version of our data and code at [Cite] https://github.com/facebookresearch/unlu.,Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.569_2_0,2021,UnNatural Language Inference,Body
2665,12673," https://github.com/kentonl/e2e-coref"," ['5 Experimental Setup']",The code for replicating these results is publicly available. [Cite_Footnote_1],1 https://github.com/kentonl/e2e-coref,"We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [Cite_Footnote_1]",Method,Code,True,Produce（引用目的）,True,N18-2108_0_0,2018,Higher-order Coreference Resolution with Coarse-to-fine Inference,Footnote
2666,12674," https://github.com/dwadden/dygiepp"," ['References']",Our code is publicly available at [Cite] https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.,,"We examine the capabilities of a unified, multi-task framework for three information extrac-tion tasks: named entity recognition, rela-tion extraction, and event extraction. Our framework (called D Y GIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) con-text. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform ex-periments comparing different techniques to construct span representations. Contextual-ized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagat-ing span representations via predicted coref-erence links can enable the model to disam-biguate challenging entity mentions. Our code is publicly available at [Cite] https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",Method,Code,True,Produce（引用目的）,True,D19-1585_0_0,2019,"Entity, Relation, and Event Extraction with Contextualized Span Representations",Body
2667,12675," http://en.wiktionary.org/wiki/Wiktionary:Idioms_that_survived_RFD"," ['5 Experiments']","Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are com-positional (e.g. from the Wiktionary guidelines for idiom inclusion, [Cite_Footnote_3] computer chess, basketball player, telephone box).",3 http://en.wiktionary.org/wiki/ Wiktionary:Idioms_that_survived_RFD,"The baseline for each dataset takes the form of looking for a user-annotated idiom tag in the Wik-tionary lexical entry for the MWE: if there is an id-iomatic tag, both components are considered to be non-compositional; otherwise, both components are considered to be compositional. We expect this method to suffer from low precision for two reasons: first, the guidelines given to the annota-tors of our datasets might be different from what Wiktionary contributors assume to be an idiom. Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are com-positional (e.g. from the Wiktionary guidelines for idiom inclusion, [Cite_Footnote_3] computer chess, basketball player, telephone box).",補足資料,Document,True,Introduce（引用目的）,False,D14-1189_0_0,2014,Detecting Non-compositional MWE Components using Wiktionary,Footnote
2668,12676," https://github.com/bsalehi/wiktionary_MWE_compositionality"," ['7 Conclusion']",All code to replicate the re-sults in this paper has been made publicly avail-able at [Cite] https://github.com/bsalehi/wiktionary_MWE_compositionality.,,"We have proposed an unsupervised approach for predicting the compositionality of an MWE rel-ative to each of its components, based on lexi-cal overlap using Wiktionary, optionally incorpo-rating synonym and translation data. Our experi-ments showed that the various instantiations of our approach are superior to previous state-of-the-art supervised methods. All code to replicate the re-sults in this paper has been made publicly avail-able at [Cite] https://github.com/bsalehi/wiktionary_MWE_compositionality.",Method,Code,True,Use（引用目的）,True,D14-1189_1_0,2014,Detecting Non-compositional MWE Components using Wiktionary,Body
2669,12677," https://github.com/google/jax"," ['8 Experiments And Results']","We have implemented the proposed hierarchical attention using Jax, an open source library [Cite_Footnote_5] for automatic gradient computation and linear alge-bra operations on GPUs and TPUs.",5 https://github.com/google/jax,"We have implemented the proposed hierarchical attention using Jax, an open source library [Cite_Footnote_5] for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax. In all our experiments in this section, we use the standard Transformer architecture described in (Vaswani et al., 2017) as the backbone for our H-Transformer-1D model. Unless specified other-wise, the model parameters are: number of lay-ers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implemen-tation so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation. This al-lows for an easy and fair comparison.",Method,Code,True,Use（引用目的）,True,2021.acl-long.294_0_0,2021,H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences,Footnote
2670,12678," https://github.com/google/flax/blob/master/flax/nn"," ['8 Experiments And Results']",We follow the API for the standard multihead scaled dot-product attention implemen-tation [Cite_Footnote_6] so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation.,6 https://github.com/google/flax/blob/master/flax/nn,"We have implemented the proposed hierarchical attention using Jax, an open source library for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax. In all our experiments in this section, we use the standard Transformer architecture described in (Vaswani et al., 2017) as the backbone for our H-Transformer-1D model. Unless specified other-wise, the model parameters are: number of lay-ers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implemen-tation [Cite_Footnote_6] so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation. This al-lows for an easy and fair comparison.",Method,Code,False,Use（引用目的）,False,2021.acl-long.294_1_0,2021,H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences,Footnote
2671,12679," https://github.com/google-research/long-range-arena"," ['8 Experiments And Results', '8.1 Long-Range Arena']","The open-source Long-Range Arena (LRA) benchmark [Cite_Footnote_7] has been proposed as a standard way to probe and quantify the capabilities of var-ious xformer (long-range Transformer) architec-tures (Tay et al., 2020c).",7 https://github.com/google-research/long-range-arena,"The open-source Long-Range Arena (LRA) benchmark [Cite_Footnote_7] has been proposed as a standard way to probe and quantify the capabilities of var-ious xformer (long-range Transformer) architec-tures (Tay et al., 2020c). In our case, it also serves to highlight the effectiveness of the inductive bias inspired by the H-Matrix method, as well as the capability of our hierarchical attention to handle long sequences.",Method,Code,False,Introduce（引用目的）,False,2021.acl-long.294_2_0,2021,H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences,Footnote
2672,12680," https://github.com/google/flax"," ['8 Experiments And Results', '8.2 Language Models Trained on One-Billion Words']","We have used Flax, an open-source library [Cite_Footnote_8] to train neural networks, as the code base for the model training.",8 https://github.com/google/flax,"We have used Flax, an open-source library [Cite_Footnote_8] to train neural networks, as the code base for the model training. Our H-Transformer-1D model uses the standard Transformer decoder implemen-tation in Flax as the backbone. Only the atten-tion is replaced with our hierarchical attention. We trained both the Transformer baseline and H-Transformer-1D on the One-Billion Word bench-mark (Chelba et al., 2014). We tried different N r (numerical rank) in our H-Transformer-1D model. These represent different inductive bias. We found that H-Transformer-1D with N r = 16 generated text with quality comparable to that of the base-line Transformer. For both Transformer baseline and H-Transformer-1D, we also tried two sets of model parameters: 1) embedding size is 512 and feed-forward module size is 2048 and hence the parameter count is 53M; 2) embedding size is 1024 and feed-forward module size is 4096 and hence the parameter count is 144M. The test per-plexity results of these four models and various SOTA models are shown in table 2.",Method,Code,True,Use（引用目的）,True,2021.acl-long.294_3_0,2021,H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences,Footnote
2673,12681," http://syntagnet.org/"," ['References']","Our service provides both a user-friendly interface, available at [Cite] http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/).",,"Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance. We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web in-terface and a RESTful API. SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at [Cite] http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/).",Method,Tool,True,Produce（引用目的）,True,2020.acl-demos.6_0_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Body
2674,12682," http://api.syntagnet.org/"," ['References']","Our service provides both a user-friendly interface, available at http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at [Cite] http://api.syntagnet.org/).",,"Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance. We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web in-terface and a RESTful API. SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at [Cite] http://api.syntagnet.org/).",Method,Tool,True,Produce（引用目的）,True,2020.acl-demos.6_1_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Body
2675,12683," https://babelnet.org/"," ['1 Introduction']","Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the wide array of languages covered by LKBs like Ba-belNet [Cite_Footnote_1] ; Bevilacqua and Navigli, 20190, or the Open Multilingual WordNet ; Bevilacqua and Navigli, 20191.",1 https://babelnet.org/,"This challenge has been tackled by exploiting huge amounts of hand-annotated data in a super-vised fashion (Raganato et al., 2017b; Bevilacqua and Navigli, 2019; Vial et al., 2019; Bevilacqua and Navigli, 2020) or, alternatively, by harnessing struc-tured information (Agirre et al., 2014; Moro et al., 2014; Scarlini et al., 2020), such as that available within existing lexical knowledge bases (LKBs) like WordNet (Fellbaum, 1998). Despite achieving better overall results, supervised systems require tremendous efforts in order to produce data for several languages (Navigli, 2018; Pasini, 2020), whereas knowledge-based approaches can easily be applied in multilingual environments due to the wide array of languages covered by LKBs like Ba-belNet [Cite_Footnote_1] (Navigli and Ponzetto, 2012), or the Open Multilingual WordNet (Bond and Foster, 2013). Moreover, it is widely acknowledged that the per-formance of a knowledge-based WSD system is strongly correlated with the structure of the LKB employed (Boyd-Graber et al., 2006; Lemnitzer et al., 2008; Navigli and Lapata, 2010; Ponzetto and Navigli, 2010). In fact, the knowledge avail-able within LKBs reflects the fact that words can be linked via two types of semantic relations: paradig-matic relations – i.e. the most frequently encoun-tered relations in LKBs – concern the substitution of lexical units, and determine to which level in a hierarchy a language unit belongs by semantic analogy with units similar to it; conversely, syn-tagmatic relations concern the positioning of such units, by linking elements belonging to the same hierarchical level (e.g., words), which appear in the same context (e.g., a sentence). As a case in point, a paradigmatic relation exists, independently of a given context, between the words farm n and workplace n (where a farm is a type of workplace), whereas a syntagmatic relation is entertained be-tween the words work v and farm n , e.g., in the sen-tence ‘her husband works in a farm as a labourer.’ In our most recent study (Maru et al., 2019, Syn-tagNet), we provided further evidence that the na-ture of LKBs impacts on system performance: the injection of syntagmatic relations – in the form of disambiguated pairs of co-occurring words – into an existing LKB biased towards paradigmatic knowledge enables knowledge-based systems to rival their supervised counterparts.",Material,Knowledge,True,Introduce（引用目的）,True,2020.acl-demos.6_2_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Footnote
2676,12684," http://wordnetcode.princeton.edu/glosstag.shtml"," ['2 Preliminaries', '2.1 Lexical Knowledge Bases']",Princeton WordNet Gloss Corpus (PWNG) is the semantically-annotated gloss corpus made available by WordNet since its 3.0 release. [Cite_Footnote_2],2 http://wordnetcode.princeton.edu/glosstag.shtml,"Princeton WordNet Gloss Corpus (PWNG) is the semantically-annotated gloss corpus made available by WordNet since its 3.0 release. [Cite_Footnote_2] Glosses are short definitions providing proper meanings for synsets, and in PWNG they have been tagged according to the senses in WordNet. Following Agirre et al. (2014), we induce new WordNet relations from PWNG by linking the synset to which the gloss refers to each of the synsets that have been tagged in the gloss itself. In this way, additional contextual relations are provided, inadvertently covering syntagmatic relations, too.",Material,Knowledge,True,Introduce（引用目的）,True,2020.acl-demos.6_3_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Footnote
2677,12685," https://github.com/SapienzaNLP/mwsd-datasets"," ['5 Evaluation']","As regards the appraisal of Synta-gRank in a multilingual setting, we used the Ger-man, Spanish, French and Italian annotations avail-able in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets [Cite_Footnote_10] , which is accordant with the BabelNet API 4.0.1 graph and enables testing on a larger number of instances than hitherto.",10 Made available at https://github.com/SapienzaNLP/mwsd-datasets.,"In order to assess its performance, we tested Syn-tagRank on the five English all-words WSD evalu-ation datasets standardized according to WordNet 3.0 in the framework of Raganato et al. (2017a), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Nav-igli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). As regards the appraisal of Synta-gRank in a multilingual setting, we used the Ger-man, Spanish, French and Italian annotations avail-able in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets [Cite_Footnote_10] , which is accordant with the BabelNet API 4.0.1 graph and enables testing on a larger number of instances than hitherto.",Material,Dataset,True,Use（引用目的）,True,2020.acl-demos.6_4_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Footnote
2678,12686," http://syntagnet.org/"," ['6 Conclusion']","We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at [Cite] http://syntagnet.org/ and http://api.syntagnet.org , respectively.",,"In this paper we presented and described the architecture of SyntagRank, our state-of-the-art knowledge-based system for multilingual Word Sense Disambiguation using syntagmatic informa-tion. We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at [Cite] http://syntagnet.org/ and http://api.syntagnet.org , respectively.",補足資料,Website,True,Introduce（引用目的）,True,2020.acl-demos.6_5_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Body
2679,12687," http://api.syntagnet.org"," ['6 Conclusion']","We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at http://syntagnet.org/ and [Cite] http://api.syntagnet.org , respectively.",,"In this paper we presented and described the architecture of SyntagRank, our state-of-the-art knowledge-based system for multilingual Word Sense Disambiguation using syntagmatic informa-tion. We also provided details concerning the use of SyntagRank’s Web interface and RESTful API, accessible at http://syntagnet.org/ and [Cite] http://api.syntagnet.org , respectively.",補足資料,Website,True,Introduce（引用目的）,True,2020.acl-demos.6_6_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Body
2680,12688," http://api.syntagnet.org/disambiguate_tokens"," ['A API Documentation']",[Cite] http://api.syntagnet.org/disambiguate_tokens,,Token Parameters id (String) word (String) lemma (String) pos (String) isTargetWord (boolean) POST [Cite] http://api.syntagnet.org/disambiguate_tokens,Material,Knowledge,True,Introduce（引用目的）,True,2020.acl-demos.6_8_0,2020,Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation,Body
2681,12689," http://scikit-learn.org/stable/"," ['3 Experiments', '3.3 Sentence Evaluation']",We then average embeddings in a sentence and give that as features to a logistic regression classifier trained with 5-fold cross validation. [Cite_Footnote_6],6 http://scikit-learn.org/stable/,Prediction We use E MBED A LIGN to annotate every word in the training set of the benchmarks above with the posterior mean embedding in con-text. We then average embeddings in a sentence and give that as features to a logistic regression classifier trained with 5-fold cross validation. [Cite_Footnote_6],Method,Code,False,Use（引用目的）,False,N18-1092_1_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Footnote
2682,12690," https://doi.org/10.3115/1073083.1073126"," ['References']",[Cite] https://doi.,,"SICK-R semantic relatedness between two sen-tences (↑Pearson); SST-14 semantic textual similarity (↑Pearson/Spearman). vised method for word sense tagging using parallel corpora. In Proceedings of 40th An-nual Meeting of the Association for Compu-tational Linguistics. Association for Compu-tational Linguistics, Philadelphia, Pennsylva-nia, USA, pages 255–262. [Cite] https://doi.org/10.3115/1073083.1073126.",補足資料,Paper,False,Introduce（引用目的）,False,N18-1092_6_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2683,12691," http://www.aclweb.org/anthology/P14-1006"," ['References']",[Cite] http://www.aclweb.,,"We are currently expanding the notion of dis-tributional context to multiple auxiliary foreign languages at once. This seems to only require minor changes to the generative story and could increase the model’s disambiguation power dra-matically. Another direction worth exploring is to extend the model’s hierarchy with respect to how parallel sentences are generated. For exam-ple, modelling sentence level latent variables may capture global constraints and expose additional correlations to the model. the 52nd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: Long Papers). Association for Compu-tational Linguistics, Baltimore, Maryland, pages 58–68. [Cite] http://www.aclweb.org/anthology/P14-1006.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_10_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2684,12692," http://www.aclweb.org/anthology/C12-1089"," ['References']",[Cite] http://www.aclweb.org/anthology/C12-1089.,,"Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of COLING 2012. The COLING 2012 Or-ganizing Committee, Mumbai, India, pages 1459–1474. [Cite] http://www.aclweb.org/anthology/C12-1089.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_12_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2685,12693," http://mt-archive.info/MTS-2005-Koehn.pdf"," ['References']",[Cite] http://mt-archive.,,"Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Confer-ence Proceedings: the tenth Machine Trans-lation Summit. AAMT, AAMT, Phuket, Thai-land, pages 79–86. [Cite] http://mt-archive.info/MTS-2005-Koehn.pdf.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_13_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2686,12694," http://dl.acm.org/citation.cfm?id=1557769.1557821"," ['References']",[Cite] http://dl.acm.org/citation.,,"Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine trans-lation. In Proceedings of the 45th An-nual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Associa-tion for Computational Linguistics, Strouds-burg, PA, USA, ACL ’07, pages 177– 180. [Cite] http://dl.acm.org/citation.cfm?id=1557769.1557821.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_14_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2687,12695," https://doi.org/10.3115/1118905.1118906"," ['References']",[Cite] https://doi.org/10.3115/1118905.1118906.,,"Computational Linguistics, Stroudsburg, PA, USA, HLT-NAACL-PARALLEL ’03, pages 1–10. [Cite] https://doi.org/10.3115/1118905.1118906.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_17_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2688,12696," https://doi.org/10.3115/1220175.1220321"," ['References']",[Cite] https://doi.org/10.3115/1220175.1220321.2016.,,"Sebastian Padó and Mirella Lapata. 2006. Opti-mal constituent alignment with edge covers for semantic projection. In Proceedings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics. Association for Computational Linguistics, Sydney, Australia, pages 1161– 1168. [Cite] https://doi.org/10.3115/1220175.1220321.2016. Word alignment without null words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-tics (Volume 2: Short Papers). Association for Computational Linguistics, Berlin, Germany, pages 169–174. http://anthology.aclweb.org/P16-2028.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_18_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2689,12697," http://anthology.aclweb.org/P16-2028"," ['References']",[Cite] http://anthology.,,"Sebastian Padó and Mirella Lapata. 2006. Opti-mal constituent alignment with edge covers for semantic projection. In Proceedings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics. Association for Computational Linguistics, Sydney, Australia, pages 1161– 1168. https://doi.org/10.3115/1220175.1220321.2016. Word alignment without null words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-tics (Volume 2: Short Papers). Association for Computational Linguistics, Berlin, Germany, pages 169–174. [Cite] http://anthology.aclweb.org/P16-2028.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_19_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2690,12698," http://www.aclweb.org/anthology/D11-1014"," ['References']",[Cite] http://www.aclweb.,,"Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Man-ning. 2011. Semi-supervised recursive au-toencoders for predicting sentiment distribu-tions. In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Lan-guage Processing. Association for Computa-tional Linguistics, Edinburgh, Scotland, UK., pages 151–161. [Cite] http://www.aclweb.org/anthology/D11-1014.ord.2016. Bilingual learning of multi-sense embeddings with discrete autoencoders. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies. Association for Compu-tational Linguistics, San Diego, California, pages 1346–1356. http://www.aclweb.org/anthology/N16-1160.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_20_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2691,12699," http://www.aclweb.org/anthology/N16-1160"," ['References']",[Cite] http://www.aclweb.,,"Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Man-ning. 2011. Semi-supervised recursive au-toencoders for predicting sentiment distribu-tions. In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Lan-guage Processing. Association for Computa-tional Linguistics, Edinburgh, Scotland, UK., pages 151–161. http://www.aclweb.org/anthology/D11-1014.ord.2016. Bilingual learning of multi-sense embeddings with discrete autoencoders. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies. Association for Compu-tational Linguistics, San Diego, California, pages 1346–1356. [Cite] http://www.aclweb.org/anthology/N16-1160.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1092_23_0,2018,Deep Generative Model for Joint Alignment and Word Representation,Body
2692,12700," http://isobs.thunlp.org/"," ['References']",The demo system of IsOBS can be found from [Cite] http: //isobs.thunlp.org/ .,,"Oracle bone script (OBS) is the earliest known ancient Chinese writing system and the ances-tor of modern Chinese. As the Chinese writ-ing system is the oldest continuously-used sys-tem in the world, the study of OBS plays an important role in both linguistic and histori-cal research. In order to utilize advanced ma-chine learning methods to automatically pro-cess OBS, we construct an information sys-tem for OBS (IsOBS) to symbolize, serial-ize, and store OBS data at the character-level, based on efficient databases and retrieval mod-ules. Moreover, we also apply few-shot learn-ing methods to build an effective OBS charac-ter recognition module, which can recognize a large number of OBS characters (especially those characters with a handful of examples) and make the system easy to use. The demo system of IsOBS can be found from [Cite] http: //isobs.thunlp.org/ . In the future, we will add more OBS data to the system, and hopefully our IsOBS can support further ef-forts in automatically processing OBS and ad-vance the scientific progress in this field.",補足資料,Paper,True,Introduce（引用目的）,True,2020.emnlp-demos.29_0_0,2020,IsOBS: An Information System for Oracle Bone Script,Body
2693,12701," https://github.com/thunlp/isobs"," ['4 Experiment and Evaluation']",The datasets and source code can be found from [Cite] https: //github.com/thunlp/isobs .,,We evaluate different character recognition mod-els on self-created dataset. The results show that our implementation of prototypical network can achieve stable and competitive results. The datasets and source code can be found from [Cite] https: //github.com/thunlp/isobs .,Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-demos.29_1_0,2020,IsOBS: An Information System for Oracle Bone Script,Body
2694,12702," https://github.com/SussexCompSem/learninghypernyms"," ['5 Evaluation', '5.1 Hyponymy with Word2Vec Vectors']","For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs [Cite_Footnote_2] from the BLESS dataset (Baroni and Lenci, 2011).",2 https://github.com/SussexCompSem/ learninghypernyms,"For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs [Cite_Footnote_2] from the BLESS dataset (Baroni and Lenci, 2011). These noun-noun word pairs include positive hy-ponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs. Their selection is balanced between positive and negative examples, so that accuracy can be used as the performance measure. For their semi-supervised experiments, ten-fold cross validation is used, where for each test set, items are removed from the associated training set if they contain any word from the test set. Thus, the vocabulary of the training and testing sets are always disjoint, thereby requiring that the models learn about the vector space and not about the words themselves. We had to perform our own 10-fold split, but apply the same procedure to filter the training set.",Material,Knowledge,True,Use（引用目的）,True,P16-1193_0_0,2016,A Vector Space for Distributional Semantics for Entailment ∗,Footnote
2695,12703," https://code.google.com/archive/p/word2vec/"," ['5 Evaluation', '5.1 Hyponymy with Word2Vec Vectors']","We could not replicate the word embeddings used in Weeds et al. (2014), so instead we use pub-licly available word embeddings. [Cite_Footnote_4]",4 https://code.google.com/archive/p/word2vec/,"We could not replicate the word embeddings used in Weeds et al. (2014), so instead we use pub-licly available word embeddings. [Cite_Footnote_4] These vectors were trained with the Word2Vec software applied to about 100 billion words of the Google-News dataset, and have 300 dimensions.",Material,Knowledge,True,Use（引用目的）,True,P16-1193_1_0,2016,A Vector Space for Distributional Semantics for Entailment ∗,Footnote
2696,12704," http://hierplane.allenai.org/explain"," ['3 Key Features']","As a companion to this section, we invite readers to use our web demonstration at [Cite] http://hierplane.allenai.org/explain to in-teractively explore examples.",,"In this section, we describe our key features for fa-cilitating a rapid understanding of linguistic struc-ture. As a companion to this section, we invite readers to use our web demonstration at [Cite] http://hierplane.allenai.org/explain to in-teractively explore examples. 1",補足資料,Website,True,Produce（引用目的）,True,D17-2009_0_0,2017,Interactive Visualization for Linguistic Structure,Body
2697,12705," http://hierplane.allenai.org/explain"," ['4 APIs']","Our web interface, hosted at [Cite] http: //hierplane.allenai.org/explain, has the architecture shown in Figure 6.",,"Our web interface, hosted at [Cite] http: //hierplane.allenai.org/explain, has the architecture shown in Figure 6. The front-end accepts text input from the user. It forwards this text to the back-end and receives a JSON that contains an annotated tree and styling instructions. Figure 7 shows a simplified version of the JSON that renders the “He ate pasta with chopsticks” subtree in Figure 5. This JSON is passed to the visualization library, which renders it in HTML and returns this HTML to the front-end.",Method,Tool,True,Produce（引用目的）,True,D17-2009_1_0,2017,Interactive Visualization for Linguistic Structure,Body
2698,12706," http://hierplane.allenai.org/explain/"," ['4 APIs', '4.1 Basic Features']",[Cite] http://hierplane.allenai.,,"• We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence “reliably great music, good food, and excellent service”). [Cite] http://hierplane.allenai.org/explain/ with http://hierplane.allenai.org/explain/stanford/",補足資料,Media,False,Produce（引用目的）,True,D17-2009_3_0,2017,Interactive Visualization for Linguistic Structure,Body
2699,12707," http://hierplane.allenai.org/explain/stanford/"," ['4 APIs', '4.1 Basic Features']",org/explain/ with [Cite] http://hierplane.allenai.,,"• We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence “reliably great music, good food, and excellent service”). http://hierplane.allenai.org/explain/ with [Cite] http://hierplane.allenai.org/explain/stanford/",Material,Knowledge,False,Use（引用目的）,True,D17-2009_4_0,2017,Interactive Visualization for Linguistic Structure,Body
2700,12708," http://hierplane.allenai.org/explain/stanford/"," ['4 APIs', '4.1 Basic Features']",For instance: [Cite] http://hierplane.allenai.,,For instance: [Cite] http://hierplane.allenai.org/explain/stanford/ He%20ate%20pasta%20with% 20chopsticks,Material,Knowledge,False,Introduce（引用目的）,False,D17-2009_5_0,2017,Interactive Visualization for Linguistic Structure,Body
2701,12709," http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool"," ['2 Related Work']","There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010 [Cite_Ref]",Awais Athar. 2010. Dependensee: A dependency parse visualization tool. http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool. Accessed: 2017-04-27.,"Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010 [Cite_Ref] ; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016) provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008).",補足資料,Paper,True,Introduce（引用目的）,True,D17-2009_6_0,2017,Interactive Visualization for Linguistic Structure,Reference
2702,12710," https://demos.explosion.ai/"," ['2 Related Work']","There are a number of libraries (Stenetorp et al., 2012; Montani, 2016 [Cite_Ref] ; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom).",Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.,"Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016 [Cite_Ref] ; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016) provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008).",補足資料,Paper,True,Introduce（引用目的）,True,D17-2009_7_0,2017,Interactive Visualization for Linguistic Structure,Reference
2703,12711," https://demos.explosion.ai/"," ['2 Related Work']","Dis-placy (Montani, 2016) [Cite_Ref] provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008).",Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.,"Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016) [Cite_Ref] provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008).",Method,Tool,True,Introduce（引用目的）,True,D17-2009_7_1,2017,Interactive Visualization for Linguistic Structure,Reference
2704,12712," http://nlpviz.bpodgursky.com"," ['2 Related Work']","Figure 1 was generated from (Podgursky, 2015) [Cite_Ref] , which provides open-source code and a web interface for a static rendering of a con-stituency parse.",Ben Podgursky. 2015. Nlpviz. http://nlpviz.bpodgursky.com. Accessed: 2017-04-27.,"Figure 1 was generated from (Podgursky, 2015) [Cite_Ref] , which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016) provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008).",Method,Tool,True,Use（引用目的）,True,D17-2009_8_0,2017,Interactive Visualization for Linguistic Structure,Reference
2705,12713," http://trainomatic.org"," ['References']",All the training data is available for research purposes at [Cite] http://trainomatic.org.,,"Annotating large numbers of sentences with senses is the heaviest requirement of current Word Sense Disambiguation. We present Train-O-Matic, a language-independent method for generating mil-lions of sense-annotated training instances for virtually all meanings of words in a language’s vocabulary. The approach is fully automatic: no human interven-tion is required and the only type of hu-man knowledge used is a WordNet-like resource. Train-O-Matic achieves consis-tently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the bur-den of manual annotation. All the training data is available for research purposes at [Cite] http://trainomatic.org.",Material,Dataset,True,Produce（引用目的）,True,D17-1008_0_0,2017,Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data,Body
2706,12714," http://babelnet.org"," ['3 Creating a Denser and Multilingual Semantic Network']","To cope with these issues, we exploit Babel-Net, [Cite_Footnote_2] a huge multilingual semantic network ob-tained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Nav-igli and Ponzetto, 2012), and create the Babel-Net subgraph induced by the WordNet vertices.",2 http://babelnet.org,"To cope with these issues, we exploit Babel-Net, [Cite_Footnote_2] a huge multilingual semantic network ob-tained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Nav-igli and Ponzetto, 2012), and create the Babel-Net subgraph induced by the WordNet vertices. The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to Word-Net (such as hyperlinks in a Wikipedia article con-necting it to other articles). The greatest contribu-tion of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related ar-ticles (e.g., the English Wikipedia Bus article is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more).",Material,DataSource,False,Use（引用目的）,True,D17-1008_1_0,2017,Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data,Footnote
2707,12715," http://trainomatic.org"," ['8 Conclusion']","All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at [Cite] http://trainomatic.org.",,"We believe that the ability of T-O-M to over-come the current paucity of annotated data for WSD, coupled with video games with a pur-pose for validation purposes (Jurgens and Nav-igli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD. All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at [Cite] http://trainomatic.org.",Material,Dataset,True,Produce（引用目的）,True,D17-1008_2_0,2017,Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data,Body
2708,12716," https://radimrehurek.com/gensim/"," ['5 Experiments']","For all experiments, we use the Word2Vec imple-mentation in Gensim [Cite_Footnote_2] to learn the skip-gram model with dimensionality 500 for each language.",2 https://radimrehurek.com/gensim/,"For all experiments, we use the Word2Vec imple-mentation in Gensim [Cite_Footnote_2] to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).",Method,Code,True,Use（引用目的）,True,N16-1072_0_0,2016,Cross-lingual Wikification Using Multilingual Embeddings,Footnote
2709,12717," https://github.com/mfaruqui/crosslingual-cca"," ['5 Experiments']","The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) [Cite_Footnote_3] in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).","3 https://github.com/mfaruqui/ crosslingual-cca proaches on various languages. Since about one-third of the test mentions are non-trivial, a baseline is 66.67 for all languages, if we pick the most common title given the mention. Bold signi-fies highest score for each column.","For all experiments, we use the Word2Vec imple-mentation in Gensim to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) [Cite_Footnote_3] in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).",Method,Code,True,Use（引用目的）,True,N16-1072_1_0,2016,Cross-lingual Wikification Using Multilingual Embeddings,Footnote
2710,12718," http://bilbo.cs.illinois.edu/~ctsai12/xlwikifier-wikidata.zip"," ['5 Experiments', '5.1 Wikipedia Dataset']",This dataset is publicly avail-able at [Cite] http://bilbo.cs.illinois.edu/˜ctsai12/xlwikifier-wikidata.zip.,,"We create this dataset from the documents in Wikipedia by taking the anchors (hyperlinked texts) as the query mentions and the corresponding English Wikipedia titles as the answers. Note that we only keep the mentions for which we can get the corre-sponding English Wikipedia titles by the language links. As observed in previous work (Ratinov et al., 2011), most of the mentions in Wikipedia docu-ments are easy, that is, the baseline of simply choos-ing the title that maximizes P r(title|mention), the most frequent title given the mention surface string, performs quite well. In order to create a more chal-lenging dataset, we randomly select mentions such that the number of easy mentions is about twice the number of hard mentions (those mentions for which the most common title is not the correct title). This generation process is inspired by (and close to) the distribution generated in the TAC KBP2015 Entity Linking Track. Another problem that occurs when creating a dataset from Wikipedia documents is that even though training documents are different from test documents, many mentions and titles actually overlap. To test that the algorithms really general-ize from training examples, we ensure that no (men-tion, title) pair in the test set appear in the training set. Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at [Cite] http://bilbo.cs.illinois.edu/˜ctsai12/xlwikifier-wikidata.zip.",Material,Dataset,True,Produce（引用目的）,True,N16-1072_2_0,2016,Cross-lingual Wikification Using Multilingual Embeddings,Body
2711,12719," http://sweaglesw.org/linguistics/ace/"," ['3 Literal versus Intended Meaning', '3.3.2 SemBanking with ERG']","To be specific, for each input sentence S, we gener-ate K-best semantic graphs G 1 , G 2 , ..., G K with an ERG-based processor, i.e. ACE [Cite_Footnote_2] .",2 http://sweaglesw.org/linguistics/ace/,"As there is no gold semantics-annotated corpus for learner English and building such a corpus from scratch is tedious and time-consuming, we exploit ERG to establish a large-scale sembank-ing with informative semantic representations. To be specific, for each input sentence S, we gener-ate K-best semantic graphs G 1 , G 2 , ..., G K with an ERG-based processor, i.e. ACE [Cite_Footnote_2] . The created grammar-licensed analyses contain both a deriva-tion tree recording the used grammar rules and lex-ical entries, and the associated semantic represen-tation constructed compositionally via this deriva-tion (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can bene-fit many NLP systems by incorporating domain-specific knowledge and reasoning.",Method,Tool,True,Introduce（引用目的）,True,2020.acl-main.606_0_0,2020,Semantic Parsing for English as a Second Language,Footnote
2712,12720," https://sites.google.com/site/naistlang8corpora/"," ['6 Parsing to Intended Meanings']",We establish a para-phrase table based on the statistical machine trans-lation between a parallel learner corpus [Cite_Footnote_3] .,3 https://sites.google.com/site/naistlang8corpora/,"We train the factorization-based model on DeepBank and examine the performance on L2 and L1 sentences as well as the revised sentences by two GEC models. The produced graphs are compared with I-silver which represents the in-tended meaning. We notice that during the compu-tation of SMATCH , some disagreements of nodes result from the discrepancy of morphological vari-ation or different collocations between the input and the standard sentence. Hence the node score may be underestimated. Therefore, we relax the standards of matching nodes. We establish a para-phrase table based on the statistical machine trans-lation between a parallel learner corpus [Cite_Footnote_3] . As long as the labels of two aligned nodes have the same stem or they form a paraphrase pair in our ta-ble, then the two nodes can be considered “match-ing”. We call the new evaluation metric as “node-relaxed SMATCH ”.",Material,Dataset,True,Use（引用目的）,True,2020.acl-main.606_1_0,2020,Semantic Parsing for English as a Second Language,Footnote
2713,12721," https://ibm.biz/acl21-loa"," ['References']","Demo site: [Cite] https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa",,"We present Logical Optimal Actions (LOA), an action decision architecture of reinforce-ment learning applications with a neuro-symbolic framework which is a combina-tion of neural network and symbolic knowl-edge acquisition approach for natural lan-guage interaction games. The demonstra-tion for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowl-edge for improving interpretability for trained rules. This demonstration also provides a com-parison module with other neuro-symbolic ap-proaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: [Cite] https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa",補足資料,Website,True,Produce（引用目的）,True,2021.acl-demo.27_0_0,2021,LOA: Logical Optimal Actions for Text-based Interaction Games,Body
2714,12722," https://github.com/ibm/loa"," ['References']","Demo site: https://ibm.biz/acl21-loa, Code: [Cite] https://github.com/ibm/loa",,"We present Logical Optimal Actions (LOA), an action decision architecture of reinforce-ment learning applications with a neuro-symbolic framework which is a combina-tion of neural network and symbolic knowl-edge acquisition approach for natural lan-guage interaction games. The demonstra-tion for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowl-edge for improving interpretability for trained rules. This demonstration also provides a com-parison module with other neuro-symbolic ap-proaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: [Cite] https://github.com/ibm/loa",補足資料,Website,True,Produce（引用目的）,True,2021.acl-demo.27_1_0,2021,LOA: Logical Optimal Actions for Text-based Interaction Games,Body
2715,12723," https://ibm.biz/acl21-loa"," ['1 Introduction']","In this demonstration (demo site: [Cite] https://ibm.biz/acl21-loa), we present a Log-ical Optimal Actions (LOA) architecture for neuro-symbolic RL applications with LNN (Riegel et al., 2020) for text-based interaction games.",,"In this demonstration (demo site: [Cite] https://ibm.biz/acl21-loa), we present a Log-ical Optimal Actions (LOA) architecture for neuro-symbolic RL applications with LNN (Riegel et al., 2020) for text-based interaction games. While natural language-based interactive agents are the ambitious but attractive target as real-world applications of neuro-symbolic, it is not easy to provide an environment for the agent. The proposed demonstration uses text-based games learning environment, called TextWorld (Côté et al., 2018), as a miniature of a natural language-based interactive environment. The demonstration provides a web-based user interface for visualizing the game interaction, which is including displaying the natural text observation from the environment, typing the action sentence, and showing the reward value from the taken action. The LOA in this demonstration also visualizes trained and pre-defined logical rules in LNN via the same interface, and this will help the human user understand the benefits of introducing the logical rules via neuro-symbolic frameworks. We also supply an open-sourced implementation for demo environment and some RL methods. This implementation contains our logical approaches and other state-of-the-art agents.",補足資料,Website,True,Produce（引用目的）,True,2021.acl-demo.27_2_0,2021,LOA: Logical Optimal Actions for Text-based Interaction Games,Body
2716,12724," https://ibm.biz/acl21-loa"," ['4 Conclusion']",We propose a novel demonstration (URL: [Cite] https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm.,,"We propose a novel demonstration (URL: [Cite] https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent. We also extend more complicated LNN for other difficult games on the demo site. At the same time, we open the source code for the demonstration (URL: https://github.com/ibm/loa).",補足資料,Media,False,Produce（引用目的）,True,2021.acl-demo.27_3_0,2021,LOA: Logical Optimal Actions for Text-based Interaction Games,Body
2717,12725," https://github.com/ibm/loa"," ['4 Conclusion']","At the same time, we open the source code for the demonstration (URL: [Cite] https://github.com/ibm/loa).",,"We propose a novel demonstration (URL: https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent. We also extend more complicated LNN for other difficult games on the demo site. At the same time, we open the source code for the demonstration (URL: [Cite] https://github.com/ibm/loa).",Method,Code,True,Produce（引用目的）,True,2021.acl-demo.27_4_0,2021,LOA: Logical Optimal Actions for Text-based Interaction Games,Body
2718,12726," https://github.com/Frozenmad/AMN_SRL"," ['3 Experiments']",We re-run our model using different initialized parameters for 4 times and re-port the average performance [Cite_Footnote_4] .,4 Our implementation is publicly available at https://github.com/Frozenmad/AMN_SRL.,"For English, We independently determine the best distance calculating method and the best merging method one after another. First, we select a distance according to the results on development set and then we determine the merging method with the selected distance method. At last we ex-plore the impact of memory size. For Chinese, we obtain the result with similar parameters as for the best model in English. The English and Chi-nese GloVe word embeddings are both trained on Wikipedia. The pretrained English ELMo model is from (Peters et al., 2018), and the Chinese one is from (Che et al., 2018), which is hosted at (Fares et al., 2017). The model is trained for maximum 20 epochs for the nearly best model based on de-velopment set results. We re-run our model using different initialized parameters for 4 times and re-port the average performance [Cite_Footnote_4] .",Mixed,Mixed,True,Produce（引用目的）,True,N19-1340_0_0,2019,Semantic Role Labeling with Associated Memory Network,Footnote
2719,12727," https://diamt.limsi.fr/eval.html"," ['2 Evaluating contextual phenomena', '2.1 Our contrastive discursive test sets']",We created two contrastive test sets to help com-pare how well different contextual MT mod-els handle (i) anaphoric pronoun translation and (ii) coherence and cohesion. [Cite_Footnote_2],2 The test sets are freely available at https://diamt.limsi.fr/eval.html.,"We created two contrastive test sets to help com-pare how well different contextual MT mod-els handle (i) anaphoric pronoun translation and (ii) coherence and cohesion. [Cite_Footnote_2] For each test set, models are assessed on their ability to rank the cor-rect translation of an ambiguous sentence higher than the incorrect translation, using the disam-biguating context provided in the previous source and/or target sentence. All examples in the test sets are hand-crafted but inspired by real examples from OpenSubtitles2016 (Lison and Tiedemann, 2016) to ensure that they are credible and that vo-cabulary and syntactic structures are varied. The method can be used to evaluate any NMT model, by making it produce a score for a given source sentence and reference translation.",Material,Dataset,True,Produce（引用目的）,True,N18-1118_0_0,2018,Evaluating Discourse Phenomena in Neural Machine Translation,Footnote
2720,12728," http://www.opensubtitles.org"," ['4 Experiments', '4.1 Data']","Models are trained and tested on fan-produced parallel subtitles from OpenSubtitles2016 [Cite_Footnote_6] (Lison and Tiedemann, 2016).",6 http://www.opensubtitles.org,"Models are trained and tested on fan-produced parallel subtitles from OpenSubtitles2016 [Cite_Footnote_6] (Lison and Tiedemann, 2016). The data is first corrected using heuristics (e.g. minor corrections of OCR and encoding errors). It is then tokenised, fur-ther cleaned (keeping subtitles ≤80 tokens) and truecased using the Moses toolkit (Koehn et al., 2007) and finally split into subword units using BPE (Sennrich et al., 2016). 7 We run all exper-iments in a high-resource setting, with a training set of ≈29M parallel sentences, with vocabulary sizes of ≈55k for English and ≈60k for French.",補足資料,Website,True,Introduce（引用目的）,True,N18-1118_1_0,2018,Evaluating Discourse Phenomena in Neural Machine Translation,Footnote
2721,12729," https://doi.org/10.4000/discours.9047"," ['2 Evaluating contextual phenomena']","Manual evaluation has always been an essential part of evaluating MT quality, and targeted transla-tion allows us to isolate a model’s performance on specific linguistic phenomena; recent work using in-depth, qualitative manual evaluation (Isabelle et al., 2017; Scarton and Specia, 2015 [Cite_Ref] ) is very in-formative.",Carolina Scarton and Lucia Specia. 2015. A Quan-titative Analysis of Discourse Phenomena in Ma-chine Translation. Discours [online] (16). https: //doi.org/10.4000/discours.9047.,"Guillou and Hardmeier’s (2016) pronoun trans-lation test suite succeeds in overcoming some of these problems by creating an automatic evalua-tion method, with a back-off manual evaluation. Manual evaluation has always been an essential part of evaluating MT quality, and targeted transla-tion allows us to isolate a model’s performance on specific linguistic phenomena; recent work using in-depth, qualitative manual evaluation (Isabelle et al., 2017; Scarton and Specia, 2015 [Cite_Ref] ) is very in-formative. Isabelle et al. (2017) focus on specially constructed challenging examples in order to anal-yse differences between systems. They cover a wide range of linguistic phenomena, but since manual evaluation is costly and time-consuming, only a few examples per phenomenon are anal-ysed, and it is difficult to obtain quick, quantitative feedback.",補足資料,Paper,True,Introduce（引用目的）,True,N18-1118_2_0,2018,Evaluating Discourse Phenomena in Neural Machine Translation,Reference
2722,12730," http://www.msxiaoice.com"," ['References']","(ii) For Chinese, we compare DocChat with XiaoIce [Cite_Footnote_2] , a famous chitchat engine in China, and side-by-side evalua-tion shows that DocChat is a perfect com-plement for chatbot engines using Q-R pairs as main source of responses.",2 http://www.msxiaoice.com,"Most current chatbot engines are designed to reply to user utterances based on exist-ing utterance-response (or Q-R) pairs. In this paper, we present DocChat, a novel information retrieval approach for chat-bot engines that can leverage unstructured documents, instead of Q-R pairs, to re-spond to utterances. A learning to rank model with features designed at different levels of granularity is proposed to mea-sure the relevance between utterances and responses directly. We evaluate our pro-posed approach in both English and Chi-nese: (i) For English, we evaluate Doc-Chat on WikiQA and QASent, two answer sentence selection tasks, and compare it with state-of-the-art methods. Reasonable improvements and good adaptability are observed. (ii) For Chinese, we compare DocChat with XiaoIce [Cite_Footnote_2] , a famous chitchat engine in China, and side-by-side evalua-tion shows that DocChat is a perfect com-plement for chatbot engines using Q-R pairs as main source of responses.",Method,Tool,True,Compare（引用目的）,True,P16-1049_0_0,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,Footnote
2723,12731," http://www.freebase.com/"," ['4 Response Ranking', '4.5 Relation-level Feature']","Given a structured knowledge base, such as Free-base [Cite_Footnote_5] , a single relation question Q (in natural language) with its answer can be first parsed into a fact formatted as he sbj ,rel,e obj i, where e sbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, e obj denotes an object entity found from the knowledge base based on e sbj and rel.",5 http://www.freebase.com/,"Given a structured knowledge base, such as Free-base [Cite_Footnote_5] , a single relation question Q (in natural language) with its answer can be first parsed into a fact formatted as he sbj ,rel,e obj i, where e sbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, e obj denotes an object entity found from the knowledge base based on e sbj and rel. Then we can get hQ,reli pairs. This rel can help for modeling semantic relationships between Q and R. For example, the Q-A pair hWhat does Jimmy Neutron do? − inventori can be parsed into hJimmy Neutron, fiction-al character occupation, inventori where the rel is fictional character occupation.",Material,Knowledge,True,Introduce（引用目的）,True,P16-1049_1_0,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,Footnote
2724,12732," http://aka.ms/WikiQA"," ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']","We select WikiQA [Cite_Footnote_6] as the evaluation data, as it is precisely constructed based on natural language questions and Wikipedia documents, which con-tains 2,118 ‘question-document’ pairs in the train-ing set, 296 ‘question-document’ pairs in devel-opment set, and 633 ‘question-document’ pairs in testing set.",6 http://aka.ms/WikiQA,"We select WikiQA [Cite_Footnote_6] as the evaluation data, as it is precisely constructed based on natural language questions and Wikipedia documents, which con-tains 2,118 ‘question-document’ pairs in the train-ing set, 296 ‘question-document’ pairs in devel-opment set, and 633 ‘question-document’ pairs in testing set. Each sentence in the document of a given question is labeled as 1 or 0, where 1 de-notes the current sentence is a correct answer sen-tence, and 0 denotes the opposite meaning. Given a question, the task of WikiQA is to select answer sentences from all sentences in a question’s corre-sponding document. The training data settings of response ranking features are described below.",Material,DataSource,True,Use（引用目的）,True,P16-1049_2_0,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,Footnote
2725,12733," http://wiki.answers.com"," ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']","For h W2W , GIZA++ is used to train word alignments on 11.6M ‘question-related question’ pairs (Fader et al., 2013) crawled from WikiAnswers. [Cite_Footnote_7] .",7 http://wiki.answers.com,"F w denotes 3 word-level features, h WM , h W2W and h W2V . For h W2W , GIZA++ is used to train word alignments on 11.6M ‘question-related question’ pairs (Fader et al., 2013) crawled from WikiAnswers. [Cite_Footnote_7] . For h W2V , Word2Vec (Mikolov et al., 2013) is used to train word embedding on sentences from Wikipedia in English.",Material,DataSource,True,Use（引用目的）,True,P16-1049_3_0,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,Footnote
2726,12734," https://answers.yahoo.com"," ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']","For h PT , GIZA++ trains word align-ments on 4M ‘question-answer’ pairs crawled from Yahoo Answers [Cite_Footnote_10] , and then a phrase ta-ble is extracted from word alignments using the intersect-diag-grow refinement.",10 https://answers.yahoo.com,"F p denotes 2 phrase-level features, h PP and h PT . For h PP , bilingual data is used to extrac-t a phrase-based translation table (Koehn et al., 2003), from which paraphrases are extracted (Sec-tion 4.2.1). For h PT , GIZA++ trains word align-ments on 4M ‘question-answer’ pairs crawled from Yahoo Answers [Cite_Footnote_10] , and then a phrase ta-ble is extracted from word alignments using the intersect-diag-grow refinement.",Material,DataSource,True,Use（引用目的）,True,P16-1049_4_0,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,Footnote
2727,12735," https://research.facebook.com/research/-babi/"," ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']","Bordes et al. (2015) released the SimpleQuestions data set [Cite_Footnote_11] , which consists of 108,442 English questions.",11 https://research.facebook.com/research/-babi/,"F r and F ty denote relation-level feature h RE and type-level feature h TE . Bordes et al. (2015) released the SimpleQuestions data set [Cite_Footnote_11] , which consists of 108,442 English questions. Each ques-tion (e.g., What does Jimmy Neutron do?) is written by human annotators based on a triple in Freebase which formatted as he sbj , rel, e obj i (e.g., hJimmy Neutron, fictional character occupation, inventori) Here, as described in Section 4.5 and 4.6, ‘question-relation’ pairs and ‘question-type’ pairs based upon SimpleQuestions data set are used to train h RE and h TE .",Material,Dataset,True,Introduce（引用目的）,True,P16-1049_5_0,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,Footnote
2728,12736," https://msnews.github.io/index.html"," ['4 Experiment', '4.1 Experimental Datasets and Settings']","The first one is the public MIND dataset (Wu et al., 2020d) [Cite_Footnote_1] .",1 We use the small version of MIND for quick experiments. This dataset is at https://msnews.github.io/index.html,"We conduct extensive experiments on two real-world datasets to evaluate the effectiveness of Hi-eRec. The first one is the public MIND dataset (Wu et al., 2020d) [Cite_Footnote_1] . It is constructed by user behavior data collected from Microsoft News from October 12 to November 22, 2019 (six weeks), where user data in the first four weeks was used to construct users’ reading history, user data in the penultimate week was used for model training and user data in the last week was used for evaluation. Besides, MIND contains off-the-shelf topic and subtopic la-bel for each news. The second one (named Feeds) is constructed by user behavior data sampled from a commercial news feeds app in Microsoft from Jan-uary 23 to April 01, 2020 (13 weeks). We randomly sample 100,000 and 10,000 impressions from the first ten weeks to construct training and validation set, and 100,000 impressions from the last three weeks to construct test data. Since Feeds only con-tains topic label of news, we implement a simpli-fied version of HieRec with only user- and topic-level interest representations on Feeds. Besides, following Wu et al. (2020d), users in Feeds were anonymized via hash algorithms and de-linked from the production system to protect user privacy. Detailed information is summarized in Table 1.",Material,Dataset,True,Use（引用目的）,True,2021.acl-long.423_0_0,2021,HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation,Footnote
2729,12737," https://github.com/JulySinceAndrew/HieRec"," ['4 Experiment', '4.1 Experimental Datasets and Settings']",All hyper-parameters of HieRec and baseline methods are manually tuned on the valida-tion set. [Cite_Footnote_2],2 https://github.com/JulySinceAndrew/HieRec,"Next, we introduce experimental settings and hyper-parameters of HieRec. We use the first 30 words and 5 entities of news titles and users’ re-cent 50 clicked news in experiments. We adopt pre-trained glove (Pennington et al., 2014) word embeddings and TransE entity embeddings (Bor-des et al., 2013) for initialization. In HieRec, the word and entity self-attention network output 400-and 100-dimensional vectors, respectively. Besides, the unified news representation is 400-dimensional. Attention networks (i.e., φ s (·), φ t (·), and φ g (·)) are implemented by single-layer dense networks. Besides, dimensions of topic and subtopic embed-dings are 400, both of which are randomly ini-tialized and fine-tuned. The hyper-parameters for combining different interest scores, i.e. λ t and λ s , are set to 0.15 and 0.7 respectively. Moreover, we utilize dropout technique (Srivastava et al., 2014) and Adam optimizer (Kingma and Ba, 2015) for training. HieRec is trained for 5 epochs with 0.0001 learning rate. All hyper-parameters of HieRec and baseline methods are manually tuned on the valida-tion set. [Cite_Footnote_2] Following Wu et al. (2019e), we use four ranking metrics, i.e., AUC, MRR, nDCG@5, and nDCG@10, for performance evaluation.",Material,Dataset,True,Use（引用目的）,True,2021.acl-long.423_1_0,2021,HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation,Footnote
2730,12739," https://github.com/sebastianruder/emnlp2021-multiqa-tutorial"," ['References']","Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [Cite_Footnote_1]",1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don’t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational,"Question answering (QA) is one of the most challenging and impactful tasks in natural language processing. Most research in QA, however, has focused on the open-domain or monolingual setting while most real-world applications deal with specific domains or languages. In this tutorial, we attempt to bridge this gap. Firstly, we introduce stan-dard benchmarks in multi-domain and multi-lingual QA. In both scenarios, we discuss state-of-the-art approaches that achieve impressive performance, ranging from zero-shot trans-fer learning to out-of-the-box training with open-domain QA systems. Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [Cite_Footnote_1]",補足資料,Document,True,Produce（引用目的）,True,2021.emnlp-tutorials.4_0_0,2021,Multi-Domain Multilingual Question Answering,Footnote
2731,12740," https://github.com/ygraham/significance-williams"," ['4 Evaluation and Discussion']","As part of this research, we have made avail-able an open-source implementation of statis-tical tests tailored to the assessment of MT metrics available at [Cite] https://github.com/ygraham/significance-williams.",,"As part of this research, we have made avail-able an open-source implementation of statis-tical tests tailored to the assessment of MT metrics available at [Cite] https://github.com/ygraham/significance-williams.",Method,Tool,True,Produce（引用目的）,True,D14-1020_0_0,2014,Testing for Significance of Increased Correlation with Human Judgment,Body
2732,12741," https://github.com/PlusLabNLP/Plot-guided-Coherence-Evaluation"," ['3 Implausible Text Construction', '3.2.1 Plot Manipulations']",Figure 2 demon-strates various proposed plot-level manipulations in dotted boxes. [Cite_Footnote_1],"1 Our proposed data, trained models and code is re-leased at https://github.com/PlusLabNLP/ Plot-guided-Coherence-Evaluation","Here we leverage this idea for generating im-plausible texts, by controllable injection of implau-sibility sources, or perturbations, into the ground-truth plots. The resulting plot-level manipulations will force the model to reflect applied implausi-bility in the generated text and will negatively im-pact the text’s plausibility. In contrast to Guan and Huang (2020), our proposed plot-level manip-ulations (M AN P LTS ) do not directly change the text at the token level instead, we inject incoher-ence into language at the concept level. The plot-guided generation guarantees the naturalness of generations since it leverages a well-trained condi-tional language model. The generated samples are also anticipated to be closer and congruous to the machine-generated texts that will be assessed dur-ing the inference time. Concept-level incoherence creates implausible factors that guide models to include that implausible sources. Figure 2 demon-strates various proposed plot-level manipulations in dotted boxes. [Cite_Footnote_1] All proposed manipulations are described in the following sections. We refer this data as ManPlts. Non-logically Ordered Plots. Logical conflict is one of the sources for implausibility that re-sults from not-logically ordered concepts in the text. While Guan and Huang (2020) covered this type of implausibility by changing the order of sentences, we hypothesize that disrupting the log-ical order at the concept-level is more efficient. To accomplish concept reordering, we first ran-domly choose verbs from the plot and leverage the COMET (Bosselut et al., 2019) model to predict their subsequent events. Then we dislocate the re-sulted concept pairs. COMET, which is trained on tuples of the form ( subject, relation, object), can be used to predict an object given a pair of subject and relation. As an example, given the pair (work, causes) COMET will predict get pay to show that work causes to get paid. We focus on COMET relations HasPrerequisite, HasFirst-Subevent, Causes and HasLastSubevent that imply ordering. In the first two relations, object should appear before subject, while in the other two the order is reversed. Therefore, subject work comes before get pay due to the causes relation that holds between them. We flip the correct order of con-cepts and attach them with or without randomly selected connection words such as then, later, sub-sequently to generate implausible texts (the purple box in Figure 2).",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.343_0_0,2021,Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation,Footnote
2733,12742," https://github.com/huggingface/transformers"," ['5 Experiments', '5.1 Datasets']","We use 70 percent of stories in ROC (ROC_LM) and WP (WP_LM) for fine-tuning GPT2 (Radford et al., 2019) lan-guage model with batch size of 4. [Cite_Footnote_2]",2 We fine-tune GPT2 language model using https://github.com/huggingface/transformers.,"Data Preparation. We split the stories from both datasets into two subsets for training generation and evaluation models, respectively. We use 70 percent of stories in ROC (ROC_LM) and WP (WP_LM) for fine-tuning GPT2 (Radford et al., 2019) lan-guage model with batch size of 4. [Cite_Footnote_2] After 3 epochs of fine-tuning, the perplexity on the validation set of ROC and WP datasets are 8.28 and 25.04, re-spectively.",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.343_1_0,2021,Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation,Footnote
2734,12743," https://github.com/huggingface/transformers"," ['5 Experiments', '5.3 Experimental Setup']","To evaluate WP with lengthy stories, we fine-tune pretrained Longformer-base model with the learning rate of 2e-5 and batch size 3 by encoding texts with at most 1024 tokens for three epochs. [Cite_Footnote_3]",3 We fine-tune RoBERTa and Longformer mod-els using https://github.com/huggingface/transformers.,"In our experiments, we have SENTAVG as the baseline model. We compare SENTAVG across more powerful classifiers – RoBERTa for ROC sto-ries and Longformer for WP stories (FT_LM). We fine-tune pretrained RoBERTa-base model with the learning rate of 2e-5 and batch size 8 for three epochs and process the ROC stories with a maxi-mum of 128 tokens. To evaluate WP with lengthy stories, we fine-tune pretrained Longformer-base model with the learning rate of 2e-5 and batch size 3 by encoding texts with at most 1024 tokens for three epochs. [Cite_Footnote_3]",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.343_2_0,2021,Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation,Footnote
2735,12744," https://github.com/prdwb/bert_hae"," ['4 A more Robust Protocol', '4.1 Reproducing the Regular Evaluation in the Semi-automatic Scenario']","We implement our own train-ing script on the basis of codes pieces from the transformers library (Wolf et al., 2019) and BERT-HAE’s authors [Cite_Footnote_1] .",1 https://github.com/prdwb/bert_hae,"To evaluate the baseline performance (semi-automatic), we train BERT-HAE and BERT-PHAE on QuAC using the protocol described by the au-thors (Qu et al., 2019a) and the same hyperparame-ters: history markers from up to 6 turns, and spe-cific optimization parameters (12 as batch size, 3e- 5 as learning rate with a linear decrease to 0 over 24k training steps). We implement our own train-ing script on the basis of codes pieces from the transformers library (Wolf et al., 2019) and BERT-HAE’s authors [Cite_Footnote_1] . Experiments are run with a Nvidia Tesla V100 GPU.",Method,Code,True,Extend（引用目的）,True,2021.acl-short.130_0_0,2021,Towards a more Robust Evaluation for Conversational Question Answering,Footnote
2736,12745," https://nyti.ms/2DycutY"," ['1 Introduction']",[Cite_Footnote_1],1 A New York Times article at https://nyti.ms/2DycutY.,"When the SWAG dataset was first announced (Zellers et al., 2018), this new task of common-sense natural language inference seemed trivial for humans (88%) and yet challenging for then-state-of-the-art models (†60%), including ELMo (Peters et al., 2018). However, BERT (Devlin et al., 2018) soon reached over 86%, almost human-level performance. One news article on this development was headlined “finally, a ma-chine that can finish your sentence.” [Cite_Footnote_1]",補足資料,Document,True,Introduce（引用目的）,True,P19-1472_0_0,2019,HellaSwag: Can a Machine Really Finish Your Sentence?,Footnote
2737,12746," https://rowanzellers.com/hellaswag"," ['1 Introduction']","We study this question by introducing Hella-Swag, [Cite_Footnote_2] a new benchmark for commonsense NLI.","2 Short for Harder Endings, Longer contexts, and Low-shot Activities for Situations With Adversarial Generations. Dataset and code at https://rowanzellers.com/hellaswag.","We study this question by introducing Hella-Swag, [Cite_Footnote_2] a new benchmark for commonsense NLI. We use Adversarial Filtering (AF), a data-collection paradigm in which a series of discrim-inators is used to select a challenging set of gen-erated wrong answers. AF is surprisingly e↵ec-tive towards this goal: the resulting dataset of 70k problems is easy for humans (95.6% accuracy), yet challenging for machines (†50%q. This result holds even when models are given a significant number of training examples, and even when the test data comes from the exact same distribution as the training data. Machine performance slips an additional 5% when evaluated on examples that cover novel concepts from the same domain.",Mixed,Mixed,True,Produce（引用目的）,True,P19-1472_1_0,2019,HellaSwag: Can a Machine Really Finish Your Sentence?,Footnote
2738,12747," https://github.com/jiachenwestlake/Entity_BERT"," ['1 Introduction']",Our code and NER datasets are released at [Cite] https://github.com/jiachenwestlake/Entity_BERT.,,"Results on the three datasets show that our method outperforms these methods and achieves the best results, which demonstrates the effec-tiveness of the proposed Char-Entity-Transformer structure for integrating entity information in LM pre-training for Chinese NER. To our knowl-edge, we are the first to investigate how to make use of the scale of the input document text for enhancing NER. Our code and NER datasets are released at [Cite] https://github.com/jiachenwestlake/Entity_BERT.",Mixed,Mixed,True,Produce（引用目的）,True,2020.emnlp-main.518_0_0,2020,Entity Enhanced BERT Pre-training for Chinese NER,Body
2739,12748," https://github.com/google-research/"," ['3 Method', '3.6 Training Procedure']","Our model is initialized using a pre-trained BERT model [Cite_Footnote_2] , and the other parameters are randomly initialized.","2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.","Our model is initialized using a pre-trained BERT model [Cite_Footnote_2] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task.",Method,Code,False,Use（引用目的）,True,2020.emnlp-main.518_1_0,2020,Entity Enhanced BERT Pre-training for Chinese NER,Footnote
2740,12749," https://github.com/PaddlePaddle/ERNIE/tree/repro"," ['4 Experiments', '4.2 Experimental Settings']","[Cite_Footnote_5] (Sun et al., 2019a,b) enhances B ERT through knowledge integration using a entity-level masked LM task and more raw text from the Web resources, which achieves the currently best results on Chinese NER.",5 https://github.com/PaddlePaddle/ERNIE/tree/repro,"ERNIE baselines. E RNIE [Cite_Footnote_5] (Sun et al., 2019a,b) enhances B ERT through knowledge integration using a entity-level masked LM task and more raw text from the Web resources, which achieves the currently best results on Chinese NER. E RNIE +F UR +E NT is a stronger baseline, which uses the same entity dictionary as ours for entity-level masking and further pre-trains E RNIE on the same raw text as ours.",補足資料,Paper,False,Introduce（引用目的）,False,2020.emnlp-main.518_2_0,2020,Entity Enhanced BERT Pre-training for Chinese NER,Footnote
2741,12750," https://catalog.ldc.upenn.edu/LDC2011T13"," ['4 Experiments', '4.2 Experimental Settings']","We use the same embeddings as (Zhang and Yang, 2018), which are pre-trained on Giga-Word [Cite_Footnote_6] us-ing Word2vec (Mikolov et al., 2013).",6 https://catalog.ldc.upenn.edu/ LDC2011T13,"LSTM baselines. We compare character-level B I L STM (Lample et al., 2016) and B I L STM +E NT , which concatenates the character embeddings and its corresponding entity embeddings as inputs. We also compare a gazetteer based method L ATTICE (Zhang and Yang, 2018) and L ATTICE (R E E NT ), which replaces the word gazetteer of L ATTICE with our entity dictionary for fair comparison. We use the same embeddings as (Zhang and Yang, 2018), which are pre-trained on Giga-Word [Cite_Footnote_6] us-ing Word2vec (Mikolov et al., 2013). The entity embeddings are randomly initialized and fine-tuned during training.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-main.518_3_0,2020,Entity Enhanced BERT Pre-training for Chinese NER,Footnote
2742,12751," http://github.com/fxsjy/jieba"," ['4 Experiments', '4.4 Analysis']","We consider (i) H ALF -E NT , which uses 50% randomly selected entities from the original entity dictionary; (ii) N- GRAMS , which uses randomly selected n-grams from the raw text; (iii) O PEN - DOMAIN , which uses an open-domain dictionary from Jieba [Cite_Footnote_7] .",7 http://github.com/fxsjy/jieba,"Ablation study. As shown in Table 4, we use two groups of ablation study to investigate the effect of entity information. (1) Entity prediction task. We consider (i) N O -E NT - C LASS , which does not use the entity classification task in pre-training; and (ii) N O -P RETRAIN , which does not use entity enhanced pre-training. Results of these methods suffer significantly decreases com-pared to F INAL , which shows that pre-training, es-pecially with the entity classification task, plays an important role in integrating the entity information. In addition, we also explore the effect of raw text quantity. The result of (iii) H ALF -R AW shows that a larger amount of the raw text is helpful. (2) Entity dictionary. We consider (i) H ALF -E NT , which uses 50% randomly selected entities from the original entity dictionary; (ii) N- GRAMS , which uses randomly selected n-grams from the raw text; (iii) O PEN - DOMAIN , which uses an open-domain dictionary from Jieba [Cite_Footnote_7] . The results of these meth-ods decrease significantly (at least over 6% F 1 ) compared to F INAL , which shows that document-specific entity dictionary benefits the performance, and the new-word discovery method is effective for collecting entity dictionary.",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.518_4_0,2020,Entity Enhanced BERT Pre-training for Chinese NER,Footnote
2743,12752," http://github.com/fxsjy/jieba"," ['References']","Finally, we add the three values MI, E L and E R as the validity score of possible new entities, re-move the common words based on an open-domain dictionary from Jieba [Cite_Footnote_8] , and save the top 50% of the remaining words as the potential input document-specific entity dictionary.",8 http://github.com/fxsjy/jieba,"Finally, we add the three values MI, E L and E R as the validity score of possible new entities, re-move the common words based on an open-domain dictionary from Jieba [Cite_Footnote_8] , and save the top 50% of the remaining words as the potential input document-specific entity dictionary.",補足資料,Website,True,Use（引用目的）,True,2020.emnlp-main.518_5_0,2020,Entity Enhanced BERT Pre-training for Chinese NER,Footnote
2744,12753," https://babelnovel.com/"," ['B Details of the Datasets', 'B.2 Novel Dataset']",We construct our corpus from a professional Chinese novel reading site named Babel Novel [Cite_Footnote_9] .,9 https://babelnovel.com/,"Data collection. We construct our corpus from a professional Chinese novel reading site named Babel Novel [Cite_Footnote_9] . Unlike news, the novel dataset cov-ers a mixture of literary style including historical novels, and martial arts novels in the genre of fan-tasy, mystery, romance, military, etc. Therefore, unique characteristics of this dataset such as novel-specific types of named entities present challenges for NER.",Material,DataSource,True,Extend（引用目的）,True,2020.emnlp-main.518_6_0,2020,Entity Enhanced BERT Pre-training for Chinese NER,Footnote
2745,12754," https://github.com/CongBao/AutoencodedMetaEmbedding"," ['4 Experiments']","How do these compare with just fusing Src and Tgt via recent meta-embedding methods like AAEME (Bollegala and Bao, 2018) [Cite_Footnote_1] ?",1 We used the implementation available at: https://github.com/CongBao/AutoencodedMetaEmbedding,"2. How do these compare with just fusing Src and Tgt via recent meta-embedding methods like AAEME (Bollegala and Bao, 2018) [Cite_Footnote_1] ?",Method,Code,True,Use（引用目的）,True,P19-1168_0_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2746,12755," http://nlp.cis.unimelb.edu.au/resources/cqadupstack/"," ['Topics and tasks']","StackExchange topics We pick four topics (Physics, Gaming, Android and Unix) from the CQADupStack [Cite_Footnote_3] dataset of questions and re-sponses.",3 http://nlp.cis.unimelb.edu.au/resources/cqadupstack/,"StackExchange topics We pick four topics (Physics, Gaming, Android and Unix) from the CQADupStack [Cite_Footnote_3] dataset of questions and re-sponses. For each topic, the available response text is divided into D T , used for training/adapt-ing embeddings, and Df T , the evaluation fold used to measure perplexity. In each topic, the target corpus D T has 2000 responses totalling roughly 1 MB. We also report results with changing sizes of D T . Depending on the method we use D T , D S , or u S to train topic-specific embeddings and eval-uate them as-is on two tasks that train task-specific layers on top of these fixed embeddings. The first is an unsupervised language modeling task where we train a LSTM on the adapted embed-dings (which are pinned) and report perplexity on Df T . The second is a Duplicate question detec-tion task. Available in each topic are human an-notated duplicate questions (statistics in Table 10 of Appendix) which we partition across train, test and dev as 50%, 40%, 10%. For contrastive train-ing, we add four times as much randomly chosen non-duplicate pairs. The goal is to predict dupli-cate/not for a question pair, for which we use word mover distance (Kusner et al., 2015, WMD) over adapted word embeddings. We found WMD more accurate than BiMPM (Wang et al., 2017). We use three splits of the target corpus, and for each re-sultant embedding, measure AUC on three random (train-)dev-test splits of question pairs, for a total of nine runs. For reporting AUC, WMD does not need the train fold.",Material,Dataset,True,Use（引用目的）,True,P19-1168_1_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2747,12756," https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"," ['Topics and tasks']",The first is an unsupervised language modeling task where we train a LSTM [Cite_Footnote_4] on the adapted embed-dings (which are pinned) and report perplexity on Df T .,4 https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py,"StackExchange topics We pick four topics (Physics, Gaming, Android and Unix) from the CQADupStack dataset of questions and re-sponses. For each topic, the available response text is divided into D T , used for training/adapt-ing embeddings, and Df T , the evaluation fold used to measure perplexity. In each topic, the target corpus D T has 2000 responses totalling roughly 1 MB. We also report results with changing sizes of D T . Depending on the method we use D T , D S , or u S to train topic-specific embeddings and eval-uate them as-is on two tasks that train task-specific layers on top of these fixed embeddings. The first is an unsupervised language modeling task where we train a LSTM [Cite_Footnote_4] on the adapted embed-dings (which are pinned) and report perplexity on Df T . The second is a Duplicate question detec-tion task. Available in each topic are human an-notated duplicate questions (statistics in Table 10 of Appendix) which we partition across train, test and dev as 50%, 40%, 10%. For contrastive train-ing, we add four times as much randomly chosen non-duplicate pairs. The goal is to predict dupli-cate/not for a question pair, for which we use word mover distance (Kusner et al., 2015, WMD) over adapted word embeddings. We found WMD more accurate than BiMPM (Wang et al., 2017). We use three splits of the target corpus, and for each re-sultant embedding, measure AUC on three random (train-)dev-test splits of question pairs, for a total of nine runs. For reporting AUC, WMD does not need the train fold.",補足資料,Document,False,Use（引用目的）,False,P19-1168_2_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2748,12757," https://github.com/vihari/we_adapt_datasets"," ['Topics and tasks']",All our data splits are made publicly available at [Cite] https://github.com/vihari/we_adapt_datasets.,,Pretrained embeddings E are trained on Wikipedia using the default settings of word2vec’s CBOW model. All our data splits are made publicly available at [Cite] https://github.com/vihari/we_adapt_datasets.,Material,Knowledge,True,Produce（引用目的）,True,P19-1168_3_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Body
2749,12758," https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm"," ['Topics and tasks']",The corresponding five down-stream tasks are text classification over the 3– [Cite_Footnote_5] fine-grained classes under each top-level class.,5 https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm,"Topics from 20 newsgroup We choose the five top-level classes in the 20 newsgroup dataset as topics; viz.: Computer, Recreation, Science, Pol-itics, Religion. The corresponding five down-stream tasks are text classification over the 3– [Cite_Footnote_5] fine-grained classes under each top-level class. Train, test, dev splits were 50%, 40%, 10%. We average over nine splits. The body text is used as D T and subject text is used for classification.",補足資料,Document,True,Produce（引用目的）,True,P19-1168_4_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2750,12759," http://qwone.com/~jason/20Newsgroups/"," ['Topics and tasks']",Topics from 20 newsgroup We choose the five top-level classes in the 20 newsgroup dataset [Cite_Footnote_6] as topics; viz.,6 http://qwone.com/˜jason/20Newsgroups/,"Topics from 20 newsgroup We choose the five top-level classes in the 20 newsgroup dataset [Cite_Footnote_6] as topics; viz.: Computer, Recreation, Science, Pol-itics, Religion. The corresponding five down-stream tasks are text classification over the 3– fine-grained classes under each top-level class. Train, test, dev splits were 50%, 40%, 10%. We average over nine splits. The body text is used as D T and subject text is used for classification.",Material,DataSource,True,Extend（引用目的）,True,P19-1168_5_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2751,12760," https://code.google.com/archive/p/word2vec/"," ['Topics and tasks', '4.2 Epochs vs. regularization results']","First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code [Cite_Footnote_7] and left unchanged in RegFreq (Yang et al., 2017)).",7 https://code.google.com/archive/p/word2vec/,"In Figure 2 we show perplexity and AUC against training epochs. Here we focus on four meth-ods: Tgt, SrcTune, RegFreq, and RegSense. First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code [Cite_Footnote_7] and left unchanged in RegFreq (Yang et al., 2017)). In contrast, Src-Tune, RegSense, and RegFreq are much better than Tgt at five epochs, saturating quickly. With respect to perplexity, SrcTune starts getting worse around 20 iterations and becomes identical to Tgt, showing catastrophic forgetting. Regularizers in RegFreq and RegSense are able to reduce such for-getting, with RegSense being more effective than RegFreq. These experiments show that any com-parison that chooses a fixed number of training epochs across all methods is likely to be unfair. Henceforth we will use a validation set for the stopping criteria. While this is standard practice for supervised tasks, most word embedding code we downloaded ran for a fixed number of epochs, making comparisons unreliable. We conclude that validation-based stopping is critical for fair evalu-ation.",Method,Code,False,Introduce（引用目的）,True,P19-1168_6_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2752,12761," https://github.com/Victor0118/cross_domain_embedding/"," ['Topics and tasks', '4.2 Epochs vs. regularization results']","First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code and left unchanged in RegFreq [Cite_Footnote_8] (Yang et al., 2017)).",8 https://github.com/Victor0118/cross_domain_embedding/,"In Figure 2 we show perplexity and AUC against training epochs. Here we focus on four meth-ods: Tgt, SrcTune, RegFreq, and RegSense. First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code and left unchanged in RegFreq [Cite_Footnote_8] (Yang et al., 2017)). In contrast, Src-Tune, RegSense, and RegFreq are much better than Tgt at five epochs, saturating quickly. With respect to perplexity, SrcTune starts getting worse around 20 iterations and becomes identical to Tgt, showing catastrophic forgetting. Regularizers in RegFreq and RegSense are able to reduce such for-getting, with RegSense being more effective than RegFreq. These experiments show that any com-parison that chooses a fixed number of training epochs across all methods is likely to be unfair. Henceforth we will use a validation set for the stopping criteria. While this is standard practice for supervised tasks, most word embedding code we downloaded ran for a fixed number of epochs, making comparisons unreliable. We conclude that validation-based stopping is critical for fair evalu-ation.",Method,Code,False,Introduce（引用目的）,True,P19-1168_7_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2753,12762," https://allennlp.org/elmo"," ['Topics and tasks', '4.4 Contextual embeddings']","We explore if contextual word embeddings obvi-ate the need for adapting source embeddings, in the ELMo (Peters et al., 2018) setting, a contex-tualized word representation model, pre-trained on a 5.5B token corpus [Cite_Footnote_11] .",11 https://allennlp.org/elmo,"We explore if contextual word embeddings obvi-ate the need for adapting source embeddings, in the ELMo (Peters et al., 2018) setting, a contex-tualized word representation model, pre-trained on a 5.5B token corpus [Cite_Footnote_11] . We compare ELMo’s contextual embeddings as-is, and also after con-catenating them with each of Tgt, SrcTune, and SrcSel embeddings in Table 8. First, ELMo+Tgt is better than Tgt and ELMo individually. This shows that contextual embeddings are useful but they do not eliminate the need for topic-sensitive embeddings. Second, ELMo+SrcSel is better than ELMo+Tgt. Although SrcSel is trained on data that is a strict subset of ELMo, it is still instru-mental in giving gains since that subset is aligned better with the target sense of words. We conclude that topic-adapted embeddings can be useful, even with ELMo-style contextual embeddings.",補足資料,Website,True,Compare（引用目的）,True,P19-1168_8_0,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,Footnote
2754,12763," http://en.wikipedia.org/wiki/AOL"," ['3 System Description', '3.2 Semantic Knowledge']","In fact, the Wikipedia page for AOL [Cite_Footnote_10] has a predicate-nominative construction which supports the compatibility of this head pair: AOL LLC (for-merly America Online) is an American global In-ternet services and media company operated by Time Warner.",10 http://en.wikipedia.org/wiki/AOL,"However, given a semantically compatible men-tion head pair, say AOL and company, one might expect to observe a reliable appositive or predicative-nominative construction involving these mentions somewhere in a large corpus. In fact, the Wikipedia page for AOL [Cite_Footnote_10] has a predicate-nominative construction which supports the compatibility of this head pair: AOL LLC (for-merly America Online) is an American global In-ternet services and media company operated by Time Warner.",補足資料,Document,True,Introduce（引用目的）,True,D09-1120_0_0,2009,Simple Coreference Resolution with Rich Syntactic and Semantic Features,Footnote
2755,12764," http://www.itl.nist.gov/iad/mig/tests/mt/2009/"," ['4 Experiments', '4.1 Common settings']",We used mteval-v13a.pl for calculating BLEU scores. [Cite_Footnote_1],1 It is available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/,"We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) except that we used various language models to compare them. We used the MOSES phrase-based SMT system (Koehn et al., 2003), together with Giza++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores. [Cite_Footnote_1]",補足資料,Document,True,Produce（引用目的）,True,D13-1082_0_0,2013,Converting Continuous-Space Language Models into N-gram Language Models for Statistical Machine Translation,Footnote
2756,12765," http://www.ark.cs.cmu.edu/MT/"," ['4 Experiments', '4.2 Experimental results']","We also performed the paired bootstrap re-sampling test (Koehn, 2004). [Cite_Footnote_3]",3 We used the code available at http://www.ark.cs.cmu.edu/MT/.,"We also performed the paired bootstrap re-sampling test (Koehn, 2004). [Cite_Footnote_3] We sampled 2000 samples for each significance test.",Method,Code,True,Use（引用目的）,True,D13-1082_1_0,2013,Converting Continuous-Space Language Models into N-gram Language Models for Statistical Machine Translation,Footnote
2757,12766," http://www.bitterlemons.org"," ['3 Corpus']",Our corpus consists of articles published on the bitterlemons website [Cite_Footnote_1] .,1 http://www.bitterlemons.org,"Our corpus consists of articles published on the bitterlemons website [Cite_Footnote_1] . The website is set up to “contribute to mutual understanding [between Palestinians and Israels] through the open exchange of ideas”. Every week an issue about Israeli-Palestinian conflict is selected for discussion, for example, “Disengagement: unilateral or coordi-nated?”, and a Palestinian editor and an Israeli edi-tor contribute a article addressing the issue. In ad-dition, the Israeli and Palestinian editors invite or interview one Israeli and one Palestinian to express their views, resulting in a total of four articles in a weekly edition.",補足資料,Website,True,Introduce（引用目的）,True,N06-3005_0_0,2006,Identifying Perspectives at the Document and Sentence Levels Using Statistical Models,Footnote
2758,12767," https://github.com/mortezaro/tr-disfluency"," ['7 Results']",Table 1. [Cite_Footnote_1],1 Experiments are reproducible from https://github.com/mortezaro/tr-disfluency,Table 1. [Cite_Footnote_1] We found our best model was the add-M,Mixed,Mixed,True,Produce（引用目的）,True,2021.acl-long.286_0_0,2021,Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental,Footnote
2759,12768," http://time2002.org/"," ['4 Question Decomposition Unit', '4.3.3 Example']","Due to the fact that question corpora used in TREC (TREC, ) and CLEF (CLEF, ) do not contain com-plex questions, the TERQAS question corpus has been chosen (Radev and Sundheim, 2002; Puste-jovsky, 2002 [Cite_Ref] ).",J. Pustejovsky. 2002. Terqas:time and event recognition for question answering systems. http://time2002.org/.,"This section presents an evaluation of the Decompo-sition Unit for the treatment of complex questions. For the evaluation a corpus of questions containing as many simple as complex questions is required. Due to the fact that question corpora used in TREC (TREC, ) and CLEF (CLEF, ) do not contain com-plex questions, the TERQAS question corpus has been chosen (Radev and Sundheim, 2002; Puste-jovsky, 2002 [Cite_Ref] ). It consists of 123 temporal questions. From these, 11 were discarded due to requiring the need of a treatment beyond the capabilities of the system introduced hereby. Questions of the type: “Who was the second man on the moon” can not be answered by applying the question decomposi-tion. They need a special treatment. For the afore-mentioned phrase, this would consist of obtaining the names of all the men having been on the moon, ordering the dates and picking the second in the or-dered list of names.",補足資料,Paper,True,Introduce（引用目的）,True,P04-1072_1_0,2004,Splitting Complex Temporal Questions for Question Answering systems,Reference
2760,12769," http://www.cs.brandeis.edu/j̃amesp/arda/time/documentation/TimeML-use-in-qa-v1.0.pdf"," ['4 Question Decomposition Unit', '4.3.3 Example']","Due to the fact that question corpora used in TREC (TREC, ) and CLEF (CLEF, ) do not contain com-plex questions, the TERQAS question corpus has been chosen (Radev and Sundheim, 2002 [Cite_Ref] ; Puste-jovsky, 2002).",D. Radev and B. Sundheim. 2002. Us-ing timeml in question answering. http://www.cs.brandeis.edu/j̃amesp/arda/time/documentation/TimeML-use-in-qa-v1.0.pdf.,"This section presents an evaluation of the Decompo-sition Unit for the treatment of complex questions. For the evaluation a corpus of questions containing as many simple as complex questions is required. Due to the fact that question corpora used in TREC (TREC, ) and CLEF (CLEF, ) do not contain com-plex questions, the TERQAS question corpus has been chosen (Radev and Sundheim, 2002 [Cite_Ref] ; Puste-jovsky, 2002). It consists of 123 temporal questions. From these, 11 were discarded due to requiring the need of a treatment beyond the capabilities of the system introduced hereby. Questions of the type: “Who was the second man on the moon” can not be answered by applying the question decomposi-tion. They need a special treatment. For the afore-mentioned phrase, this would consist of obtaining the names of all the men having been on the moon, ordering the dates and picking the second in the or-dered list of names.",補足資料,Paper,True,Introduce（引用目的）,True,P04-1072_2_0,2004,Splitting Complex Temporal Questions for Question Answering systems,Reference
2761,12770," https://github.com/rtmaww/SENT"," ['1 Introduction']",Our codes are publicly available at Github [Cite_Footnote_1] .,1 https://github.com/rtmaww/SENT,"In this work, we propose the use of negative training (NT) (Kim et al., 2019) for distant RE. Different from positive training (PT), NT trains a model by selecting the complementary labels of the given label, regarding that “the input sentence does not belong to this complementary label”. Since the probability of selecting a true label as a complementary label is low, NT decreases the risk of providing noisy information and prevents the model from overfitting the noisy data. Moreover, the model trained with NT is able to separate the noisy data from the training data (a histogram in Fig.3 shows the separated data distribution during NT). Based on NT, we propose SENT, a sentence-level framework for distant RE. During SENT training, the noisy instances are not only filtered with a noise-filtering strategy, but also transformed into useful training data with a re-labeling method. We further design an iterative training algorithm to take full advantage of these data-refining processes, which significantly boost performance. Our codes are publicly available at Github [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,2021.acl-long.484_0_0,2021,SENT: Sentence-level Distant Relation Extraction via Negative Training,Footnote
2762,12771," https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR"," ['4 Experiments']","In this part, we adopt the NYT-10 data set for sentence-level training, following the setting of Jia et al. (2019), who publishes a manually labeled sentence-level test set. [Cite_Footnote_4]",4 https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR,"The first part is the effectiveness study on sentence-level evaluation for distant RE. Different from bag-level evaluation, a sentence-level evalua-tion compute Precision (Prec.), Recall (Rec.) and F1 metric directly on all of the individual instances in the dataset. In this part, we adopt the NYT-10 data set for sentence-level training, following the setting of Jia et al. (2019), who publishes a manually labeled sentence-level test set. [Cite_Footnote_4] Besides, they also publish a test set for evaluating noise-filtering ability. Details of the adopted dataset are shown in Table 1.",Material,Dataset,True,Introduce（引用目的）,True,2021.acl-long.484_1_0,2021,SENT: Sentence-level Distant Relation Extraction via Negative Training,Footnote
2763,12772," https://github.com/yuhaozhang/tacred-relation"," ['4 Experiments']","Since no labeled training data are available in the distant supervision setting, we construct a noisy dataset with 30% noise from a labeled dataset, TACRED (Zhang et al., 2017) [Cite_Footnote_5] .",5 https://github.com/yuhaozhang/tacred-relation,"We construct the second part of experiments (Sec.4.4) to better understand SENT’s behaviors. Since no labeled training data are available in the distant supervision setting, we construct a noisy dataset with 30% noise from a labeled dataset, TACRED (Zhang et al., 2017) [Cite_Footnote_5] . We regard this constructed dataset as noisy-TACRED. The reason we choose this dataset is that 80% instances in the training data are “no relation”. This “NA” rate is similar to the NYT data which contains 70% “NA” relation type, thus analysis on this dataset is more credible.",Material,Dataset,True,Extend（引用目的）,True,2021.acl-long.484_2_0,2021,SENT: Sentence-level Distant Relation Extraction via Negative Training,Footnote
2764,12773," http://nlp.cs.nyu.edu/evalb"," ['4 Indicative Current Results', '4.2 Comparative Acccuracy']","The entries show the proportion of fully accurate parses, the f-score average of bracket precision and recall, and average crossing brackets, as obtained by EVALB (Sekine and Collins, 1997) [Cite_Ref] .",Satoshi Sekine and Michael Collins. 1997. EvalB. Available at http://nlp.cs.nyu.edu/evalb,"Table 2 primarily compares the accuracy of the Collins model 3 and RH parsers. The entries show the proportion of fully accurate parses, the f-score average of bracket precision and recall, and average crossing brackets, as obtained by EVALB (Sekine and Collins, 1997) [Cite_Ref] . The RH f-score is currently somewhat lower, but the proportion of fully correct parses is significantly higher.",Method,Tool,True,Use（引用目的）,True,N07-2031_1_0,2007,RH: A Retro Hybrid Parser,Reference
2765,12774," http://www.jaist.ac.jp/~h-yamada/"," ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']","Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [Cite_Footnote_2] .",2 http://www.jaist.ac.jp/˜h-yamada/,"We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [Cite_Footnote_2] . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka’s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default.",Material,Knowledge,True,Use（引用目的）,True,P07-2052_0_0,2007,Minimally Lexicalized Dependency Parsing,Footnote
2766,12775," http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/postagger/"," ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']","During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka’s tagger [Cite_Footnote_4] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001).",4 http://www-tsujii.is.s.u-tokyo.ac.jp/˜tsuruoka/postagger/,"We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka’s tagger [Cite_Footnote_4] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default.",Method,Tool,True,Use（引用目的）,True,P07-2052_1_0,2007,Minimally Lexicalized Dependency Parsing,Footnote
2767,12776," http://chasen.org/~taku-ku/software/yamcha/"," ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']","During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka’s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [Cite_Footnote_5] (Kudo and Matsumoto, 2001).",5 http://chasen.org/˜taku-ku/software/yamcha/,"We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka’s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [Cite_Footnote_5] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default.",Method,Tool,True,Use（引用目的）,True,P07-2052_2_0,2007,Minimally Lexicalized Dependency Parsing,Footnote
2768,12777," http://chasen.org/~taku-ku/software/TinySVM/"," ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']","We used an SVMs package, TinySVM [Cite_Footnote_6] ,and trained the SVMs classifiers using a third-order polynomial kernel.",6 http://chasen.org/˜taku-ku/software/TinySVM/,"We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank. We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada . During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka’s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM [Cite_Footnote_6] ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default.",Method,Tool,True,Use（引用目的）,True,P07-2052_3_0,2007,Minimally Lexicalized Dependency Parsing,Footnote
2769,12778," https://github.com/zxshamson/dy-conv-rec"," ['1 Introduction']","For experiments [Cite_Footnote_1] , we collect Reddit conversa-tions from three subreddits — “technology”, “to-dayilearned”, and “funny”, each exhibiting differ-ent data statistics, discussion topics, and language styles.",1 The datasets and codes are available at: https://github.com/zxshamson/dy-conv-rec,"For experiments [Cite_Footnote_1] , we collect Reddit conversa-tions from three subreddits — “technology”, “to-dayilearned”, and “funny”, each exhibiting differ-ent data statistics, discussion topics, and language styles. An absolute date is used to separate training data (before the date) from test and validation data (after the date). In this way, most conversations in the test and validation parts are new conversations that have not been counted before. This presents a more realistic setup than previous studies (Zeng et al., 2018, 2019b), which let training data contain partial context for any conversations to allow the possibility of predicting users’ future engagement for recommendation.",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.305_0_0,2020,Dynamic Online Conversation Recommendation,Footnote
2770,12780," https://github.com/SapienzaNLP/sgl"," ['References']",We release our code and our models for research purposes at [Cite] https: //github.com/SapienzaNLP/sgl.,,"Graph-based semantic parsing aims to repre-sent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several di-verse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multi-ple formalisms as Multilingual Neural Ma-chine Translation ( MNMT ), and propose SGL , a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora com-ing from Machine Translation: we report com-petitive performances on AMR and UCCA pars-ing, especially once paired with pre-trained ar-chitectures. Furthermore, we find that mod-els trained under this configuration scale re-markably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its com-petitors by a large margin without even explic-itly seeing non-English to AMR examples at training time and, once these examples are in-cluded as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at [Cite] https: //github.com/SapienzaNLP/sgl.",Method,Code,True,Produce（引用目的）,True,2021.naacl-main.30_0_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Body
2771,12781," https://github.com/RikVN/AMR"," ['3 Speak the Graph Languages ( SGL )', '3.1 Graph Linearizations']","For both phases, we use the scripts released by van Noord and Bos (2017). [Cite_Footnote_6]",6 https://github.com/RikVN/AMR,"For AMR parsing, as in van Noord and Bos (2017), we first simplify AMR graphs by remov-ing variables and wiki links. We then convert these stripped AMR graphs into trees by duplicating co-referring nodes. At this point, in order to obtain the final linearized version of a given AMR , we concatenate all the lines of its PENMAN notation (Goodman, 2020) together, replacing newlines and multiple spaces with single spaces (Figure 1a and 1b). Conversely, delinearization is performed by assigning a variable to each predicted concept, per-forming Wikification, restoring co-referring nodes and, where possible, repairing any syntactically malformed subgraph. For both phases, we use the scripts released by van Noord and Bos (2017). [Cite_Footnote_6]",Material,Dataset,True,Use（引用目的）,True,2021.naacl-main.30_1_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Footnote
2772,12782," https://github.com/berniey/hanziconv"," ['4 Experimental Setup', '4.2 Datasets and Preprocessing']","Preprocessing We do not perform any prepro-cessing or tokenization, except for the graph lin-earizations explained in §3.1 and Chinese simpli-fication. [Cite_Footnote_12]",12 We use the hanziconv library (https://github.com/berniey/hanziconv).,"UCCA We replicate the setting of the CoNLL 2019 Shared Task (Oepen et al., 2019). We train our models using the freely available 10 UCCA portion of the training data; this corpus amounts to 6 572 sentence-graph pairs, drawn from the English Web Treebank (2012T13) and English Wikipedia arti-cles on celebrities. As no official development set was included in the data release, following Hersh-covich and Arviv (2019), we reserve 500 instances and use them as the validation set. To the best of our knowledge, the full evaluation data have not been released yet and, therefore, we compare with state-of-the-art alternatives and report results only on The Little Prince, a released subset consisting of 100 manually-tagged sentences sampled from the homonymous novel. Parallel Data We use English-centric paral-lel corpora in four languages, namely, Chinese, German, Italian and Spanish; we employ Mul-tiUN (Tiedemann, 2012) for Chinese and Spanish, ParaCrawl (Esplà et al., 2019) for German, and Europarl (Tiedemann, 2012) for Italian. We per-form a mild filtering over all the available parallel sentences and then take the first 5M out of these. Preprocessing We do not perform any prepro-cessing or tokenization, except for the graph lin-earizations explained in §3.1 and Chinese simpli-fication. [Cite_Footnote_12] Instead, we directly apply subword to-kenization with a Unigram Model (Kudo, 2018). When working with Cross in a single-task setting on AMR or UCCA , we follow Ge et al. (2019) and use a vocabulary size of 20k subwords; instead, when working in the multilingual setting, we in-crease this value to 50k so as to better accom-modate the increased amount of languages. Con-versely, when using mBART, we always use the original vocabulary consisting of 250k subwords.",Material,Knowledge,True,Use（引用目的）,True,2021.naacl-main.30_2_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Footnote
2773,12783," https://github.com/snowblink14/smatch"," ['4 Experimental Setup', '4.3 Evaluation']","We evaluate AMR and cross-lingual AMR parsing by using the Smatch score [Cite_Footnote_13] (Cai and Knight, 2013), a metric that computes the overlap between two graphs.",13 https://github.com/snowblink14/smatch,"We evaluate AMR and cross-lingual AMR parsing by using the Smatch score [Cite_Footnote_13] (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems’ performances, we also re-port the fine-grained scores as computed by the evaluation toolkit 14 of Damonte et al. (2017). For UCCA parsing, we employ the official evaluation metric 15 of the shared task, conceptually similar to the Smatch score.",Method,Code,True,Use（引用目的）,True,2021.naacl-main.30_3_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Footnote
2774,12784," https://github.com/mdtux89/amr-evaluation"," ['4 Experimental Setup', '4.3 Evaluation']","Furthermore, in order to have a better picture of the systems’ performances, we also re-port the fine-grained scores as computed by the evaluation toolkit [Cite_Footnote_14] of Damonte et al. (2017).",14 https://github.com/mdtux89/ amr-evaluation,"We evaluate AMR and cross-lingual AMR parsing by using the Smatch score 13 (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems’ performances, we also re-port the fine-grained scores as computed by the evaluation toolkit [Cite_Footnote_14] of Damonte et al. (2017). For UCCA parsing, we employ the official evaluation metric of the shared task, conceptually similar to the Smatch score.",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.30_4_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Footnote
2775,12785," https://github.com/cfmrp/mtool"," ['4 Experimental Setup', '4.3 Evaluation']","For UCCA parsing, we employ the official evaluation metric [Cite_Footnote_15] of the shared task, conceptually similar to the Smatch score.",15 https://github.com/cfmrp/mtool,"We evaluate AMR and cross-lingual AMR parsing by using the Smatch score 13 (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems’ performances, we also re-port the fine-grained scores as computed by the evaluation toolkit of Damonte et al. (2017). For UCCA parsing, we employ the official evaluation metric [Cite_Footnote_15] of the shared task, conceptually similar to the Smatch score.",Method,Code,True,Use（引用目的）,True,2021.naacl-main.30_5_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Footnote
2776,12786," http://svn.nlpl.eu/mrp/2019/public/resources.txt"," ['5 Results', '5.3 UCCA Parsing']","Finally, we wish to point out that direct com-parability between our system and those reported is hindered by the fact that our training setting is significantly different from theirs; in particular, we limit ourselves to two frameworks only and leverage resources (the parallel corpora from MT ) whose usage was forbidden to the shared task par-ticipants. [Cite_Footnote_19]",19 Allowed resources are specified at: http://svn.nlpl.eu/mrp/2019/public/resources.txt,"Finally, we wish to point out that direct com-parability between our system and those reported is hindered by the fact that our training setting is significantly different from theirs; in particular, we limit ourselves to two frameworks only and leverage resources (the parallel corpora from MT ) whose usage was forbidden to the shared task par-ticipants. [Cite_Footnote_19] Nevertheless, we believe that their re-sults are needed here to better contextualize the performances SGL obtains.",補足資料,Document,True,Introduce（引用目的）,True,2021.naacl-main.30_6_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Footnote
2777,12787," https://github.com/OpenNMT/OpenNMT-py"," ['References']","Cross For Cross, we leverage the implementa-tion available in OpenNMT-py [Cite_Footnote_20] (Klein et al., 2017) and define a Transformer model closely following Vaswani et al. (2017), except for the embedding modification we described.",20 https://github.com/OpenNMT/OpenNMT-py,"Cross For Cross, we leverage the implementa-tion available in OpenNMT-py [Cite_Footnote_20] (Klein et al., 2017) and define a Transformer model closely following Vaswani et al. (2017), except for the embedding modification we described. We use 128 as the em-bedding size and 512 as the Transformer hidden size when training in single-task settings; when scaling to the multilingual framework, we augment the hidden size to 768 so as to increase the model capacity. The number of layers in both the encoder and the decoder is set to 6, while the number of attention heads to 8; therefore, Cross st contains 46M trainable parameters, while Cross mt 105M. We optimize the models parameters using Adam (Kingma and Ba, 2015) and the original scheduler of Vaswani et al. (2017). We train with an effective batch size of 8000 tokens and for a maximum of 300k steps on a NVIDIA GeForce RTX 2080 Ti, using semantic parsing accuracy as the early stop-ping criterion with 25k steps of patience; training lasted 1 day for Cross st and roughly 4 days for Cross mt . We did not perform any significant tun-ing of decoding time parameters: we use 5 beams and, following Ge et al. (2019), we set the GNMT length penalty parameter alpha to 1.0.",補足資料,Website,True,Introduce（引用目的）,True,2021.naacl-main.30_7_0,2021,SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation,Footnote
2778,12788," https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset"," ['References']","Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art ap-proaches on Chinese literature text [Cite_Footnote_1] .","1 The Chinese literature text corpus for relation classi-fication, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset","Relation classification is an important seman-tic processing task in the field of natural lan-guage processing. In this paper, we propose the task of relation classification for Chinese literature text. A new dataset of Chinese liter-ature text is constructed to facilitate the study in this task. We present a novel model, named Structure Regularized Bidirectional Recurrent Convolutional Neural Network (SR-BRCNN), to identify the relation between entities. The proposed model learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized de-pendency tree, which has the benefits of re-ducing the complexity of the whole model. Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art ap-proaches on Chinese literature text [Cite_Footnote_1] .",Material,Dataset,True,Produce（引用目的）,True,N18-2059_0_0,2018,Structure Regularized Neural Network for Entity Relation Classification for Chinese Literature Text,Footnote
2779,12789," https://math-qa.github.io/math-QA/"," ['References']",[Cite] https: //math-qa.github.io/math-QA/.,,"We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map prob-lems to operation programs. Due to an-notation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational an-notations over diverse problem types. We introduce a new representation language to model precise operation programs correspond-ing to each math problem that aim to im-prove both the performance and the inter-pretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational pro-grams. We additionally introduce a neu-ral sequence-to-program model enhanced with automatic problem categorization. Our exper-iments show improvements over competitive baselines in our MathQA as well as the AQuA datasets. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future re-search. Our dataset is available at: [Cite] https: //math-qa.github.io/math-QA/.",Material,Dataset,True,Produce（引用目的）,True,N19-1245_0_0,2019,MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms,Body
2780,12790," https://math-qa.github.io/math-QA/"," ['1 Introduction']","We use this representation lan-guage to construct MathQA [Cite_Footnote_1] , a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math do-main categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017).",1 The dataset is available at: https://math-qa.github.io/math-QA/,"In this paper, we introduce a new operation-based representation language for solving math word problems. We use this representation lan-guage to construct MathQA [Cite_Footnote_1] , a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math do-main categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017). We introduce a neu-ral model for mapping problems to operation pro-grams with domain categorization.",Material,Dataset,True,Produce（引用目的）,True,N19-1245_1_0,2019,MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms,Footnote
2781,12791," https://www.figure-eight.com"," ['4 Dataset', '4.1 Annotation using Crowd Workers']",We take advantage of the annotation platform in Figure Eight. [Cite_Footnote_5],5 https://www.figure-eight.com,"High Quality Crowd Workers We dynami-cally evaluate and employ high-quality annotators through a collection of quality-control questions. We take advantage of the annotation platform in Figure Eight. [Cite_Footnote_5] The annotators are randomly eval-uated through a pre-defined set of test questions, and they have to maintain an accuracy threshold to be able to continue their annotations. If an an-notator’s accuracy drops below a threshold, their previous annotations are labeled as untrusted and will be added to the pool of annotations again.",補足資料,Document,True,Introduce（引用目的）,True,N19-1245_2_0,2019,MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms,Footnote
2782,12792," http://ipsc.jrc.ec.europa.eu/index.php?id=198"," ['3 Experiment']",Our EN–FR dataset is from the pub-licly available JRC-Acquis corpus. [Cite_Footnote_4],4 http://ipsc.jrc.ec.europa.eu/index. php?id=198,"Table 1 shows a summary of our datasets. The EN–ZH dataset is a translation memory from Symantec. Our EN–FR dataset is from the pub-licly available JRC-Acquis corpus. [Cite_Footnote_4] Word align-ment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and.",Material,DataSource,True,Extend（引用目的）,True,P16-2045_0_0,2016,Phrase-Level Combination of SMT and TM Using Constrained Word Lattice,Footnote
2783,12793," http://www.psychpark.org"," ['5 Experimental Results']","The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark ( [Cite] http://www.psychpark.org) (Bai, 2001) and John Tung Foundation (http://www.jtf.org.tw).",,"To evaluate the performance of the CIP, we built a prototype system and provided a set of seed patterns. The seed patterns were collected by re-ferring to the well-defined instruments for as-sessing negative life events (Brostedt and Peder-sen, 2003; Pagano et al., 2004). A total of 20 seed patterns were selected by the health profes-sionals. Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark ( [Cite] http://www.psychpark.org) (Bai, 2001) and John Tung Foundation (http://www.jtf.org.tw).",補足資料,Website,True,Introduce（引用目的）,True,P06-2121_0_0,2006,HAL-based Cascaded Model for Variable-Length Semantic Pattern Induction from Psychiatry Web Resources,Body
2784,12794," http://www.jtf.org.tw"," ['5 Experimental Results']","The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark (http://www.psychpark.org) (Bai, 2001) and John Tung Foundation ( [Cite] http://www.jtf.org.tw).",,"To evaluate the performance of the CIP, we built a prototype system and provided a set of seed patterns. The seed patterns were collected by re-ferring to the well-defined instruments for as-sessing negative life events (Brostedt and Peder-sen, 2003; Pagano et al., 2004). A total of 20 seed patterns were selected by the health profes-sionals. Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark (http://www.psychpark.org) (Bai, 2001) and John Tung Foundation ( [Cite] http://www.jtf.org.tw).",補足資料,Website,True,Introduce（引用目的）,True,P06-2121_1_0,2006,HAL-based Cascaded Model for Variable-Length Semantic Pattern Induction from Psychiatry Web Resources,Body
2785,12795," https://github.com/allenai/natural-perturbations"," ['6 Conclusion']","While this is not a dataset paper (since our focus is on more on the value of natural perturbations for robust model design), we provide the natural per-turbations resource for B OOL Q constructed during the course of this study. [Cite_Footnote_4]",4 https://github.com/allenai/natural-perturbations,"While this is not a dataset paper (since our focus is on more on the value of natural perturbations for robust model design), we provide the natural per-turbations resource for B OOL Q constructed during the course of this study. [Cite_Footnote_4]",Mixed,Mixed,False,Produce（引用目的）,True,2020.emnlp-main.12_0_0,2020,More Bang for Your Buck: Natural Perturbation for Robust Question Answering,Footnote
2786,12796," https://youtu.be/"," ['7:453–466.']",[Cite] https://youtu.be/,,Here is an screen cast of the relevant anno-tation interface interface: [Cite] https://youtu.be/,補足資料,Media,True,Introduce（引用目的）,True,2020.emnlp-main.12_1_0,2020,More Bang for Your Buck: Natural Perturbation for Robust Question Answering,Body
2787,12797," https://super.gluebenchmark.com/leaderboard/"," ['C Performances Across Datasets']","The human performance on B OOL Q and M ULTI RC are directly reported from SuperGLUE (Wang et al., 2019) leaderboard. [Cite_Footnote_5]",5 https://super.gluebenchmark.com/leaderboard/,"The results are summarized in Table 2. Most of the rows are R O BERT A trained on a specified dataset. We have also included a row correspond-ing a system trained on the union of B OOL Q and B OOL Q , referred to as B OOL Q++ for brevity. Most of the datasets are slightly skewed between the two classes, which is why the majority la-bel baseline (Always-Yes or Always-No) achieves scores above 50%. Rows indicated with * are re-ported directly from prior work. The human predic-tion on B OOL Q is the majority label of 5 indepen-dent AMT annotators. The human performance on B OOL Q and M ULTI RC are directly reported from SuperGLUE (Wang et al., 2019) leaderboard. [Cite_Footnote_5] Here are the key observations in this table:",補足資料,Website,False,Introduce（引用目的）,False,2020.emnlp-main.12_2_0,2020,More Bang for Your Buck: Natural Perturbation for Robust Question Answering,Footnote
2788,12798," https://github.com/decompositional-semantics-initiative/improved-ParaBank-rewriter"," ['E Rule-Based Perturbations']",Here we’re showing examples of perturbations generated via a recent machine paraphraser sys-tem. [Cite_Footnote_6],6 https://github.com/decompositional-semantics-initiative/improved-ParaBank-rewriter,Here we’re showing examples of perturbations generated via a recent machine paraphraser sys-tem. [Cite_Footnote_6],Method,Tool,False,Introduce（引用目的）,False,2020.emnlp-main.12_3_0,2020,More Bang for Your Buck: Natural Perturbation for Robust Question Answering,Footnote
2789,12799," https://paracrawl.eu/"," ['1 Introduction']","Misaligned segments were by far the most prevalent type of noise in the ParaCrawl [Cite_Footnote_1] parallel corpus they used, twice as common as accepted segments.",1 https://paracrawl.eu/,"NMT systems have been shown to be sensitive to noise in the training data (Khayrallah and Koehn, 2018), where noise is defined as segments that de-crease output quality of systems trained on the data. It is, therefore, important to be able to ac-curately align multilingual texts and precisely fil-ter out misalignments and bad translations that ad-versely affect performance. In the study, conducted on the impact of various types of noise on MT quality, untranslated and misaligned segments had the most detrimental effect. Misaligned segments were by far the most prevalent type of noise in the ParaCrawl [Cite_Footnote_1] parallel corpus they used, twice as common as accepted segments. However, misalign-ments vary; a segment can have one extraneous word, it can have twice the content its counterpart has, or anything in between. It can be very useful to understand the intricacies of the effects different types and levels of noise have, why it is important not to have noise and whether some kinds of noise are more acceptable than others. This leads us to our first research question:",Material,Dataset,True,Introduce（引用目的）,True,2020.acl-srw.25_0_0,2020,Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions,Footnote
2790,12800," https://spacy.io/"," ['3 Experimental Framework', '3.3 Tools and Models']","For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy. [Cite_Footnote_2]",2 https://spacy.io/,"In Section 4, we will discuss some of the methods we will be experimenting with. These include ap-plying a variety of available tools and models as well as developing our own. ABLTagger (Stein-grímsson et al., 2019) will be used for PoS-tagging Icelandic texts. The tagger employs biLSTMs and an external morphological lexicon (Bjarnadóttir et al., 2019). Lemmatising will be carried out using Nefnir (Ingólfsdóttir et al., 2019). For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy. [Cite_Footnote_2]",Method,Tool,True,Use（引用目的）,True,2020.acl-srw.25_1_0,2020,Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions,Footnote
2791,12801," http://acube.di.unipi.it/tmn-dataset/"," ['3 Experiment Setup', '3.1 Datasets']",We use the news titles as in-stances from the benchmark classification dataset released by Vitale et al. (2012). [Cite_Footnote_5],5 http://acube.di.unipi.it/tmn-dataset/,"TagMyNews. We use the news titles as in-stances from the benchmark classification dataset released by Vitale et al. (2012). [Cite_Footnote_5] This dataset con-tains English news from really simple syndication (RSS) feeds. Each news feed (with its title) is an-notated with one from seven labels, e.g., sci-tech. Twitter. This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track. Follow-ing previous settings (Yan et al., 2013; Li et al., 2016a), hashtags, i.e., user-annotated topic la-bels in each tweet such as “#Trump” and “#Su-perBowl”, serve as our ground-truth class labels.",Material,Dataset,True,Use（引用目的）,True,D18-1351_0_0,2018,Topic Memory Networks for Short Text Classification,Footnote
2792,12802," http://trec.nist.gov/data/tweets"," ['3 Experiment Setup', '3.1 Datasets']","This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track. [Cite_Footnote_6]",6 http://trec.nist.gov/data/tweets,"TagMyNews. We use the news titles as in-stances from the benchmark classification dataset released by Vitale et al. (2012). This dataset con-tains English news from really simple syndication (RSS) feeds. Each news feed (with its title) is an-notated with one from seven labels, e.g., sci-tech. Twitter. This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track. [Cite_Footnote_6] Follow-ing previous settings (Yan et al., 2013; Li et al., 2016a), hashtags, i.e., user-annotated topic la-bels in each tweet such as “#Trump” and “#Su-perBowl”, serve as our ground-truth class labels.",Material,Dataset,True,Use（引用目的）,True,D18-1351_1_0,2018,Topic Memory Networks for Short Text Classification,Footnote
2793,12803," https://radimrehurek.com/gensim/utils.html"," ['3 Experiment Setup', '3.1 Datasets']","We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer [Cite_Footnote_8] for tokenization.",8 https://radimrehurek.com/gensim/utils.html,"Table 2 shows the statistic information of the four datasets. Each dataset is randomly split into 80% for training and 20% for test. 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer [Cite_Footnote_8] for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016). Differently, we use the raw word se-quence (without words removal) for the inputs of classification x Seq as is done in previous work of text classification (Kim, 2014; Liu et al., 2017).",Method,Tool,True,Use（引用目的）,True,D18-1351_2_0,2018,Topic Memory Networks for Short Text Classification,Footnote
2794,12804," https://github.com/FudanNLP/fnlp"," ['3 Experiment Setup', '3.1 Datasets']","As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [Cite_Footnote_9] for word segmentation.",9 https://github.com/FudanNLP/fnlp,"Table 2 shows the statistic information of the four datasets. Each dataset is randomly split into 80% for training and 20% for test. 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [Cite_Footnote_9] for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016). Differently, we use the raw word se-quence (without words removal) for the inputs of classification x Seq as is done in previous work of text classification (Kim, 2014; Liu et al., 2017).",Method,Tool,True,Use（引用目的）,True,D18-1351_3_0,2018,Topic Memory Networks for Short Text Classification,Footnote
2795,12805," https://radimrehurek.com/gensim/parsing/preprocessing.html"," ['3 Experiment Setup', '3.1 Datasets']","In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words [Cite_Footnote_10] and words occurring less than 3 times.",10 https://radimrehurek.com/gensim/parsing/preprocessing.html,"Table 2 shows the statistic information of the four datasets. Each dataset is randomly split into 80% for training and 20% for test. 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words [Cite_Footnote_10] and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016). Differently, we use the raw word se-quence (without words removal) for the inputs of classification x Seq as is done in previous work of text classification (Kim, 2014; Liu et al., 2017).",補足資料,Document,True,Introduce（引用目的）,True,D18-1351_4_0,2018,Topic Memory Networks for Short Text Classification,Footnote
2796,12806," http://nlp.stanford.edu/data/glove.6B.zip"," ['3 Experiment Setup', '3.2 Model Settings']","For Snippets and TagMyNews datasets, we use pre-trained GloVe embed-dings (Pennington et al., 2014) [Cite_Footnote_11] .",11 http://nlp.stanford.edu/data/glove.6B.zip (200d),"We use pre-trained embeddings to initialize all word embeddings. For Snippets and TagMyNews datasets, we use pre-trained GloVe embed-dings (Pennington et al., 2014) [Cite_Footnote_11] . For Twitter and Weibo datasets, we pre-train embeddings on large-scale external data with 99M tweets and 467M Weibo messages, respectively. For the number of topics, we follow previous settings (Yan et al., 2013; Das et al., 2015; Dieng et al., 2016) to set K = 50. For all the other hyperparame-ters, we tune them on the development set by grid search. For our classifier, we employ CNN in experiment because of its better performance in short text classification than its counterparts such as RNN (Wang et al., 2017a). The hidden size of CNN is set as 500. The dimension of word em-bedding E = 200. γ = 0.8 for trading off θ and P , and λ = 1.0 for controlling the effects of topic model and classification. In the learning process, we run our model for 800 epochs with early-stop strategy applied (Caruana et al., 2000).",Material,Knowledge,True,Use（引用目的）,True,D18-1351_5_0,2018,Topic Memory Networks for Short Text Classification,Footnote
2797,12807," https://github.com/dice-group/Palmetto"," ['4 Experimental Results', '4.2 Topic Coherence Comparison']","We use the C V metric (Röder et al., 2015) computed by Palmetto toolkit [Cite_Footnote_12] to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI (Bouma, 2009).",12 https://github.com/dice-group/ Palmetto,"In Section 4.1, we find that TMN can significantly outperform comparison models on short text clas-sification. In this section, we study whether jointly learning topic models and classification can be helpful in producing coherent and meaningful top-ics. We use the C V metric (Röder et al., 2015) computed by Palmetto toolkit [Cite_Footnote_12] to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI (Bouma, 2009). Table 4 shows the compar-ison results of LDA, BTM, NTM, and TMN on the three English datasets. Note that we do not re-port C V scores for Chinese Weibo dataset as the Palmetto toolkit cannot process Chinese topics.",Method,Tool,True,Use（引用目的）,True,D18-1351_6_0,2018,Topic Memory Networks for Short Text Classification,Footnote
2798,12808," http://www.mturk.com"," ['3 Annotation of Social Variables']","In order to study how entrainment in various dimen-sions correlated with perceived social behaviors of our subjects, we asked Amazon Mechanical Turk [Cite_Footnote_1] annotators to label the 168 Objects games in our cor-pus for an array of social behaviors perceived for each of the speakers, which we term here “social variables.”",1 http://www.mturk.com,"In order to study how entrainment in various dimen-sions correlated with perceived social behaviors of our subjects, we asked Amazon Mechanical Turk [Cite_Footnote_1] annotators to label the 168 Objects games in our cor-pus for an array of social behaviors perceived for each of the speakers, which we term here “social variables.”",補足資料,Website,True,Use（引用目的）,True,N12-1002_0_0,2012,Acoustic-Prosodic Entrainment and Social Behavior,Footnote
2799,12809," https://github.com/yanaiela/demog-text-removal"," ['1 Introduction']",We explore means for im-proving the effectiveness of the adversarial train-ing procedure (section 5.2). [Cite_Footnote_1],1 The code and data acquisition are available in: https: //github.com/yanaiela/demog-text-removal,"This suggests that when working with text data it is very easy to condition on sensitive properties by mistake. Even when explicitly using the adver-sarial training method to remove such properties, one should not blindly trust the adversary, and be careful to ensure the protected attributes are in-deed fully removed. We explore means for im-proving the effectiveness of the adversarial train-ing procedure (section 5.2). [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D18-1002_0_0,2018,Adversarial Removal of Demographic Attributes from Text Data,Footnote
2800,12810," http://www.seggu.net/ccl/"," ['3 Method', '3.3.1 Constituency structure induction']","CCL While proposed in 2007, CCL [Cite_Footnote_2] is still con-sidered a state-of-the-art unsupervised parser.",2 http://www.seggu.net/ccl/,"To induce constituency structures, we compare two different techniques: the pre-neural statistical com-mon cover link parser (CCL, Seginer, 2007) and the neural parser Deep Inside-Outside Recursive Auto-encoder (DIORA, Drozdov et al., 2019). CCL While proposed in 2007, CCL [Cite_Footnote_2] is still con-sidered a state-of-the-art unsupervised parser. Con-trary to other popular parsers from the 2000s (e.g. Klein and Manning, 2004, 2005; Ponvert et al., 2011; Reichart and Rappoport, 2010), it does not require POS-annotation of the words in the corpus, making it appropriate for our setup.",Method,Tool,True,Introduce（引用目的）,True,2020.emnlp-main.270_0_0,2020,The Grammar of Emergent Languages,Footnote
2801,12811," https://github.com/iesl/diora"," ['3 Method', '3.3.1 Constituency structure induction']","DIORA In addition to CCL, we also experiment with the more recent neural unsupervised parser DIORA [Cite_Footnote_3] .",3 https://github.com/iesl/diora,"CCL is an incremental and greedy parser, that aims to incrementally add cover links to all words in a sentence. From these sets of cover links, con-stituency trees can be constructed. To limit the search space, CCL incorporates a few assumptions based on knowledge about natural language, such as the fact that constituency trees are generally skewed and the word distribution zipfian. In our experiments, we use the default settings for CCL. DIORA In addition to CCL, we also experiment with the more recent neural unsupervised parser DIORA [Cite_Footnote_3] . As the name suggests, DIORA is built on the application of recursive auto-encoders.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.270_1_0,2020,The Grammar of Emergent Languages,Footnote
2802,12812," https://nlp.stanford.edu/software/GloVe-1.2.zip"," ['3 Method', '3.3.1 Constituency structure induction']","We use the GloVe framework [Cite_Footnote_4] (Pennington et al., 2014) to pretrain word-embeddings for our corpus; using an embedding size of 16.",4 https://nlp.stanford.edu/software/GloVe-1.2.zip,"In our experiments with DIORA, we use a tree- LSTM with a hidden dimension of 50, and train for a maximum of 5 epochs with a batch size of 128. We use the GloVe framework [Cite_Footnote_4] (Pennington et al., 2014) to pretrain word-embeddings for our corpus; using an embedding size of 16.",Method,Tool,True,Use（引用目的）,True,2020.emnlp-main.270_2_0,2020,The Grammar of Emergent Languages,Footnote
2803,12813," https://github.com/pld/BMM_labels/"," ['3 Method', '3.3.2 Constituency labelling']",We use the BMM implementation provided by Borensztajn and Zuidema (2007) [Cite_Footnote_5] .,5 https://github.com/pld/BMM_labels/,"The BMM algorithm starts from a set of con-stituency trees in which each constituent is given its own unique label. It defines an iterative search procedure that merges labels to reduce the joint de-scription length of the data (DDL) and the grammar that can be inferred from the labelling (GDL). To find the next best merge step, the algorithm com-putes the effect of merging two labels on the sum of the GDL and DDL after doing the merge, where the GDL is defined as the number of bits to encode the grammar that can be inferred from the current labelled treebank with relative frequency estima-tion, and the DDL as the negative log-likelihood of the corpus given this grammar. To facilitate the search and avoid local minima, several heuris-tics and a look-ahead procedure are used to im-prove the performance of the algorithm. We use the BMM implementation provided by Borensztajn and Zuidema (2007) [Cite_Footnote_5] .",Method,Tool,False,Use（引用目的）,True,2020.emnlp-main.270_3_0,2020,The Grammar of Emergent Languages,Footnote
2804,12814," https://github.com/i-machine-think/emergent_grammar_induction"," ['6 Conclusion']","To facilitate such analysis, we bundled our tests in a comprehensive and easily usable evaluation frame-work. [Cite_Footnote_9]",9 https://github.com/i-machine-think/ emergent_grammar_induction,"We argue that while the extent to which syntax develops in different types of referential games is an interesting question in its own right, a better understanding of the syntactic structure of emer-gent languages could also provide pivotal in better understanding their semantics, especially if this is considered from a compositional point of view. To facilitate such analysis, we bundled our tests in a comprehensive and easily usable evaluation frame-work. [Cite_Footnote_9] We hope to have inspired other researchers to apply syntactic analysis techniques and encour-age them to use our code to evaluate new emergent languages trained in other scenarios.",Method,Code,False,Use（引用目的）,True,2020.emnlp-main.270_4_0,2020,The Grammar of Emergent Languages,Footnote
2805,12815," http://gutenberg.org/"," ['6 Experiments', '6.1 Corpora']",[Cite_Footnote_7] as a self-trained corpus.,7 http://gutenberg.org/,"We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kučera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg [Cite_Footnote_7] as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). 10 The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.",Material,DataSource,True,Use（引用目的）,True,N10-1004_0_0,2010,Automatic Domain Adaptation for Parsing,Footnote
2806,12816," http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/"," ['6 Experiments', '6.1 Corpora']","Our biomedi-cal data comes from the GENIA treebank [Cite_Footnote_8] (Tateisi et al., 2005), a corpus of abstracts from the Med-line database.",8 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/,"We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kučera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank [Cite_Footnote_8] (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). 10 The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.",Material,DataSource,True,Extend（引用目的）,True,N10-1004_1_0,2010,Automatic Domain Adaptation for Parsing,Footnote
2807,12817," http://www.ncbi.nlm.nih.gov/PubMed/"," ['6 Experiments', '6.1 Corpora']","Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. [Cite_Footnote_9]",9 http://www.ncbi.nlm.nih.gov/PubMed/,"We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kučera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. [Cite_Footnote_9] We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). 10 The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.",Material,DataSource,True,Extend（引用目的）,True,N10-1004_2_0,2010,Automatic Domain Adaptation for Parsing,Footnote
2808,12818," http://nclt.computing.dcu.ie/~jfoster/resources/"," ['6 Experiments', '6.1 Corpora']","Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [Cite_Footnote_10]","10 http://nclt.computing.dcu.ie/˜jfoster/resources/, downloaded January 8th, 2009.","We use news articles portion of the Wall Street Journal corpus ( WSJ ) from the Penn Treebank (Mar-cus et al., 1993) in conjunction with the self-trained North American News Text Corpus ( NANC , Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation 6 of broadcast news in Arabic. For literature, we use the BROWN cor-pus (Francis and Kučera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg 7 as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations. While the original trees include disflu-ency information, we assume our speech corpora have had speech repairs excised (e.g. using a sys-tem such as Johnson et al. (2004)). Our biomedi-cal data comes from the GENIA treebank 8 (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. 9 We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE . These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain perfor-mance. Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [Cite_Footnote_10] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.",Material,DataSource,True,Use（引用目的）,True,N10-1004_3_0,2010,Automatic Domain Adaptation for Parsing,Footnote
2809,12819," http://wapiti.limsi.fr"," ['3 Reranking Framework', '3.1 Conditional Random Fields']","The other is described in (Lavergne et al., 2010) and has been implemented in the software wapiti [Cite_Footnote_1] .",1 available at http://wapiti.limsi.fr,"Two particular effective implementations of CRFs have been recently proposed. One is described in (Hahn et al., 2009) and uses a margin based criterion for probabilities estimation. The other is described in (Lavergne et al., 2010) and has been implemented in the software wapiti [Cite_Footnote_1] . The latter solution in partic-ular trains the model using two different regulariza-tion factors at the same time:",Method,Tool,True,Introduce（引用目的）,True,D11-1102_0_0,2011,Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding,Footnote
2810,12820," http://wapiti.limsi.fr"," ['5 Experiments']","We didn’t carry out optimization for parameters ρ 1 and ρ [Cite_Footnote_2] of the elastic net (see section 3.1), default values lead in most cases to very accurate models.",2 available at http://wapiti.limsi.fr,"For our CRF models, both Automatic Concept An-notation and Attribute Value Extraction SLU phases, we used wapiti 2 (Lavergne et al., 2010). The CRF model for the first SLU phase integrates a tradi-tional set of features like word prefixes and suffixes (of length up to 5), plus some Yes/No features like “Does the word start with capital letter ?”, “Does the word contain non alphanumeric characters ?”, “Is the word preceded by non alphanumeric char-acteris ?” etc. The CRF model for AVE integrates only words, prefixes and suffixes (length 3 and 4) concatenated with concepts. Since in this case la-bels are attribute values, which are a huge set with respect to concepts (7̃00 VS 99), using a lot of fea-tures would make model training problematic. De-spite the reduced set of features, training error rate at both token and sentence level is under 1%. We didn’t carry out optimization for parameters ρ 1 and ρ [Cite_Footnote_2] of the elastic net (see section 3.1), default values lead in most cases to very accurate models.",Method,Code,False,Introduce（引用目的）,False,D11-1102_1_0,2011,Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding,Footnote
2811,12821," http://disi.unitn.it/moschitti/Tree-Kernel.htm"," ['5 Experiments']",Reranking models based on SVM and PTK have been trained with “SVM-Light-TK” [Cite_Footnote_3] .,3 available at http://disi.unitn.it/moschitti/Tree-Kernel.htm,"Reranking models based on SVM and PTK have been trained with “SVM-Light-TK” [Cite_Footnote_3] . Kernel param-eters M and SVM parameter C have been optimized on the development set, as well as thresholds for the WRR (see section 4.2).",Method,Tool,True,Use（引用目的）,True,D11-1102_2_0,2011,Hypotheses Selection Criteria in a Reranking Framework for Spoken Language Understanding,Footnote
2812,12822," https://github.com/facebookresearch/fairseq-py"," ['5 Experimental evaluation: improving WMT’14 En-De NMT systems', '5.1 Baseline NMT systems']","We have performed all our experiments with the freely available Sequence-to-Sequence Py-Torch toolkit from Facebook AI Research, [Cite_Footnote_3] called fairseq-py.",3 https://github.com/facebookresearch/ fairseq-py,"We have performed all our experiments with the freely available Sequence-to-Sequence Py-Torch toolkit from Facebook AI Research, [Cite_Footnote_3] called fairseq-py. It implements a convolutional model which achieves very competitive results (Gehring et al., 2017). We use this system to show the improvements obtained by filtering the stan-dard training data and by integrating additional mined data. We will freely share this data so that it can be used to train different NMT architectures.",Method,Tool,True,Use（引用目的）,True,P18-2037_0_0,2018,Filtering and Mining Parallel Data in a Joint Multilingual Space,Footnote
2813,12823," https://fasttext.cc/docs/en/language-identification.html"," ['5 Experimental evaluation: improving WMT’14 En-De NMT systems', '5.2 Filtering Common Crawl']",It is surpris-ing that almost 6% of the data seems to have the wrong source or target language. [Cite_Footnote_5],"5 LID itself may also commit errors, we used https://fasttext.cc/docs/en/language-identification.html","After some initial experiments, it turned out that some additional steps are needed before cal-culating the distances (see Table 5): 1) remove sentences with more than 3 commas. Those are indeed often enumerations of names, cities, etc. While such sentences maybe useful to train NMT systems, the multilingual distance is not very reli-able to distinguish list of named entities; 2) limit to sentences with less than 50 words; 3) perform LID on source and target sentences; These steps discarded overall 19% of the data. It is surpris-ing that almost 6% of the data seems to have the wrong source or target language. [Cite_Footnote_5]",Method,Tool,True,Use（引用目的）,True,P18-2037_1_0,2018,Filtering and Mining Parallel Data in a Joint Multilingual Space,Footnote
2814,12824," http://paracrawl.eu/download.html"," ['6 Conclusion']",We will apply it to the data mined by the European ParaCrawl project. [Cite_Footnote_6],6 http://paracrawl.eu/download.html,"There are many directions to extend this re-search, in particular to scale-up to larger corpora. We will apply it to the data mined by the European ParaCrawl project. [Cite_Footnote_6] The proposed multilingual sentence distance could be also used in MT con-fidence estimation, or to filter back-translations of monolingual data (Sennrich et al., 2016a).",Material,Dataset,True,Use（引用目的）,True,P18-2037_2_0,2018,Filtering and Mining Parallel Data in a Joint Multilingual Space,Footnote
2815,12825," https://FN.icsi.berkeley.edu/fndrupal/"," ['1 Introduction']",† See the details in [Cite] https://FN.icsi.berkeley.edu/fndrupal/,,† See the details in [Cite] https://FN.icsi.berkeley.edu/fndrupal/,補足資料,Website,False,Introduce（引用目的）,False,2021.acl-long.407_0_0,2021,A Knowledge-Guided Framework for Frame Identification,Body
2816,12826," http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip"," ['1 Introduction']","(4) By adding word-knowledge features and us-ing learning-based multi-sieve approach, we im-prove the performance of the state-of-the-art system of (Bengtson and Roth, 2008) by 3 MUC, [Cite_Footnote_2] B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset.",2 The extracted attributes and the related re-sources are available for public download at http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip,"(4) By adding word-knowledge features and us-ing learning-based multi-sieve approach, we im-prove the performance of the state-of-the-art system of (Bengtson and Roth, 2008) by 3 MUC, [Cite_Footnote_2] B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset. We report our experimen-tal results in Sec. 6 and conclude with discussion in Sec. 7.",Mixed,Mixed,True,Produce（引用目的）,True,D12-1113_0_0,2012,Learning-based Multi-Sieve Co-reference Resolution with Knowledge ∗,Footnote
2817,12827," http://en.wikipedia.org/wiki/Efraim"," ['3 Wikipedia as Knowledge', 'Named Entities with Wikipedia Disambiguation']","If the mention head is an NE matched to a Wikipedia page p by GLOW, we import all the knowledge at-tributes from p. GLOW allows us to map “Ephraim Sneh” to [Cite] http://en.wikipedia.org/wiki/Efraim Sneh and to assign it the Israeli nationality, male gender, and the fine-grained entity types: {member, politi-cian, person, minister, alumnus, physician, gen-eral}.",,"If the mention head is an NE matched to a Wikipedia page p by GLOW, we import all the knowledge at-tributes from p. GLOW allows us to map “Ephraim Sneh” to [Cite] http://en.wikipedia.org/wiki/Efraim Sneh and to assign it the Israeli nationality, male gender, and the fine-grained entity types: {member, politi-cian, person, minister, alumnus, physician, gen-eral}.",補足資料,Document,True,Use（引用目的）,True,D12-1113_1_0,2012,Learning-based Multi-Sieve Co-reference Resolution with Knowledge ∗,Body
2818,12828," http://cogcomp.cs.illinois.edu/page/software_view/Wikifier"," ['3 Wikipedia as Knowledge', '3.2 Injecting Knowledge Attributes']","Additionally, while (Rahman and Ng, 2011) uses the union of all possible meanings a mention may have in Wikipedia, we deploy GLOW (Ratinov et al., 2011) [Cite_Footnote_7] , a context-sensitive system for disam-biguation to Wikipedia.",7 Available at: http://cogcomp.cs.illinois.edu/page/software_view/Wikifier,"Additionally, while (Rahman and Ng, 2011) uses the union of all possible meanings a mention may have in Wikipedia, we deploy GLOW (Ratinov et al., 2011) [Cite_Footnote_7] , a context-sensitive system for disam-biguation to Wikipedia. Using context-sensitive dis-ambiguation to Wikipedia as well as high-precision set of knowledge attributes allows us to inject the knowledge to more mention pairs when compared to (Rahman and Ng, 2011). Our exact heuristic for injecting knowledge attributes to mentions is as fol-lows:",Method,Tool,True,Use（引用目的）,True,D12-1113_2_0,2012,Learning-based Multi-Sieve Co-reference Resolution with Knowledge ∗,Footnote
2819,12829," https://github.com/shashiongithub/XSum"," ['References']","We demonstrate exper-imentally that this architecture captures long-range dependencies in a document and recog-nizes pertinent content, outperforming an or-acle extractive system and state-of-the-art ab-stractive approaches when evaluated automat-ically and by humans. [Cite_Footnote_1]","1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum.","We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large scale dataset for this task by harvesting online articles from the British Broadcasting Corpo-ration (BBC). We propose a novel abstrac-tive model which is conditioned on the ar-ticle’s topics and based entirely on convolu-tional neural networks. We demonstrate exper-imentally that this architecture captures long-range dependencies in a document and recog-nizes pertinent content, outperforming an or-acle extractive system and state-of-the-art ab-stractive approaches when evaluated automat-ically and by humans. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D18-1206_0_0,2018,"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",Footnote
2820,12830," https://github.com/abisee/pointer-generator"," ['4 Experimental Setup']","For S EQ 2S EQ , P T G EN and P T G EN -C OVG , we used the best settings reported on the CNN and DailyMail data (See et al., 2017). [Cite_Footnote_2]",2 We used the code available at https://github.com/abisee/pointer-generator.,"For S EQ 2S EQ , P T G EN and P T G EN -C OVG , we used the best settings reported on the CNN and DailyMail data (See et al., 2017). [Cite_Footnote_2] All three mod-els had 256 dimensional hidden states and 128 di-mensional word embeddings. They were trained using Adagrad (Duchi et al., 2011) with learning rate 0.15 and an initial accumulator value of 0.1. We used gradient clipping with a maximum gradi-ent norm of 2, but did not use any form of regular-ization. We used the loss on the validation set to implement early stopping.",Method,Code,True,Use（引用目的）,True,D18-1206_1_0,2018,"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",Footnote
2821,12831," https://github.com/facebookresearch/fairseq-py"," ['4 Experimental Setup']","For C ONV S2S [Cite_Footnote_3] and T-C ONV S2S, we used 512 dimensional hidden states and 512 dimensional word and position embeddings.",3 We used the code available at https://github.com/facebookresearch/fairseq-py.,"For C ONV S2S [Cite_Footnote_3] and T-C ONV S2S, we used 512 dimensional hidden states and 512 dimensional word and position embeddings. We trained our convolutional models with Nesterov’s accelerated gradient method (Sutskever et al., 2013) using a momentum value of 0.99 and renormalized gra-dients if their norm exceeded 0.1 (Pascanu et al., 2013). We used a learning rate of 0.10 and once the validation perplexity stopped improving, we reduced the learning rate by an order of magnitude after each epoch until it fell below 10 −4 . We also applied a dropout of 0.2 to the embeddings, the decoder outputs and the input of the convolutional blocks. Gradients were normalized by the number of non-padding tokens per mini-batch. We also used weight normalization for all layers except for lookup tables.",Method,Code,True,Use（引用目的）,True,D18-1206_2_0,2018,"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",Footnote
2822,12832," https://github.com/dr97531/SparseSCVB0"," ['6 Experiments']","In this section we study the performance of our SparseSCVB0 [Cite_Footnote_1] algorithm, on small as well as large corpora to validate the proposed method for topic models such as LDA and MMSGTM, and to compare with other state-of-the-art algorithms.",1 Code implementing SparseSCVB0 can be found at https://github.com/dr97531/SparseSCVB0.,"In this section we study the performance of our SparseSCVB0 [Cite_Footnote_1] algorithm, on small as well as large corpora to validate the proposed method for topic models such as LDA and MMSGTM, and to compare with other state-of-the-art algorithms.",Method,Code,True,Produce（引用目的）,True,N19-1291_0_0,2019,Scalable Collapsed Inference for High-Dimensional Topic Models,Footnote
2823,12833," https://github.com/huggingface/transformers"," ['4 Experiments', '4.1 Settings']",Network Training We implement our proposed method based on huggingface Transformers [Cite_Footnote_1] .,1 https://github.com/huggingface/transformers,"Network Training We implement our proposed method based on huggingface Transformers [Cite_Footnote_1] . Fol-lowing Wolf et al. (2019), we use a batch size of 32, and 3 training epochs to ensure convergence of op-timization. Following Wu and Dredze (2019), we freeze the parameters of the embedding layer and the bottom three layers of BERT BASE . For the op-timizers, we use AdamW (Loshchilov and Hutter, 2017) with learning rate of 5e − 5 for teacher mod-els (Wolf et al., 2019), and 1e − 4 for the student model (Yang et al., 2019) to converge faster. As for language similarity measuring (i.e., Eq. 10), we set γ = 0.01 following Pinheiro (2018). Besides, we use a low-rank approximation for the bilinear operator M, i.e., M = U T V where U, V ∈ R d×m with d m, and we empirically set d = 64.",Method,Tool,True,Extend（引用目的）,True,2020.acl-main.581_0_0,2020,Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language,Footnote
2824,12834," https://github.com/thunlp/MuGNN"," ['References']",Source code and data used in the experiments can be accessed at [Cite] https://github.com/thunlp/MuGNN.,,"Entity alignment typically suffers from the is-sues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by ro-bustly encoding two KGs via multiple chan-nels. Each channel encodes KGs via differ-ent relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive enti-ties respectively, which are further combined via pooling techniques. Moreover, we also in-fer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed align-ments. Extensive experiments on five pub-licly available datasets demonstrate our su-perior performance (5% Hits@1 up on av-erage). Source code and data used in the experiments can be accessed at [Cite] https://github.com/thunlp/MuGNN.",Mixed,Mixed,True,Produce（引用目的）,True,P19-1140_0_0,2019,Multi-Channel Graph Neural Network for Entity Alignment,Body
2825,12835," https://www.mpi-inf.mpg.de/"," ['3 KG Completion', '3.1 Rule Inference and Transfer']",Its source code is available online [Cite_Footnote_1] .,1 https://www.mpi-inf.mpg.de/,"Since the acquirement of rule knowledge is not our focus in this paper, we utilize AMIE+ (Galárraga et al., 2015), a modern rule mining system, to effi-ciently find Horn rules from large-scale KG, such as marriedTo(x, y) ∧ liveIn(x, z) ⇒ liveIn(y, z). Its source code is available online [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,True,P19-1140_1_0,2019,Multi-Channel Graph Neural Network for Entity Alignment,Footnote
2826,12836," https://github.com/valentinhofmann/dga"," ['1 Introduction']",The model produces embeddings that capture information about the compatibility of af-fixes and stems in derivation and can be used as pretrained input to other NLP applications. [Cite_Footnote_1],1 We make all our code and data publicly available at https://github.com/valentinhofmann/dga.,"This study takes a first step towards computation-ally modeling the MWF of English derivatives. We present a derivational graph auto-encoder (DGA) that combines semantic and syntactic information with associative information from the mental lexi-con, achieving very good results on MWF predic-tion and performing on par with a character-based LSTM at a fraction of the number of trainable pa-rameters. The model produces embeddings that capture information about the compatibility of af-fixes and stems in derivation and can be used as pretrained input to other NLP applications. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.106_0_0,2020,"A Graph Auto-encoder Model of Derivational Morphology Valentin Hofmann *‡ , Hinrich Schütze ‡ , Janet B. Pierrehumbert †*",Footnote
2827,12837," http://wordnet.princeton.edu"," ['5 Parameterization', '5.2 Features']",The third feature is in-voked if two heads are synonyms or derivations that are extracted from the WordNet [Cite_Footnote_4] .,4 http://wordnet.princeton.edu,"The first feature is an indicator invoked only at specific values. On the other hand, the rest of the features are invoked across multiple values, allow-ing general patterns to be learned. The second fea-ture is invoked if two heads are identical or a head is a substring of another. The third feature is in-voked if two heads are synonyms or derivations that are extracted from the WordNet [Cite_Footnote_4] . The fourth feature is invoked if the cosine similarity between word embeddings of two heads is larger than a threshold. The fifth feature is invoked when the heads are both prepositions to capture their differ-ent natures from the content words. The last two features are for categories; the sixth one is invoked at each category pair, while the seventh feature is invoked if the input categories are the same.",Material,DataSource,True,Use（引用目的）,True,D17-1001_0_0,2017,Monolingual Phrase Alignment on Parse Forests,Footnote
2828,12838," http://ilk.uvt.nl/conll/software.html"," ['6 Evaluation', '6.3 Evaluation Metric']","Dependencies were extracted from the output HPSG trees, and evaluated using the official script [Cite_Footnote_6] .",6 http://ilk.uvt.nl/conll/software.html,"Parsing Quality The parsing quality was evalu-ated using the CONLL-X (Buchholz and Marsi, 2006) standard. Dependencies were extracted from the output HPSG trees, and evaluated using the official script [Cite_Footnote_6] . Due to this conversion, the accuracy on the relation labels is less important. Thus, we reported only the unlabeled attachment score (UAS) . The development and test sets pro-vide 2, 371 and 6, 957 dependencies, respectively.",Method,Code,False,Use（引用目的）,True,D17-1001_1_0,2017,Monolingual Phrase Alignment on Parse Forests,Footnote
2829,12839," http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/"," ['Supplemental Material']","The supplemental material is available at our web site [Cite_Footnote_9] that provides proofs of the theorems, pseudo-codes of the algorithms, and more experiment re-sults with examples.",9 http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/,"The supplemental material is available at our web site [Cite_Footnote_9] that provides proofs of the theorems, pseudo-codes of the algorithms, and more experiment re-sults with examples.",補足資料,Document,True,Produce（引用目的）,True,D17-1001_2_0,2017,Monolingual Phrase Alignment on Parse Forests,Footnote
2830,12840," http://www1.cs.columbia.edu/nlp/tides/SEEManual.pdf"," ['2 Case Study Analysis']","Despite its pit-falls, ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods (Lin, 2004; Over and James, 2004; Over et al., 2007; Nenkova et al., 2007; Louis and Nenkova, 2013; Peyrard et al., 2017), such as SEE (Lin, 2001) [Cite_Ref] , responsiveness (NIST, 2006) and Pyramid (Nenkova et al., 2007).",Chin-Yew Lin. 2001. Summary evaluation en-vironment user guide. http://www1.cs.columbia.edu/nlp/tides/SEEManual.pdf.,"Next, we turn to investigate our research ques-tion. In this paper, we examine it with respect to automatic summary evaluation, which has become most common for system development and evalua-tion, thanks to its speed and low cost. Specifically, we use several variants of the ROUGE metric (Lin, 2004), which is almost exclusively utilized as an automatic evaluation metric class for summariza-tion. ROUGE variants are based on word sequence overlap between a system summary and a refer-ence summary, where each variant measures a dif-ferent aspect of text comparison. Despite its pit-falls, ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods (Lin, 2004; Over and James, 2004; Over et al., 2007; Nenkova et al., 2007; Louis and Nenkova, 2013; Peyrard et al., 2017), such as SEE (Lin, 2001) [Cite_Ref] , responsiveness (NIST, 2006) and Pyramid (Nenkova et al., 2007).",補足資料,Paper,True,Introduce（引用目的）,True,D18-1087_0_0,2018,Evaluating Multiple System Summary Lengths: A Case Study,Reference
2831,12841," https://duc.nist.gov/"," ['1 Introduction']","For that, the earliest Document Un-derstand Conference (DUC) (NIST, 2011) [Cite_Ref] bench-marks, in 2001 and 2002, defined several tar-get summary lengths and evaluated each summary against (manually written) reference summaries of the same length.",NIST. 2011. Document Understanding Conferences. https://duc.nist.gov/.,"It was originally assumed that summarization systems should be assessed across multiple sum-mary lengths. For that, the earliest Document Un-derstand Conference (DUC) (NIST, 2011) [Cite_Ref] bench-marks, in 2001 and 2002, defined several tar-get summary lengths and evaluated each summary against (manually written) reference summaries of the same length.",補足資料,Website,True,Introduce（引用目的）,True,D18-1087_1_0,2018,Evaluating Multiple System Summary Lengths: A Case Study,Reference
2832,12842," https://tac.nist.gov/"," ['1 Introduction']","However, due to the high cost incurred, subse-quent DUC and TAC (NIST, 2018) [Cite_Ref] benchmarks (2003-2014), as well as the more recently popular datasets CNN/Daily Mail (Nallapati et al., 2016) and Gigaword (Graff et al., 2003), included refer-ences and evaluation for just one summary length per input text.",NIST. 2018. Text Analysis Conferences. https://tac.nist.gov/.,"However, due to the high cost incurred, subse-quent DUC and TAC (NIST, 2018) [Cite_Ref] benchmarks (2003-2014), as well as the more recently popular datasets CNN/Daily Mail (Nallapati et al., 2016) and Gigaword (Graff et al., 2003), included refer-ences and evaluation for just one summary length per input text. Accordingly, systems were asked to produce a single summary, of corresponding length. This decision was partly supported by an observation that system rankings tended to corre-late across different summary lengths (Over et al., 2007), even though, as we show in Section 2, this correlation is limited.",補足資料,Website,True,Introduce（引用目的）,True,D18-1087_2_0,2018,Evaluating Multiple System Summary Lengths: A Case Study,Reference
2833,12843," https://github.com/lilt/alignment-scripts"," ['6 Experiments', '6.1 Data']",We use the same experimental setup [Cite_Footnote_5] as de-scribed by Zenkel et al. (2019) and used by Garg et al. (2019).,5 https://github.com/lilt/ alignment-scripts,"We use the same experimental setup [Cite_Footnote_5] as de-scribed by Zenkel et al. (2019) and used by Garg et al. (2019). It contains three language pairs: German→English, Romanian→English and English→French (Och and Ney, 2000a; Mihalcea and Pedersen, 2003). We learn a joint byte pair encoding (BPE) for the source and the target lan-guage with 40k merge operation (Sennrich et al., 2016). To convert from alignments between word pieces to alignments between words, we align a source word to a target word if an alignment link exists between any of its word pieces.",補足資料,Website,True,Use（引用目的）,True,2020.acl-main.146_0_0,2020,End-to-End Neural Word Alignment Outperforms GIZA++,Footnote
2834,12844," https://github.com/anuradha1992/EDOS"," ['Philip Ball. 2011. How movies mirror our mimicry.']",The resultant procedure can be used to create sim-ilar datasets in the same domain as well as in other domains. [Cite_Footnote_1],1 The datasets and the code are publicly accessible at https://github.com/anuradha1992/EDOS.,"Recent development in NLP shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where emo-tion plays an important role. However, exist-ing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a large-scale silver dataset consisting of 1M di-alogues annotated with 32 fine-grained emo-tions, eight empathetic response intents, and the Neutral category. To achieve this goal, we have developed a novel data curation pipeline starting with a small seed of manually anno-tated data and eventually scaling it to a satis-factory size. We compare its quality against a state-of-the-art gold dataset using offline ex-periments and visual validation methods. The resultant procedure can be used to create sim-ilar datasets in the same domain as well as in other domains. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.emnlp-main.96_0_0,2021,A Large-Scale Dataset for Empathetic Response Generation,Footnote
2835,12845," https://opus.nlpl.eu/OpenSubtitles-v2018.php"," ['7 Ethical considerations']","EDOS contains dialogues derived from the Open-Subtitles corpus (Lison et al., 2019), which is pub-licly available. [Cite_Footnote_2]",2 https://opus.nlpl.eu/OpenSubtitles-v2018.php,"EDOS contains dialogues derived from the Open-Subtitles corpus (Lison et al., 2019), which is pub-licly available. [Cite_Footnote_2] It is part of the OPUS (Open Par-allel corpUS), which is based on open source prod-ucts and is delivered as an open content package. The workers annotating the dataset were compen-sated with $0.4 per HIT, which takes 4.12 minutes on average to complete (excluding the time taken by workers who took an unusually long time to complete the task) and a bonus of $0.1 if they com-pleted at least 3 out of 5 quiz questions correctly. Fair compensation was determined based on the US minimum wage of $7.12 per hour. Since the dataset is in English, the annotators recruited from AMT were restricted the majority native English-speaking countries: US; UK; Canada; Australia; and New Zealand. The fact that the dataset is English-only potentially perpetuates an English bias in NLP systems.",Material,DataSource,True,Extend（引用目的）,True,2021.emnlp-main.96_1_0,2021,A Large-Scale Dataset for Empathetic Response Generation,Footnote
2836,12846," http://nlp.stanford.edu/data/glove.6B.zip"," ['4 Experiments and Results', '4.1 Obtaining Word-Pair Embedding']","To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) ∈ T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [Cite_Footnote_1] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path .",1 http://nlp.stanford.edu/data/glove.6B.zip,"To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) ∈ T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [Cite_Footnote_1] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 · 10 −7 , following Joshi et al. (2019).",Material,Dataset,True,Use（引用目的）,True,D19-1357_0_0,2019,Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling,Footnote
2837,12847," https://github.com/tombosc/cpae"," ['4 Experiments and Results', '4.2 Definition Embedding']","Embedding Benchmarks projects [Cite_Footnote_3] , following Bosc and Vincent (2018).",3 https://github.com/tombosc/cpae,"For the evaluation of definition embeddings, we used the modified Word Embedding Benchmarks projects [Cite_Footnote_3] , following Bosc and Vincent (2018). These benchmarks include SimLex999 (SL999), SimLex333 (SL333) (Hill et al., 2015), SimVerb (SV) (Gerz et al., 2016), MEN (Bruni et al., 2014), RG (Rubenstein and Goodenough, 1965), WS353 (Finkelstein et al., 2002), SCWS (Huang et al., 2012), and MTurk (Radinsky et al., 2011; Halawi et al., 2012). To evaluate the definition embed-dings, we scored word pairs in the benchmarks us-ing the cosine similarity between the correspond-ing definition embeddings and calculated Spear-man’s correlation to the ground truth. The defini-tions in WordNet were used to train the definition encoder. The development sets of the SimVerb and MEN were used for the hyperparameter tuning.",補足資料,Website,True,Use（引用目的）,True,D19-1357_1_0,2019,Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling,Footnote
2838,12848," https://code.google.com/archive/p/word2vec/"," ['4 Experiments and Results', '4.2 Definition Embedding']",The word embeddings in the def-inition encoder were initialized by the Google Word2Vec vectors [Cite_Footnote_4] .,4 https://code.google.com/archive/p/word2vec/,"We implemented the CPAE (Section 2.1) as a baseline and compared it to CPAE with the word-pair embeddings (Section 3.1), which is our pro-posed method. The word embeddings in the def-inition encoder were initialized by the Google Word2Vec vectors [Cite_Footnote_4] . Google Word2Vec vectors and GloVe were used as the other baselines.",Method,Tool,True,Use（引用目的）,True,D19-1357_2_0,2019,Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling,Footnote
2839,12849," https://nlp.stanford.edu/projects/glove/"," ['4 Evaluation', '4.2 Baselines']","We used the 300-dimensional pre-trained GloVe (Pennington et al., 2014) [Cite_Footnote_1] and represented word pairs as described in Section 2.1.",1 https://nlp.stanford.edu/projects/glove/,"VecOff. We used the 300-dimensional pre-trained GloVe (Pennington et al., 2014) [Cite_Footnote_1] and represented word pairs as described in Section 2.1.",Method,Code,True,Use（引用目的）,True,D18-1058_0_0,2018,Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space,Footnote
2840,12850," https://github.com/sinantie/NeuralAmr"," ['5 Experiments']","We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [Cite_Footnote_1]",1 We used the evaluation script available at https://github.com/sinantie/NeuralAmr.,"We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [Cite_Footnote_1] We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017). Hyperparameters of each model were tuned on the development set of LDC2015E86. For the GCN components, we use two layers, ReLU activations, and tanh highway layers. We use single layer LSTMs. We train with SGD with the initial learning rate set to 1 and decay to 0.8. Batch size is set to 100.",Method,Code,True,Use（引用目的）,True,N19-1366_0_0,2019,Structural Neural Encoders for AMR-to-text Generation,Footnote
2841,12851," https://github.com/mdtux89/OpenNMT-py-AMR-to-text"," ['5 Experiments']",Batch size is set to 100. [Cite_Footnote_2],2 Our code is available at https://github.com/mdtux89/OpenNMT-py-AMR-to-text.,"We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017). Hyperparameters of each model were tuned on the development set of LDC2015E86. For the GCN components, we use two layers, ReLU activations, and tanh highway layers. We use single layer LSTMs. We train with SGD with the initial learning rate set to 1 and decay to 0.8. Batch size is set to 100. [Cite_Footnote_2]",Method,Code,True,Produce（引用目的）,True,N19-1366_1_0,2019,Structural Neural Encoders for AMR-to-text Generation,Footnote
2842,12852," https://github.com/mdtux89/OpenNMT-py"," ['5 Experiments', '5.1 Reentrancies', '5.1.2 Contrastive Pairs']","From the test split of LDC2017T10, we gener-ated 251 contrastive examples due to antecedent replacements, 912 due to pronoun type replace-ments, 1840 due to number replacements and 95 due to gender replacements. [Cite_Footnote_5]",5 The generated contrastive examples are available at https://github.com/mdtux89/OpenNMT-py.,"From the test split of LDC2017T10, we gener-ated 251 contrastive examples due to antecedent replacements, 912 due to pronoun type replace-ments, 1840 due to number replacements and 95 due to gender replacements. [Cite_Footnote_5] The results are shown in Table 6. The sequential encoder per-forms surprisingly well at this task, with better or on par performance with respect to the tree encoder. The graph encoder outperforms the se-quential encoder only for pronoun number and gender replacements. Future work is required to more precisely analyze if the different models cope with pronomial mentions in significantly dif-ferent ways. Other approaches to inspect phenom-ena of co-reference and control verbs can also be explored, for instance by devising specific training objectives (Linzen et al., 2016).",補足資料,Document,True,Produce（引用目的）,True,N19-1366_2_0,2019,Structural Neural Encoders for AMR-to-text Generation,Footnote
2843,12853," https://github.com/tensorflow/tensor2tensor"," ['3 Experiments', '3.1 Neural Machine Translation']","Experimental Setup For subword-level NMT, we evaluate our models on seven NMT datasets us-ing the Tensor2Tensor [Cite_Footnote_1] framework (Vaswani et al., 2018), namely IWSLT’14 De→En, IWSLT’14 Ro→En, IWSLT’15 En→Vi, IWSLT’17 En→Id, WMT’17 En→Et, SETIMES En→Mk, and the well-established large-scale WMT’16 En→De.",1 https://github.com/tensorflow/ tensor2tensor,"For neural machine translation (NMT), we evaluate on both the subword-level and character-level tasks. Experimental Setup For subword-level NMT, we evaluate our models on seven NMT datasets us-ing the Tensor2Tensor [Cite_Footnote_1] framework (Vaswani et al., 2018), namely IWSLT’14 De→En, IWSLT’14 Ro→En, IWSLT’15 En→Vi, IWSLT’17 En→Id, WMT’17 En→Et, SETIMES En→Mk, and the well-established large-scale WMT’16 En→De.",Method,Tool,False,Use（引用目的）,True,2021.acl-short.48_0_0,2021,On Orthogonality Constraints for Transformers,Footnote
2844,12854," http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/"," ['1 Introduction']",The dataset and baseline models will be freely available online. [Cite_Footnote_1],1 http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/,"This work presents the data collection approach, a summary of the data structure, as well as a se-ries of analyses of the data statistics. To show the potential and usefulness of the proposed Mul-tiWOZ corpus, benchmarking baselines of belief tracking, natural language generation and end-to-end response generation have been conducted and reported. The dataset and baseline models will be freely available online. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,D18-1547_0_0,2018,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,Footnote
2845,12855," https://github.com/osmanio2/multi-domain-belief-tracking"," ['5 MultiWOZ as a New Benchmark', '5.1 Dialogue State Tracking']","Further-more, the model parameters are independent of the ontology and belief states, therefore the number of the parameters does not increase with the size of the domain itself. [Cite_Footnote_4]",4 The model is publicly available at https://github.com/osmanio2/ multi-domain-belief-tracking,"A robust natural language understanding and dia-logue state tracking is the first step towards build-ing a good conversational system. Since multi-domain dialogue state tracking is still in its infancy and there are not many comparable approaches available (Rastogi et al., 2017), we instead report our state-of-the-art result on the restaurant subset of the MultiWOZ corpus as the reference baseline. The proposed method (Ramadan et al., 2018) ex-ploits the semantic similarity between dialogue ut-terances and the ontology terms which allows the information to be shared across domains. Further-more, the model parameters are independent of the ontology and belief states, therefore the number of the parameters does not increase with the size of the domain itself. [Cite_Footnote_4]",Method,Code,True,Produce（引用目的）,True,D18-1547_1_0,2018,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,Footnote
2846,12856," https://github.com/budzianowski/multiwoz"," ['5 MultiWOZ as a New Benchmark', '5.2 Dialogue-Context-to-Text Generation']","In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [Cite_Footnote_5]",5 The model is publicly available at https://github.com/budzianowski/multiwoz,"After a robust dialogue state tracking module is built, the next challenge becomes the dia-logue management and response generation com-ponents. These problems can either be addressed separately (Young et al., 2013), or jointly in an end-to-end fashion (Bordes et al., 2017; Wen et al., 2017; Li et al., 2017). In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [Cite_Footnote_5]",Method,Code,True,Produce（引用目的）,True,D18-1547_2_0,2018,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,Footnote
2847,12857," https://github.com/andy194673/nlg-sclstm-multiwoz"," ['5 MultiWOZ as a New Benchmark', '5.3 Dialogue-Act-to-Text Generation']","The best model for both datasets was found through a grid search over a set of hyper-parameters such as the size of embeddings, learn-ing rate, and number of LSTM layers. [Cite_Footnote_6]",6 The model is publicly available at https://github.com/andy194673/ nlg-sclstm-multiwoz,"Table 4 : Performance comparison of two different model architectures using a corpus-based evaluation. logue turns. To give more statistics about the two datasets: the SFX corpus has 9 different act types with 12 slots comparing to 12 acts and 14 slots in our corpus. The best model for both datasets was found through a grid search over a set of hyper-parameters such as the size of embeddings, learn-ing rate, and number of LSTM layers. [Cite_Footnote_6]",Method,Code,True,Produce（引用目的）,True,D18-1547_3_0,2018,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,Footnote
2848,12858," http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/"," ['References']",The data is avail-able at [Cite] http://dialogue.mi.eng.cam.,,"This work was funded by a Google Faculty Re-search Award (RG91111), an EPSRC studentship (RG80792), an EPSRC grant (EP/M018946/1) and by Toshiba Research Europe Ltd, Cam-bridge Research Laboratory (RG85875). The authors thank many excellent Mechanical Turk contributors for building this dataset. The au-thors would also like to thank Thang Minh Luong for his support for this project ad Nikola Mrkšić and anonymous reviewers for their constructive feedback. The data is avail-able at [Cite] http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/.",Material,Dataset,True,Produce（引用目的）,True,D18-1547_4_0,2018,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,Body
2849,12859," http://www.purl.com/net/ArabicSentiment"," ['1 Introduction']",We also cre-ated a substantial amount of sentiment labeled data pertaining to Arabic social media texts and their En-glish translations which is made freely available. [Cite_Footnote_1],1 http://www.purl.com/net/ArabicSentiment,"In the process of developing these experiments to study how translation alters sentiment, we created a state-of-the-art Arabic sentiment analysis system by porting NRC-Canada’s competition winning system (Kiritchenko et al., 2014) to Arabic. We also cre-ated a substantial amount of sentiment labeled data pertaining to Arabic social media texts and their En-glish translations which is made freely available. [Cite_Footnote_1] This is the first such resource where text in one language and its translations into another language (both manually and automatically produced) are each manually labeled for sentiment.",Material,Dataset,True,Produce（引用目的）,True,N15-1078_0_0,2015,Sentiment after Translation: A Case-Study on Arabic Social Media Posts,Footnote
2850,12860," https://catalog.ldc.upenn.edu/LDC2012T09"," ['3 Method for Determining Sentiment Predictability on Translation']","DATA: Since manual translation of text from Ara-bic to English is a costly exercise, we chose, for our experiments, an existing Arabic social media dataset that has already been translated – the BBN Arabic-Dialect/English Parallel Text (Zbib et al., 2012). [Cite_Footnote_2]",2 https://catalog.ldc.upenn.edu/LDC2012T09,"DATA: Since manual translation of text from Ara-bic to English is a costly exercise, we chose, for our experiments, an existing Arabic social media dataset that has already been translated – the BBN Arabic-Dialect/English Parallel Text (Zbib et al., 2012). [Cite_Footnote_2] It contains about 3.5 million tokens of Arabic dialect sentences and their English translations. We use a randomly chosen subset of 1200 Levantine dialectal sentences, which we will refer to as the BBN posts or BBN dataset, in our experiments. Additionally, we also conduct experiments on a dataset of 2000 tweets originating from Syria (a country where Levantine dialectal Arabic is commonly spoken). These tweets were collected in May 2014 by polling the Twitter API. We will refer to this dataset as the Syrian tweets or Syrian dataset. Note, however, that manual trans-lations of the Syrian dataset are not available.",Material,Dataset,True,Use（引用目的）,True,N15-1078_1_0,2015,Sentiment after Translation: A Case-Study on Arabic Social Media Posts,Footnote
2851,12861," http://www.crowdflower.com"," ['5 Creating sentiment labeled data in Arabic and English']",[Cite_Footnote_3] for three BBN datasets and two Syrian datasets:,3 http://www.crowdflower.com,Manual sentiment annotations were performed on the crowdsourcing platform CrowdFlower [Cite_Footnote_3] for three BBN datasets and two Syrian datasets:,補足資料,Website,True,Use（引用目的）,True,N15-1078_2_0,2015,Sentiment after Translation: A Case-Study on Arabic Social Media Posts,Footnote
2852,12862," http://www.purl.com/net/lexicons"," ['6 English Sentiment Analysis']","The sentiment lex-icon features are derived from existing, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Moham-mad and Turney, 2013), Bing Liu’s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically gen-erated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014). [Cite_Footnote_4]",4 http://www.purl.com/net/lexicons,"A linear-kernel Support Vector Machine (Chang and Lin, 2011) classifier is trained on the avail-able training data. The classifier leverages a vari-ety of surface-form, semantic, and sentiment lexi-con features described below. The sentiment lex-icon features are derived from existing, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Moham-mad and Turney, 2013), Bing Liu’s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically gen-erated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014). [Cite_Footnote_4]",Material,Knowledge,True,Use（引用目的）,True,N15-1078_3_0,2015,Sentiment after Translation: A Case-Study on Arabic Social Media Posts,Footnote
2853,12863," http://www.purl.com/net/ArabicSentiment"," ['9 Conclusions']","The resources created as part of this project (Arabic sentiment lex-icons, Arabic sentiment annotations of social me-dia posts, and English sentiment annotations of their translations) are made freely available. [Cite_Footnote_5]",5 http://www.purl.com/net/ArabicSentiment,"We presented a set of experiments to systemati-cally study the impact of English translation (man-ual and automatic) on sentiment analysis of Arabic social media posts. Our experiments show that au-tomatic sentiment analysis of English translations (even of automatic translations) can lead to com-petitive results—results that are similar to that ob-tained by current state-of-the-art Arabic sentiment analysis systems. Our results also show that auto-matic sentiment analysis of automatic translations outperforms the manual sentiment annotations of the automatically translated text. This suggests that SMT errors impact human perception of sentiment markedly more than automatic sentiment systems. This is an interesting avenue for future exploration. We also show that translated texts tend to lose some of the sentiment information and there is a relatively higher percentage of neutral instances in the trans-lated text than in the original dataset. The resources created as part of this project (Arabic sentiment lex-icons, Arabic sentiment annotations of social me-dia posts, and English sentiment annotations of their translations) are made freely available. [Cite_Footnote_5]",Mixed,Mixed,True,Produce（引用目的）,True,N15-1078_4_0,2015,Sentiment after Translation: A Case-Study on Arabic Social Media Posts,Footnote
2854,12864," https://pilehvar.github.io/wic/"," ['References']",WiC is released in [Cite] https://pilehvar.github.io/wic/.,,"By design, word embeddings are unable to model the dynamic nature of words’ seman-tics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized mean-ing representation techniques such as sense or contextualized embeddings have been pro-posed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contex-tual Word Similarity, and highlight its short-comings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on anno-tations curated by experts, for generic evalua-tion of context-sensitive representations. WiC is released in [Cite] https://pilehvar.github.io/wic/.",Material,Dataset,True,Produce（引用目的）,True,N19-1128_0_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Body
2855,12865," https://www.wiktionary.org/"," ['2 WiC: the Word-in-Context dataset', '2.1 Construction']","Contextual sentences in WiC were extracted from example usages provided for words in three lexi-cal resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) Verb-Net (Kipper-Schuler, 2005), the largest domain-independent verb-based resource; and (3) Wik-tionary [Cite_Footnote_3] , a large collaborative-constructed online dictionary.",3 https://www.wiktionary.org/,"Contextual sentences in WiC were extracted from example usages provided for words in three lexi-cal resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) Verb-Net (Kipper-Schuler, 2005), the largest domain-independent verb-based resource; and (3) Wik-tionary [Cite_Footnote_3] , a large collaborative-constructed online dictionary. We used WordNet as our core re-source, exploiting BabelNet’s mappings (Navigli and Ponzetto, 2012) as a bridge between Wik-tionary and VerbNet to WordNet. Lexicographer examples constitute a reliable base for the con-struction of the dataset, as they are curated in a way to be clearly distinguishable across different senses of a word.",Material,DataSource,True,Extend（引用目的）,True,N19-1128_1_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Footnote
2856,12866," https://github.com/orenmel/context2vec"," ['3 Experiments']",We used the 600-d UkWac pre-trained models [Cite_Footnote_7] .,7 https://github.com/orenmel/context2vec,"Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models [Cite_Footnote_7] . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, 12 layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). 9 Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT’s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords’ embeddings.",Material,Knowledge,True,Use（引用目的）,True,N19-1128_2_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Footnote
2857,12867," https://www.tensorflow.org/hub/modules/google/elmo/1"," ['3 Experiments']","We used the 1024-d pre-trained models [Cite_Footnote_8] for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM.",8 https://www.tensorflow.org/hub/modules/google/elmo/1,"Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models [Cite_Footnote_8] for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, 12 layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). 9 Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT’s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords’ embeddings.",Method,Code,False,Use（引用目的）,True,N19-1128_3_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Footnote
2858,12868," https://github.com/google-research/bert/blob/master/"," ['3 Experiments']","We experimented with two pre-trained BERT models: base (768 dimensions, layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). [Cite_Footnote_9]",9 https://github.com/google-research/bert/blob/master/,"Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models 7 . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models 8 for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). [Cite_Footnote_9] Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT’s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords’ embeddings.",Method,Code,True,Use（引用目的）,True,N19-1128_4_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Footnote
2859,12869," https://github.com/uhh-lt/sensegram"," ['3 Experiments']","JBT [Cite_Footnote_11] (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embed-dings and computes embedding for each cluster (sense).",11 https://github.com/uhh-lt/sensegram,"Multi-prototype embeddings. We experiment with three recent techniques that release 300-d pre-trained multi-prototype embeddings . JBT [Cite_Footnote_11] (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embed-dings and computes embedding for each cluster (sense). DeConf 12 (Pilehvar and Collier, 2016) exploits the knowledge encoded in WordNet. For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelev-ina et al. (2016): for each example we retrieve the closest sense embedding to the context vec-tor, which is computed by averaging its contained words’ embeddings.",Method,Tool,True,Introduce（引用目的）,True,N19-1128_5_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Footnote
2860,12870," https://pilehvar.github.io/deconf/"," ['3 Experiments']","We experimented with two pre-trained BERT models: base (768 dimensions, [Cite_Footnote_12] layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters).",12 https://pilehvar.github.io/deconf/,"Contextualized word embeddings. One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models 7 . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context. ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models 8 for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019). The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, [Cite_Footnote_12] layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT’s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords’ embeddings.",Method,Code,True,Use（引用目的）,True,N19-1128_6_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Footnote
2861,12871," http://lcl.uniroma1.it/sw2v"," ['3 Experiments']","SW2V [Cite_Footnote_13] (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result.",13 http://lcl.uniroma1.it/sw2v,"Multi-prototype embeddings. We experiment with three recent techniques that release 300-d pre-trained multi-prototype embeddings . JBT (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embed-dings and computes embedding for each cluster (sense). DeConf 12 (Pilehvar and Collier, 2016) exploits the knowledge encoded in WordNet. For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V [Cite_Footnote_13] (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelev-ina et al. (2016): for each example we retrieve the closest sense embedding to the context vec-tor, which is computed by averaging its contained words’ embeddings.",Material,Knowledge,True,Introduce（引用目的）,True,N19-1128_7_0,2019,WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations,Footnote
2862,12872," https://github.com/facebookresearch/KILT"," ['References']",KILT data and code are available at [Cite] https://github.com/facebookresearch/KILT.,,"Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, exter-nal knowledge sources. While some models do well on individual tasks, developing gen-eral models is difficult as each task might re-quire computationally expensive indexing of custom knowledge sources, in addition to ded-icated infrastructure. To catalyze research on models that condition on specific informa-tion in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reduc-ing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architec-tures. We test both task-specific and gen-eral baselines, evaluating downstream perfor-mance in addition to the ability of the mod-els to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, out-performing more tailor-made approaches for fact checking, open-domain question answer-ing and dialogue, and yielding competitive re-sults on entity linking and slot filling, by gen-erating disambiguated text. KILT data and code are available at [Cite] https://github.com/facebookresearch/KILT. 1",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.200_0_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Body
2863,12873," https://huggingface.co/datasets?search=kilt"," ['References']",KILT data and code are available at https://github.com/facebookresearch/KILT. [Cite_Footnote_1],1 and at https://huggingface.co/datasets? search=kilt,"Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, exter-nal knowledge sources. While some models do well on individual tasks, developing gen-eral models is difficult as each task might re-quire computationally expensive indexing of custom knowledge sources, in addition to ded-icated infrastructure. To catalyze research on models that condition on specific informa-tion in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reduc-ing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architec-tures. We test both task-specific and gen-eral baselines, evaluating downstream perfor-mance in addition to the ability of the mod-els to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, out-performing more tailor-made approaches for fact checking, open-domain question answer-ing and dialogue, and yielding competitive re-sults on entity linking and slot filling, by gen-erating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-main.200_1_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2864,12874," https://evalai.cloudcv.org/web/challenges/challenge-page/689"," ['3 Tasks']","These test sets are not publicly released, but are used for the KILT challenge on EvalAI (Yadav et al., 2019) where participants can upload their models’ predictions and be listed on the public leaderboard. [Cite_Footnote_5]",5 available at https://evalai.cloudcv.org/web/challenges/challenge-page/689.,"We consider five tasks that use Wikipedia as a knowledge source for KILT: fact checking, open domain question answering, slot filling, entity link-ing, and dialogue. The diversity of these tasks challenge models to represent knowledge flexibly. Some tasks require a discrete prediction (e.g., an entity), others, such as extractive question answer-ing, can copy the output directly from a Wikipedia page, while still other tasks must synthesize mul-tiple pieces of knowledge in an abstractive way to produce an output. KILT also provides a variety of ways to seek knowledge, from a claim to verify to a text chunk to annotate, from a structured or natural question to a conversation (see Table 1 for details). We are able to include the test set for all datasets in KILT, either because the test set is public, or because we were able to obtain the test set from the authors of the original dataset. These test sets are not publicly released, but are used for the KILT challenge on EvalAI (Yadav et al., 2019) where participants can upload their models’ predictions and be listed on the public leaderboard. [Cite_Footnote_5]",Material,Dataset,True,Introduce（引用目的）,True,2021.naacl-main.200_3_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2865,12875," http://lemurproject.org/clueweb12"," ['3 Tasks', '3.2 Entity Linking']","WNED-CWEB (Guo and Barbosa, 2018) is a dataset created with the same strategy as WNED-WIKI, but sampling from the ClueWeb 2012 cor-pora annotated with the FACC1 system. [Cite_Footnote_7]",7 http://lemurproject.org/clueweb12,"WNED-CWEB (Guo and Barbosa, 2018) is a dataset created with the same strategy as WNED-WIKI, but sampling from the ClueWeb 2012 cor-pora annotated with the FACC1 system. [Cite_Footnote_7] Similarly, we randomly split into dev and test.",Material,Dataset,True,Introduce（引用目的）,True,2021.naacl-main.200_4_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2866,12876," https://yjernite.github.io/lfqa.html"," ['3 Tasks', '3.4 Open Domain Question Answering']","ELI5 (Fan et al., 2019b) [Cite_Footnote_8] is a collection of question-answer-evidence triples where the ques-tions are complex, and the answers are long, ex-planatory, and free-form.",8 https://yjernite.github.io/lfqa.html,"ELI5 (Fan et al., 2019b) [Cite_Footnote_8] is a collection of question-answer-evidence triples where the ques-tions are complex, and the answers are long, ex-planatory, and free-form. For dev and test, we col-lect annotations using Amazon Mechanical Turk, asking evaluators to select which supporting docu-ments from Wikipedia can be used to answer the question. We treat these as gold provenance anno-tations for evaluation (details in section 4).",Material,Dataset,True,Introduce（引用目的）,True,2021.naacl-main.200_5_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2867,12877," https://www.wikidata.org"," ['8 Discussion']","For instance, subject entities can be used for lookups by title in Wikipedia to retrieve knowledge (this heuristic will always work for zsRE), and structured human-curated resources (such as Wikidata [Cite_Footnote_13] ) could be used to get all an-swers right.",13 https://www.wikidata.org,"There are custom solutions that can easily simplify the slot filling task. For instance, subject entities can be used for lookups by title in Wikipedia to retrieve knowledge (this heuristic will always work for zsRE), and structured human-curated resources (such as Wikidata [Cite_Footnote_13] ) could be used to get all an-swers right. Nevertheless, we are interested in test-ing if a general model can extract attributes about specific entities from a large body of text.",Material,Dataset,True,Introduce（引用目的）,True,2021.naacl-main.200_6_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2868,12878," https://ai.google.com/research/"," ['11 Acknowledgment']","The authors would like to greatly thank the team be-hind Natural Questions [Cite_Footnote_14] for the held out data, that defines our NQ test set; FEVER 15 , HotpotQA 16 and TriviaQA teams for sharing official test data for the KILT leaderboard; Luke Zettlemoyer and Scott Wen-tau Yih for helpful discussions; Rishabh Jain for the help in setting up the EvalAI challenge.",14 https://ai.google.com/research/,"The authors would like to greatly thank the team be-hind Natural Questions [Cite_Footnote_14] for the held out data, that defines our NQ test set; FEVER 15 , HotpotQA 16 and TriviaQA teams for sharing official test data for the KILT leaderboard; Luke Zettlemoyer and Scott Wen-tau Yih for helpful discussions; Rishabh Jain for the help in setting up the EvalAI challenge. A Appendix Wikipedia Representation We represent the KILT knowledge source as a collection of JSON records, one per Wikipedia page. Each record is assigned: (i) a unique Wikipedia id; (ii) a unique Wikipedia title; (iii) a text field containing a list of strings, one for each paragraph, bulleted list item, and section header (for which we preserve the hier-archical structure); (iv) a list of anchors elements, one for each hyperlink in the original page text, with span reference in the text field and page linked; (v) a list of categories; (vi) a url redirecting to the original html for the page, with timestamp of the last page revision before the considered snapshot. Datasets Mapping Details In FEVER, often multiple pieces of knowledge must be combined to produce an output. For example, 30% of claims have more than one equally-valid provenance and 16% require the combination of multiple evidence spans. The second iteration (FEVER2.0, Thorne et al., 2019) introduces a collection of adversar-ial instances. For KILT, we merge the two ver-sions of FEVER into a single resource and con-sider only supported refuted claims. We exclude all claims classified as not having enough information since these instances have no evidence to assess the claim and cannot be mapped to the KILT knowl-edge source. Therefore we cannot asses whether such label is still appropriated given our snapshot. Moreover, we design KILT as an in-KB resource where each instance can be answered and corrobo-rated by information in the knowledge source.",補足資料,Website,True,Other（引用目的）,True,2021.naacl-main.200_7_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2869,12879," https://nlp.cs.washington.edu/triviaqa"," ['11 Acknowledgment']","The authors would like to greatly thank the team be-hind Natural Questions for the held out data, that defines our NQ test set; FEVER 15 , HotpotQA 16 and TriviaQA [Cite_Footnote_17] teams for sharing official test data for the KILT leaderboard; Luke Zettlemoyer and Scott Wen-tau Yih for helpful discussions; Rishabh Jain for the help in setting up the EvalAI challenge.",17 https://nlp.cs.washington.edu/ triviaqa,"The authors would like to greatly thank the team be-hind Natural Questions for the held out data, that defines our NQ test set; FEVER 15 , HotpotQA 16 and TriviaQA [Cite_Footnote_17] teams for sharing official test data for the KILT leaderboard; Luke Zettlemoyer and Scott Wen-tau Yih for helpful discussions; Rishabh Jain for the help in setting up the EvalAI challenge. A Appendix Wikipedia Representation We represent the KILT knowledge source as a collection of JSON records, one per Wikipedia page. Each record is assigned: (i) a unique Wikipedia id; (ii) a unique Wikipedia title; (iii) a text field containing a list of strings, one for each paragraph, bulleted list item, and section header (for which we preserve the hier-archical structure); (iv) a list of anchors elements, one for each hyperlink in the original page text, with span reference in the text field and page linked; (v) a list of categories; (vi) a url redirecting to the original html for the page, with timestamp of the last page revision before the considered snapshot. Datasets Mapping Details In FEVER, often multiple pieces of knowledge must be combined to produce an output. For example, 30% of claims have more than one equally-valid provenance and 16% require the combination of multiple evidence spans. The second iteration (FEVER2.0, Thorne et al., 2019) introduces a collection of adversar-ial instances. For KILT, we merge the two ver-sions of FEVER into a single resource and con-sider only supported refuted claims. We exclude all claims classified as not having enough information since these instances have no evidence to assess the claim and cannot be mapped to the KILT knowl-edge source. Therefore we cannot asses whether such label is still appropriated given our snapshot. Moreover, we design KILT as an in-KB resource where each instance can be answered and corrobo-rated by information in the knowledge source.",補足資料,Website,True,Other（引用目的）,True,2021.naacl-main.200_10_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2870,12880," https://hotpotqa.github.io"," ['7:453–466.']","[Cite_Footnote_19] , that is quite far from what achieved by our general solutions.",19 https://hotpotqa.github.io,"Performance Impact Of The Mapping Strategy We want to assess if the performance we obtain after mapping each dataset to a unified Wikipedia snapshot are in line with what reported in previous work. Thorne and Vlachos (2020) report a 2-way accuracy of 79.09 for the FEVER dev set when considering purely claims in input to a RoBERTa-based classifier (Liu et al., 2019). Our dev set in-cludes also the adversarial examples of FEVER 2.0, nevertheless the performance of BART are in line (80.67 dev, 78.93 test). Karpukhin et al. (2020) re-port 41.5 for EM on the open domain version of the NQ dev set . With our setting, DPR achieves an on-par performance on the dev set, with a 42.58 EM (50.43 F1-score). Results on our brand new NQ test set are 3/4 points lower for EM and F1-score than dev results. We don’t evaluate multi-hop specific baselines on KILT but the current best F1-score for HotpotQA is 75.43 according to the official lead-earboard [Cite_Footnote_19] , that is quite far from what achieved by our general solutions. BLINK results are in line with what reported in the GitHub repository for all three entity linking datasets. The Tranformer MemNet of Dinan et al. (2019) achieves a F1-score of 14.3 on the original version of the WW dataset while 11.5 in our setting, probably because in KILT we consider an harder open-domain setting.",補足資料,Document,False,Compare（引用目的）,False,2021.naacl-main.200_11_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2871,12881," https://github.com/facebookresearch/BLINK"," ['7:453–466.']",BLINK results are in line with what reported in the GitHub repository [Cite_Footnote_20] for all three entity linking datasets.,20 https://github.com/facebookresearch/ BLINK,"Performance Impact Of The Mapping Strategy We want to assess if the performance we obtain after mapping each dataset to a unified Wikipedia snapshot are in line with what reported in previous work. Thorne and Vlachos (2020) report a 2-way accuracy of 79.09 for the FEVER dev set when considering purely claims in input to a RoBERTa-based classifier (Liu et al., 2019). Our dev set in-cludes also the adversarial examples of FEVER 2.0, nevertheless the performance of BART are in line (80.67 dev, 78.93 test). Karpukhin et al. (2020) re-port 41.5 for EM on the open domain version of the NQ dev set . With our setting, DPR achieves an on-par performance on the dev set, with a 42.58 EM (50.43 F1-score). Results on our brand new NQ test set are 3/4 points lower for EM and F1-score than dev results. We don’t evaluate multi-hop specific baselines on KILT but the current best F1-score for HotpotQA is 75.43 according to the official lead-earboard , that is quite far from what achieved by our general solutions. BLINK results are in line with what reported in the GitHub repository [Cite_Footnote_20] for all three entity linking datasets. The Tranformer MemNet of Dinan et al. (2019) achieves a F1-score of 14.3 on the original version of the WW dataset while 11.5 in our setting, probably because in KILT we consider an harder open-domain setting.",補足資料,Document,True,Compare（引用目的）,True,2021.naacl-main.200_12_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2872,12882," https://github.com/easonnie/combine-FEVER-NSMN"," ['7:453–466.']","We use the public model [Cite_Footnote_22] pre-trained on FEVER, and con-sider not enough information predictions as false.",22 available at https://github.com/easonnie/ combine-FEVER-NSMN,"For fact checking, we consider NSMN (Nie et al., 2019), the highest scoring system from the FEVER shared task (Thorne et al., 2018b). We use the public model [Cite_Footnote_22] pre-trained on FEVER, and con-sider not enough information predictions as false. Moreover, we develop a fact checking baseline that combines a BERT-base classifier with passages re-turned from DPR where the claim and retrieved passage are input. The classifier is trained to label the claim-passage pair as supported or refuted with an additional neutral class for negative-sampled unrelated passages. Unrelated passages are sam-pled from two sources: (1) DPR-retrieved passages from pages that are not in the list of pages in the instance’s provenance and (2) passages sampled uniformly at random from pages in the instance’s provenance. At inference, we classify the first sen-tence of the Wikipedia pages retrieved by the top- 100 DPR passages against the claim. Using pages labelled as supported or refuted, we label the claim through majority voting. For claim provenance, we re-rank passages by probability according to this label.",Method,Tool,True,Use（引用目的）,True,2021.naacl-main.200_13_0,2021,KILT: a Benchmark for Knowledge Intensive Language Tasks,Footnote
2873,12883," http://kaldi.sf.net"," ['4 Experiments', '4.1 Baseline Systems']","First, we build an HMM-GMM system using the Kaldi open-source toolkit [Cite_Footnote_2] (Povey et al., 2011).",2 http://kaldi.sf.net,"First, we build an HMM-GMM system using the Kaldi open-source toolkit [Cite_Footnote_2] (Povey et al., 2011). The baseline recognizer has 8,986 sub-phone states and 200K Gaussians trained using maximum likelihood. Input features are speaker-adapted MFCCs. Overall, the baseline GMM system setup largely follows the existing s5b Kaldi recipe, and we defer to previous work for details (Vesely et al., 2013).",Method,Tool,True,Use（引用目的）,True,N15-1038_0_0,2015,Lexicon-Free Conversational Speech Recognition with Neural Networks,Footnote
2874,12884," https://github.com/Lucien-qiang/Rhetoric-Generator"," ['4 Experiments', '4.1 Datasets and Setups']",We conduct all experiments on two datasets [Cite_Footnote_1] .,1 https://github.com/Lucien-qiang/Rhetoric-Generator,"We conduct all experiments on two datasets [Cite_Footnote_1] . One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training. To this end, we built a clas-sifier to predict the rhetoric label automatically. We sampled about 15,000 sentences from the orig-inal poetry dataset and annotated the data manu-ally with three categories, i.e., metaphor, personi-fication, and other. This dataset was divided into a training set, validation set, and test set. Three clas-sifiers, including LSTM, Bi-LSTM, and Bi-LSTM with a self-attention model, were trained on this dataset. The Bi-LSTM with self-attention classi-fier (Yang et al., 2016) outperforms the other mod-els and achieves the best accuracy of 0.83 on the test set. In this classifier, the sizes of word embed-ding, hidden state and the attention size are set to 128, 256, 30 respectively, and a two-layer LSTM is used. The results for different classes are given in Table 2.",Material,Dataset,True,Use（引用目的）,True,P19-1192_0_0,2019,Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation,Footnote
2875,12885," http://www.shigeku.com/"," ['4 Experiments', '4.1 Datasets and Setups']","We collected the modern Chinese poetry dataset from an online poetry website [Cite_Footnote_2] and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites.",2 http://www.shigeku.com/,"We conduct all experiments on two datasets . One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website [Cite_Footnote_2] and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training. To this end, we built a clas-sifier to predict the rhetoric label automatically. We sampled about 15,000 sentences from the orig-inal poetry dataset and annotated the data manu-ally with three categories, i.e., metaphor, personi-fication, and other. This dataset was divided into a training set, validation set, and test set. Three clas-sifiers, including LSTM, Bi-LSTM, and Bi-LSTM with a self-attention model, were trained on this dataset. The Bi-LSTM with self-attention classi-fier (Yang et al., 2016) outperforms the other mod-els and achieves the best accuracy of 0.83 on the test set. In this classifier, the sizes of word embed-ding, hidden state and the attention size are set to 128, 256, 30 respectively, and a two-layer LSTM is used. The results for different classes are given in Table 2.",Material,DataSource,True,Extend（引用目的）,True,P19-1192_1_0,2019,Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation,Footnote
2876,12886," https://www.aclweb.org/anthology/"," ['1 Introduction']","The Association of Computational Linguistics (ACL) is one of the largest publishers of articles in natural language processing research: it maintains the open-access ACL Anthology [Cite_Footnote_1] of articles that date back to the 1960s, offering a rich resource for studying NLP publications.",1 https://www.aclweb.org/anthology/,"TACL of 750 Number 500 250 0 of rapid growth (see Figure 1). The Association of Computational Linguistics (ACL) is one of the largest publishers of articles in natural language processing research: it maintains the open-access ACL Anthology [Cite_Footnote_1] of articles that date back to the 1960s, offering a rich resource for studying NLP publications. While the aforementioned analyses have mainly focused on incoming citations, our work targets outgoing citations. We focus on the age of citations in the References section of articles published at ACL venues between 2010 and 2019 (Sec. 2), with a view to studying three questions:",補足資料,Website,True,Introduce（引用目的）,True,2020.acl-main.699_0_0,2020,On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology,Footnote
2877,12887," https://gitlab.freedesktop.org/poppler/poppler"," ['2 Data']","Extracting citations To extract a list of refer-ences from an article, we first extract the text stream from the PDF file via pdftotext, [Cite_Footnote_2] then feed it into ParsCit (Councill et al., 2008) to obtain the references.",2 https://gitlab.freedesktop.org/poppler/poppler,"Anthology statistics Figure 1 shows the distri-bution of the articles in the corpus: the number of articles published in these venues steadily in-creases from 2010–2019. The CL and TACL jour-nals publish articles at a steady rate; the ACL con-ference fluctuates in size, depending on whether it is co-located with NAACL; and the EACL confer-ence nearly doubles in size each time it takes place. In terms of whether the field is rapidly growing, we note that there was a year-on-year increase of 42% between in 2017–2018 due to the increase in the number of papers published at NAACL and EMNLP, and a 34% increase between 2018–2019. Extracting citations To extract a list of refer-ences from an article, we first extract the text stream from the PDF file via pdftotext, [Cite_Footnote_2] then feed it into ParsCit (Councill et al., 2008) to obtain the references. For each reference in this list, we then extract and keep the parsed “date”, “author”, and “title” entries. For 1.4% of the input files, this pipeline fails to extract any references; spot-checking reveals that many of those are not regular papers (but, e.g., book reviews or front matter), some PDFs have no embedded text, and others silently fail to parse.",Method,Tool,True,Use（引用目的）,True,2020.acl-main.699_1_0,2020,On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology,Footnote
2878,12888," https://github.com/coastalcph/acl-citations"," ['3 Analysis', '3.1 Are more recently published papers citing more recently published papers?']","The mean age of a cited paper has steadily decreased since 2013, from 7.69 years to 5.53 years in 2019; the median has dropped from [Cite_Footnote_6] to 3 years in the same period.",6 Datasets and code are available at: https://github.com/coastalcph/acl-citations,"Figure 2 shows the distribution of the age of cited articles with respect to the year in which the source article was published; Table 1 gives some comple-mentary statistics. The mean age of a cited paper has steadily decreased since 2013, from 7.69 years to 5.53 years in 2019; the median has dropped from [Cite_Footnote_6] to 3 years in the same period.",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-main.699_2_0,2020,On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology,Footnote
2879,12889," https://github.com/un33k/python-slugify"," ['A Fuzzy paper matching']","Figure [Cite_Footnote_10] shows the average number of citations per paper, analogous to Figure 7, but for a larger number of citation ages.",10 We achieve this by using https://github.com/un33k/python-slugify.,"Figure [Cite_Footnote_10] shows the average number of citations per paper, analogous to Figure 7, but for a larger number of citation ages. citations of 40 Number citations of 40 Number",Method,Tool,True,Use（引用目的）,True,2020.acl-main.699_3_0,2020,On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology,Footnote
2880,12890," https://github.com/seatgeek/fuzzywuzzy"," ['A Fuzzy paper matching']",tance ratio [Cite_Footnote_12] is ≤ 95%.,12 As implemented by https://github.com/seatgeek/fuzzywuzzy.,"Figure 7: Average number of citations per paper that are 15 years or older, by venue of publication. tance ratio [Cite_Footnote_12] is ≤ 95%.",Method,Code,False,Introduce（引用目的）,False,2020.acl-main.699_4_0,2020,On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology,Footnote
2881,12891," http://qwone.com/~jason/20Newsgroups/"," ['3 High Water Mark for Tandem Anchors', '3.1 Experimental Setup']","We use the well-known 20 Newsgroups dataset (20 NEWS ) used in previous interactive topic mod-eling work: 18,846 Usenet postings from 20 dif-ferent newgroups in the early 1990s. [Cite_Footnote_1]",1 http://qwone.com/˜jason/20Newsgroups/,"We use the well-known 20 Newsgroups dataset (20 NEWS ) used in previous interactive topic mod-eling work: 18,846 Usenet postings from 20 dif-ferent newgroups in the early 1990s. [Cite_Footnote_1] We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes. We then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.",Material,Dataset,True,Use（引用目的）,True,P17-1083_0_0,2017,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,Footnote
2882,12892," http://hunch.net/~vw/"," ['3 High Water Mark for Tandem Anchors', '3.2 Experimental Results']",We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit [Cite_Footnote_4] with topic-word pairs as features.,4 http://hunch.net/˜vw/,"Our first evaluation is a classification task to pre-dict documents’ newsgroup membership. Thus, we do not aim for state-of-the-art accuracy, but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than Gram-Schmidt anchors. After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tan-dem anchors and the Gram-Schmidt single word anchors. For multiword anchors, we use each of the combiner functions from Section 2.2. The anchor algorithm only gives the topic-word dis-tributions and not word-level topic assignments, so we infer token-level topic assignments using LDA Latent Dirichlet Allocation (Blei et al., 2003) with fixed topics discovered by the anchor method. We use our own implementation of Gibbs sam-pling with fixed topics and a symmetric document-topic Dirichlet prior with concentration α = .01. Since the topics are fixed, this inference is very fast and can be parallelized on a per-document ba-sis. We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit [Cite_Footnote_4] with topic-word pairs as features. Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features. For both training and test, we exclude words outside the LDA vocabulary.",Method,Tool,True,Use（引用目的）,True,P17-1083_1_0,2017,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,Footnote
2883,12893," http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download"," ['6 Experiments', '6.1 Development Tests']",We use the C&C supertagger [Cite_Footnote_1] to assign a set of probable lexical categories to each input word using the gold-standard order.,1 http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download.,"In practice, it is often unnecessary to leave lexi-cal category disambiguation completely to the gram-maticality improvement system. When it is reason-able to assume that the input sentence for the gram-maticality improvement system is sufficiently fluent, a list of candidate lexical categories can be assigned automatically to each word via supertagging (Clark and Curran, 2007) on the input sequence. We use the C&C supertagger [Cite_Footnote_1] to assign a set of probable lexical categories to each input word using the gold-standard order. When the input is noisy, the accuracy of a supertagger tends to be lower than when the in-put is grammatical. One way to address this problem is to allow the supertagger to produce a larger list of possible supertags for each input word, and leave the ambiguity to the grammatical improvement sys-tem. We simulate the noisy input situation by using a small probability cutoff (β) value in the supertag-ger, and supertag correctly ordered input sentences before breaking them into bags of words. With a β value of 0.0001, there are 5.4 lexical categories for each input word in the development test (which is smaller than the dictionary case).",Method,Tool,True,Use（引用目的）,True,D11-1106_0_0,2011,Syntax-Based Grammaticality Improvement using CCG and Guided Search,Footnote
2884,12894," http://statmt.org/wmt16"," ['4 Experiments', '4.1 Neural Machine Translation']","Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [Cite_Footnote_2] .",2 http://statmt.org/wmt16,"Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [Cite_Footnote_2] . Further details of the datasets and the experimental setup are shown in Appendix A.2.",補足資料,Website,True,Introduce（引用目的）,True,2020.emnlp-main.735_0_0,2020,Improving Text Generation with Student-Forcing Optimal Transport,Footnote
2885,12895," http://www.statmt.org/wmt17/"," ['4 Experiments', '4.3 Neural Language Generation']","Following recent unconditional long text genera-tion work (Caccia et al., 2018), we perform exper-iments on EMNLP2017 WMT News dataset [Cite_Footnote_3] .",3 http://www.statmt.org/wmt17/,"Following recent unconditional long text genera-tion work (Caccia et al., 2018), we perform exper-iments on EMNLP2017 WMT News dataset [Cite_Footnote_3] . All sentences are longer than 20, making the dataset appropriate for testing the exposure bias problem.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-main.735_1_0,2020,Improving Text Generation with Student-Forcing Optimal Transport,Footnote
2886,12896," https://github.com/tensorflow/nmt"," ['4 Experiments', '4.1 Neural Machine Translation']","We compare SFOT with a variety of meth-ods: MLE (Luong et al., 2017) [Cite_Ref] , Scheudle Sam-pling (SS), TFOT and several RL-based models, i.e. RAML (Norouzi et al., 2016), SPG (Ding and Soricut, 2017), and MIXER (Ranzato et al., 2016b).","Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. 2017. Neural machine translation (seq2seq) tutorial. https://github.com/tensorflow/nmt.","We compare SFOT with a variety of meth-ods: MLE (Luong et al., 2017) [Cite_Ref] , Scheudle Sam-pling (SS), TFOT and several RL-based models, i.e. RAML (Norouzi et al., 2016), SPG (Ding and Soricut, 2017), and MIXER (Ranzato et al., 2016b). The results are summarized in Tables 1 and 2. The proposed SFOT approach consis-tently improves upon MLE training and outper-forms other models in all experimental setups. Besides the quantitative results, we observe that SFOT correctly maintains the information from the source side to make correct translations (Ta-ble 3). As can be seen from these examples, our model can better preserve information from the source and is less likely to transfer words incor-rectly. Notice that most errors in the baseline models occur in the latter part of sequences, due to error accumulation from exposure-bias, which SFOT addresses by matching the free-running out-puts to the ground-truth. In conjunction with the quantitative results presented above, these qualita-tive observations confirm that our model can gen-erate more reliable translation for long sentences and address the exposure-bias problem.",補足資料,Document,True,Use（引用目的）,True,2020.emnlp-main.735_2_0,2020,Improving Text Generation with Student-Forcing Optimal Transport,Reference
2887,12897," http://statmt.org/wmt16"," ['References']","For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [Cite_Footnote_5] , which contains 4.5M sentence pairs.",5 http://statmt.org/wmt16,"Dataset Two standard datasets are tested for NMT tasks. The first one is a small-scale English-Vietnamese corpus from the IWSLT 2015 Evalu-ation Campaign (Cettolo et al., 2015), which is a parallel corpus of TED-talks and contains 133K sentence pairs. We follow the pre-processing pro-cedure in (Luong and Manning, 2015) by replac-ing words with frequencies less than 5 with hunki. As a result, our vocabulary reduces to 17K for English and 7.7K for Vietnamese. We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [Cite_Footnote_5] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set. We conduct the sub-word tokenization on the corpus using the Byte Pair Encoding (BPE) method (Sen-nrich et al., 2015). Following Klein et al. (2017), we set the vocabulary size of both English and German to 32K.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-main.735_3_0,2020,Improving Text Generation with Student-Forcing Optimal Transport,Footnote
2888,12898," https://github.com/tensorflow/nmt"," ['References']","Setup We use Google’s Neural Machine Trans-lation (GNMT) system (Wu et al., 2016) as our baseline MLE model, which follows the standard architecture and hyper-parameters [Cite_Footnote_6] for fair com-parison.",6 https://github.com/tensorflow/nmt,"Setup We use Google’s Neural Machine Trans-lation (GNMT) system (Wu et al., 2016) as our baseline MLE model, which follows the standard architecture and hyper-parameters [Cite_Footnote_6] for fair com-parison. All other models are built on top of with same network structure. We evaluate the model performance using BLEU scores (Papineni et al., 2002). We set OT weighting parameter λ = 0.1 and order-preserving penalty weighting parameter β = 0.1.",Method,Code,True,Introduce（引用目的）,True,2020.emnlp-main.735_4_0,2020,Improving Text Generation with Student-Forcing Optimal Transport,Footnote
2889,12899," http://www.rulequest.com/see5-info.html"," ['6 Comparison with other Models']","Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, [Cite] http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin.",,"To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, [Cite] http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004. LIB-SVM: a Library for Support Vector Machines, http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features.",Method,Code,True,Compare（引用目的）,True,H05-1112_0_0,2005,A Semantic Scattering Model for the Automatic Interpretation of Genitives,Body
2890,12900," http://jbnc.sourceforge.net"," ['6 Comparison with other Models']","Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, [Cite] http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin.",,"To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, [Cite] http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004. LIB-SVM: a Library for Support Vector Machines, http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features.",Method,Code,True,Compare（引用目的）,True,H05-1112_1_0,2005,A Semantic Scattering Model for the Automatic Interpretation of Genitives,Body
2891,12901," http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf"," ['6 Comparison with other Models']","LIB-SVM: a Library for Support Vector Machines, [Cite] http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf).",,"To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004. LIB-SVM: a Library for Support Vector Machines, [Cite] http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features.",Method,Code,True,Compare（引用目的）,True,H05-1112_2_0,2005,A Semantic Scattering Model for the Automatic Interpretation of Genitives,Body
2892,12902," http://www.ukp.tu-darmstadt.de/fnwkde/"," ['References']",The created resource is publicly available at [Cite] http://www.ukp.tu-darmstadt.de/fnwkde/.,,"We present a new bilingual FrameNet lex-icon for English and German. It is cre-ated through a simple, but powerful ap-proach to construct a FrameNet in any language using Wiktionary as an inter-lingual representation. Our approach is based on a sense alignment of FrameNet and Wiktionary, and subsequent transla-tion disambiguation into the target lan-guage. We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexical-semantic resources. The created resource is publicly available at [Cite] http://www.ukp.tu-darmstadt.de/fnwkde/.",Mixed,Mixed,True,Produce（引用目的）,True,P13-1134_0_0,2013,FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection,Body
2893,12903," http://en.wiktionary.org/wiki/Wiktionary:Statistics"," ['2 Resource Overview']","The English language edition contains over 500,000 word senses. [Cite_Footnote_2]","2 as of May 2013, see http://en.wiktionary.org/wiki/Wiktionary:Statistics.","Wiktionary is a collaboratively created dictio-nary available in over 500 language editions. It is continuously extended and revised by a commu-nity of volunteer editors. The English language edition contains over 500,000 word senses. [Cite_Footnote_2]",補足資料,Document,True,Introduce（引用目的）,True,P13-1134_1_0,2013,FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection,Footnote
2894,12904," http://www.ukp.tu-darmstadt.de/fnwkde/"," ['9 Discussion: a Multilingual FrameNet']","For many of the top 30 languages in FNWKxx, the Wiktionary edi-tions seem sufficiently large to provide targets for translation disambiguation, [Cite_Footnote_4] and they are contin-uously extended.",4 see overview table at http://www.ukp.tu-darmstadt.de/fnwkde/.,"To create those FrameNet lexicons, the transla-tion disambiguation approach used for FNWKde (step 2 in Fig. 1) needs to be adapted to other lan-guages. The approach is in theory applicable to any language, but there are some obstacles: first, it relies on the availability of the target sense in the target language Wiktionary. For many of the top 30 languages in FNWKxx, the Wiktionary edi-tions seem sufficiently large to provide targets for translation disambiguation, [Cite_Footnote_4] and they are contin-uously extended. Second, our approach requires access to the target language Wiktionary, but the data format across Wiktionary language editions is not standardized. Third, the approach requires machine translation into the target language. For languages, where such a tool is not available, we could default to the first-sense-heuristic, or en-courage the Wiktionary community to link the translations to their target Wiktionary senses in-spired by Sajous et al. (2010).",補足資料,Document,True,Introduce（引用目的）,True,P13-1134_2_0,2013,FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection,Footnote
2895,12905," http://www.ukp.tu-darmstadt.de/fnwkde/"," ['10 Conclusion']","We make both resources publicly available in the standardized format UBY-LMF (Eckle-Kohler et al., 2012), which supports automatic processing of the resources via the UBY Java API, see [Cite] http://www.ukp.tu-darmstadt.de/fnwkde/.",,"We make both resources publicly available in the standardized format UBY-LMF (Eckle-Kohler et al., 2012), which supports automatic processing of the resources via the UBY Java API, see [Cite] http://www.ukp.tu-darmstadt.de/fnwkde/.",Method,Code,True,Introduce（引用目的）,True,P13-1134_3_0,2013,FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection,Body
2896,12907," http://www.tartarus.org/~martin/PorterStemmer"," ['4 Generation of Domain Model', '4.2 Taxonomy Generation']","This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v [Cite_Footnote_1] .",1 http://www.tartarus.org/˜martin/PorterStemmer,"The Clusterer generates individual levels of the taxonomy by using text clustering. We used CLUTO package for doing text clustering. We experimented with all the available clustering functions in CLUTO but no one clustering al-gorithm consistently outperformed others. Also, there was not much difference between various algorithms based on the available goodness met-rics. Hence, we used the default repeated bisec-tion technique with cosine function as the similar-ity metric. We ran this algorithm on a collection of 2000 transcriptions multiple times. First we generate 5 clusters from the 2000 transcriptions. Next we generate 10 clusters from the same set of transcriptions and so on. At the finest level we split them into 100 clusters. To generate the topic taxonomy, these sets containing 5 to 100 clusters are passed through the Taxonomy Builder compo-nent. This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v [Cite_Footnote_1] . Now v 1 and v 2 become nodes in adjacent layers in the taxonomy. Here we found the taxonomy to be a tree but in general it can be a DAG. Now onwards, each node in the taxonomy will be referred to as a topic.",Method,Tool,False,Introduce（引用目的）,False,P06-1093_0_0,2006,Automatic Generation of Domain Models for Call Centers from Noisy Transcriptions,Footnote
2897,12908," http://glaros.dtc.umn.edu/gkhome/views/cluto"," ['4 Generation of Domain Model', '4.2 Taxonomy Generation']",We used CLUTO package [Cite_Footnote_2] for doing text clustering.,2 http://glaros.dtc.umn.edu/gkhome/views/cluto,"The Clusterer generates individual levels of the taxonomy by using text clustering. We used CLUTO package [Cite_Footnote_2] for doing text clustering. We experimented with all the available clustering functions in CLUTO but no one clustering al-gorithm consistently outperformed others. Also, there was not much difference between various algorithms based on the available goodness met-rics. Hence, we used the default repeated bisec-tion technique with cosine function as the similar-ity metric. We ran this algorithm on a collection of 2000 transcriptions multiple times. First we generate 5 clusters from the 2000 transcriptions. Next we generate 10 clusters from the same set of transcriptions and so on. At the finest level we split them into 100 clusters. To generate the topic taxonomy, these sets containing 5 to 100 clusters are passed through the Taxonomy Builder compo-nent. This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v . Now v 1 and v 2 become nodes in adjacent layers in the taxonomy. Here we found the taxonomy to be a tree but in general it can be a DAG. Now onwards, each node in the taxonomy will be referred to as a topic.",Method,Tool,True,Use（引用目的）,True,P06-1093_1_0,2006,Automatic Generation of Domain Models for Call Centers from Noisy Transcriptions,Footnote
2898,12909," https://rajpurkar.github.io/SQuAD-explorer/"," ['1 Introduction']","Their immense ability in capturing various linguistic properties has led these state-of-the-art language models to master different NLP tasks, even surpassing human accuracy on some benchmarks such as SQuAD [Cite_Footnote_1] .",1 https://rajpurkar.github.io/SQuAD-explorer/,"High-performance deep neural language models such as BERT (Devlin et al., 2018), XLNet (Z. Yang et al., 2019), and GPT-2 (Radford et al., 2019) have brought breakthroughs to a wide range of Natural Language Processing (NLP) tasks including text classification, sentiment analysis, textual entailment, natural language inference, machine translation, and question answering. Their immense ability in capturing various linguistic properties has led these state-of-the-art language models to master different NLP tasks, even surpassing human accuracy on some benchmarks such as SQuAD [Cite_Footnote_1] .",Material,Dataset,True,Introduce（引用目的）,True,2021.emnlp-main.117_0_0,2021,Evaluating the Robustness of Neural Language Models to Input,Footnote
2899,12910," https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003"," ['2 NLP tasks']","CoNLL-2003 [Cite_Footnote_2] is a Named Entity Recognition (NER) dataset containing news stories from the Reuters corpus with more than 200K tokens annotated as Person, Organization, Location, Miscellaneous, or Other.",2 https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,"CoNLL-2003 [Cite_Footnote_2] is a Named Entity Recognition (NER) dataset containing news stories from the Reuters corpus with more than 200K tokens annotated as Person, Organization, Location, Miscellaneous, or Other.",Material,Dataset,True,Introduce（引用目的）,True,2021.emnlp-main.117_1_0,2021,Evaluating the Robustness of Neural Language Models to Input,Footnote
2900,12911," https://github.com/mmoradi-iut/NLP-perturbation"," ['4 Perturbation methods']",The source code is available at [Cite] https://github.com/mmoradi-iut/NLP-perturbation.,,We designed and implemented various character-level and word-level perturbation methods that simulate different types of noise an NLP system may encounter in real-world situations. The perturbations can be produced for every dataset regardless of the underlying language model or NLP system being tested. Table 2 presents an example for every perturbation method. The perturbation methods were implemented in Python using the NLTK library. The source code is available at [Cite] https://github.com/mmoradi-iut/NLP-perturbation.,Method,Code,True,Produce（引用目的）,True,2021.emnlp-main.117_2_0,2021,Evaluating the Robustness of Neural Language Models to Input,Body
2901,12912," https://github.com/deepset-ai/FARM"," ['3 Language models']","We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [Cite_Footnote_3] libraries to implement the transformer-based models.",3 https://github.com/deepset-ai/FARM,"We retrieved the pretrained models, fine-tuned them separately on each downstream task using the training and development sets, and tested them on the test sets. We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [Cite_Footnote_3] libraries to implement the transformer-based models. A complete list of hyperparameter values is presented in Appendix A.",Method,Code,True,Use（引用目的）,True,2021.emnlp-main.117_3_0,2021,Evaluating the Robustness of Neural Language Models to Input,Footnote
2902,12913," https://doi.org/10.1016/j.knosys.2019.105210"," ['1 Introduction']","Other studies focused on evaluating robustness to perturbed inputs for machine translation (Belinkov and Bisk, 2018; Niu et al., 2020), perturbation sensitivity analysis for detecting unintended model biases (Prabhakaran et al., 2019), or robustness to adversarial perturbations (Alshemali and Kalita, 2020 [Cite_Ref] ; Ebrahimi et al., 2018; Liang et al., 2018).","Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.","Applying automatic or human-controlled perturbations to textual inputs has been shown to be effective for evaluating the robustness of NLP systems, investigating their vulnerabilities, and finding their bugs. Recently, CheckList (Ribeiro et al., 2020) provided a framework for behavioral testing of NLP systems inspired by black-box testing in software engineering. CheckList enabled generating new (perturbed) test samples through abstracting different test types aimed at testing linguistic capabilities. Other studies focused on evaluating robustness to perturbed inputs for machine translation (Belinkov and Bisk, 2018; Niu et al., 2020), perturbation sensitivity analysis for detecting unintended model biases (Prabhakaran et al., 2019), or robustness to adversarial perturbations (Alshemali and Kalita, 2020 [Cite_Ref] ; Ebrahimi et al., 2018; Liang et al., 2018). However, a comprehensive methodology for evaluating the performance of NLP models under real-world conditions is still missing.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.117_5_0,2021,Evaluating the Robustness of Neural Language Models to Input,Reference
2903,12914," https://doi.org/10.1016/j.knosys.2019.105210"," ['6 Related work']","Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [Cite_Ref] ; Ren et al., 2019; Zhang et al., 2020).","Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.","Typical performance measures such as accuracy, precision, recall, etc. may not properly reflect how NLP systems behave in real-world use cases. This has motivated many studies to devise novel methods for investigating different capabilities and vulnerabilities of text processing systems. Behavioral testing introduces targeted changes to textual inputs to test linguistic capabilities of systems (Ribeiro et al., 2020). Explanations provide simplified representations of what a complex NLP model has learned (Moradi and Samwald, 2021a, b; Ribeiro et al., 2016). This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [Cite_Ref] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise. Moreover, most of previous work on adversarial perturbation to NLP models focused on misspelling attacks (Jones et al., 2020; Pruthi et al., 2019; L. Sun et al., 2020). The perturbation methods implemented in this paper represented a wide range of noises that an NLP system may face in real-world situations.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.117_5_1,2021,Evaluating the Robustness of Neural Language Models to Input,Reference
2904,12915," https://doi.org/10.1016/j.engappai.2020.103641"," ['5 Experimental results', '5.1 Performance on perturbed inputs']","Since it has been proven that sentences that contain few typos, misspellings, or minor character-level errors can be still fully understandable to humans (Belinkov and Bisk, 2018; Xu and Du, 2020 [Cite_Ref] ), character-level perturbations are not expected to change the text’s meaning in most cases.","Jincheng Xu, and Qingfeng Du. 2020. TextTricker: Loss-based and gradient-based adversarial attacks on text classification models. Engineering Applications of Artificial Intelligence, 92: 103641. https://doi.org/10.1016/j.engappai.2020.103641.","Since it has been proven that sentences that contain few typos, misspellings, or minor character-level errors can be still fully understandable to humans (Belinkov and Bisk, 2018; Xu and Du, 2020 [Cite_Ref] ), character-level perturbations are not expected to change the text’s meaning in most cases. Therefore, they can be automatically produced and used for testing the robustness of NLP systems.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.117_19_0,2021,Evaluating the Robustness of Neural Language Models to Input,Reference
2905,12916," https://doi.org/10.1145/3374217"," ['1 Introduction']","Some types of perturbation utilized in this work were already tested in previous work on adversarial attacks on NLP systems (Zeng et al., 2020; Zhang et al., 2020 [Cite_Ref] ).","Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.","Some types of perturbation utilized in this work were already tested in previous work on adversarial attacks on NLP systems (Zeng et al., 2020; Zhang et al., 2020 [Cite_Ref] ). However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016). In order to generate effective adversarial examples, most attack methods need to have access to the NLP model structure, internal weights, and hyperparameters, which may not be possible in every testing scenario (Zhang et al., 2020). Furthermore, adversarial perturbations should not be perceived by humans (Liang et al., 2018). This is a serious challenge, since even small changes to a text may be easily recognized by the user.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.117_20_0,2021,Evaluating the Robustness of Neural Language Models to Input,Reference
2906,12917," https://doi.org/10.1145/3374217"," ['1 Introduction']","In order to generate effective adversarial examples, most attack methods need to have access to the NLP model structure, internal weights, and hyperparameters, which may not be possible in every testing scenario (Zhang et al., 2020) [Cite_Ref] .","Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.","Some types of perturbation utilized in this work were already tested in previous work on adversarial attacks on NLP systems (Zeng et al., 2020; Zhang et al., 2020). However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016). In order to generate effective adversarial examples, most attack methods need to have access to the NLP model structure, internal weights, and hyperparameters, which may not be possible in every testing scenario (Zhang et al., 2020) [Cite_Ref] . Furthermore, adversarial perturbations should not be perceived by humans (Liang et al., 2018). This is a serious challenge, since even small changes to a text may be easily recognized by the user.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.117_20_1,2021,Evaluating the Robustness of Neural Language Models to Input,Reference
2907,12918," https://doi.org/10.1145/3374217"," ['4 Perturbation methods']","Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [Cite_Ref] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList.","Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.","Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [Cite_Ref] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList. Among the word-level perturbations, Deletion, Repetition, Singular/plural verbs, Word order, and Verb tense were not already used to test the robustness. However, Negation was included in CheckList, and Replacement with Synonyms was used for adversarial attack (Dong et al., 2020; Ren et al., 2019).",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.117_20_2,2021,Evaluating the Robustness of Neural Language Models to Input,Reference
2908,12919," https://doi.org/10.1145/3374217"," ['6 Related work']","Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [Cite_Ref] ).","Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.","Typical performance measures such as accuracy, precision, recall, etc. may not properly reflect how NLP systems behave in real-world use cases. This has motivated many studies to devise novel methods for investigating different capabilities and vulnerabilities of text processing systems. Behavioral testing introduces targeted changes to textual inputs to test linguistic capabilities of systems (Ribeiro et al., 2020). Explanations provide simplified representations of what a complex NLP model has learned (Moradi and Samwald, 2021a, b; Ribeiro et al., 2016). This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [Cite_Ref] ). However, adversarial samples resemble a very specific type of noise. Moreover, most of previous work on adversarial perturbation to NLP models focused on misspelling attacks (Jones et al., 2020; Pruthi et al., 2019; L. Sun et al., 2020). The perturbation methods implemented in this paper represented a wide range of noises that an NLP system may face in real-world situations.",補足資料,Paper,True,Introduce（引用目的）,True,2021.emnlp-main.117_20_3,2021,Evaluating the Robustness of Neural Language Models to Input,Reference
2909,12920," http://mat-annotation.sourceforge.net/"," ['2 Related Work']","MAT [Cite_Footnote_1] (Bayer, 2015) is designed for what they call the “tag-a-little, learn-a-little (TALLAL) loop” to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production.",1 http://mat-annotation.sourceforge.net/,"Neves and Ševa (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT [Cite_Footnote_1] (Bayer, 2015) is designed for what they call the “tag-a-little, learn-a-little (TALLAL) loop” to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.",Method,Tool,True,Introduce（引用目的）,False,2021.naacl-demos.12_0_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2910,12921," https://www.tagtog.net/"," ['2 Related Work']","The tools tagtog [Cite_Footnote_2] (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality.",2 https://www.tagtog.net/ 3 https://prodi.gy/,"Neves and Ševa (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) is designed for what they call the “tag-a-little, learn-a-little (TALLAL) loop” to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog [Cite_Footnote_2] (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.",Method,Tool,True,Compare（引用目的）,True,2021.naacl-demos.12_1_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2911,12922," https://prodi.gy/"," ['2 Related Work']","The tools tagtog [Cite_Footnote_2] (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality.",2 https://www.tagtog.net/ 3 https://prodi.gy/,"Neves and Ševa (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) is designed for what they call the “tag-a-little, learn-a-little (TALLAL) loop” to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog [Cite_Footnote_2] (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.",Method,Tool,True,Compare（引用目的）,True,2021.naacl-demos.12_2_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2912,12923," https://www.lighttag.io/"," ['2 Related Work']","The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag [Cite_Footnote_4] are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality.","4 https://www.lighttag.io/ Demonstrations, pages 99–105","Neves and Ševa (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) is designed for what they call the “tag-a-little, learn-a-little (TALLAL) loop” to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag [Cite_Footnote_4] are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.",Method,Tool,True,Compare（引用目的）,True,2021.naacl-demos.12_3_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2913,12924," https://github.com/doccano/doccano"," ['2 Related Work']","Another annotation tool with limited sup-port for document-level annotations is Doccano [Cite_Footnote_5] (Nakayama et al., 2018), though it is not mentioned in the evaluation of Neves and Ševa.",5 https://github.com/doccano/doccano,"Another annotation tool with limited sup-port for document-level annotations is Doccano [Cite_Footnote_5] (Nakayama et al., 2018), though it is not mentioned in the evaluation of Neves and Ševa. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks.",Method,Tool,True,Compare（引用目的）,True,2021.naacl-demos.12_4_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2914,12925," https://github.com/pemistahl/lingua"," ['4 Annotation Generator']",The first one automatically detects the language of the generator input using the language detection library Lingua [Cite_Footnote_6] .,6 https://github.com/pemistahl/lingua,"Currently, A CTIVE A NNO has three inbuilt an-notation generator implementations. The first one automatically detects the language of the generator input using the language detection library Lingua [Cite_Footnote_6] . This is an example of a statistical and rule-based annotation generator as compared to a machine learning-based generator. The second annotation generator allows calling an external machine learn-ing service through a URL for tag set annotation definitions, which will take the response and map it into the tag set options from the annotation defi-nition. This can be used when an already trained machine learning model exists. The model would have to be wrapped by an HTTP API to comply with the API definition of A CTIVE A NNO for this annotation generator. The API is structured to al-low for multiple documents to be predicted at once. The third annotation generator is similar to the sec-ond one, but also supports automatically updating the external machine learning model by sending an HTTP request with the training data in the body. To support this functionality, the concept of an up-datable annotation generator exists. This kind of generator extends the base annotation generator, but also requires its subclasses to implement an update method, where the training data will be aggregated and used to train or update the anno-tation generator. For this, updatable annotation generators also need to define a threshold when to start the training and when to update an exist-ing model. For example, the first model should be trained after 100 training samples are generated, and then it should be updated for every 25 new training samples. An updatable annotation gen-erator is versioned with a version number and an update state, to ensure the version is actually usable to generate new annotations.",補足資料,Website,True,Introduce（引用目的）,True,2021.naacl-demos.12_5_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2915,12926," http://mat-annotation.sourceforge.net/"," ['2 Related Work']","MAT (Bayer, 2015) [Cite_Ref] is designed for what they call the “tag-a-little, learn-a-little (TALLAL) loop” to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production.",Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.,"Neves and Ševa (2019) conducted an extensive re-view of 78 annotation tools in total, comparing 15 which met their minimum criteria. Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) [Cite_Ref] is designed for what they call the “tag-a-little, learn-a-little (TALLAL) loop” to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system. Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.",Method,Tool,True,Introduce（引用目的）,True,2021.naacl-demos.12_6_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Reference
2916,12927," http://mat-annotation.sourceforge.net/"," ['4 Annotation Generator', '4.2 Active Learning Process']","This is similar to the “tag-a-little, learn-a-little loop” from MAT (Bayer, 2015) [Cite_Ref] .",Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.,"This process can then be repeated until the ma-chine learning model is performing well enough to be partly or fully automated. This is similar to the “tag-a-little, learn-a-little loop” from MAT (Bayer, 2015) [Cite_Ref] . If combined with enabling pre-annotations, it is also very similar to the annotation process of RapTAT (Gobbel et al., 2014). To partly automate the process, the project has to be configured to treat the generator as an annotator and to require one annotator per document. Additionally, the configu-ration option of the finalize condition has to be set to some confidence threshold, for example, 80%. Then, only the documents with a confidence value below 80% will be required to be annotated further. To fully automate the process, the finalize condition should be set to always to accept the annotations automatically without additional conditions.",Method,Tool,True,Compare（引用目的）,True,2021.naacl-demos.12_6_1,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Reference
2917,12928," https://github.com/MaxMello/ActiveAnno"," ['5 Use Cases']",Please refer to the demo [Cite_Footnote_7] and video to see this use case in action.,"7 Source code, documentation and the link to the demo can be found here: https://github.com/MaxMello/ ActiveAnno","Finally, as both experiments were conducted on non-publicly available data, we created another example project based on the OffensEval 2019 shared task (Zampieri et al., 2019), specifically sub-task A. The task is directly integrated as an example project in A CTIVE A NNO , including a machine learning component that can be updated through A CTIVE A NNO . Please refer to the demo [Cite_Footnote_7] and video to see this use case in action.",Mixed,Mixed,True,Produce（引用目的）,True,2021.naacl-demos.12_7_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2918,12929," https://youtu.be/ryCi4XeReDg"," ['5 Use Cases']",Please refer to the demo and video [Cite_Footnote_8] to see this use case in action.,8 Demo Video: https://youtu.be/ryCi4XeReDg,"Finally, as both experiments were conducted on non-publicly available data, we created another example project based on the OffensEval 2019 shared task (Zampieri et al., 2019), specifically sub-task A. The task is directly integrated as an example project in A CTIVE A NNO , including a machine learning component that can be updated through A CTIVE A NNO . Please refer to the demo and video [Cite_Footnote_8] to see this use case in action.",補足資料,Media,True,Produce（引用目的）,True,2021.naacl-demos.12_8_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Footnote
2919,12930," https://github.com/doccano/doccano"," ['2 Related Work']","Another annotation tool with limited sup-port for document-level annotations is Doccano (Nakayama et al., 2018) [Cite_Ref] , though it is not mentioned in the evaluation of Neves and Ševa.","Hiroki Nakayama, Takahiro Kubo, Junya Kamura, Ya-sufumi Taniguchi, and Xu Liang. 2018. doccano: Text annotation tool for human. Software available from https://github.com/doccano/doccano.","Another annotation tool with limited sup-port for document-level annotations is Doccano (Nakayama et al., 2018) [Cite_Ref] , though it is not mentioned in the evaluation of Neves and Ševa. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks.",Method,Tool,True,Compare（引用目的）,True,2021.naacl-demos.12_9_0,2021,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,Reference
2920,12931," http://ls6-www.informatik.uni-dortmund.de/ir/projects/freeWAIS-sf/"," ['4 Virtual Examples for Text Classification', 'We assume on text classification the following:']","We to-kenize a document to words, downcase them and then remove stopwords, where the stopword list of freeWAIS-sf [Cite_Footnote_2] is used.",2 Available at http://ls6-www.informatik.uni-dortmund.de/ir/projects/freeWAIS-sf/,"Before describing our methods, we describe text representation which we used in this study. We to-kenize a document to words, downcase them and then remove stopwords, where the stopword list of freeWAIS-sf [Cite_Footnote_2] is used. Stemming is not performed. We adopt binary feature vectors where word fre-quency is not used.",Material,Knowledge,True,Use（引用目的）,True,W03-1027_0_0,2003,Virtual Examples for Text Classification with Support Vector Machines,Footnote
2921,12932," http://www.daviddlewis.com/resources/testcollections/reuters21578/"," ['5 Experimental Results and Discussion', '5.1 Test Set Collection']",We used the Reuters-21578 dataset [Cite_Footnote_3] to evaluate the proposed methods.,3 Available from David D. Lewis’s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/,"We used the Reuters-21578 dataset [Cite_Footnote_3] to evaluate the proposed methods. The dataset has several splits of a training set and a test set. We used here “ModApte” split, which is most widely used in the literature on text classification. This split has 9,603 training ex-amples and 3,299 test examples. More than 100 cat-egories are in the dataset. We use, however, only the most frequent 10 categories. Table 2 shows the 10 categories and the number of training and test exam-ples in each of the categories.",Material,Dataset,True,Use（引用目的）,True,W03-1027_1_0,2003,Virtual Examples for Text Classification with Support Vector Machines,Footnote
2922,12933," http://mastarpj.nict.go.jp/IWSLT2008/downloads/"," ['3 Hidden Event Language Model']","One such technique was introduced by the organizers of the IWSLT evaluation campaign, who suggested duplicating the ending punctuation sym-bol to the beginning of each sentence before training the language model [Cite_Footnote_1] .",1 http://mastarpj.nict.go.jp/IWSLT2008/downloads/ case+punc tool using SRILM.instructions.txt,"Thus, in practice, special techniques are usually required on top of using a hidden event language model in order to overcome long range dependen-cies. Examples include relocating or duplicating punctuation symbols to different positions of a sen-tence such that they appear closer to the indicative words (e.g., “how much” indicates a question sen-tence). One such technique was introduced by the organizers of the IWSLT evaluation campaign, who suggested duplicating the ending punctuation sym-bol to the beginning of each sentence before training the language model [Cite_Footnote_1] . Empirically, the technique has demonstrated its effectiveness in predicting question marks in English, since most of the indicative words for English question sentences appear at the begin-ning of a question. However, such a technique is specially designed and may not be widely applica-ble in general or to languages other than English. Furthermore, a direct application of such a method may fail in the event of multiple sentences per utter-ance without clearly annotated sentence boundaries within an utterance.",Material,Dataset,False,Use（引用目的）,False,D10-1018_0_0,2010,Better Punctuation Prediction with Dynamic Conditional Random Fields,Footnote
2923,12934," http://www.cis.upenn.edu/∼treebank/tokenization.html"," ['6 Experiments']","For all the experiments, we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with the Penn Treebank tokenizer [Cite_Footnote_2] .",2 http://www.cis.upenn.edu/∼treebank/tokenization.html,"We perform experiments on part of the corpus of the IWSLT09 evaluation campaign (Paul, 2009), where both Chinese and English conversational speech texts are used. Two multilingual datasets are consid-ered, the BTEC (Basic Travel Expression Corpus) dataset and the CT (Challenge Task) dataset. The former consists of tourism-related sentences, and the latter consists of human-mediated cross-lingual di-alogs in travel domain. The official IWSLT09 BTEC training set consists of 19,972 Chinese-English ut-terance pairs, and the CT training set consists of 10,061 such pairs. We randomly split each of the two datasets into two portions, where 90% of the ut-terances are used for training the punctuation predic-tion models, and the remaining 10% for evaluating the prediction performance. For all the experiments, we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with the Penn Treebank tokenizer [Cite_Footnote_2] . We list the statistics of the two datasets after processing in Table 3. The proportions of sentence types in the two datasets are listed. The majority of the sentences are declarative sentences. However, question sentences are more frequent in the BTEC dataset compared to the CT dataset. Exclamatory sentences contribute less than 1% for all datasets and are not listed. We also count how often each utterance consists of multiple sen-tences. The utterances from the CT dataset are much longer (with more words per utterance), and there-fore more CT utterances actually consist of multiple sentences.",Method,Tool,True,Use（引用目的）,True,D10-1018_1_0,2010,Better Punctuation Prediction with Dynamic Conditional Random Fields,Footnote
2924,12935," http://code.google.com/p/berkeleyaligner/"," ['6 Experiments', '6.3 Performance in Translation']","The state-of-the-art un-supervised Berkeley aligner [Cite_Footnote_3] (Liang et al., 2006) is used for aligning the training bitext.",3 http://code.google.com/p/berkeleyaligner/,"In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for train-ing the translation system. The state-of-the-art un-supervised Berkeley aligner [Cite_Footnote_3] (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexi-calized reordering model enabled. This is because lexicalized reordering gives better performance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reorder-ing model (msd-bidirectional-fe) is used.",Method,Tool,True,Use（引用目的）,True,D10-1018_2_0,2010,Better Punctuation Prediction with Dynamic Conditional Random Fields,Footnote
2925,12936," http://mallet.cs.umass.edu/grmm/"," ['5 Factorial Conditional Random Fields']","In this work, we use the G RMM package (Sutton, 2006) [Cite_Ref] for building both the linear-chain CRF (L-C RF ) and factorial CRF (F-C RF ).",C. Sutton. 2006. GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/.,"In this work, we use the G RMM package (Sutton, 2006) [Cite_Ref] for building both the linear-chain CRF (L-C RF ) and factorial CRF (F-C RF ). The tree-based reparameterization (TRP) schedule for belief propa-gation (Wainwright et al., 2001) is used for approxi-mate inference.",Method,Tool,True,Use（引用目的）,True,D10-1018_3_0,2010,Better Punctuation Prediction with Dynamic Conditional Random Fields,Reference
2926,12937," http://www.dcs.shef.ac.uk/nlp/meter"," ['References']","A prototype browser-based demo of both the GST algorithm and alignment program, allow-ing users to test arbitrary text pairs for simi-larity, is now available [Cite_Footnote_4] and will continue to be enhanced.",4 See http://www.dcs.shef.ac.uk/nlp/meter.,"A prototype browser-based demo of both the GST algorithm and alignment program, allow-ing users to test arbitrary text pairs for simi-larity, is now available [Cite_Footnote_4] and will continue to be enhanced. helpful comments on earlier drafts.",補足資料,Media,True,Produce（引用目的）,True,P02-1020_0_0,2002,METER: MEasuring TExt Reuse,Footnote
2927,12938," https://bit.ly/2Hw0xWc"," ['1 Introduction']","Whereas a decade ago most mon NLP models, compared to familiar consumption. [Cite_Footnote_1]",1 Sources: (1) Air travel and per-capita consumption: https://bit.ly/2Hw0xWc; (2) car lifetime: https: //bit.ly/2Qbr0w1.,"Advances in techniques and hardware for train-ing deep neural networks have recently en-abled impressive accuracy improvements across many fundamental NLP tasks (Bahdanau et al., 2015; Luong et al., 2015; Dozat and Man-ning, 2017; Vaswani et al., 2017), with the most computationally-hungry models obtaining the highest scores (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; So et al., 2019). As a result, training a state-of-the-art model now re-quires substantial computational resources which demand considerable energy, along with the as-sociated financial and environmental costs. Re-search and development of new models multiplies these costs by thousands of times by requiring re-training to experiment with model architectures and hyperparameters. Whereas a decade ago most mon NLP models, compared to familiar consumption. [Cite_Footnote_1] NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances.",補足資料,Document,True,Introduce（引用目的）,True,P19-1355_0_0,2019,Energy and Policy Considerations for Deep Learning in NLP,Footnote
2928,12939," https://bit.ly/2Qbr0w1"," ['1 Introduction']","Whereas a decade ago most mon NLP models, compared to familiar consumption. [Cite_Footnote_1]",1 Sources: (1) Air travel and per-capita consumption: https://bit.ly/2Hw0xWc; (2) car lifetime: https: //bit.ly/2Qbr0w1.,"Advances in techniques and hardware for train-ing deep neural networks have recently en-abled impressive accuracy improvements across many fundamental NLP tasks (Bahdanau et al., 2015; Luong et al., 2015; Dozat and Man-ning, 2017; Vaswani et al., 2017), with the most computationally-hungry models obtaining the highest scores (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; So et al., 2019). As a result, training a state-of-the-art model now re-quires substantial computational resources which demand considerable energy, along with the as-sociated financial and environmental costs. Re-search and development of new models multiplies these costs by thousands of times by requiring re-training to experiment with model architectures and hyperparameters. Whereas a decade ago most mon NLP models, compared to familiar consumption. [Cite_Footnote_1] NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances.",補足資料,Document,True,Introduce（引用目的）,True,P19-1355_1_0,2019,Energy and Policy Considerations for Deep Learning in NLP,Footnote
2929,12940," https://bit.ly/30sGEbi"," ['2 Methods']",In-terface [Cite_Footnote_2] to sample the GPU power consumption and report the average over all samples.,2 nvidia-smi: https://bit.ly/30sGEbi,"We measure energy use as follows. We train the models described in §2.1 using the default settings provided, and sample GPU and CPU power con-sumption during training. Each model was trained for a maximum of 1 day. We train all models on a single NVIDIA Titan X GPU, with the excep-tion of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeat-edly query the NVIDIA System Management In-terface [Cite_Footnote_2] to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel’s Running Average Power Limit interface.",Method,Tool,True,Use（引用目的）,True,P19-1355_2_0,2019,Energy and Policy Considerations for Deep Learning in NLP,Footnote
2930,12941," https://bit.ly/2LObQhV"," ['2 Methods']","To sample CPU power consumption, we use Intel’s Running Average Power Limit interface. [Cite_Footnote_3]",3 RAPL power meter: https://bit.ly/2LObQhV,"We measure energy use as follows. We train the models described in §2.1 using the default settings provided, and sample GPU and CPU power con-sumption during training. Each model was trained for a maximum of 1 day. We train all models on a single NVIDIA Titan X GPU, with the excep-tion of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeat-edly query the NVIDIA System Management In-terface to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel’s Running Average Power Limit interface. [Cite_Footnote_3]",Method,Tool,True,Use（引用目的）,True,P19-1355_3_0,2019,Energy and Policy Considerations for Deep Learning in NLP,Footnote
2931,12942," https://bit.ly/2JTbGnI"," ['2 Methods']","Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [Cite_Footnote_5] and Germany (Burger, 2019).",5 U.S. Dept. of Energy: https://bit.ly/2JTbGnI,"Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [Cite_Footnote_5] and Germany (Burger, 2019).",補足資料,Document,True,Introduce（引用目的）,True,P19-1355_4_0,2019,Energy and Policy Considerations for Deep Learning in NLP,Footnote
2932,12943," https://bit.ly/2QHE5O3"," ['2 Methods']","Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [Cite_Footnote_5] and Germany (Burger, 2019).",5 China Electricity Council; trans. China Energy Portal: https://bit.ly/2QHE5O3,"Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [Cite_Footnote_5] and Germany (Burger, 2019).",補足資料,Document,True,Introduce（引用目的）,True,P19-1355_5_0,2019,Energy and Policy Considerations for Deep Learning in NLP,Footnote
2933,12944," http://www.cs.toronto.edu/~aditya/g2p-tl-rr"," ['4 Experiments']",Relevant code and scripts associated with our experimental results are available online [Cite_Footnote_1] .,1 http://www.cs.toronto.edu/˜aditya/ g2p-tl-rr,"Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [Cite_Footnote_1] .",Method,Code,True,Produce（引用目的）,False,N12-1044_0_0,2012,Leveraging supplemental representations for sequential transduction,Footnote
2934,12945," https://github.com/VinAIResearch/BERTweet"," ['References']",[Cite] https://github.com/VinAIResearch/BERTweet.,,"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same ar-chitecture as BERT base (Devlin et al., 2019), is trained using the RoBERTa pre-training pro-cedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa base and XLM-R base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tag-ging, Named-entity recognition and text clas-sification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at: [Cite] https://github.com/VinAIResearch/BERTweet.",Material,Knowledge,True,Produce（引用目的）,True,2020.emnlp-demos.2_0_0,2020,BERTweet: A pre-trained language model for English Tweets,Body
2935,12946," https://archive.org/details/twitterstream"," ['2 BERTweet', 'Pre-training data']","• We first download the general Twitter Stream grabbed by the Archive Team, [Cite_Footnote_1] containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter.",1 https://archive.org/details/ twitterstream,"• We first download the general Twitter Stream grabbed by the Archive Team, [Cite_Footnote_1] containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter. To identify English Tweets, we employ the language identification compo-nent of fastText (Joulin et al., 2017). We to-kenize those English Tweets using “TweetTo-kenizer” from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). We also normal-ize the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. We filter out retweeted Tweets and the ones shorter than 10 or longer than 64 word tokens. This pre-process results in the first corpus of 845M English Tweets.",Material,DataSource,True,Use（引用目的）,True,2020.emnlp-demos.2_1_0,2020,BERTweet: A pre-trained language model for English Tweets,Footnote
2936,12947," https://pypi.org/project/emoji"," ['2 BERTweet', 'Pre-training data']","We to-kenize those English Tweets using “TweetTo-kenizer” from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). [Cite_Footnote_2]",2 https://pypi.org/project/emoji,"• We first download the general Twitter Stream grabbed by the Archive Team, containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter. To identify English Tweets, we employ the language identification compo-nent of fastText (Joulin et al., 2017). We to-kenize those English Tweets using “TweetTo-kenizer” from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). [Cite_Footnote_2] We also normal-ize the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. We filter out retweeted Tweets and the ones shorter than 10 or longer than 64 word tokens. This pre-process results in the first corpus of 845M English Tweets.",Method,Code,True,Use（引用目的）,True,2020.emnlp-demos.2_2_0,2020,BERTweet: A pre-trained language model for English Tweets,Footnote
2937,12948," https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz"," ['3 Experimental setup', 'Downstream task datasets']","For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter [Cite_Footnote_4] (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 (Liu et al., 2018).",4 https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz),"For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter [Cite_Footnote_4] (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-demos.2_3_0,2020,BERTweet: A pre-trained language model for English Tweets,Footnote
2938,12949," https://github.com/Oneplus/Tweebank"," ['3 Experimental setup', 'Downstream task datasets']","For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 [Cite_Footnote_5] (Liu et al., 2018).",5 https://github.com/Oneplus/Tweebank,"For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 [Cite_Footnote_5] (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-demos.2_4_0,2020,BERTweet: A pre-trained language model for English Tweets,Footnote
2939,12950," https://github.com/guitaowufeng/TPANN"," ['3 Experimental setup', 'Downstream task datasets']","For Ritter11-T-POS, we employ a 70/15/15 training/validation/test pre-split available from Gui et al. (2017). [Cite_Footnote_6]",6 https://github.com/guitaowufeng/TPANN,"For Ritter11-T-POS, we employ a 70/15/15 training/validation/test pre-split available from Gui et al. (2017). [Cite_Footnote_6] ARK-Twitter contains two files daily547.conll and oct27.conll in which oct27.conll is further split into files oct27.traindev and oct27.test. Fol-lowing Owoputi et al. (2013) and Gui et al. (2017), we employ daily547.conll as a test set. In addition, we use oct27.traindev and oct27.test as training and validation sets, re-spectively. For the T WEEBANK - V 2, WNUT16 and WNUT17 datasets, we use their existing training/validation/test split. The SemEval2017- Task4A and SemEval2018-Task3A datasets are provided with training and test sets only (i.e. there is not a standard split for validation), thus we sam-ple 10% of the training set for validation and use the remaining 90% for training.",Material,Dataset,True,Use（引用目的）,True,2020.emnlp-demos.2_5_0,2020,BERTweet: A pre-trained language model for English Tweets,Footnote
2940,12951," http://luululu.com/tweet/"," ['3 Experimental setup', 'Downstream task datasets']","We also apply a “hard” strategy by further applying lexical normalization dictio-naries (Aramaki, 2010 [Cite_Ref] ; Liu et al., 2012; Han et al., 2012) to normalize word tokens in Tweets.",Eiji Aramaki. 2010. TYPO CORPUS. http://luululu.com/tweet/.,"We use a “soft” normalization strategy to all of the experimental datasets by translating word to-kens of user mentions and web/url links into spe-cial tokens @USER and HTTPURL, respectively, and converting emotion icon tokens into corre-sponding strings. We also apply a “hard” strategy by further applying lexical normalization dictio-naries (Aramaki, 2010 [Cite_Ref] ; Liu et al., 2012; Han et al., 2012) to normalize word tokens in Tweets.",Material,Knowledge,True,Use（引用目的）,True,2020.emnlp-demos.2_6_0,2020,BERTweet: A pre-trained language model for English Tweets,Reference
2941,12952," https://github.com/vzhong/e3"," ['References']",We release source code for our models and experiments at [Cite] https://github.com/vzhong/e3.,,"Conversational machine reading systems help users answer high-level questions (e.g. deter-mine if they qualify for particular govern-ment benefits) when they do not know the ex-act rules by which the determination is made (e.g. whether they need certain income levels or veteran status). The key challenge is that these rules are only provided in the form of a procedural text (e.g. guidelines from govern-ment website) which the system must read to figure out what to ask the user. We present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational his-tory and which still need to be edited to create questions for the user. On the recently intro-duced ShARC conversational machine read-ing dataset, our Entailment-driven Extract and Edit network (E 3 ) achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In addition, by explicitly highlighting which information still needs to be gathered, E 3 provides a more ex-plainable alternative to prior work. We release source code for our models and experiments at [Cite] https://github.com/vzhong/e3.",Method,Code,True,Produce（引用目的）,True,P19-1223_0_0,2019,E 3 : Entailment-driven Extracting and Editing for Conversational Machine Reading,Body
2942,12953," https://github.com/vzhong/e3"," ['1 Introduction']",We re-lease source code for E 3 and the BERTQA model at [Cite] https://github.com/vzhong/e3.,,"We compare E 3 to the previous-best systems as well as a new, strong, BERT-based extrac-tive question answering model (BERTQA) on the recently proposed ShARC CMR dataset (Saeidi et al., 2018). Our results show that E 3 is more accurate in its decisions and generates more rele-vant inquiries. In particular, E 3 outperforms the previous-best model by 5.7% in micro-averaged decision accuracy and 4.3 in inquiry BLEU4. Similarly, E 3 outperforms the BERTQA base-line by 4.0% micro-averaged decision accuracy and 2.4 in inquiry BLEU4. In addition to out-performing previous methods, E 3 is explainable in the sense that one can visualize what rules the model extracted and how previous interactions and inquiries ground to the extracted rules. We re-lease source code for E 3 and the BERTQA model at [Cite] https://github.com/vzhong/e3.",Method,Code,True,Produce（引用目的）,True,P19-1223_1_0,2019,E 3 : Entailment-driven Extracting and Editing for Conversational Machine Reading,Body
2943,12954," https://github.com/jekbradbury/revtok"," ['4 Experiment', '4.1 Experimental setup']","We tokenize using revtok [Cite_Footnote_1] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014).",1 https://github.com/jekbradbury/revtok,"We tokenize using revtok [Cite_Footnote_1] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al. (2019) (e.g. bert-base-uncased). We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4.",Method,Tool,True,Use（引用目的）,True,P19-1223_2_0,2019,E 3 : Entailment-driven Extracting and Editing for Conversational Machine Reading,Footnote
2944,12955," https://github.com/huggingface/pytorch-pretrained-BERT"," ['4 Experiment', '4.1 Experimental setup']","We fine-tune the smaller, uncased pretrained BERT model by Devlin et al. (2019) (e.g. bert-base-uncased). [Cite_Footnote_2]",2 We use the BERT implementation from https://github.com/huggingface/ pytorch-pretrained-BERT,"We tokenize using revtok and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al. (2019) (e.g. bert-base-uncased). [Cite_Footnote_2] We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4.",Method,Tool,True,Use（引用目的）,True,P19-1223_3_0,2019,E 3 : Entailment-driven Extracting and Editing for Conversational Machine Reading,Footnote
2945,12956," http://www.ted.com"," ['2 Related Work']",Of-fline speech translation of TED [Cite_Footnote_1] talks has been ad-dressed through the IWSLT 2011 and 2012 evalua-tion tracks.,1 http://www.ted.com,"Speech translation of European Parliamentary speeches has been addressed as part of the TC-STAR project (Vilar et al., 2005; Fügen et al., 2006). The project focused primarily on offline translation of speeches. Simultaneous translation of lectures and speeches has been addressed in (Hamon et al., 2009; Fügen et al., 2007). However, the work fo-cused on a single speaker in a limited domain. Of-fline speech translation of TED [Cite_Footnote_1] talks has been ad-dressed through the IWSLT 2011 and 2012 evalua-tion tracks. The talks are from a variety of speakers with varying dialects and cover a range of topics. The study presented in this work is the first effort on real-time speech translation of TED talks. In com-parison with previous work, we also present a sys-tematic study of the accuracy versus latency tradeoff for both offline and real-time translation on the same dataset.",補足資料,Website,True,Use（引用目的）,True,N13-1023_0_0,2013,Segmentation Strategies for Streaming Speech Translation,Footnote
2946,12957," http://www.research.att.com/sw/tools/fsm/"," ['5 Speech Translation Models', '5.1 Acoustic and Language Model']","We used the AT&T FSM toolkit (Mohri et al., 1997) [Cite_Ref] to train a trigram lan-guage model (LM) for each component (corpus).","M. Mohri, F. Pereira, and M. Riley. 1997. At&t general-purpose finite-state machine software tools, http://www.research.att.com/sw/tools/fsm/.","The English language model was built using the permissible data in the IWSLT 2011 evaluation. The texts were normalized using a variety of cleanup, number and spelling normalization techniques and filtered by restricting the vocabulary to the top 375000 types; i.e., any sentence containing a to-ken outside the vocabulary was discarded. First, we removed extraneous characters beyond the ASCII range followed by removal of punctuations. Sub-sequently, we normalized hyphenated words and re-moved words with more than 25 characters. The re-sultant text was normalized using a variety of num-ber conversion routines and each corpus was fil-tered by restricting the vocabulary to the top 150000 types; i.e., any sentence containing a token outside the vocabulary was discarded. The vocabulary from all the corpora was then consolidated and another round of filtering to the top 375000 most frequent types was performed. The OOV rate on the TED dev2010 set is 1.1%. We used the AT&T FSM toolkit (Mohri et al., 1997) [Cite_Ref] to train a trigram lan-guage model (LM) for each component (corpus). Fi-nally, the component language models were interpo-lated by minimizing the perplexity on the dev2010 set. The results are shown in Table 2.",Method,Tool,True,Use（引用目的）,True,N13-1023_1_0,2013,Segmentation Strategies for Streaming Speech Translation,Reference
2947,12958," http://peir.path.uab.edu/library/"," ['4 Experiments', '4.1 Datasets']",PEIR Gross The Pathology Education Informa-tional Resource (PEIR) digital library [Cite_Footnote_3] is a pub-lic medical image library for medical education.,"3 PEIR is c University of Alabama at Birmingham, De-partment of Pathology. (http://peir.path.uab.edu/library/)","PEIR Gross The Pathology Education Informa-tional Resource (PEIR) digital library [Cite_Footnote_3] is a pub-lic medical image library for medical education. We collected the images together with their de-scriptions in the Gross sub-collection, resulting in the PEIR Gross dataset that contains 7,442 image-caption pairs from 21 different sub-categories. Different from the IU X-Ray dataset, each caption in PEIR Gross contains only one sentence. We used this dataset to evaluate our model’s ability of generating single-sentence report.",補足資料,Website,True,Introduce（引用目的）,True,P18-1240_0_0,2018,On the Automatic Generation of Medical Imaging Reports,Footnote
2948,12959," https://github.com/tylin/coco-caption"," ['4 Experiments', '4.4 Quantitative Results']","We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image cap-tioning evaluation tool [Cite_Footnote_4] which provides evalua-tion on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedan-tam et al., 2015).",4 https://github.com/tylin/coco-caption,"We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image cap-tioning evaluation tool [Cite_Footnote_4] which provides evalua-tion on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedan-tam et al., 2015).",Method,Tool,True,Use（引用目的）,True,P18-1240_1_0,2018,On the Automatic Generation of Medical Imaging Reports,Footnote
2949,12960," https://sjmielke.com/papers/syncretism"," ['6 Conclusion']","The code is availabile at [Cite] https://sjmielke.com/papers/syncretism, along with type-disambiguated unigram counts for all lexicons pro-vided by the UniMorph project (100+ languages).",,"We have presented a novel generative latent-variable model for resolving ambiguity in unigram counts, notably due to syncretism. Given a lexicon, an unsupervised model partitions the corpus count for each ambiguous form among its analyses listed in a lexicon. We empirically evaluated our method on 5 languages under two evaluation metrics. The code is availabile at [Cite] https://sjmielke.com/papers/syncretism, along with type-disambiguated unigram counts for all lexicons pro-vided by the UniMorph project (100+ languages).",Method,Code,True,Use（引用目的）,False,N18-2087_0_0,2018,Unsupervised Disambiguation of Syncretism in Inflected Lexicons,Body
2950,12961," https://github.com/duyvuleo/Transformer-DyNet"," ['4 Experiments', '4.1 Setup']","For the Transformer, we use Transformer-DyNet [Cite_Footnote_7] implementation and extend it for our context-aware NMT model.",7 https://github.com/duyvuleo/Transformer-DyNet,"All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based NMT implementation in mantis (Cohn et al., 2016). The encoder is a sin-gle layer bidirectional GRU (Cho et al., 2014) and the decoder is a 2-layer GRU with embeddings and hidden dimensions set to 512. The dropout rate for the output layer is set to 0.2. For the Transformer, we use Transformer-DyNet [Cite_Footnote_7] implementation and extend it for our context-aware NMT model. The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1. We also employ all four types of dropouts as in the original Transformer with a rate of 0.1 for the sentence-based model and 0.2 for our context-aware model.",Method,Tool,True,Extend（引用目的）,True,N19-1313_0_0,2019,Selective Attention for Context-aware Neural Machine Translation,Footnote
2951,12962," https://github.com/sameenmaruf/selective-attn"," ['4 Experiments', '4.1 Setup']","For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [Cite_Footnote_8]",8 The code is available at https://github.com/sameenmaruf/selective-attn,"All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based NMT implementation in mantis (Cohn et al., 2016). The encoder is a sin-gle layer bidirectional GRU (Cho et al., 2014) and the decoder is a 2-layer GRU with embeddings and hidden dimensions set to 512. The dropout rate for the output layer is set to 0.2. For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [Cite_Footnote_8] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1. We also employ all four types of dropouts as in the original Transformer with a rate of 0.1 for the sentence-based model and 0.2 for our context-aware model.",Method,Code,True,Extend（引用目的）,True,N19-1313_1_0,2019,Selective Attention for Context-aware Neural Machine Translation,Footnote
2952,12963," http://www.lexalytics.com/webdemo"," ['2 Analyzing Sentiment in Texts']","For example, the company Lexalytics has in its website an available demo [Cite_Footnote_1] for sentiment detection.",1 http://www.lexalytics.com/webdemo,"In the internet, we can find many systems and companies related with sentiment analysis. For example, the company Lexalytics has in its website an available demo [Cite_Footnote_1] for sentiment detection. This demo shows an interface which highlights positive and negative words in the text. The interface also shows entities, categories associated, a summary and the top terms.",Method,Tool,True,Introduce（引用目的）,True,N12-3002_0_0,2012,A Graphical User Interface for Feature-Based Opinion Mining,Footnote
2953,12964," http://www.rankspeed.com/"," ['2 Analyzing Sentiment in Texts']",The RankSpeed [Cite_Footnote_2] is a website for product comparison.,2 http://www.rankspeed.com/,"The RankSpeed [Cite_Footnote_2] is a website for product comparison. The website includes in the search the sentiment associated with each product. In the interface, the user can input a list of sentiment words, like “excellent”, “cool”, “easy” or “powerful” that the system will organize the results according the frequency of those words in reviews related to the products.",補足資料,Website,True,Introduce（引用目的）,True,N12-3002_1_0,2012,A Graphical User Interface for Feature-Based Opinion Mining,Footnote
2954,12965," http://www.thestocksonar.com/"," ['2 Analyzing Sentiment in Texts']",The Stock Sonar [Cite_Footnote_3] has a timeline chart as the main interface.,3 http://www.thestocksonar.com/,"The Stock Sonar [Cite_Footnote_3] has a timeline chart as the main interface. In this timeline, both positive and negative sentiments are displayed throughout time. The sentiments are retrieved from real-time news associated with a particular company. In the same timeline, it is possible to follow-up the increase or decrease of the stock prices for that company in that period of time. In this application, the sentiment is used to forecast market actions such as buy and sell stocks.",補足資料,Website,True,Introduce（引用目的）,True,N12-3002_2_0,2012,A Graphical User Interface for Feature-Based Opinion Mining,Footnote
2955,12966," http://www.epinions.com"," ['3 The System and its Interface']","As an example, Figure 1 shows the organization of this xml file from a review retrieved from the website epinions.com ( [Cite] http://www.epinions.com).",,"The tool accepts as input pure text files or xml files. The xml files follow a specific format which allows the system to retrieve metadata information. It is also possible to retrieve web pages from the web. The tool offers the possibility to retrieve a single webpage, given the URL, or a collection of pages by crawling. To crawl, for example, reviews webpages, the user need to setup some crawling and information extraction rules defined by a template in the configuration file. The files retrieved from the web are converted in xml format, which allows preserving the metadata information. As an example, Figure 1 shows the organization of this xml file from a review retrieved from the website epinions.com ( [Cite] http://www.epinions.com).",Material,DataSource,True,Use（引用目的）,False,N12-3002_3_0,2012,A Graphical User Interface for Feature-Based Opinion Mining,Body
2956,12967," https://pytorch.org/"," ['3 Methodology', '4.1 Experimental Setup']",All experiments were run using the PyTorch [Cite_Footnote_1] framework.,1 https://pytorch.org/,"Hardware Details We trained all models using a single NVIDIA V100 GPU. The batch size was set to 64. We used mixed-precision training (Micike-vicius et al., 2018) to expedite the training proce-dure. All experiments were run using the PyTorch [Cite_Footnote_1] framework.",Method,Tool,True,Use（引用目的）,True,2021.emnlp-main.526_0_0,2021,Towards Zero-Shot Knowledge Distillation for Natural Language Processing,Footnote
2957,12968," https://www.mindspore.cn/"," ['Acknowledgments']",We thank Mindspore [Cite_Footnote_3] which is a new deep learning computing framework for the partial support of this work.,3 https://www.mindspore.cn/,We thank Mindspore [Cite_Footnote_3] which is a new deep learning computing framework for the partial support of this work.,Method,Tool,True,Other（引用目的）,True,2021.emnlp-main.526_1_0,2021,Towards Zero-Shot Knowledge Distillation for Natural Language Processing,Footnote
2958,12969," https://github.com/Jess1ca/CoordinationExtPTB"," ['References']","We make the coordination anno-tation publicly available, in hope that they will facilitate further research into coordi-nation disambiguation. [Cite_Footnote_1]",1 The data is available in: https://github.com/Jess1ca/CoordinationExtPTB,"Coordination is an important and common syntactic construction which is not han-dled well by state of the art parsers. Co-ordinations in the Penn Treebank are miss-ing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsisten-cies. In this work, we initiated manual an-notation process for solving these issues. We identify the different elements in a co-ordination phrase and label each element with its function. We add phrase bound-aries when these are missing, unify incon-sistencies, and fix errors. The outcome is an extension of the PTB that includes con-sistent and detailed structures for coordi-nations. We make the coordination anno-tation publicly available, in hope that they will facilitate further research into coordi-nation disambiguation. [Cite_Footnote_1]",Material,Dataset,True,Produce（引用目的）,True,P16-1079_0_0,2016,Coordination Annotation Extension in the Penn Tree Bank,Footnote
2959,12970," https://github.com/JRC1995/Multilingual-BERT-Disaster"," ['References']","Our code, dataset, and other resources are available on Github. [Cite_Footnote_1]",1 https://github.com/JRC1995/ Multilingual-BERT-Disaster,"Distinguishing informative and actionable messages from a social media platform like Twitter is critical for facilitating disaster man-agement. For this purpose, we compile a multi-lingual dataset of over 130K samples for multi-label classification of disaster-related tweets. We present a masking-based loss function for partially labeled samples and demonstrate the effectiveness of Manifold Mixup in the text domain. Our main model is based on Multi-lingual BERT, which we further improve with Manifold Mixup. We show that our model generalizes to unseen disasters in the test set. Furthermore, we analyze the capability of our model for zero-shot generalization to new lan-guages. Our code, dataset, and other resources are available on Github. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,2020.acl-srw.39_0_0,2020,Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup,Footnote
2960,12971," https://crisisnlp.qcri.org/"," ['3 Aggregated Dataset']","To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, [Cite_Footnote_2] to-gether with two resources from CrisisLex.",2 https://crisisnlp.qcri.org/,"To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, [Cite_Footnote_2] to-gether with two resources from CrisisLex. Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex. The original classes in each resource, together with the mapping to the new classes included in our data set, can be seen in Table 1. Some examples from the dataset are shown in Table 2. For the dataset construction, the following classes were included:",Material,DataSource,True,Extend（引用目的）,True,2020.acl-srw.39_1_0,2020,Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup,Footnote
2961,12972," https://crisislex.org/data-collections.html"," ['3 Aggregated Dataset']","To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, to-gether with two resources from CrisisLex. [Cite_Footnote_3]",3 https://crisislex.org/data-collections.html,"To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, to-gether with two resources from CrisisLex. [Cite_Footnote_3] Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex. The original classes in each resource, together with the mapping to the new classes included in our data set, can be seen in Table 1. Some examples from the dataset are shown in Table 2. For the dataset construction, the following classes were included:",Material,DataSource,True,Extend（引用目的）,True,2020.acl-srw.39_2_0,2020,Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup,Footnote
2962,12973," https://github.com/JRC1995/Multilingual-BERT-Disaster"," ['4 Methods', '5.1 Experimental Setup']",The exact hyperpa-rameters are available on Github. [Cite_Footnote_5] .,5 https://github.com/JRC1995/ Multilingual-BERT-Disaster,"We use four datasets for testing: Russia Meteor, Cy-clone Pam, Philippines Flood, and Mixed disasters. To demonstrate the generalization capabilities of our models, we ensured that the first three datasets are from disasters that are absent in the training set. For M-BERT-based models, we use a mini batch size of 32, a learning rate of 10 −3 for non-BERT parameters, and a fine-tuning rate of 2 × 10 −5 for M-BERT parameters. We set the parameter α of the Beta distribution for the Mixup equation to 2. We run each model five times and report the mean and standard deviation of the results obtained in the 5 runs. For the other models, we import the parame-ter settings from their corresponding paper and then perform light manual tuning. The exact hyperpa-rameters are available on Github. [Cite_Footnote_5] . For significance testing, we used the paired t-test (p ≤ 0.05) (Dror et al., 2018) Note that the CNN baseline is also similar to the model used by Nguyen et al. (2017) which was demonstrated to be a strong performer in disaster-related classification.",Method,Code,True,Produce（引用目的）,False,2020.acl-srw.39_3_0,2020,Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup,Footnote
2963,12974," http://davidmlane.com/hyperstat/A34739.html"," ['5 Evaluations']","The Pearson’s correlation coefficient [Cite_Footnote_5] measures the strength and direction of a linear relationship be-tween any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.","5 For a quick overview of the Pearson’s coefficient, see: http://davidmlane.com/hyperstat/A34739.html.","The Pearson’s correlation coefficient [Cite_Footnote_5] measures the strength and direction of a linear relationship be-tween any two variables, i.e. automatic metric score and human assigned mean coverage score in our case. It ranges from +1 to -1. A correlation of 1 means that there is a perfect positive linear rela-tionship between the two variables, a correlation of -1 means that there is a perfect negative linear rela-tionship between them, and a correlation of 0 means that there is no linear relationship between them. Since we would like to use automatic evaluation metric not only in comparing systems but also in in-house system development, a good linear correlation with human judgment would en-able us to use automatic scores to predict corre-sponding human judgment scores. Therefore, Pearson’s correlation coefficient is a good measure to look at.",補足資料,Document,True,Introduce（引用目的）,True,P04-1077_0_0,2004,Automatic Evaluation of Machine Translation Quality Using Longest Com-mon Subsequence and Skip-Bigram Statistics,Footnote
2964,12975," http://davidmlane.com/hyperstat/A62436.html"," ['5 Evaluations']",Spearman’s correlation coefficient [Cite_Footnote_6] is also a measure of correlation between two variables.,"6 For a quick overview of the Spearman’s coefficient, see: http://davidmlane.com/hyperstat/A62436.html.","Spearman’s correlation coefficient [Cite_Footnote_6] is also a measure of correlation between two variables. It is a non-parametric measure and is a special case of the Pearson’s correlation coefficient when the val-ues of data are converted into ranks before comput-ing the coefficient. Spearman’s correlation coefficient does not assume the correlation be-tween the variables is linear. Therefore it is a use-ful correlation indicator even when good linear correlation, for example, according to Pearson’s correlation coefficient between two variables could not be found. It also suits the NIST MT evaluation scenario where multiple systems are ranked ac-cording to some performance metrics.",補足資料,Document,True,Introduce（引用目的）,True,P04-1077_1_0,2004,Automatic Evaluation of Machine Translation Quality Using Longest Com-mon Subsequence and Skip-Bigram Statistics,Footnote
2965,12976," http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf"," ['1 Introduction']",A variant of B LEU developed by NIST (2002) [Cite_Ref] has been used in two recent large-scale machine translation evalua-tions.,NIST. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-Occurrence Statistics. AAAAAAAAAAA http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf,"Using objective functions to automatically evalu-ate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) ex-tended the idea to accommodate multiple refer-ences. Nießen et al. (2000) calculated the length-normalized edit distance, called word error rate (WER), between a candidate and multiple refer-ence translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word posi-tion, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and ref-erence translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence meas-ure, B LEU , proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of B LEU developed by NIST (2002) [Cite_Ref] has been used in two recent large-scale machine translation evalua-tions.",補足資料,Paper,True,Introduce（引用目的）,True,P04-1077_2_0,2004,Automatic Evaluation of Machine Translation Quality Using Longest Com-mon Subsequence and Skip-Bigram Statistics,Reference
2966,12977," http://lil.nlp.cornell.edu/nlvr/"," ['1 Introduction']",NLVR2 is available at [Cite] http://lil.nlp.cornell.edu/nlvr/.,,"This paper includes four main contributions: (1) a procedure for collecting visually rich im-ages paired with semantically-diverse language descriptions; (2) NLVR2, which contains 107,292 examples of captions and image pairs, includ-ing 29,680 unique sentences and 127,502 im-ages; (3) a qualitative linguistically-driven data analysis showing that our process achieves a broader representation of linguistic phenomena compared to other resources; and (4) an evalu-ation with several baselines and state-of-the-art visual reasoning methods on NLVR2. The rel-atively low performance we observe shows that NLVR2 presents a significant challenge, even for methods that perform well on existing vi-sual reasoning tasks. NLVR2 is available at [Cite] http://lil.nlp.cornell.edu/nlvr/.",Material,Dataset,True,Introduce（引用目的）,True,P19-1644_0_0,2019,A Corpus for Reasoning About Natural Language Grounded in Photographs,Body
2967,12979," http://lic.nlp.cornell.edu/nlvr/"," ['8 Conclusion']",Procedures for evaluating on the unre-leased test set and a leaderboard are available at [Cite] http://lic.nlp.cornell.edu/nlvr/. ported by the National Science Foundation Grad-uate Research Fellowship under Grant No.,,"We introduce the NLVR2 corpus for study-ing semantically-rich joint reasoning about pho-tographs and natural language captions. Our fo-cus on visually complex, natural photographs and human-written captions aims to reflect the chal-lenges of compositional visual reasoning better than existing corpora. Our analysis shows that the language contains a wide range of linguistic phe-nomena including numerical expressions, quan-tifiers, coreference, and negation. This demon-strates how our focus on complex visual stim-uli and data collection procedure result in com-positional and diverse language. We experiment with baseline approaches and several methods for visual reasoning, which result in relatively low performance on NLVR2. These results and our analysis exemplify the challenge that NLVR2 in-troduces to methods for visual reasoning. We release training, development, and public test sets, and provide scripts to break down perfor-mance on the 800 examples we manually ana-lyzed (Section 4) according to the analysis cat-egories. Procedures for evaluating on the unre-leased test set and a leaderboard are available at [Cite] http://lic.nlp.cornell.edu/nlvr/. ported by the National Science Foundation Grad-uate Research Fellowship under Grant No. DGE-1650441. We thank Mark Yatskar, Noah Snavely, and Valts Blukis for their comments and sugges-tions, the workers who participated in our data col-lection for their contributions, and the anonymous reviewers for their feedback.",補足資料,Document,True,Produce（引用目的）,False,P19-1644_2_0,2019,A Corpus for Reasoning About Natural Language Grounded in Photographs,Body
2968,12980," https://github.com/Detectron.facebookresearch/detectron"," ['6 Evaluation Systems']","We detect the objects in the images using a Mask R-CNN model (He et al., 2017; Girshick et al., 2018 [Cite_Ref] ) pre-trained on the COCO detection task (Lin et al., 2014).","Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. 2018. https://github.com/Detectron.facebookresearch/detectron.","We use two baselines that consider both lan-guage and vision inputs. The CNN+RNN base-line concatenates the encoding of the text and im-ages, computed similar to the T EXT and I MAGE baselines, and applies a multilayer perceptron to predict a truth value. The M AX E NT baseline com-putes features from the sentence and objects de-tected in the paired images. We detect the objects in the images using a Mask R-CNN model (He et al., 2017; Girshick et al., 2018 [Cite_Ref] ) pre-trained on the COCO detection task (Lin et al., 2014). We use a detection threshold of 0.5. For each n-gram with a numerical phrase in the caption and object class detected in the images, we compute features based on the number present in the n-gram and the detected object count. We create features for each image and for both together, and use these features in a maximum entropy classifier.",補足資料,Paper,True,Introduce（引用目的）,True,P19-1644_3_0,2019,A Corpus for Reasoning About Natural Language Grounded in Photographs,Reference
2969,12981," http://alt.qcri.org/clef2018-factcheck/"," ['2 Related Work']","As fact checking is mainly done for English, non-English datasets are rare and often unnatural, e.g., translated from En-glish, and focusing on US politics. [Cite_Footnote_2]","2 See for example the CLEF-2018 lab on Automatic Iden-tification and Verification of Claims in Political Debates, which features US political debates translated to Arabic: http://alt.qcri.org/clef2018-factcheck/","Fact checking is very time-consuming, and thus most datasets focus on claims that have been al-ready checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Popat et al., 2016). As fact checking is mainly done for English, non-English datasets are rare and often unnatural, e.g., translated from En-glish, and focusing on US politics. [Cite_Footnote_2] In contrast, we start with claims that are not only relevant to the Arab world, but that were also originally made in Arabic, thus producing the first publicly avail-able Arabic fact-checking dataset.",補足資料,Document,False,Introduce（引用目的）,False,N18-2004_0_0,2018,Integrating Stance Detection and Fact Checking in a Unified Corpus,Footnote
2970,12982," http://www.fakenewschallenge.org/"," ['2 Related Work']","(Mohammad et al., 2016) and from the Fake News Challenge (FNC). [Cite_Footnote_3]",3 http://www.fakenewschallenge.org/,"Stance detection has been studied so far dis-jointly from fact checking. While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-2016 Task 6 (Mohammad et al., 2016) and from the Fake News Challenge (FNC). [Cite_Footnote_3] De-spite its name, the latter has no annotations for fac-tuality, but consists of article-claim pairs labeled for stance: agrees, disagrees, discusses, and unre-lated. In contrast, we retrieve documents for each claim, which yields an arguably more natural dis-tribution of stance labels compared to FNC.",補足資料,Website,True,Introduce（引用目的）,True,N18-2004_1_0,2018,Integrating Stance Detection and Fact Checking in a Unified Corpus,Footnote
2971,12983," http://www.verify-sy.com"," ['3 The Corpus']",V ERIFY [Cite_Footnote_4] is a project that was established to expose false claims made about the war in Syria and other related Middle Eastern issues.,4 http://www.verify-sy.com,"Claim Extraction We consider two websites as the source of our claims. V ERIFY [Cite_Footnote_4] is a project that was established to expose false claims made about the war in Syria and other related Middle Eastern issues. It is an independent platform that debunks claims made by all parties to the conflict. To the best of our knowledge, this is the only platform that publishes fact-checked claims in Arabic.",補足資料,Website,True,Produce（引用目的）,True,N18-2004_2_0,2018,Integrating Stance Detection and Fact Checking in a Unified Corpus,Footnote
2972,12984," http://ara.reuters.com"," ['3 The Corpus']","After extracting the false claims from V ERIFY , we collected the true claims of our corpus from R EUTERS [Cite_Footnote_5] by extracting headlines of news docu-ments.",5 http://ara.reuters.com,"After extracting the false claims from V ERIFY , we collected the true claims of our corpus from R EUTERS [Cite_Footnote_5] by extracting headlines of news docu-ments. We used a list of manually selected key-words to extract claims with the same topics as those extracted from V ERIFY . Then, we manually excluded claims that contained political rhetorical statements (see example 3 be-low), multiple facts, accusations or denials, and ultimately we only kept those claims that discuss factual events, i.e., that can be verified.",補足資料,Website,True,Introduce（引用目的）,True,N18-2004_3_0,2018,Integrating Stance Detection and Fact Checking in a Unified Corpus,Footnote
2973,12985," http://groups.csail.mit.edu/sls/downloads/"," ['3 The Corpus']","Table 1 shows the distribution over the stance labels, [Cite_Footnote_7] which turns out to be very similar to that for the FNC dataset.",7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/,"Table 1 shows the distribution over the stance labels, [Cite_Footnote_7] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance.",Material,Knowledge,True,Produce（引用目的）,True,N18-2004_4_0,2018,Integrating Stance Detection and Fact Checking in a Unified Corpus,Footnote
2974,12986," http://alt.qcri.org/resources/"," ['3 The Corpus']","Table 1 shows the distribution over the stance labels, [Cite_Footnote_7] which turns out to be very similar to that for the FNC dataset.",7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/,"Table 1 shows the distribution over the stance labels, [Cite_Footnote_7] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance.",Material,Knowledge,True,Produce（引用目的）,True,N18-2004_5_0,2018,Integrating Stance Detection and Fact Checking in a Unified Corpus,Footnote
2975,12987," https://github.com/hiaoxui/D2T-Grounding"," ['References']","Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. [Cite_Footnote_1]",1 Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding.,"Previous work on grounded language learn-ing did not fully capture the semantics under-lying the correspondences between structured world state representations and texts, espe-cially those between numerical values and lex-ical terms. In this paper, we attempt at learn-ing explicit latent semantic annotations from paired structured tables and texts, establishing correspondences between various types of val-ues and texts. We model the joint probabil-ity of data fields, texts, phrasal spans, and la-tent annotations with an adapted semi-hidden Markov model, and impose a soft statistical constraint to further improve the performance. As a by-product, we leverage the induced an-notations to extract templates for language generation. Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. [Cite_Footnote_1]",Method,Tool,True,Produce（引用目的）,True,D18-1411_0_0,2018,Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data,Footnote
2976,12988," http://www.itl.nist.gov/iad/mig/tests/mt/"," ['References']","We carried out a study that involved monolin-gual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 2008 [Cite_Footnote_1] test sets, being assisted by statistical machine translation systems trained on data created under the GALE research program.",1 http://www.itl.nist.gov/iad/mig/tests/mt/,"We carried out a study that involved monolin-gual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 2008 [Cite_Footnote_1] test sets, being assisted by statistical machine translation systems trained on data created under the GALE research program.",Material,Dataset,True,Use（引用目的）,True,N10-1078_0_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2977,12989," http://www.darpa.mil/ipto/programs/gale/gale.asp"," ['References']","We carried out a study that involved monolin-gual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 2008 test sets, being assisted by statistical machine translation systems trained on data created under the GALE [Cite_Footnote_2] research program.",2 http://www.darpa.mil/ipto/programs/gale/gale.asp,"We carried out a study that involved monolin-gual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 2008 test sets, being assisted by statistical machine translation systems trained on data created under the GALE [Cite_Footnote_2] research program.",補足資料,Website,True,Introduce（引用目的）,True,N10-1078_1_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2978,12990," http://www.trados.com/"," ['2 Human Translation', '2.1 Translation Tools']","The use of computers has also led to the adoption of tools such as translation memories [Cite_Footnote_3] (databases of translated material that are queried for fuzzy matches, i.e. translated sentences similar to the one to be processed), monolingual and bilingual concor-dances (showing words used in context, and their translations), terminology databases, online dictio-naries and thesauri, and basic editing tools such as word processors and spell checkers (Desilets, 2009).","3 for instance: Trados, http://www.trados.com/","The use of computers has also led to the adoption of tools such as translation memories [Cite_Footnote_3] (databases of translated material that are queried for fuzzy matches, i.e. translated sentences similar to the one to be processed), monolingual and bilingual concor-dances (showing words used in context, and their translations), terminology databases, online dictio-naries and thesauri, and basic editing tools such as word processors and spell checkers (Desilets, 2009).",Material,Dataset,True,Introduce（引用目的）,True,N10-1078_2_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2979,12991," http://en.wikipedia.org/wiki/Wikipedia:Translation"," ['2 Human Translation', '2.2 Translation Skills']","To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles [Cite_Footnote_4] (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks.",4 http://en.wikipedia.org/wiki/Wikipedia:Translation,"Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles [Cite_Footnote_4] (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks.",補足資料,Document,False,Introduce（引用目的）,False,N10-1078_3_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2980,12992," http://l10n.kde.org/"," ['2 Human Translation', '2.2 Translation Skills']","To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, [Cite_Footnote_5] movie subtitles, and even material such as the TED conference talks.",5 http://l10n.kde.org/,"Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, [Cite_Footnote_5] movie subtitles, and even material such as the TED conference talks.",補足資料,Website,False,Introduce（引用目的）,False,N10-1078_4_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2981,12993," http://www.opensubtitles.org/"," ['2 Human Translation', '2.2 Translation Skills']","To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, [Cite_Footnote_6] and even material such as the TED conference talks.",6 http://www.opensubtitles.org/,"Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, [Cite_Footnote_6] and even material such as the TED conference talks.",補足資料,Website,False,Introduce（引用目的）,True,N10-1078_5_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2982,12994," http://www.ted.com/translate/"," ['2 Human Translation', '2.2 Translation Skills']","To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks. [Cite_Footnote_7]",7 http://www.ted.com/translate/,"Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks. [Cite_Footnote_7]",補足資料,Website,False,Introduce（引用目的）,True,N10-1078_6_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2983,12995," http://translate.google.com/toolkit/"," ['2 Human Translation', '2.2 Translation Skills']","Assistance may be as limited as offering machine translation in a post-editing environment, as for instance provided by Google Translator Toolkit [Cite_Footnote_8] (Galvez and Bhansali, 2009) which provides a special function to translate Wikipedia articles.",8 http://translate.google.com/toolkit/,"Research has shown that less qualified transla-tors are able to increase their productivity and qual-ity disproportionally when given automatic assis-tance (Koehn and Haddow, 2009). Assistance may be as limited as offering machine translation in a post-editing environment, as for instance provided by Google Translator Toolkit [Cite_Footnote_8] (Galvez and Bhansali, 2009) which provides a special function to translate Wikipedia articles.",Method,Tool,True,Introduce（引用目的）,True,N10-1078_7_0,2010,Enabling Monolingual Translators: Post-Editing vs. Options,Footnote
2984,12996," http://github.com/jacobandreas/neuralese"," ['References']",We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmat-ics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward rel-ative to players with a common language. [Cite_Footnote_1],"1 We have released code and data at http://github.com/jacobandreas/neuralese. denotes a gated recurrent unit (Cho et al., 2014). Dashed lines represent recurrent connections.","Several approaches have recently been pro-posed for learning decentralized deep mul-tiagent policies that coordinate via a dif-ferentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communi-cation strategies has remained a challenge. Here we propose to interpret agents’ mes-sages by translating them. Unlike in typi-cal machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural lan-guage strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmat-ics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward rel-ative to players with a common language. [Cite_Footnote_1]",Mixed,Mixed,True,Produce（引用目的）,True,P17-1022_0_0,2017,Translating Neuralese,Footnote
2985,12997," http://anserini.io/"," ['3 System Architecture', '3.1 Anserini Retriever']","We use a post-v0.3.0 branch of Anserini, [Cite_Footnote_1] with BM25 as the ranking function (Anserini’s default parameters).",1 http://anserini.io/,"At inference time, we retrieve k text segments (one of the above conditions) using the question as a “bag of words” query. We use a post-v0.3.0 branch of Anserini, [Cite_Footnote_1] with BM25 as the ranking function (Anserini’s default parameters).",Method,Tool,True,Use（引用目的）,True,N19-4013_0_0,2019,End-to-End Open-Domain Question Answering with BERTserini,Footnote
2986,12998," https://github.com/google-research/bert"," ['3 System Architecture', '3.2 BERT Reader']",Our BERT reader is based on Google’s refer-ence implementation [Cite_Footnote_2] (TensorFlow 1.12.0).,2 https://github.com/google-research/bert,"Our BERT reader is based on Google’s refer-ence implementation [Cite_Footnote_2] (TensorFlow 1.12.0). For training, we begin with the BERT-Base model (un-cased, 12-layer, 768-hidden, 12-heads, 110M pa-rameters) and then fine tune the model on the train-ing set of SQuAD (v1.1). All inputs to the reader are padded to 384 tokens; the learning rate is set to 3 × 10 −5 and all other defaults settings are used.",Method,Code,True,Extend（引用目的）,True,N19-4013_1_0,2019,End-to-End Open-Domain Question Answering with BERTserini,Footnote
2987,12999," https://www.washingtonpost.com/news/the-intersect/wp/2016/05/11/you-probably-havent-even-noticed-googles-sketchy-quest-to-control-the-worlds-knowledge/"," ['8 Ethical Considerations']",[Cite] https://www.washingtonpost.com/news/the-intersect/wp/2016/05/11/you-probably-havent-even-noticed-googles-sketchy-quest-to-control-the-worlds-knowledge/.,,"Jinyin Chen, Yangyang Wu, Xuanheng Xu, Yixian Chen, Haibin Zheng, and Qi Xuan. 2018. Fast gradient attack on network embedding. CoRR, abs/1809.02797. and Xifeng Yan. 2009. Identifying bug signatures using discriminative graph mining. In Proceedings of the 18th International Symposium on Software Testing and Analysis (ISSTA’09), pages 141–152, Chicago, IL. haven’t even noticed google’s sketchy quest to control the world’s knowledge. [Cite] https://www.washingtonpost.com/news/the-intersect/wp/2016/05/11/you-probably-havent-even-noticed-googles-sketchy-quest-to-control-the-worlds-knowledge/.",補足資料,Document,True,Introduce（引用目的）,True,2021.emnlp-main.432_2_0,2021,"Adversarial Attack against Cross-lingual Knowledge Graph Alignment Zeru Zhang 1 , Zijie Zhang 1 , Yang Zhou 1 , Lingfei Wu 2 , Sixing Wu 3 , Xiaoying Han 1 ,",Body
2988,13000," http://www.nist.gov/speech/tests/ace/2005"," ['2 Related Work']","ACE mention detection systems (e.g., see (ACE, 2005 [Cite_Ref] ; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes.",ACE. 2005. NIST ACE evaluation website. In http://www.nist.gov/speech/tests/ace/2005.,"Semantic class tagging is most closely related to named entity recognition (NER), mention detec-tion, and semantic lexicon induction. NER sys-tems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Sev-eral bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, how-ever, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or han-dle semantic classes that are not associated with proper named entities (e.g., symptoms). ACE mention detection systems (e.g., see (ACE, 2005 [Cite_Ref] ; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with super-vised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different lan-guages (e.g., (Zitouni and Florian, 2009)).",補足資料,Website,True,Introduce（引用目的）,True,P10-1029_0_0,2010,Inducing Domain-specific Semantic Class Taggers from (Almost) Nothing,Reference
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m      7\u001b[0m     AutoModel,\n\u001b[1;32m      8\u001b[0m     AutoTokenizer,\n\u001b[1;32m      9\u001b[0m     BitsAndBytesConfig,\n\u001b[1;32m     10\u001b[0m     HfArgumentParser,\n\u001b[1;32m     11\u001b[0m     TrainingArguments,\n\u001b[1;32m     12\u001b[0m     pipeline,\n\u001b[1;32m     13\u001b[0m     logging,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/utils/import_utils.py:1782\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1784\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/utils/import_utils.py:1781\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1779\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/utils/import_utils.py:1793\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1793\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1797\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     _BaseAutoBackboneClass,\n\u001b[1;32m     23\u001b[0m     _BaseAutoModelClass,\n\u001b[1;32m     24\u001b[0m     _LazyAutoMapping,\n\u001b[1;32m     25\u001b[0m     auto_class_update,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n\u001b[1;32m     30\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[1;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     46\u001b[0m CLASS_DOCSTRING \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m    This is a generic model class that will be instantiated as one of the model classes of the library when created\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m    with the [`~BaseAutoModelClass.from_pretrained`] class method or the [`~BaseAutoModelClass.from_config`] class\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m    This class cannot be instantiated directly using `__init__()` (throws an error).\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/utils/import_utils.py:1781\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1779\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/utils/import_utils.py:1793\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1793\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1797\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/generation/utils.py:53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_constraints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DisjunctiveConstraint, PhrasalConstraint\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcandidate_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     54\u001b[0m     AssistedCandidateGenerator,\n\u001b[1;32m     55\u001b[0m     AssistedCandidateGeneratorDifferentTokenizers,\n\u001b[1;32m     56\u001b[0m     CandidateGenerator,\n\u001b[1;32m     57\u001b[0m     EarlyExitCandidateGenerator,\n\u001b[1;32m     58\u001b[0m     PromptLookupCandidateGenerator,\n\u001b[1;32m     59\u001b[0m     _crop_past_key_values,\n\u001b[1;32m     60\u001b[0m     _prepare_attention_mask,\n\u001b[1;32m     61\u001b[0m     _prepare_token_type_ids,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     NEED_SETUP_CACHE_CLASSES_MAPPING,\n\u001b[1;32m     65\u001b[0m     QUANT_BACKEND_CLASSES_MAPPING,\n\u001b[1;32m     66\u001b[0m     GenerationConfig,\n\u001b[1;32m     67\u001b[0m     GenerationMode,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogits_process\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     70\u001b[0m     EncoderNoRepeatNGramLogitsProcessor,\n\u001b[1;32m     71\u001b[0m     EncoderRepetitionPenaltyLogitsProcessor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     UnbatchedClassifierFreeGuidanceLogitsProcessor,\n\u001b[1;32m     96\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/transformers/generation/candidate_generator.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sklearn_available\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynamicCache\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isin_mps_friendly\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/sklearn/metrics/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Score functions, performance metrics, pairwise metrics and distance computations.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     accuracy_score,\n\u001b[1;32m      9\u001b[0m     balanced_accuracy_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     zero_one_loss,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/sklearn/metrics/cluster/__init__.py:28\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     adjusted_mutual_info_score,\n\u001b[1;32m     14\u001b[0m     adjusted_rand_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     v_measure_score,\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     calinski_harabasz_score,\n\u001b[1;32m     30\u001b[0m     davies_bouldin_score,\n\u001b[1;32m     31\u001b[0m     silhouette_samples,\n\u001b[1;32m     32\u001b[0m     silhouette_score,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing, check_random_state, check_X_y\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _atol_for_type\n",
      "File \u001b[0;32m~/miniconda3/envs/ME_241211/lib/python3.12/site-packages/sklearn/preprocessing/__init__.py:26\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Binarizer,\n\u001b[1;32m      8\u001b[0m     KernelCenterer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     scale,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_discretization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KBinsDiscretizer\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_encoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, OrdinalEncoder\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_function_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionTransformer\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1191\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Literal, Optional, TypedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLCiteDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    create dataset\n",
    "    - init\n",
    "    - len\n",
    "    - getitem\n",
    "    '''\n",
    "    def __init__(self, texts: list[str]):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 30 14:14:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           On  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              37W / 250W |  23856MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-32GB           On  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              23W / 250W |      4MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    145237      C   /home/z40350r/TSC/.venv/bin/python        23852MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size::: 2690\n",
      "test_data_size::: 299\n"
     ]
    }
   ],
   "source": [
    "csv_dataset = pd.read_csv(\"/data/group1/z40436a/ME/URL_Citation_Classification_Intermediate/data/all_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "seed = 111 # fixed\n",
    "train_df, eval_df = train_test_split(csv_dataset, test_size = 0.1, random_state=seed)\n",
    "print(\"train_data_size:::\", len(train_df))\n",
    "print(\"test_data_size:::\", len(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "CITE_TOKEN = \"[URL_CITE]\"\n",
    "\n",
    "def replace_tag(sentences: pd.Series) -> list[str]:\n",
    "    # replace [Cite_****] to [Cite] token\n",
    "    rule = re.compile(r'\\[Cite[^\\[\\] ]*\\]')\n",
    "    sentences_replaced:list[str] = list()\n",
    "    for sentence in sentences:\n",
    "        sentences_replaced.append(rule.sub(CITE_TOKEN, sentence))\n",
    "\n",
    "    return sentences_replaced\n",
    "\n",
    "def get_3sent(paragraphs:list[str]) -> list[str]:\n",
    "    ret:list[list[str]] = list()\n",
    "    for paragraph in paragraphs:\n",
    "        sentences: list[str] = nltk.sent_tokenize(paragraph)\n",
    "        if not len(sentences):\n",
    "            print('!!!')\n",
    "        if len(sentences) < 4:\n",
    "            ret.append(sentences)\n",
    "            continue\n",
    "        else:\n",
    "            for i in range(len(sentences)):\n",
    "                if CITE_TOKEN in sentences[i]:\n",
    "                    if i == 0:\n",
    "                        ret.append(sentences[i:i+2])\n",
    "                    elif i == len(sentences)-1:\n",
    "                        ret.append(sentences[i-1:i+1])\n",
    "                    else:\n",
    "                        ret.append(sentences[i-1:i+2])\n",
    "                    break\n",
    "                if i == len(sentences)-1:\n",
    "                    # print(sentences)\n",
    "                    pass\n",
    "    cont_3sent = [\" \".join(sent) for sent in ret]\n",
    "    return cont_3sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_icl(file_path:str) -> list[list[int]]:\n",
    "    '''\n",
    "    return icl_idx top-k (from left)\n",
    "    '''\n",
    "    icl_idxs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line == '\\n':\n",
    "                break\n",
    "            icl_idxs.append(json.loads(line))\n",
    "    return icl_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_path = f\"/data/group1/z40436a/ME/URL_Citation_Classification_Intermediate/icl/random/{str(seed)}.txt\"\n",
    "icl_idxs = read_icl(icl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inst(train_df:pd.DataFrame, test_df:pd.DataFrame, icl_method:str, k:int=5) -> list[str]:\n",
    "    texts: list[str] = []\n",
    "\n",
    "    icl_idxs = read_icl(f\"/data/group1/z40436a/ME/URL_Citation_Classification_Intermediate/icl/{icl_method}/{str(seed)}.txt\")\n",
    "    \n",
    "    train_replaced_sentences = replace_tag(train_df['citation-paragraph'])\n",
    "    train_conts = get_3sent(train_replaced_sentences)\n",
    "\n",
    "    test_replaced_sentences = replace_tag(test_df['citation-paragraph'])\n",
    "    test_conts = get_3sent(test_replaced_sentences)\n",
    "\n",
    "    for test_cont, (i, row) in zip(test_conts, test_df.iterrows()):\n",
    "        reset_idx = 0\n",
    "        instruction = [\n",
    "            {\"role\":\"System\", \"content\": f\"\"\"Your task is to classify the type of artifact (TYPE) reffered to the URL and the citation reason (FUNCTION). I will provide you with a URL and citation context, section titles.\\n\n",
    "Here is the classification schema for the artifact type:\n",
    "1. Tool: toolkit, software, system\n",
    "2. Code: codebase, library, API\n",
    "3. Dataset: corpus, image, sets\n",
    "4. Knowledge: lexicon, knowledge graph\n",
    "5. DataSource: source data for the Dataset/Knowledge\n",
    "6. Document: specifications, guidelines\n",
    "7. Paper: scholarly papers\n",
    "8. Media: games, music, videos\n",
    "9. Website: services, homepages\n",
    "10. Mixed: citations referring to multiple resources\n",
    "    \n",
    "Here is the classification schema for the citation reason:\n",
    "1. Use: Used in the citing paper’s research\n",
    "2. Produce: First produced or released by the citing paper’s research\n",
    "3. Compare: Compared with other resources\n",
    "4. Extend: Used in the citing paper’s research but are improved, upgraded, or changed during the research\n",
    "5. Introduce: The resources or the related information\n",
    "6. Other: The URL citation does not belong to the above categories\"\"\"}\n",
    "        ]\n",
    "\n",
    "        if k == 0:\n",
    "            pass\n",
    "        elif k > 0 and k <=5:\n",
    "            for top_k in range(k):\n",
    "                icl_idx = icl_idxs[reset_idx][top_k]\n",
    "                icl_df = train_df.iloc[icl_idx]\n",
    "                # print(icl_df)\n",
    "                icl_input = f\"\"\"Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
    "URL: {icl_df['url']}\n",
    "Citation Context: {train_conts[icl_idx]}\n",
    "Footnote or Reference text (if exists): {icl_df['citation-info']}\n",
    "Section Titles (if exists): {icl_df['passage-title']}\"\"\"\n",
    "                instruction.append({\"role\":\"user\", \"content\": icl_input})\n",
    "                instruction.append({\"role\":\"assistant\", \"content\": f\"\"\"TYPE: {icl_df['type']}\\nFUNCTION: {row['function'].split(\"（\")[0]}\"\"\"})\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "        test_input = f\"\"\"Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
    "URL: {row['url']}\n",
    "Citation Context: {test_cont}\n",
    "Footnote or Reference text (if exists): {row['citation-info']}\n",
    "Section Titles (if exists): {row['passage-title']}\"\"\"\n",
    "        instruction.append({\"role\":\"user\", \"content\": test_input})\n",
    "\n",
    "        reset_idx += 1\n",
    "\n",
    "        texts.append(instruction)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len is OK!, len is 299\n",
      "random\n",
      "===System===\n",
      "Your task is to classify the type of artifact (TYPE) reffered to the URL and the citation reason (FUNCTION). I will provide you with a URL and citation context, section titles.\n",
      "\n",
      "Here is the classification schema for the artifact type:\n",
      "1. Tool: toolkit, software, system\n",
      "2. Code: codebase, library, API\n",
      "3. Dataset: corpus, image, sets\n",
      "4. Knowledge: lexicon, knowledge graph\n",
      "5. DataSource: source data for the Dataset/Knowledge\n",
      "6. Document: specifications, guidelines\n",
      "7. Paper: scholarly papers\n",
      "8. Media: games, music, videos\n",
      "9. Website: services, homepages\n",
      "10. Mixed: citations referring to multiple resources\n",
      "    \n",
      "Here is the classification schema for the citation reason:\n",
      "1. Use: Used in the citing paper’s research\n",
      "2. Produce: First produced or released by the citing paper’s research\n",
      "3. Compare: Compared with other resources\n",
      "4. Extend: Used in the citing paper’s research but are improved, upgraded, or changed during the research\n",
      "5. Introduce: The resources or the related information\n",
      "6. Other: The URL citation does not belong to the above categories\n",
      "###DEMO\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://ufal.mff.cuni.cz/\n",
      "Citation Context: Most of the labels [URL_CITE] are self-explanatory: Pa-tient (PAT), Actor (ACT), Time (TWHEN), Effect (EFF), Location (LOC), Manner (MANN), Ad-dressee (ADDR), Extent (EXT). CPHR marks the nominal part of a complex predicate, as in “to have [a plan] CPHR ”, and DIR3 indicates destination.\n",
      "Footnote or Reference text (if exists): 2 http://ufal.mff.cuni.cz/ ∼ toman/pcedt/en/functors.html\n",
      "Section Titles (if exists):  ['5 Results', '5.2 Argument Classification']\n",
      "===assistant===\n",
      "TYPE: Knowledge\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.lpl.univ-\n",
      "Citation Context: Multext-East References (Copernicus 106). [URL_CITE] http://www.lpl.univ-\n",
      "Footnote or Reference text (if exists): nan\n",
      "Section Titles (if exists):  ['3 The Methodology', '3.5 Handling Unknown Words']\n",
      "===assistant===\n",
      "TYPE: Paper\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.aclweb.org/anthology/P16-1101\n",
      "Citation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016 [URL_CITE] ), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\n",
      "Footnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064–1074. http://www.aclweb.org/anthology/P16-1101.\n",
      "Section Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']\n",
      "===assistant===\n",
      "TYPE: Paper\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  https://writing.wisc.edu/Handbook/Transitions.html\n",
      "Citation Context: The first set of features that we use are length and count-based features, such as word length, word count, sentence length, count of transition phrases [URL_CITE] etc. (Persing and Ng, 2015; Zesch et al., 2015).\n",
      "Footnote or Reference text (if exists): 6 https://writing.wisc.edu/Handbook/Transitions.html\n",
      "Section Titles (if exists):  ['4 Features', '4.1 Text-based Features']\n",
      "===assistant===\n",
      "TYPE: Document\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.verify-sy.com\n",
      "Citation Context: Claim Extraction We consider two websites as the source of our claims. V ERIFY [URL_CITE] is a project that was established to expose false claims made about the war in Syria and other related Middle Eastern issues. It is an independent platform that debunks claims made by all parties to the conflict.\n",
      "Footnote or Reference text (if exists): 4 http://www.verify-sy.com\n",
      "Section Titles (if exists):  ['3 The Corpus']\n",
      "===assistant===\n",
      "TYPE: Website\n",
      "FUNCTION: Introduce\n",
      "###TEST\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.itl.nist.gov/iad/mig/tests/mt/2006/\n",
      "Citation Context: The Chinese word “HE ZHANG,” denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-lation (OpenMT) 2006 evaluation [URL_CITE] . The resulting alignment is shown in Figure 1(b).\n",
      "Footnote or Reference text (if exists): 2 http://www.itl.nist.gov/iad/mig/tests/mt/2006/\n",
      "Section Titles (if exists):  ['1 Introduction']\n",
      "bm25\n",
      "===System===\n",
      "Your task is to classify the type of artifact (TYPE) reffered to the URL and the citation reason (FUNCTION). I will provide you with a URL and citation context, section titles.\n",
      "\n",
      "Here is the classification schema for the artifact type:\n",
      "1. Tool: toolkit, software, system\n",
      "2. Code: codebase, library, API\n",
      "3. Dataset: corpus, image, sets\n",
      "4. Knowledge: lexicon, knowledge graph\n",
      "5. DataSource: source data for the Dataset/Knowledge\n",
      "6. Document: specifications, guidelines\n",
      "7. Paper: scholarly papers\n",
      "8. Media: games, music, videos\n",
      "9. Website: services, homepages\n",
      "10. Mixed: citations referring to multiple resources\n",
      "    \n",
      "Here is the classification schema for the citation reason:\n",
      "1. Use: Used in the citing paper’s research\n",
      "2. Produce: First produced or released by the citing paper’s research\n",
      "3. Compare: Compared with other resources\n",
      "4. Extend: Used in the citing paper’s research but are improved, upgraded, or changed during the research\n",
      "5. Introduce: The resources or the related information\n",
      "6. Other: The URL citation does not belong to the above categories\n",
      "###DEMO\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  https://github.com/neural-dialogue-metrics/rouge\n",
      "Citation Context: In addition, we also use a ROUGE evaluation library [URL_CITE] https://github.com/neural-dialogue-metrics/rouge , which supports the evaluation of ROUGE series metrics (ROUGE-N, ROUGE-L and ROUGE-W).\n",
      "Footnote or Reference text (if exists): nan\n",
      "Section Titles (if exists):  ['A Appendices', 'A.1 Hierarchical Fusion Decoder']\n",
      "===assistant===\n",
      "TYPE: Code\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  https://github.com/Maluuba/nlg-eval\n",
      "Citation Context: The final output state reaches through the feed-forward and add&norm layer like the general trans-former, calculated as the following equation: where W 1 , W 2 , b 1 and b 2 are trainable parameters. A.2 Evalution Metrics We use the nmtpytorch evaluation library https: //github.com/lium-lst/nmtpytorch suggested by the How2 Challenge, which includes BLEU (1, 2, 3, 4), ROUGE-L, METEOR, and CIDEr eval-uation metrics. As an alternative, nlg-eval [URL_CITE] https://github.com/Maluuba/nlg-eval can obtain the same evaluation score as nmtpytorch.\n",
      "Footnote or Reference text (if exists): nan\n",
      "Section Titles (if exists):  ['A Appendices', 'A.1 Hierarchical Fusion Decoder']\n",
      "===assistant===\n",
      "TYPE: Code\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  https://github.com/Maluuba/nlg-evalcan\n",
      "Citation Context: The final output state reaches through the feed-forward and add&norm layer like the general trans-former, calculated as the following equation: where W 1 , W 2 , b 1 and b 2 are trainable parameters. A.2 Evalution Metrics We use the nmtpytorch evaluation library [URL_CITE] https: //github.com/lium-lst/nmtpytorch suggested by the How2 Challenge, which includes BLEU (1, 2, 3, 4), ROUGE-L, METEOR, and CIDEr eval-uation metrics. As an alternative, nlg-eval https://github.com/Maluuba/nlg-eval can obtain the same evaluation score as nmtpytorch.\n",
      "Footnote or Reference text (if exists): nan\n",
      "Section Titles (if exists):  ['A Appendices', 'A.1 Hierarchical Fusion Decoder']\n",
      "===assistant===\n",
      "TYPE: Code\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://berouge.com/default.aspx\n",
      "Citation Context: Both ROUGE and BE have been implemented and included in the ROUGE/BE evaluation toolkit [URL_CITE] , which has been used as the default evaluation tool in the summarization track in the Document Un-derstanding Conference (DUC) and Text Analysis Conference (TAC). DUC and TAC also manually evaluated machine generated summaries by adopt-ing the Pyramid method. Besides evaluating with ROUGE/BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall re-sponsiveness.\n",
      "Footnote or Reference text (if exists): 1 http://berouge.com/default.aspx\n",
      "Section Titles (if exists):  ['2 Related Work']\n",
      "===assistant===\n",
      "TYPE: Tool\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  https://github.com/eladsegal/tag-based-multi-span-extraction\n",
      "Citation Context: Evaluation was performed with the official evaluation scripts of DROP and Q UOREF . Our full implementation can be found at [URL_CITE] https://github.com/eladsegal/tag-based-multi-span-extraction.\n",
      "Footnote or Reference text (if exists): nan\n",
      "Section Titles (if exists):  ['7:453–466.']\n",
      "===assistant===\n",
      "TYPE: Mixed\n",
      "FUNCTION: Introduce\n",
      "###TEST\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.itl.nist.gov/iad/mig/tests/mt/2006/\n",
      "Citation Context: The Chinese word “HE ZHANG,” denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-lation (OpenMT) 2006 evaluation [URL_CITE] . The resulting alignment is shown in Figure 1(b).\n",
      "Footnote or Reference text (if exists): 2 http://www.itl.nist.gov/iad/mig/tests/mt/2006/\n",
      "Section Titles (if exists):  ['1 Introduction']\n",
      "encoder\n",
      "===System===\n",
      "Your task is to classify the type of artifact (TYPE) reffered to the URL and the citation reason (FUNCTION). I will provide you with a URL and citation context, section titles.\n",
      "\n",
      "Here is the classification schema for the artifact type:\n",
      "1. Tool: toolkit, software, system\n",
      "2. Code: codebase, library, API\n",
      "3. Dataset: corpus, image, sets\n",
      "4. Knowledge: lexicon, knowledge graph\n",
      "5. DataSource: source data for the Dataset/Knowledge\n",
      "6. Document: specifications, guidelines\n",
      "7. Paper: scholarly papers\n",
      "8. Media: games, music, videos\n",
      "9. Website: services, homepages\n",
      "10. Mixed: citations referring to multiple resources\n",
      "    \n",
      "Here is the classification schema for the citation reason:\n",
      "1. Use: Used in the citing paper’s research\n",
      "2. Produce: First produced or released by the citing paper’s research\n",
      "3. Compare: Compared with other resources\n",
      "4. Extend: Used in the citing paper’s research but are improved, upgraded, or changed during the research\n",
      "5. Introduce: The resources or the related information\n",
      "6. Other: The URL citation does not belong to the above categories\n",
      "###DEMO\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  https://github.com/sinantie/NeuralAmr\n",
      "Citation Context: We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [URL_CITE] We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017).\n",
      "Footnote or Reference text (if exists): 1 We used the evaluation script available at https://github.com/sinantie/NeuralAmr.\n",
      "Section Titles (if exists):  ['5 Experiments']\n",
      "===assistant===\n",
      "TYPE: Code\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.itl.nist.gov/iad/mig/tests/mt/2009/\n",
      "Citation Context: We used mteval-v13a.pl for calculating BLEU scores. [URL_CITE]\n",
      "Footnote or Reference text (if exists): 1 It is available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/\n",
      "Section Titles (if exists):  ['4 Experiments', '4.1 Common settings']\n",
      "===assistant===\n",
      "TYPE: Document\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.statmt.org/moses/\n",
      "Citation Context: Evaluation Following the WAT ’16 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 [URL_CITE] (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 (Isozaki et al., 2010). Follow-ing Cho et al.\n",
      "Footnote or Reference text (if exists): 6 http://www.statmt.org/moses/\n",
      "Section Titles (if exists):  ['4 Experiments', '4.1 Setup']\n",
      "===assistant===\n",
      "TYPE: Tool\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\n",
      "Citation Context: Evaluation Following the WAT ’16 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [URL_CITE] (Isozaki et al., 2010). Follow-ing Cho et al.\n",
      "Footnote or Reference text (if exists): 7 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\n",
      "Section Titles (if exists):  ['4 Experiments', '4.1 Setup']\n",
      "===assistant===\n",
      "TYPE: Tool\n",
      "FUNCTION: Introduce\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://github.com/andreasvc/discodop\n",
      "Citation Context: For the evaluation, we use the corresponding module of discodop. [URL_CITE] We report several metrics (as implemented in discodop):\n",
      "Footnote or Reference text (if exists): 4 http://github.com/andreasvc/discodop\n",
      "Section Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']\n",
      "===assistant===\n",
      "TYPE: Tool\n",
      "FUNCTION: Introduce\n",
      "###TEST\n",
      "===user===\n",
      "Please classify the artifact type and the citation reason for the following URL and citation sentence.\n",
      "URL:  http://www.itl.nist.gov/iad/mig/tests/mt/2006/\n",
      "Citation Context: The Chinese word “HE ZHANG,” denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-lation (OpenMT) 2006 evaluation [URL_CITE] . The resulting alignment is shown in Figure 1(b).\n",
      "Footnote or Reference text (if exists): 2 http://www.itl.nist.gov/iad/mig/tests/mt/2006/\n",
      "Section Titles (if exists):  ['1 Introduction']\n"
     ]
    }
   ],
   "source": [
    "def show_demo(icl_lst:list[dict[str, str]], method_name:str) -> None:\n",
    "    print(method_name)\n",
    "    for lst_idx, icl in enumerate(icl_lst):\n",
    "        if icl == icl_lst[-1]:\n",
    "            print(\"###TEST\")\n",
    "        elif lst_idx > 0 and icl == icl_lst[1]:\n",
    "            print(\"###DEMO\")\n",
    "        print(f\"==={icl['role']}===\")\n",
    "        print(icl['content'])\n",
    "\n",
    "    return None\n",
    "\n",
    "### test_code create_inst\n",
    "k = 5\n",
    "random_icl = create_inst(train_df, eval_df, \"random\", k)\n",
    "bm25_icl = create_inst(train_df, eval_df, \"bm25\", k)\n",
    "encoder_icl = create_inst(train_df, eval_df, \"encoder\", k)\n",
    "\n",
    "if len(random_icl) == len(bm25_icl) == len(encoder_icl):\n",
    "    print(f\"len is OK!, len is {len(random_icl)}\")\n",
    "\n",
    "test_code_idx = 100\n",
    "show_demo(random_icl[test_code_idx], \"random\")\n",
    "show_demo(bm25_icl[test_code_idx], \"bm25\")\n",
    "show_demo(encoder_icl[test_code_idx], \"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_dir = f\"/data/group1/z40436a/ME/URL_Citation_Classification_Intermediate/icl/{ICL_METHOD}\"\n",
    "with open(f\"{output_dir}/{str(seed)}.txt\", \"w\") as jsonl_file:\n",
    "    for icl in icls:\n",
    "        json.dump(icl, jsonl_file)\n",
    "        jsonl_file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ME_241211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import torch\n",
    "# from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoModel,\n",
    "#     AutoTokenizer,\n",
    "#     BitsAndBytesConfig,\n",
    "#     HfArgumentParser,\n",
    "#     TrainingArguments,\n",
    "#     pipeline,\n",
    "#     logging,\n",
    "# )\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Literal, Optional, TypedDict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICL_METHOD = \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split_ids(ids_path:str) -> list[int]:\n",
    "    with open(ids_path, 'r', encoding='utf-8') as ids_file:\n",
    "        ids_lst = ids_file.readlines()\n",
    "    ids_lst = [int(id) for id in ids_lst]\n",
    "    return ids_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ids = read_split_ids('../data/few_data_split/dev_ids.txt')\n",
    "train_ids = read_split_ids('../data/few_data_split/dev_ids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'name': 'UUParser', 'fullname': 'N/A', 'description': ['a near-SOTA model', 'a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle'], 'citationtag': ['de Lhoneux et al. (2017b)'], 'role': 'Method', 'type': 'Code', 'func': 'Use', 'url': 'https://github.com/mdelhoneux/uuparser-composition', 'section_title': '4 Composition in a K&G Parser', 'add_info': '4 The code can be found at https://github.com/mdelhoneux/uuparser-composition', 'text': 'Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [Cite_Footnote_4] . The S WAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarse-grained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below the best system, which uses an ensemble system as well as ELMo embed-dings, as introduced by Peters et al. (2018). Note, however, that we use a slightly impoverished ver-sion of the model used for the shared task which is described in Smith et al. (2018a): we use a less ac-curate POS tagger (UDPipe) and we do not make use of multi-treebank models. In addition, Smith et al. (2018a) use the three top items of the stack as well as the first item of the buffer to represent the configuration, while we only use the two top items of the stack and the first item of the buffer. Smith et al. (2018a) also use an extended feature set as introduced by Kiperwasser and Goldberg (2016b) where they also use the rightmost and left-most children of the items of the stack and buffer that they consider. We do not use that extended feature set. This is to keep the parser settings as simple as possible and avoid adding confounding factors. It is still a near-SOTA model. We evaluate parsing models on the development sets and report the average of the 5 best results in 30 epochs and 5 runs with different random seeds.'}, {'id': 1, 'name': 'Universal Depen-dencies 2.0 treebanks', 'fullname': 'N/A', 'description': ['N/A'], 'citationtag': ['Straka and Strakov, 2017', 'Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88–99.'], 'role': 'Material', 'type': 'Knowledge', 'func': 'Use', 'url': 'http://hdl.handle.net/11234/1-2364', 'section_title': '5 What Correlates with Difficulty?', 'add_info': 'Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88–99. Documented models at http://hdl.handle.net/11234/1-2364.', 'text': 'Head-POS Entropy Dehouck and Denis (2018) propose an alternative measure of morphosyntactic complexity. Given a corpus of dependency graphs, they estimate the conditional entropy of the POS tag of a random token’s parent, conditioned on the token’s type. In a language where this HPE-mean metric is low, most tokens can predict the POS of their parent even without context. We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017)  .'}, {'id': 2, 'name': 'N/A', 'fullname': 'N/A', 'description': ['the re-versible language-agnostic tokenizer'], 'citationtag': ['Mielke and Eisner (2018)'], 'role': 'Method', 'type': 'Tool', 'func': 'Use', 'url': 'http://sjmielke.com/papers/tokenize/', 'section_title': 'D Data selection: Europarl', 'add_info': '31 http://sjmielke.com/papers/tokenize/', 'text': 'Finally, it should be said that the text in CoStEP itself contains some markup, marking reports, el-lipses, etc., but we strip this additional markup to obtain the raw text. We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [Cite_Footnote_31] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set. This way we ensure uniform division over sessions of the parliament and sizes of 2 / 3 , 1 / 6 , and 1 / 6 , respectively.'}, {'id': 3, 'name': 'Twitter API', 'fullname': 'N/A', 'description': ['N/A'], 'citationtag': ['N/A'], 'role': 'Method', 'type': 'Tool', 'func': 'Use', 'url': 'https://developer.twitter.com/en/docs.html', 'section_title': '2 Problem Formulation 2.2 Data', 'add_info': None, 'text': '• Negative examples: We have col-lected 1% of tweets from Twitter’s daily feed using the Twitter API (  https://developer.twitter.com/en/docs.html) to use as negative examples.'}, {'id': 4, 'name': 'MTurk', 'fullname': 'N/A', 'description': ['N/A'], 'citationtag': ['Amazon, 2005', 'Amazon. 2005'], 'role': 'Method', 'type': 'Tool', 'func': 'Use', 'url': 'https://www.mturk.com/', 'section_title': '5 User study', 'add_info': 'Amazon. 2005. MTurk. (https://www.mturk.com/).', 'text': 'To verify whether human evaluators are in agree-ment with our characterization model, we con-ducted a user study using MTurk (Amazon, 2005)  .'}]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/few_data_split/input_data.json', 'r', encoding='utf-8') as json_file:\n",
    "    input_data = json.load(json_file)\n",
    "print(input_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_icl(input_data:list[dict], train_ids:list[int], dev_ids:list[int], method:str, k:int=5) -> list[list[str]]:\n",
    "    icl_idxs = []\n",
    "    dev_data = [input_data[dev_id] for dev_id in dev_ids]\n",
    "    train_data = [input_data[train_id] for train_id in train_ids]\n",
    "\n",
    "    # random\n",
    "    if method == \"random\":\n",
    "        for i, cont in enumerate(dev_data):\n",
    "            random.seed(i)\n",
    "            icl_idxs.append(random.sample(train_ids, k))\n",
    "\n",
    "    #bm25\n",
    "    elif method == \"bm25\":\n",
    "        tokenized_corpus = [cont['text'].split(\" \") for cont in train_data]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        for i, cont in enumerate(dev_data):\n",
    "            bm25_scores = bm25.get_scores(cont['text'].split(\" \"))\n",
    "            reset_ids = np.argsort(bm25_scores)[-k:][::-1].tolist()\n",
    "            icl_idxs.append([train_data[reset_id]['id'] for reset_id in reset_ids])\n",
    "    # elif method == \"encoder\":\n",
    "    #     model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "    #     tokenized_corpus = model.encode(train_cont_3sent, convert_to_tensor=True)\n",
    "    #     for cont, (i, row) in zip(test_cont_3sent, test_df.iterrows()):\n",
    "    #         tokenized_query = model.encode(test_cont_3sent)\n",
    "    #         cos_scores = util.cos_sim(tokenized_query, tokenized_corpus)[0]\n",
    "\n",
    "    #         icl_idxs.append(np.argsort(-cos_scores)[:k].tolist())\n",
    "    else:\n",
    "        print(\"select other method\")\n",
    "    \n",
    "    return icl_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123, 335, 395, 220, 187], [297, 413, 349, 64, 62], [12, 118, 242, 308, 331], [391, 195, 175, 61, 141], [391, 282, 18, 409, 318], [64, 121, 285, 225, 139], [72, 242, 261, 220, 396], [85, 90, 409, 13, 176], [293, 141, 423, 61, 147], [139, 141, 185, 297, 320], [72, 396, 286, 318, 231], [332, 170, 139, 187, 195], [82, 185, 285, 112, 279], [220, 81, 320, 293, 279], [18, 285, 47, 185, 64], [137, 231, 298, 396, 180], [308, 82, 318, 420, 335], [298, 335, 282, 308, 81], [320, 62, 332, 433, 391], [395, 298, 62, 187, 196], [90, 220, 166, 85, 72], [331, 335, 420, 318, 68], [297, 47, 225, 332, 320], [81, 242, 5, 195, 370], [123, 321, 320, 68, 331], [423, 231, 68, 370, 82], [196, 137, 312, 316, 175], [318, 350, 420, 196, 176], [143, 61, 175, 316, 429], [323, 176, 112, 316, 117], [175, 81, 225, 137, 64], [231, 82, 143, 409, 279], [176, 68, 279, 282, 391], [72, 331, 293, 350, 318], [285, 121, 321, 225, 293], [323, 433, 61, 354, 90], [433, 12, 5, 420, 242], [117, 118, 187, 396, 141], [335, 286, 18, 349, 308], [137, 220, 123, 225, 147], [129, 321, 285, 396, 47], [423, 433, 293, 331, 123], [143, 225, 350, 47, 191], [396, 420, 279, 139, 141], [284, 298, 175, 143, 429], [185, 335, 261, 64, 242], [176, 211, 395, 195, 293], [121, 349, 312, 323, 129], [323, 353, 61, 170, 60], [349, 112, 284, 143, 85], [51, 185, 308, 47, 82], [47, 45, 323, 180, 293], [185, 13, 187, 318, 141], [68, 129, 45, 318, 298], [297, 109, 170, 282, 318], [118, 196, 90, 282, 242], [170, 231, 82, 298, 282], [395, 141, 195, 117, 5], [321, 196, 137, 395, 147], [191, 242, 129, 5, 297], [370, 420, 72, 90, 220], [51, 320, 170, 68, 85], [72, 429, 349, 391, 139], [109, 81, 64, 318, 118], [82, 62, 211, 60, 5], [335, 420, 187, 191, 109], [176, 370, 312, 47, 332], [176, 143, 284, 139, 335], [139, 45, 143, 117, 321], [396, 166, 331, 349, 117], [62, 81, 129, 332, 297], [85, 187, 231, 220, 90], [176, 316, 320, 112, 175], [350, 62, 45, 318, 413], [187, 143, 353, 62, 370], [332, 321, 312, 129, 395], [141, 139, 123, 196, 282], [64, 85, 196, 391, 147]]\n"
     ]
    }
   ],
   "source": [
    "icls = create_icl(input_data=input_data, train_ids=train_ids, dev_ids=dev_ids, method=ICL_METHOD)\n",
    "print(icls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_dir = f\"../icl/few_data_split/icl_for_dev\"\n",
    "with open(f\"{output_dir}/{ICL_METHOD}.txt\", \"w\") as jsonl_file:\n",
    "    for icl in icls:\n",
    "        json.dump(icl, jsonl_file)\n",
    "        jsonl_file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laptop_me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250124までのアノテーションデータ\n",
    "ANNO_IDX = 435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role</th>\n",
       "      <th>type</th>\n",
       "      <th>function</th>\n",
       "      <th>func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Method</td>\n",
       "      <td>Code</td>\n",
       "      <td>Use（引用目的）</td>\n",
       "      <td>Use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Material</td>\n",
       "      <td>Knowledge</td>\n",
       "      <td>Use（引用目的）</td>\n",
       "      <td>Use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Method</td>\n",
       "      <td>Tool</td>\n",
       "      <td>Use（引用目的）</td>\n",
       "      <td>Use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Method</td>\n",
       "      <td>Tool</td>\n",
       "      <td>Use（引用目的）</td>\n",
       "      <td>Use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Method</td>\n",
       "      <td>Tool</td>\n",
       "      <td>Use（引用目的）</td>\n",
       "      <td>Use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       role       type   function func\n",
       "0    Method       Code  Use（引用目的）  Use\n",
       "1  Material  Knowledge  Use（引用目的）  Use\n",
       "2    Method       Tool  Use（引用目的）  Use\n",
       "3    Method       Tool  Use（引用目的）  Use\n",
       "4    Method       Tool  Use（引用目的）  Use"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# role, type, funcを取得\n",
    "csv_dataset = pd.read_csv(\"../data/all_data.csv\", encoding=\"utf-8\", usecols=['role', 'type', 'function'], index_col=0)\n",
    "choice_data = csv_dataset.query(\"role == 'Method' | role == 'Material'\").reset_index()\n",
    "choice_data['func'] = choice_data['function'].apply(lambda x: x.split('（')[0])\n",
    "display(choice_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    # 余分なセパレータを削除\n",
    "    pattern = r'\\n\\n(<Separator_Footnote>|<Seperator_Reference>)\\n'\n",
    "    # 余分なURL引用タグを削除\n",
    "    pattern2 = r'(\\[Cite\\]|\\[Cite_Ref\\])'\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # idをキーとして付与\n",
    "        whole_id:int = 0\n",
    "        # 1行ずつ読み込む\n",
    "        for line in file:\n",
    "            output_data = {}\n",
    "            # 辞書型に変換\n",
    "            row_data = json.loads(line.strip())\n",
    "            # print(row_data)\n",
    "\n",
    "            # initialize span\n",
    "            name_span = []\n",
    "            fullname_span = []\n",
    "            genericmention_span = []\n",
    "            description_span = []\n",
    "            citationtag_span = []\n",
    "\n",
    "            # initialize value\n",
    "            ## 辞書は下記のキーを持つ\n",
    "            output_data['id'] = whole_id\n",
    "            output_data['name'] = []\n",
    "            output_data['fullname'] = []\n",
    "            output_data['genericmention'] = []\n",
    "            output_data['description'] = []\n",
    "            output_data['citationtag'] = []\n",
    "            output_data['role'] = choice_data['role'][whole_id]\n",
    "            output_data['type'] = choice_data['type'][whole_id]\n",
    "            output_data['func'] = choice_data['func'][whole_id]\n",
    "\n",
    "            # スパンのアノテーションを走査\n",
    "            for me_span in row_data['label']:\n",
    "                # 各スパンのラベルをチェック\n",
    "                if me_span[-1] == 'URL':\n",
    "                    url_span = me_span[:-1]\n",
    "                    # print(url_span)\n",
    "                elif me_span[-1] == 'Name':\n",
    "                    name_span = me_span[:-1]\n",
    "                    output_data['name'].append(row_data['data'][name_span[0]:name_span[1]])\n",
    "                elif me_span[-1] == \"Full name\":\n",
    "                    # print('HELLO')\n",
    "                    fullname_span = me_span[:-1]\n",
    "                    output_data['fullname'].append(row_data['data'][fullname_span[0]:fullname_span[1]])\n",
    "                    # print(output_data['fullname'])\n",
    "                elif me_span[-1] == 'Description':\n",
    "                    description_span = me_span[:-1]\n",
    "                    output_data['description'].append(row_data['data'][description_span[0]:description_span[1]])\n",
    "                elif me_span[-1] == 'Citation tag':\n",
    "                    citationtag_span = me_span[:-1]\n",
    "                    output_data['citationtag'].append(row_data['data'][citationtag_span[0]:citationtag_span[1]])\n",
    "                elif me_span[-1] == 'Generic mention':\n",
    "                    genericmention_span = me_span[:-1]\n",
    "                    output_data['genericmention'].append(row_data['data'][genericmention_span[0]:genericmention_span[1]])\n",
    "                else:\n",
    "                    print(\"Error\")\n",
    "                    print(row_data)\n",
    "                    return\n",
    "            \n",
    "            # COMMENT ME!!!\n",
    "            # print(row_data)\n",
    "            # 最終的な値の決定\n",
    "            ## 入力として用いるテキストや諸情報\n",
    "            output_data['url'] = row_data['data'][url_span[0]:url_span[1]]\n",
    "            output_data['section_title'] = row_data['data'].split('\\n\\n')[0]\n",
    "            text = re.split(pattern, row_data['data'].split(output_data['section_title']+'\\n\\n')[1])[0]\n",
    "            if re.search(pattern, row_data['data']):\n",
    "                # print('HELLO')\n",
    "                output_data['add_info'] = re.sub(pattern2, '', re.split(pattern, row_data['data'])[-1])\n",
    "            else:\n",
    "                output_data['add_info'] = None\n",
    "            output_data['text'] = re.sub(pattern2, '', text)\n",
    "            ## もしそのメタデータがなければ、'N/A'とする\n",
    "            if output_data['name'] == []:\n",
    "                output_data['name'] = 'N/A'\n",
    "            else:\n",
    "                output_data['name'] = output_data['name'][0]\n",
    "            if output_data['fullname'] == []:\n",
    "                output_data['fullname'] = 'N/A'\n",
    "            else:\n",
    "                output_data['fullname'] = output_data['fullname'][0]\n",
    "            if output_data['genericmention'] == []:\n",
    "                output_data['genericmention'] = [\"N/A\"]\n",
    "            if output_data['description'] == []:\n",
    "                output_data['description'] = [\"N/A\"]\n",
    "            if output_data['citationtag'] == []:\n",
    "                output_data['citationtag'] = [\"N/A\"]\n",
    "            data.append(output_data)\n",
    "            whole_id += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNO_IDX = 435\n",
    "dict_data = read_jsonl('../data/all.jsonl')[:ANNO_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'name': 'UUParser', 'fullname': 'N/A', 'genericmention': ['the parser'], 'description': ['a near-SOTA model', 'a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle'], 'citationtag': ['de Lhoneux et al. (2017b)'], 'role': 'Method', 'type': 'Code', 'func': 'Use', 'url': 'https://github.com/mdelhoneux/uuparser-composition', 'section_title': '4 Composition in a K&G Parser', 'add_info': '4 The code can be found at https://github.com/mdelhoneux/uuparser-composition', 'text': 'Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [Cite_Footnote_4] . The S WAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarse-grained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below the best system, which uses an ensemble system as well as ELMo embed-dings, as introduced by Peters et al. (2018). Note, however, that we use a slightly impoverished ver-sion of the model used for the shared task which is described in Smith et al. (2018a): we use a less ac-curate POS tagger (UDPipe) and we do not make use of multi-treebank models. In addition, Smith et al. (2018a) use the three top items of the stack as well as the first item of the buffer to represent the configuration, while we only use the two top items of the stack and the first item of the buffer. Smith et al. (2018a) also use an extended feature set as introduced by Kiperwasser and Goldberg (2016b) where they also use the rightmost and left-most children of the items of the stack and buffer that they consider. We do not use that extended feature set. This is to keep the parser settings as simple as possible and avoid adding confounding factors. It is still a near-SOTA model. We evaluate parsing models on the development sets and report the average of the 5 best results in 30 epochs and 5 runs with different random seeds.'}, {'id': 1, 'name': 'Universal Depen-dencies 2.0 treebanks', 'fullname': 'N/A', 'genericmention': ['N/A'], 'description': ['N/A'], 'citationtag': ['Straka and Strakov, 2017', 'Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88–99.'], 'role': 'Material', 'type': 'Knowledge', 'func': 'Use', 'url': 'http://hdl.handle.net/11234/1-2364', 'section_title': '5 What Correlates with Difficulty?', 'add_info': 'Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88–99. Documented models at http://hdl.handle.net/11234/1-2364.', 'text': 'Head-POS Entropy Dehouck and Denis (2018) propose an alternative measure of morphosyntactic complexity. Given a corpus of dependency graphs, they estimate the conditional entropy of the POS tag of a random token’s parent, conditioned on the token’s type. In a language where this HPE-mean metric is low, most tokens can predict the POS of their parent even without context. We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017)  .'}, {'id': 2, 'name': 'N/A', 'fullname': 'N/A', 'genericmention': ['the re-versible language-agnostic tokenizer'], 'description': ['the re-versible language-agnostic tokenizer'], 'citationtag': ['Mielke and Eisner (2018)'], 'role': 'Method', 'type': 'Tool', 'func': 'Use', 'url': 'http://sjmielke.com/papers/tokenize/', 'section_title': 'D Data selection: Europarl', 'add_info': '31 http://sjmielke.com/papers/tokenize/', 'text': 'Finally, it should be said that the text in CoStEP itself contains some markup, marking reports, el-lipses, etc., but we strip this additional markup to obtain the raw text. We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [Cite_Footnote_31] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set. This way we ensure uniform division over sessions of the parliament and sizes of 2 / 3 , 1 / 6 , and 1 / 6 , respectively.'}]\n"
     ]
    }
   ],
   "source": [
    "print(dict_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/few_data_split/input_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(dict_data, json_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laptop_me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

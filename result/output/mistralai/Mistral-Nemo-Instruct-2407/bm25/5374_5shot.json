[
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH [URL_CITE] and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 5 http://pytorch.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/solr/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR [URL_CITE] for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/solr/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE [URL_CITE] and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 2 https://lucene.apache.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y [URL_CITE] for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 1 https://spacy.io\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) [URL_CITE] for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): Mart\u00edn Abadi et al. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN [URL_CITE] , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 4 http://scikit-learn.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf\nCitation Context: 2004. LIB-SVM: a Library for Support Vector Machines, [URL_CITE] http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Comparison with other Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2XjkE2T\nCitation Context: To demonstrate the relative effectiveness of our proposed approach, we compared its quality to state-of-the-art alternative baselines. On TREC-COVID, we directly compared against the three best automatic performing systems [URL_CITE] (out of 141 system runs submitted to the Round-1 challenge by 56 different teams), namely: sabir, IRIT markers and unipd.it.\nFootnote or Reference text (if exists): 8 The details of these systems as well as other competing systems are available in https://bit.ly/2XjkE2T\nSection Titles (if exists):  ['4 Evaluation', '4.3 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/18814\nCitation Context: But, the ground truth evidence and labels of the test set are not publicly available. Instead, once extracted evidence sets/sentences and predicted labels of the test set by a model are sub-mitted to the online evaluation system [URL_CITE] , its perfor-mance is measured and displayed at the system. In this work, we train and tune our hyper-parameters on training and development sets, respectively.\nFootnote or Reference text (if exists): 1 https://competitions.codalab.org/competitions/18814\nSection Titles (if exists):  ['5 Experiments', '5.1 Experiment Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/budzianowski/multiwoz\nCitation Context: In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [URL_CITE]\nFootnote or Reference text (if exists): 5 The model is publicly available at https://github.com/budzianowski/multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.2 Dialogue-Context-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs.stanford.edu/\nCitation Context: We see many opportunities for improvement, considering the poor performance of oracle training relative to the super-vised state-of-the-art, and in turn the poor perfor-mance of unsupervised state-of-the-art relative to the oracle models. [URL_CITE] To this end, it would be instructive to understand both the linguistic and statistical na-ture of the sweet spot, and to test its universality.\nFootnote or Reference text (if exists): 11 To facilitate future work, all of our models are publicly available at http://cs.stanford.edu/ \u223c valentin/.\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (The URL points to the homepage of the Computer Science department at Stanford University, which is not directly related to the citation context. The relevant resource is the specific link to the author's models"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://facebook.github.io/react\nCitation Context: The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [URL_CITE] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:\nFootnote or Reference text (if exists): 8 https://facebook.github.io/react\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/\nCitation Context: The supplemental material is available at our web site [URL_CITE] that provides proofs of the theorems, pseudo-codes of the algorithms, and more experiment re-sults with examples.\nFootnote or Reference text (if exists): 9 http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/\nSection Titles (if exists):  ['Supplemental Material']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2017-non-factoid-qa\nCitation Context: The source-code of our system is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2017-non-factoid-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.elastic.co/products/elasticsearch\nCitation Context: ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [URL_CITE] search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers.\nFootnote or Reference text (if exists): 3 https://www.elastic.co/products/ elasticsearch\nSection Titles (if exists):  ['2 System Architecture and Components', '2.1 System Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: We propose a novel demonstration (URL: [URL_CITE] https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://132.70.6.148:at:8080/exploration\nCitation Context: The web application of our system is available [URL_CITE] http://132.70.6.148:at:8080/exploration\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Application to the Health-care Domain']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/airsplay/R2R-EnvDrop\nCitation Context: Empirically, we show that our agent is substantially better at generalizabil-ity when fine-tuned with these triplets, outper-forming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code, data, and models publicly available at: https://github.com/airsplay/R2R-EnvDrop\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asmadotgh/neural_chat_web\nCitation Context: For further information and instructions on server configuration, please read the server doc-umentation available at [URL_CITE] https://github.com/asmadotgh/neural_chat_web. We hope that this platform will allow others to host their own bots and evaluate them in an interactive setting.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['F Interactive bot platform details', 'F.1 Website server setup and configuration']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/airsplay/R2R-EnvDrop\nCitation Context: We compare our agent with the baseline agent (Tan et al., 2019) [URL_CITE] on the R2R test leaderboard. Our syntax-aware agent achieves 47.8% in success rate and 0.45 in SPL, improving the baseline model by 2.1% in success rate and 2% in SPL.\nFootnote or Reference text (if exists): 3 The baseline is the re-implementation of their model without back translation based on code: https://github.com/airsplay/R2R-EnvDrop\nSection Titles (if exists):  ['5 Results and Analysis', '5.1 Room-to-Room Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://datasets.maluuba.com/Frames\nCitation Context: At the end of each dialogue, the agent receives a positive reward of 2\u21e4max turn (max turn = 60 in our experiments) for success, or a negative re-ward of max turn for failure. Furthermore, at each turn, the agent receives a reward of [URL_CITE] so that shorter dialogue sessions are encouraged.\nFootnote or Reference text (if exists): 1 https://datasets.maluuba.com/Frames\nSection Titles (if exists):  ['4 Experiments and Results', '4.3 User Simulator']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/chaitanyamalaviya/lang-reps\nCitation Context: Experiments show that the pro-posed method is able to infer not only syn-tactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages\u2019 geographic and phy-logenetic neighbors. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and learned vectors are available at http://github.com/chaitanyamalaviya/lang-reps\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jialuli-luka/SyntaxVLN\nCitation Context: We also show that our agent is better at aligning instructions with the cur-rent visual information via qualitative visual-izations. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and models: https://github.com/jialuli-luka/SyntaxVLN\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cornell.edu/People/pabo/movie-review-data/\nCitation Context: And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset [URL_CITE] (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews.\nFootnote or Reference text (if exists): 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~mdredze/datasets/sentiment/\nCitation Context: And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD [URL_CITE] (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews.\nFootnote or Reference text (if exists): 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/~jrennie/20Newsgroups/\nCitation Context: In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [URL_CITE] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).\nFootnote or Reference text (if exists): 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dennybritz/cnn-text-classification-tf\nCitation Context: The embedding matrix W covers the subset of a vocabulary of interests for a particular NLP task, note that the value of V is known to be very large due to the rich variations in human languages. Here we showcase the em-bedding matrix size of a popular text classification model [URL_CITE] on AG-news dataset (Zhang et al., 2015) in Table 1. From which we can easily observe that the embedding matrix is commonly occupy-ing most of the parameter capacity, which could be the bottleneck in many real-world applications with limited computation resources.\nFootnote or Reference text (if exists): 2 https://github.com/dennybritz/ cnn-text-classification-tf\nSection Titles (if exists):  ['2 Vocabulary Selection', '2.1 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/quankiquanki/\nCitation Context: Following ZR18 we also consider a more chal-lenging setup where the English source domain consists of user airline (A) reviews (Nguyen, 2015) [URL_CITE] . We use the dataset of ZR18, consisting of 1000 positive and 1000 negative reviews in the la-beled set, and 39396 reviews as the unlabeled set.\nFootnote or Reference text (if exists): Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset\nCitation Context: We use one of the classic NLP tasks that commonly suffer profanity issues - text style transfer, to eval-uate the effectiveness of the proposed framework. Particularly, we conduct experiments on a subset of the widely used Yelp dataset [URL_CITE] . The dataset consists of product reviews aligned with sentiment ratings from 1 to 5.\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 4 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wuch15/NRHUB\nCitation Context: We conducted experiments on a real-world news recommendation dataset [URL_CITE] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/\nCitation Context: The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [URL_CITE] . The statistics are shown in Table 1.\nFootnote or Reference text (if exists): 3 https://dev.twitter.com/\nSection Titles (if exists):  ['3 Experiments and Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: Online news platforms such as Google News and MSN News [URL_CITE] have attracted many users to read news online (Das et al., 2007). Massive news ar-ticles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is very important for online news platforms to target user interests and allevi-ate information overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 2 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://news.google.com/\nCitation Context: Online news platforms such as Google News [URL_CITE] and MSN News have attracted many users to read news online (Das et al., 2007). Massive news ar-ticles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is very important for online news platforms to target user interests and allevi-ate information overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 1 https://news.google.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs in one month (Dec. 13, 2018 to Jan. 12, 2019). The detailed statistics are shown in Ta-ble 1.\nFootnote or Reference text (if exists): 3 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/postagger/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger [URL_CITE] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 4 http://www-tsujii.is.s.u-tokyo.ac.jp/\u02dctsuruoka/postagger/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mynlp/jigg\nCitation Context: We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [URL_CITE] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.\nFootnote or Reference text (if exists): 9 https://github.com/mynlp/jigg\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-1983xxx\nCitation Context: We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [URL_CITE] tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=\u00a72-21, dev=\u00a722, test=\u00a723), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy \u2248 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task\u2019s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016).\nFootnote or Reference text (if exists): Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://arxiv.org/abs/1809.00794\nCitation Context: Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io. [URL_CITE]\nFootnote or Reference text (if exists): 1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/MIME\nCitation Context: We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at [URL_CITE] https: //github.com/declare-lab/MIME .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/QAML/S3QACoreFramework\nCitation Context: Our cQA pipeline is available for download [URL_CITE] and is distributed under the terms of the Apache 2.0 Li-cense. By taking advantage of the Apache Maven project management tool, most dependencies are automatically handled.\nFootnote or Reference text (if exists): 1 https://github.com/QAML/ S3QACoreFramework\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/\nCitation Context: Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub . Detailed documentation and tutorials are available on Hugging Face\u2019s website [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://huggingface.co/transformers/ in small details, but need to be in sync with pretraining. (Left) Transformer architectures specialized for different tasks, e.g. understanding versus generation, or for specific use-cases, e.g. speed, image+text. (Top) heads allow a Transformer to be used for different tasks. Here we assume the input token sequence is x 1:N from a vocabulary V, and y represents different possible outputs, possibly from a class set C. Example datasets represent a small subset of example code distributed with the library.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub [URL_CITE] . Detailed documentation and tutorials are available on Hugging Face\u2019s website .\nFootnote or Reference text (if exists): 1 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.texar.io.1\nCitation Context: The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at [URL_CITE] https: //www.texar.io. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/lda\nCitation Context: A classical proba-bilistic topic model. We use the LDA pack-age [URL_CITE] for its implementation. We use the set-tings with iteration number n = 2000, the Dirichlet parameter for distribution over top-ics \u03b1 = 0.1 and the Dirichlet parameter for distribution over words \u03b7 = 0.01.\nFootnote or Reference text (if exists): 4 https://pypi.python.org/pypi/lda\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/martin-arvidsson/InterpretableWordEmbeddings\nCitation Context: We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling [URL_CITE] . We set the size of the embeddings K = 100, use a context window size of 8 and \u03c3 = 1 throughout all experiments.\nFootnote or Reference text (if exists): 1 Code is available at: https://github.com/martin-arvidsson/InterpretableWordEmbeddings\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/F1score\nCitation Context: On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure [URL_CITE] for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set \u03bb = 0.5 and \u03c4 = 0.4 unless other-wise mentioned.\nFootnote or Reference text (if exists): 12 http://en.wikipedia.org/wiki/F1 score\nSection Titles (if exists):  ['5 Experimental Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [URL_CITE] are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://badaa.mngl.net/docs.php?p=trans_table\nCitation Context: We romanize all the Katakana words in the dictionary and index them using consecutive N 1 [URL_CITE] http://badaa.mngl.net/docs.php?p=trans_table (May, 2006) characters. We also romanize each candidate loanword when use as a query. We experimentally set N = 2, and use the Okapi BM25 (Robertson et al., 1995) for the retrieval model.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Methodology', '3.6 N-gram retrieval']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/\nCitation Context: It is a sparsity-enhanced non-probabilistic topic model. We use the code released by the authors [URL_CITE] . We set the regularization constants as \u03bb = 0.3, \u03c1 = 0.0001 and the maximum number of itera-tions of hierarchical sparse coding, dictionary learning as 100.\nFootnote or Reference text (if exists): 5 http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [URL_CITE] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description.\nFootnote or Reference text (if exists): 5 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipediadatabase\nCitation Context: Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [URL_CITE] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/Wikipedia database\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isw3.naist.jp/IS/TechReport/report-list.html#2009\nCitation Context: For the description of features used in our adap-tation of the Shimbo-Hara model to Japanese, see (Okuma et al., 2009) [URL_CITE] . In this model, all features are defined as indicator functions asking whether one or more attributes (e.g., surface form, part-of-speech) take specific values at the neighbor of an arc. One example of a feature assigned to a diag-onal arc at row i and column j of the alignment graph is where POS [i] denotes the part-of-speech of the ith word in a sentence.\nFootnote or Reference text (if exists): H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.\nSection Titles (if exists):  ['2 Alignment-based coordinate structure analysis', '2.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2008/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wikitravel.org/en/Wikitravel:Articletemplates/Sections\nCitation Context: Each article describes a city or a larger geo-graphic area that is of interest to tourists. In addition, all articles contain sections [URL_CITE] describing different as-pects of a city from a tourism point of view (e.g., places to see, transportation, shopping and eating). For our proof of concept, we performed IE only on the \u2018places to see\u2019 sections.\nFootnote or Reference text (if exists): 7 http://wikitravel.org/en/Wikitravel:Articletemplates/Sections\nSection Titles (if exists):  ['4 Human Subject Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ntu.edu.au/education/langs/ielex/IE-DATA1\nCitation Context: This information is obtained from manually curated cognate lists such as the data of Dyen et al. (1997) [URL_CITE] .\nFootnote or Reference text (if exists): I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.\nSection Titles (if exists):  ['1 Introduction', '1.1 Previous work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhaozj89/TensorEmbeddingNLP\nCitation Context: The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at [URL_CITE] https://github.com/zhaozj89/TensorEmbeddingNLP.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www2.parc.com/isl/groups/nltt/xle/\nCitation Context: One area of current research which has similarities with this work is on Lexical Functional Grammars (LFGs). Both approaches attempt to abstract away from the surface level syntax of the sentence (e.g., the XLE system [URL_CITE] ). The most obvious difference be-tween the approaches is that we use SRL data to train our system, avoiding the need to have labeled data specific to our simplification scheme.\nFootnote or Reference text (if exists): 3 http://www2.parc.com/isl/groups/nltt/xle/\nSection Titles (if exists):  ['9 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nCitation Context: One of the quirks of the TED talks is that the speaker annotation correlates with the topic of their talk to a high degree. Although the topics that a speaker talks about can be considered as a man-ifestation of speaker traits, we also perform a con-trol experiment on a different dataset to verify that our model is indeed learning more than just topical information. Specifically, we train our models on a speaker annotated version of the Europarl corpus (Rabinovich et al., 2017), on the en-de language pair [URL_CITE] .\nFootnote or Reference text (if exists): 6 available here: https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nSection Titles (if exists):  ['5 Experiments', '5.3 Further experiments on the Europarl corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php\nCitation Context: The French Treebank [URL_CITE] [FTB] (Abeille\u0301 et al., 2003) is a syntactically annotated corpus made up of jour-nalistic articles from Le Monde newspaper. We used the latest edition of the corpus (June 2010) that we preprocessed with the Stanford Parser pre-processing tools (Green et al., 2011).\nFootnote or Reference text (if exists): 2 http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php\nSection Titles (if exists):  ['4 Resources', '4.1 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ntu.edu.au/education/langs/ielex/IE-DATA1\nCitation Context: Since we used automatic tools for preparing our corpus rather than careful linguistic analysis, our cognate list is much noiser in terms of the pres-ence of borrowed words and phonemeic transcrip-tion errors compared to the ones used by previous approaches (Swadesh, 1955; Dyen et al., 1997 [URL_CITE] ). The benefit of our mechanical preprocessing is that more cognate data can easily be made available, allowing us to effectively train richer models.\nFootnote or Reference text (if exists): I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.\nSection Titles (if exists):  ['4 Experiments', '4.1 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doccano/doccano\nCitation Context: Another annotation tool with limited sup-port for document-level annotations is Doccano [URL_CITE] (Nakayama et al., 2018), though it is not mentioned in the evaluation of Neves and \u0160eva. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks.\nFootnote or Reference text (if exists): 5 https://github.com/doccano/doccano\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doccano/doccano\nCitation Context: Another annotation tool with limited sup-port for document-level annotations is Doccano (Nakayama et al., 2018) [URL_CITE] , though it is not mentioned in the evaluation of Neves and \u0160eva. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks.\nFootnote or Reference text (if exists): Hiroki Nakayama, Takahiro Kubo, Junya Kamura, Ya-sufumi Taniguchi, and Xu Liang. 2018. doccano: Text annotation tool for human. Software available from https://github.com/doccano/doccano.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip\nCitation Context: We applied our annotation scheme to the product re-view dataset [URL_CITE] released by Hu and Liu (2004). We annotated the data for 3 out of 5 products.\nFootnote or Reference text (if exists): 4 http://www.cs.uic.edu/\u02dcliub/FBS/CustomerReviewData.zip\nSection Titles (if exists):  ['5 Data and Annotation Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EdoFrank/emnlp2019_morio_egawa\nCitation Context: The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [URL_CITE]\nFootnote or Reference text (if exists): 1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://katfuji.lab.tuat.ac.jp/nlp_datasets/\nCitation Context: The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [URL_CITE]\nFootnote or Reference text (if exists): 1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/\u223cpdtb/tools.shtml#annotator\nCitation Context: The source of these files is Xinhua newswire. The annotation is carried out on the PDTB annotation tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 http://www.seas.upenn.edu/\u223cpdtb/tools.shtml#annotator\nSection Titles (if exists):  ['4 Annotation experiment', '4.1 Set-up']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projects.ldc.upenn.edu/gale/data/Catalog.html\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [URL_CITE] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.\nFootnote or Reference text (if exists): 2 http://projects.ldc.upenn.edu/gale/data/Catalog.html\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.translationautomation.com/\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS. [URL_CITE]\nFootnote or Reference text (if exists): 4 http://www.translationautomation.com/\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opus.nlpl.eu/\nCitation Context: For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [URL_CITE] (Tiedemann, 2012) and (2) ParaCrawl (Espl\u00e0 et al., 2019). From OPUS, we use the JW300 corpus (Agic\u0301 and Vulic\u0301, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014).\nFootnote or Reference text (if exists): 3 http://opus.nlpl.eu/\nSection Titles (if exists):  ['2 A FRO MT benchmark', '2.2 Data Sources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tticoin/AntonymDetection\nCitation Context: In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on aver-age) and 380,873 antonymy pairs (6.5 per word). [URL_CITE] As in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (panga-sius, animal)).\nFootnote or Reference text (if exists): 5 https://github.com/tticoin/AntonymDetection\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://casmacat.eu/corpus/global-voices.html\nCitation Context: Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, [URL_CITE] EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh\u2013English and Kazakh\u2013Russian data from WMT, to be able to evaluate on Kazakh.\nFootnote or Reference text (if exists): 10 http://casmacat.eu/corpus/global-voices.html\nSection Titles (if exists):  ['B Data Details for Replication']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.public.asu.edu/~jye02/Software/MALSAR\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [URL_CITE] 3 Wu and Huang (2015) 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): J. Zhou, J. Chen, and J. Ye. 2011. MAL-SAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University. http://www.public.asu.edu/\u02dcjye02/Software/MALSAR.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/AD3\nCitation Context: We trained a struc-tured SVM with stochastic subgradient descent; the cost-augmented inference problems are re-laxed and solved with AD 3 , as described in \u00a73.3. [URL_CITE] We followed the procedure described in Berg-Kirkpatrick et al. (2011) to reduce the number of candidate sentences: scores were defined for each sentence (the sum of the scores of the concepts they cover), and the best-scored sentences were greedily selected up to a limit of 1,000 words.\nFootnote or Reference text (if exists): 8 We use the AD 3 implementation in http://www.ark.cs.cmu.edu/AD3, setting the maximum number of iterations to 200 at training time and 1000 at test time. We extended the code to handle the knapsack and budget factors; the modified code will be part of the next release (AD 3 2.1).\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nCitation Context: (4) By adding word-knowledge features and us-ing learning-based multi-sieve approach, we im-prove the performance of the state-of-the-art system of (Bengtson and Roth, 2008) by 3 MUC, [URL_CITE] B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset. We report our experimen-tal results in Sec.\nFootnote or Reference text (if exists): 2 The extracted attributes and the related re-sources are available for public download at http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/CrowdTruth/\nCitation Context: Formality NTERPERS GYAFC (Rao and Tetreault, 2018) 224k given 2 formal (50%), informal (50%) Y web N clsf. Politeness I StanfPolite (Danescu et al., 2013) 10k given 2 polite (49.6%), impolite (50.3%) Humor ShortHumor (CrowdTruth, 2016) [URL_CITE] 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017) 463k random 2 humor (50%), non-humor (50%) IGURATIVE Sarcasm SarcGhosh (Ghosh and Veale, 2016) 43k given 2 sarcastic (45%), non-sarcastic (55%) EmoBank valence (Buechel and Hahn, 2017) 10k random 1 negative, positive EmoBank arousal (Buechel and Hahn, 2017) 10k random 1 calm, excited EmoBank dominance (Buechel and Hahn, 2017) 10k random 1 being_controlled, being_in_control FFECTIVE DailyDialog (Li et al., 2017) 102k given 7 noemotion(83%), happy(12%).. Offense HateOffensive (Davidson et al., 2017) 24k given 3 hate(6.8%), offensive(76.3%).. A Romance ShortRomance 2k random 2 romantic (50%), non-romantic (50%) Sentiment SentiBank (Socher et al., 2013) 239k given 2 positive (54.6%), negative (45.4%) Gender PASTEL (Kang et al., 2019) 41k given 3 Female (61.2%), Male (38.0%).. 41k given 8 35-44 (15.3%), 25-34 (42.1%).. ERSONAL Age PASTEL (Kang et al., 2019)\nFootnote or Reference text (if exists): CrowdTruth. 2016. Short Text Corpus For Hu-mor Detection. http://github.com/CrowdTruth/ Short-Text-Corpus-For-Humor-Detection . [On-line; accessed 1-Oct-2019].\nSection Titles (if exists):  ['2 Related Work', '3.1 Style selection and groupings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/amoudgl/short-jokes-dataset\nCitation Context: Formality NTERPERS GYAFC (Rao and Tetreault, 2018) 224k given 2 formal (50%), informal (50%) Y web N clsf. Politeness I StanfPolite (Danescu et al., 2013) 10k given 2 polite (49.6%), impolite (50.3%) Humor ShortHumor (CrowdTruth, 2016) 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017) [URL_CITE] 463k random 2 humor (50%), non-humor (50%) IGURATIVE Sarcasm SarcGhosh (Ghosh and Veale, 2016) 43k given 2 sarcastic (45%), non-sarcastic (55%) EmoBank valence (Buechel and Hahn, 2017) 10k random 1 negative, positive EmoBank arousal (Buechel and Hahn, 2017) 10k random 1 calm, excited EmoBank dominance (Buechel and Hahn, 2017) 10k random 1 being_controlled, being_in_control FFECTIVE DailyDialog (Li et al., 2017) 102k given 7 noemotion(83%), happy(12%).. Offense HateOffensive (Davidson et al., 2017) 24k given 3 hate(6.8%), offensive(76.3%).. A Romance ShortRomance 2k random 2 romantic (50%), non-romantic (50%) Sentiment SentiBank (Socher et al., 2013) 239k given 2 positive (54.6%), negative (45.4%) Gender PASTEL (Kang et al., 2019) 41k given 3 Female (61.2%), Male (38.0%).. 41k given 8 35-44 (15.3%), 25-34 (42.1%).. ERSONAL Age PASTEL (Kang et al., 2019)\nFootnote or Reference text (if exists): Abhinav Moudgil. 2017. short jokes dataset. https://github.com/amoudgl/short-jokes-dataset. [On-line; accessed 1-Oct-2019].\nSection Titles (if exists):  ['2 Related Work', '3.1 Style selection and groupings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) 3 Wu and Huang (2015) [URL_CITE] 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html\nCitation Context: 1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) [URL_CITE] (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).\nFootnote or Reference text (if exists): Anthony Kroch and Ann Taylor. 2000. Penn-Helsinki Parsed Corpus of Middle English, second edition. http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/PCEEC-manual/index.htm\nCitation Context: 1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) [URL_CITE] (2.2 million words).\nFootnote or Reference text (if exists): Ann Taylor, Arja Nurmi, Anthony Warner, Susan Pintzuk, and Terttu Nevalainen. 2006. Parsed Corpus of Early English Correspondence. Com-piled by the CEEC Project Team. York: Uni-versity of York and Helsinki: University of Helsinki. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/PCEEC-manual/index.htm.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.linguist.is/icelandic_treebank\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011) [URL_CITE] , French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Joel Wallenberg, Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson. 2011. Icelandic Parsed Historical Corpus (IcePaHC) version 0.4. http://www.linguist.is/icelandic_treebank.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/YCOE/YcoeHome.htm\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003) [URL_CITE] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.tycho.iel.unicamp.br/~tycho/corpus/en/index.html\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010) [URL_CITE] , totaling 4.5 million words.\nFootnote or Reference text (if exists): Charlotte Galves and Pabol Faria. 2010. Tycho Brahe Parsed Corpus of Historical Portuguese. http://www.tycho.iel.unicamp.br/\u02dctycho/corpus/en/index.html.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCEME-RELEASE-2/index.html\nCitation Context: 1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) [URL_CITE] (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).\nFootnote or Reference text (if exists): Anthony Kroch, Beatrice Santorini, and Ariel Diertani. 2004. Penn-Helsinki Parsed Cor-pus of Early Modern English. http: //www.ling.upenn.edu/hist-corpora/ PPCEME-RELEASE-2/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ltrc.iiit.ac.in/download.php\nCitation Context: Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words [URL_CITE] and most frequent Telugu words . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists.\nFootnote or Reference text (if exists): 2 https://ltrc.iiit.ac.in/download.php\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Dataset details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wiktionary.org/Frequencylists/Telugu\nCitation Context: Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words and most frequent Telugu words [URL_CITE] . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists.\nFootnote or Reference text (if exists): 3 https://en.wiktionary.org/Frequencylists/Telugu\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Dataset details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://snowball.tartarus.org/credits.php\nCitation Context: 4 For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970) [URL_CITE] , and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content words were excluded.\nFootnote or Reference text (if exists): Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php.\nSection Titles (if exists):  ['4 Experiments', '4.1 Matching on Multilingual Topics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/OpenBookQA\nCitation Context: For MultiRC, we also apply an off-the-shelf coreference resolution model 4 and re-place the mentions when they resolve to pronouns occurring in a different sentence . For Open-BookQA, we use the exact same retrieval as re-leased by the authors of OpenBookQA [URL_CITE] and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query.\nFootnote or Reference text (if exists): 6 https://github.com/allenai/OpenBookQA\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/P96-1011\nCitation Context: For our models, we adopt the pruning strate-gies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with \u03b2 = 0.00001, and utilize a tag dictio-nary, which maps frequent words to the possible supertags 7 . Unless otherwise stated, we only al-low normal form parses (Eisner, 1996 [URL_CITE] ; Hocken-maier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).\nFootnote or Reference text (if exists): Jason Eisner. 1996. Efficient Normal-Form Parsing for Combinatory Categorial Grammar. In 34th Annual Meeting of the Association for Computational Lin-guistics. http://aclweb.org/anthology/P96-1011.\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\nCitation Context: The encoder-decoder model is a conditional lan-guage model that predicts a correct output se-quence from an input sequence (Sutskever et al., 2014) [URL_CITE] . This model consists of two RNNs: an en-coder and decoder. The encoder extracts a context of the input sequence as a real-valued vector, and the decoder predicts the output sequences from the context individually.\nFootnote or Reference text (if exists): Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural In-formation Processing Systems 27 (NIPS 2014), Curran Associates, Inc., pages 3104\u20133112. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aka.ms/WikiQA\nCitation Context: In SIN-CONV, we first use a convolution layer to obtain phrase representations for the two sen-tences s 1 and s 2 , and the SIN interaction proce-dure is then applied to these phrase representations as before to model phrase interactions. The aver-age of all hidden states are treated as sentence vec-tors v scnn 1 and v scnn [URL_CITE] . Thus, SIN-CONV is SIN with word vectors substituted by phrase vectors.\nFootnote or Reference text (if exists): 2 http://aka.ms/WikiQA\nSection Titles (if exists):  ['3 Method', '3.3 SIN with Convolution (SIN-CONV)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T07\nCitation Context: \u2022 Caciularu et al. (2021) \u2013 graph-based Bh2019 model using representations from a Long-2 [URL_CITE] https://catalog.ldc.upenn.edu/LDC2011T07 former (Beltagy et al., 2020) with cross-document attention during pre-training\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experimental Setup', '4.4 Baselines and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ASRU.2015.7404872\nCitation Context: The objective of this task is to re-rank a list of candidate answers according to a non-factoid question, where the best-ranked candidate is selected as an answer. Models usually learn to generate dense vector representations for questions and candidates, where representations of a question and an associated correct answer should lie closely together within the vector space (Feng et al., 2015) [URL_CITE] . Accordingly, the ranking score can be determined with a simple similarity met-ric.\nFootnote or Reference text (if exists): Minwei Feng, Bing Xiang, Michael R. Glass, Li-dan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding. pages 813\u2013 820. https://doi.org/10.1109/ASRU.2015.7404872.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\nCitation Context: ResNeXt WSL We first experiment with im-age representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion pub-lic images (Mahajan et al., 2018), with subse-quent fine-tuning on the ImageNet1K dataset (Rus-sakovsky et al., 2015) [URL_CITE] . The output of this model is a 2048-dimensional vector, and we refer to these representations as \u201cResNeXt WSL\" features.\nFootnote or Reference text (if exists): 4 https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014), which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016) [URL_CITE] , MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., pages 271\u2013 279. http://papers.nips.cc/paper/ 6066-f-gan-training-generative- neural-samplers-using-variational-divergence-minimization.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: And just as binomial adversar-ial networks have been applied to numerous tasks (e.g. image generation (Goodfellow et al., 2014) [URL_CITE] , domain adaptation (Ganin et al., 2016), cross-lingual text classification (Chen et al., 2016)), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ccsasuke/man\nCitation Context: Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multino-mial adversarial network [URL_CITE] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theo-retical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multi-ple probability distributions.\nFootnote or Reference text (if exists): 1 The source code of MAN is available at https://github.com/ccsasuke/man.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [URL_CITE] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017 [URL_CITE] ). Ganin et al.\nFootnote or Reference text (if exists): Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence genera-tive adversarial nets with policy gradient. In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014) [URL_CITE] , which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmdrr/testSignificanceNLP.git\nCitation Context: We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental re-sults, statistical significance testing is often ig-nored or misused. We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [URL_CITE] .\nFootnote or Reference text (if exists): 1 The code for all statistical tests detailed in this pa-per is found on: https://github.com/rtmdrr/testSignificanceNLP.git\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/\nCitation Context: ATIS and SNIPS, respectively), named entity recog- The results in Table 1 (above) support the observa-nition, part-of-speech tagging and semantic role tion that, in general, bidirectional models do have labeling (Weischedel et al., 2013); and four for a better non-incremental performance than LSTMs sentence classification: intent (Hemphill et al., (except for IntentATIS and ProsCons) and that there marginal difference in the evaluation metrics. We thus present 4 The code is available at [URL_CITE] https://github.com/ the results using the pre-trained model only, and leave more briemadu/inc-bidirectional. For more details on exploration of fine-tuning for future work.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.york.ac.uk/semeval-2012/task6/\nCitation Context: An example of such an approach, as applied to the task of grammat-ical error detection, can be found in (Madnani et al., 2011). [URL_CITE] Second, we believe that the PAN corpus\u2014 with Turker simulated plagiarism\u2014contains much more realistic examples of paraphrase and should be incorporated into future evaluations of paraphrase identification. In order to encourage this, we are re-leasing our PAN dataset containing 13,000 sentence pairs.\nFootnote or Reference text (if exists): 3 A good approximation is to use an ordinal scale for the human judgments as in the Semantic Textual Similarity task of SemEval 2012. See http://www.cs.york.ac.uk/semeval-2012/task6/ for more details.\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://duc.nist.gov/\nCitation Context: It was originally assumed that summarization systems should be assessed across multiple sum-mary lengths. For that, the earliest Document Un-derstand Conference (DUC) (NIST, 2011) [URL_CITE] bench-marks, in 2001 and 2002, defined several tar-get summary lengths and evaluated each summary against (manually written) reference summaries of the same length.\nFootnote or Reference text (if exists): NIST. 2011. Document Understanding Conferences. https://duc.nist.gov/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/drgriffis/\nCitation Context: TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at [URL_CITE] https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/monoses\nCitation Context: We thus conclude that, contrary to recent trend, future work in BLI should not focus exclusively in direct retrieval approaches, nor should BLI be the only evaluation task for cross-lingual embeddings. Our code is available at [URL_CITE] https://github.com/artetxem/monoses.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions and future work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.icst.pku.edu.cn/lcwm/grass\nCitation Context: which yields a signifi-cant improvement over the best published result in the literature. Our implementa-tion is available at [URL_CITE] http://www.icst.pku.edu.cn/lcwm/grass.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thu-coai/earl\nCitation Context: We selected the model performing best in the validation set to evaluate in test sets. Our code is available at: [URL_CITE] https://github.com/thu-coai/earl.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Makwen1995/LDGNMLTC\nCitation Context: We select the best parameter configuration based on performance on the valida-tion set and evaluate the configuration on the test set. Our code is available on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/Makwen1995/LDGN MLTC\nSection Titles (if exists):  ['3 Experiment', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [URL_CITE] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence. The GVDB is the result of a large crowdsourced annotation effort. This an-notation is ongoing, and the GVDB will be regularly updated with new data and new layers of annotation, making it an interesting and challenging data set on which to evaluate state-of-the-art NLP tools.\nFootnote or Reference text (if exists): 2 http://gun-violence.org/\nSection Titles (if exists):  ['3 The Gun Violence Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/WordMoversEmbeddings\nCitation Context: Moreover, WME often achieves orders of magnitude speed-up compared to KNN-WMD while obtaining the same testing accuracy. Our code and data is available at [URL_CITE] https://github.com/IBM/WordMoversEmbeddings.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rycolab/info-theoretic-probing\nCitation Context: We evaluate on a set of ten typologically diverse languages often under-represented in NLP research\u2014plus English\u2014 totalling eleven languages. Our implementa-tion is available in [URL_CITE] https://github.com/rycolab/info-theoretic-probing.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) [URL_CITE] and the 2016 International Workshop on Spoken Language Translation (IWSLT16) . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 4 WMT16: http://www.statmt.org/wmt16/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.slt.atr.jp/IWSLT2006/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation [URL_CITE] , NIST Machine Translation Evaluation , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 1 http://www.slt.atr.jp/IWSLT2006/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/index.htm\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation [URL_CITE] , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt/index.htm\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.863data.org.cn/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation , and HTRDP Evaluation [URL_CITE] were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 3 http://www.863data.org.cn/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://workshop2016.iwslt.org/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) [URL_CITE] . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 5 IWSLT16: https://workshop2016.iwslt.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleylm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 [URL_CITE] (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 6 http://code.google.com/p/berkeleylm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/redpony/cdec\nCitation Context: \u2022 cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://github.com/redpony/cdec\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Languagefamily\nCitation Context: Language family is a group of languages re-lated through descent from a common ancestral language or parental language [URL_CITE] . There are dif-ferent kinds of taxonomies for language family in the world, among which, Ethnologue (Paul et al., 2009) is one of the most authoritative and com-monly accepted taxonomies.\nFootnote or Reference text (if exists): 2 https://en.wikipedia.org/wiki/Language family\nSection Titles (if exists):  ['3 Multilingual NMT with Language Clustering', '3.1 Prior Knowledge Based Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: \u2022 Lang, SeqAE: Pretrained with the language model and autoencoder model proposed in (Dai and Le, 2015) [URL_CITE] . The language model is the de-coder part of the encoder-decoder model using a zero vector as the initial hidden layer value, and the autoencoder model is the same structure of the encoder-decoder model, where input and output are the same.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['3 Experiments', '3.3 Compared Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.sri.com/projects/srilm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM [URL_CITE] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 5 http://www.speech.sri.com/projects/srilm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/flax/blob/master/flax/nn\nCitation Context: Unless specified other-wise, the model parameters are: number of lay-ers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implemen-tation [URL_CITE] so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation. This al-lows for an easy and fair comparison.\nFootnote or Reference text (if exists): 6 https://github.com/google/flax/blob/master/flax/nn\nSection Titles (if exists):  ['8 Experiments And Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lilt/alignment-scripts\nCitation Context: This is imple-mented by masking the self attention probabilities, i.e. while computing the representation for the i th target token, the decoder can only self-attend to the representations of {1, [URL_CITE] . .\nFootnote or Reference text (if exists): 2 https://github.com/lilt/ alignment-scripts\nSection Titles (if exists):  ['4 Proposed Method', '4.3 Providing Full Target Context']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/72.410363\nCitation Context: In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015); in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015) [URL_CITE] . In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value.\nFootnote or Reference text (if exists): Karl Moritz Hermann, Toma\u0301s\u030c Koc\u030cisky\u0301, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom. 2015. Teaching Ma-chines to Read and Comprehend. In Advances in Neural Information Processing Systems (NIPS 2015). pages 1\u201314. https://doi.org/10.1109/72.410363.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1146/annurev.neuro.26.041002.131047\nCitation Context: In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015) [URL_CITE] ; in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015). In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value.\nFootnote or Reference text (if exists): Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Rep-resentations. https://doi.org/10.1146/annurev.neuro.26.041002.131047.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download\nCitation Context: When it is reason-able to assume that the input sentence for the gram-maticality improvement system is sufficiently fluent, a list of candidate lexical categories can be assigned automatically to each word via supertagging (Clark and Curran, 2007) on the input sequence. We use the C&C supertagger [URL_CITE] to assign a set of probable lexical categories to each input word using the gold-standard order. When the input is noisy, the accuracy of a supertagger tends to be lower than when the in-put is grammatical.\nFootnote or Reference text (if exists): 1 http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download.\nSection Titles (if exists):  ['6 Experiments', '6.1 Development Tests']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kaushalshetty/Structured-Self-Attention\nCitation Context: Through the channel-attention network, we can observe the attention matrix A for each input doc-ument. By summing up the attention weights of each word embedding vector, we can visualize the overall attention for that word with the visualiza-tion tool [URL_CITE] . We randomly select three movies from IMDb testing set (See Figure 5).\nFootnote or Reference text (if exists): 5 The visualization tool is provided by https://github.com/kaushalshetty/ Structured-Self-Attention\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.4 More Analysis in Rank-AE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0\nCitation Context: Named entity counts and concreteness ratio. For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [URL_CITE] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn.\nFootnote or Reference text (if exists): 6 Version 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0. pdf Section 2.6.\nSection Titles (if exists):  ['4 Pragmatic correlations with analysts\u2019 pre-call judgments', '4.1 Pragmatic lexical features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jekbradbury/revtok\nCitation Context: We tokenize using revtok [URL_CITE] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al.\nFootnote or Reference text (if exists): 1 https://github.com/jekbradbury/revtok\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fenchri/dsre-vae\nCitation Context: Experimental results on two datasets created via distant supervision indi-cate that multi-task learning results in perfor-mance benefits. Additional exploration of em-ploying Knowledge Base priors into the VAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results [URL_CITE] .\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/fenchri/dsre-vae\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dmoz.org\nCitation Context: In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 categories : tennis players, golfers, actors, politicians and scien-tists, (10 names from each category) from the dmoz directory [URL_CITE] . For each pair of names in our dataset, we measure the association between the two names using the proposed method and baselines.\nFootnote or Reference text (if exists): 7 http://dmoz.org\nSection Titles (if exists):  ['4 Experiments', '4.4 Named Entity Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://stanfordnlp.github.io/CoreNLP/coref.html\nCitation Context: As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit [URL_CITE] , we use their released model as baselines. For the End-to-end model, we also use their re-leased code .\nFootnote or Reference text (if exists): 6 https://stanfordnlp.github.io/CoreNLP/coref.html\nSection Titles (if exists):  ['5 The Experiment', '5.3 Baseline Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nCitation Context: To identify the replaceable entities, we apply named entity recognition (NER) and POS tag-ging using the Stanford CoreNLP toolkit (Man-ning et al., 2014). We allow for replacement of only those entities that are well-known, according to the Microsoft Knowledge Base [URL_CITE] . This improves the likelihood that the terms are familiar to both headline editors and humor judges.\nFootnote or Reference text (if exists): 1 http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nSection Titles (if exists):  ['2 The Humor Dataset', '2.1 Task Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fdalvi.github.io\nCitation Context: Fahim Dalvi, Software Engineer, Qatar Comput-ing Research Institute, Qatar Email: faimaduddin@hbku.edu.qa Website: [URL_CITE] https://fdalvi.github.io Fahim Dalvi is an experienced Software En-gineer with a demonstrated history of working in the research industry and is currently employed at the Qatar Computing Research Institute. Fahim\u2019s research is centered around the intersection of Natural Language Processing and Deep Learning, and he has worked on wide variety of problems in these fields including Machine Translation, Language Modelling and Explainability in Deep Neural Networks.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Instructor Information (Alphabetic order']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/jacobe/naacl09.html\nCitation Context: Source code is available at [URL_CITE] http://people.csail.mit.edu/jacobe/naacl09.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/HKUST-KnowComp/RINANTE\nCitation Context: Our code is available at [URL_CITE] https://github.com/HKUST-KnowComp/RINANTE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~cyl/ROUGE/\nCitation Context: Table 2 shows the results obtained on these two data sets for different graph settings. The table also lists baseline results, obtained on summaries generated by 1 R OUGE is available at [URL_CITE] http://www.isi.edu/\u02dccyl/ROUGE/. taking the first sentences in each document.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Evaluation', '-', 'the DUC 2002 document summarization evaluations.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ntunlpsg.github.io/project/rst-parser\nCitation Context: We make our code available at [URL_CITE] https://ntunlpsg.github.io/project/rst-parser\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/sabine/chunklink/\nCitation Context: 1 Software available at [URL_CITE] http://ilk.uvt.nl/sabine/chunklink/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Unsupervised Relation Extraction Problem', '2.1 Context Vector and Feature Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.eccox.io/\nCitation Context: More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntac-tic information. Ecco is available at [URL_CITE] https: //www.eccox.io/ . 2\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/lsl_acl20\nCitation Context: Code, data, and experiments are available at https: //github.com/jayelm/lsl and on CodaLab at [URL_CITE] https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jayelm/lsl\nCitation Context: Code, data, and experiments are available at [URL_CITE] https: //github.com/jayelm/lsl and on CodaLab at https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/MIME\nCitation Context: We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at [URL_CITE] https: //github.com/declare-lab/MIME .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SimonHFL/CWEB\nCitation Context: This approach has successfully been used in previous work for devising web-based corpora (Scha\u0308fer, 2015; Bie-mann et al., 2007). The data is available at [URL_CITE] https: //github.com/SimonHFL/CWEB . covering a range of topics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/bcEysXmR09c\nCitation Context: Ecco is available at https: //www.eccox.io/ . [URL_CITE]\nFootnote or Reference text (if exists): 2 Video demo available at https://youtu.be/ bcEysXmR09c\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) [URL_CITE] and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/lex-parser.shtml\nCitation Context: The syntactic backoff behavior is calculated in an analogous fashion: it verifies for the existence of n-grams of POS tags in a POS-tagged LM. The POS tags of target sentence are produced by the Stanford Parser [URL_CITE] (integrated in Q U E ST ++ ).\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/lex-parser.shtml\nSection Titles (if exists):  ['3 Features', '3.1 Word level']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://web.stanford.edu/~jurafsky/ws97/manual.august1.html\nCitation Context: The SwDA cor-pus conforms to the damsl tag schema. [URL_CITE] We as-sembled the tag sets referring to easy damsl (Iso-mura et al., 2009) into 9 tags (Table 2) in order to consolidate tags with a significantly low fre-quency. The SwDA corpus provides transcriptions of 1, 155 conversations with 219, 297 utterances.\nFootnote or Reference text (if exists): 1 https://web.stanford.edu/\u02dcjurafsky/ws97/manual.august1.html\nSection Titles (if exists):  ['4 Experiment', '4.1 Switch Board Dialogue Act Corpus (SwDA)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: Templates 1, 2 and 3 express word uni-grams and bi-grams. In MSU-based tagging, we can uti-lize the POS information, which we get from the Stanford Chinese POS Tagger [URL_CITE] . In template 4, the type of word refers to whether it is a number, an English word or a Chinese word.\nFootnote or Reference text (if exists): 6 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['2 System Architecture', '2.2 MSU Based Tagging', '2.2.3 Feature templates']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/mturk/welcome\nCitation Context: In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [URL_CITE] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of \u201cto-morrow\u201d.\nFootnote or Reference text (if exists): 4 https://www.mturk.com/mturk/welcome\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mgiza\nCitation Context: Postprocessing To perform unknown word re-placement (Luong et al., 2015a), we built a bilin-gual English-Japanese dictionary from all of the three million translation pairs. The dictionary was extracted with the MGIZA++ 0.7.0 [URL_CITE] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.\nFootnote or Reference text (if exists): 5 https://github.com/moses-smt/mgiza\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/bsd/licensee/2014_stats/baseline_med_filecount.html\nCitation Context: The first DT is computed based on Medline [URL_CITE] ab-stracts. This thesaurus uses the left and the right word as context features. To include multi-word expressions, we allow the number of tokens that form a term to be up to the length of three.\nFootnote or Reference text (if exists): 4 http://www.nlm.nih.gov/bsd/licensee/2014_stats/baseline_med_filecount.html\nSection Titles (if exists):  ['4 Resources', '4.1 Distributional thesauri (DTs)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aspell.net/\nCitation Context: (2010)). We use the Aspell dictionary (v6.06) [URL_CITE] to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length \u2265 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs.\nFootnote or Reference text (if exists): 2 http://aspell.net/\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/isobs\nCitation Context: We evaluate different character recognition mod-els on self-created dataset. The results show that our implementation of prototypical network can achieve stable and competitive results. The datasets and source code can be found from [URL_CITE] https: //github.com/thunlp/isobs .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiment and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.iso.org/iso-639-language-codes.html\nCitation Context: The language clustering based on language family is shown in Figure 2, which results in groups given our 23 languages. All the language names and their corresponding ISO-639-1 code [URL_CITE] can be found in Supplementary Materials (Section 2).\nFootnote or Reference text (if exists): 9 https://www.iso.org/iso-639-language-codes.html.\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor\nCitation Context: The language clustering based on language family is shown in Figure 2, which results in [URL_CITE] groups given our 23 languages. All the language names and their corresponding ISO-639-1 code can be found in Supplementary Materials (Section 2).\nFootnote or Reference text (if exists): 8 https://github.com/tensorflow/tensor2tensor\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.opendata.aws/amazon-reviews-ml/readme.html\nCitation Context: The Multilingual Amazon Reviews Corpus (MARC) can be found at https://registry.opendata.aws/amazon-reviews-ml/. The dataset description, code snippets, and li-cense agreement can be retrieved at [URL_CITE] https: //docs.opendata.aws/amazon-reviews-ml/ readme.html .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://registry.opendata.aws/amazon-reviews-ml/\nCitation Context: The Multilingual Amazon Reviews Corpus (MARC) can be found at [URL_CITE] https://registry.opendata.aws/amazon-reviews-ml/. The dataset description, code snippets, and li-cense agreement can be retrieved at https: //docs.opendata.aws/amazon-reviews-ml/ readme.html .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hohocode.github.io/textSimilarityConvNet/\nCitation Context: Everything necessary to replicate our experimen-tal results can be found in our open-source code repository. [URL_CITE]\nFootnote or Reference text (if exists): 4 http://hohocode.github.io/textSimilarityConvNet/\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/vecmap\nCitation Context: (2017). [URL_CITE] However, because the methods pre-sented in this work feature tunable hyperparame-ters, we use a portion of the training set as devel-opment data. 2 In all experiments, a single target word is predicted for each source word, and full points are awarded if it is one of the listed correct translations.\nFootnote or Reference text (if exists): 1 https://github.com/artetxem/vecmap\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: Word embedding layer (W E \u2208 R N\u00d7300 ): Sup-posing the word number of the vocabulary is N, this layer devotes to transform each word to a distributed embedding representation. Here, we adopt the pre-trained embeddings by GloVe based on a large Wikipedia dataset [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Neural Sparse Topical Coding', '3.3 Neural Sparse Topical Coding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zapier.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/tools/lextool.html\nCitation Context: If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. [URL_CITE] The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited.\nFootnote or Reference text (if exists): 1 http://www.speech.cs.cmu.edu/tools/lextool.html\nSection Titles (if exists):  ['3 Target Recovery', '3.3 Phonetic Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/search\nCitation Context: Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API [URL_CITE] us-ing a query composed of the ambiguous word and its hypernym, e.g. \u201cjaguar animal\u201d.\nFootnote or Reference text (if exists): 9 https://azure.microsoft.com/en-us/services/cognitive-services/search\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [URL_CITE] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [URL_CITE] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 3 We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [URL_CITE] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 4 We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the first word piece as the whole word representation.\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bert-as-service.github.com/hanxiao/bert-as-service\nCitation Context: Note that for BERT we evaluated all pooling strategies available in bert-as-service (Xiao, 2018) [URL_CITE] applied to either the last or second-to-last layers and report results for the best-performing combi-nation, which was mean-pooling on the last layer for both model sizes. Our results are presented in Table 1.\nFootnote or Reference text (if exists): Han Xiao. 2018. https://bert-as-service.github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [URL_CITE] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1.\nFootnote or Reference text (if exists): 7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiments', '4.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt16/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) [URL_CITE] , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 2 https://www.statmt.org/wmt16/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wit3.fbk.eu/\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) [URL_CITE] , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 1 https://wit3.fbk.eu/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nCitation Context: We use translation benchmarks IWSLT14 En\u2013De, En\u2013Fr, En\u2013It, and IWSLT15 En\u2013Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [URL_CITE] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set.\nFootnote or Reference text (if exists): 3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nSection Titles (if exists):  ['6 Experiments', '6.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: For the syntax-based NMT, we take syntac-tic trees of source texts as inputs. We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task [URL_CITE] . Both sides are tokenized and split into subwords using BPE with 8000 merge operations.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes\nCitation Context: Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score [URL_CITE] , , i.e., the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes.\nFootnote or Reference text (if exists): 3 Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt14/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) [URL_CITE] . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 3 https://www.statmt.org/wmt14/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lemurproject.org/clueweb12/\nCitation Context: Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb [URL_CITE] about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents). There are certain limitations in applying existing re-lation extraction algorithms to CQA data, i.e., they typically consider sentences independently and ig-nore the discourse of QnA pair text.\nFootnote or Reference text (if exists): 3 http://www.lemurproject.org/clueweb12/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/phrase2vec\nCitation Context: More concretely, we train our n-gram embed-dings using phrase2vec [URL_CITE] , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs.\nFootnote or Reference text (if exists): 1 https://github.com/artetxem/ phrase2vec\nSection Titles (if exists):  ['3 Principled unsupervised SMT', '3.1 Initial phrase-table']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/mt-para\nCitation Context: We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [URL_CITE]\nFootnote or Reference text (if exists): 4 The data is available at http://bit.ly/mt-para.\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tticoin/AntonymDetection\nCitation Context: In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on aver-age) and 380,873 antonymy pairs (6.5 per word). [URL_CITE] As in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (panga-sius, animal)).\nFootnote or Reference text (if exists): 5 https://github.com/tticoin/AntonymDetection\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.pascal-network.org/Challenges/RTE,March\nCitation Context: The algorithm propagating verb arguments was tested on a set of 106 pairs of phrases with simi-lar meaning for which argument structures could be built. These phrases were selected from pairs of questions and their correct answers from the set of factoid questions in TREC 2004 and also from the pairs of scenarios and hypotheses from first edition of PASCAL RTE Challenge (Dagan et al., 2005) [URL_CITE] . Table 6 shows algorithm performance.\nFootnote or Reference text (if exists): Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. Recognising Textual Entailment Challenge, http://www.pascal-network.org/Challenges/RTE,March.\nSection Titles (if exists):  ['4 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SussexCompSem/learninghypernyms\nCitation Context: For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs [URL_CITE] from the BLESS dataset (Baroni and Lenci, 2011). These noun-noun word pairs include positive hy-ponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs.\nFootnote or Reference text (if exists): 2 https://github.com/SussexCompSem/ learninghypernyms\nSection Titles (if exists):  ['5 Evaluation', '5.1 Hyponymy with Word2Vec Vectors']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: For parsing, since the derivation tree in a lexicalized TAG is a type of dependency tree (Rambow and Joshi, 1994), we can directly apply dependency parsing models. In particular, we use the biaffine graph-based parser proposed by Dozat and Manning (2017) [URL_CITE] together with our novel techniques for supertagging.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) [URL_CITE] and semantic role la-beling (Chen and Rambow, 2003). In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance. These re-sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/K17-3014\nCitation Context: Nguyen et al. (2017) [URL_CITE] incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing perfor-mance.\nFootnote or Reference text (if exists): Dat Quoc Nguyen, Mark Dras, and Mark John-son. 2017. A novel neural network model for joint pos tagging and graph-based dependency pars-ing. In CoNLL 2017 Shared Task: Multilin-gual Parsing from Raw Text to Universal Depen-dencies. Association for Computational Linguis-tics, pages 134\u2013142. https://doi.org/10.18653/v1/K17-3014.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.amap.com\nCitation Context: How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked. Thus we use wealthy geographic information about or-ganizations and locations in electronic maps (e.g., Amap [URL_CITE] ) to complete the KG.\nFootnote or Reference text (if exists): 5 https://www.amap.com\nSection Titles (if exists):  ['2 Knowledge Graph Constructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nCitation Context: This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article\u2019s text. [URL_CITE] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity.\nFootnote or Reference text (if exists): 2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nSection Titles (if exists):  ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: We used the English and Chinese parsers in Petrov and Klein (2007) [URL_CITE] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).\nFootnote or Reference text (if exists): 5 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ictclas.org/\nCitation Context: (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) [URL_CITE] with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees.\nFootnote or Reference text (if exists): 1 http://ictclas.org/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nCitation Context: We perform 5-shot KG completion task for all the methods. Our implementation for KG embedding baselines is based on OpenKE [URL_CITE] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature. During training, all triples in back-ground KG G 0 and training set, as well as few-shot reference triples of validation and testing set are used to train models.\nFootnote or Reference text (if exists): 2 https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nSection Titles (if exists):  ['5 Experiments', '5.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ownthink.com\nCitation Context: There are four types of personal information in a Chinese loan application form: \u201cSchool\u201d, \u201cCom-pany\u201d, \u201cResidence\u201d and \u201cBirthPlace\u201d. To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG [URL_CITE] and crawl triplets that are directly related to them. How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked.\nFootnote or Reference text (if exists): 3 https://www.ownthink.com\nSection Titles (if exists):  ['2 Knowledge Graph Constructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/N10-1138\nCitation Context: Key PCG factors for 2013: The causal links in PCG are more generic (Table 2) than the ones described in CGRAPH, supporting the hypothesis that latent word relationships do exist that go be-yond the scope of a single news article. The nodes of CGRAPH are tuples extracted from a seman-tic parser (SEMAFOR (Das et al., 2010) [URL_CITE] ) based on evidence of causality in a sentence. PCG poses no such restriction and derives topical (unfriended, FB) and inter-topical (healthcare, AMZN), sparse, latent and semantic relationships.\nFootnote or Reference text (if exists): Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Human Language Tech-nologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Com-putational Linguistics, Los Angeles, California, 948\u2013956. https://www.aclweb.org/anthology/N10-1138\nSection Titles (if exists):  ['5 Results', '5.2 Prediction Performance of PCG']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ananthnyu/faithful-causal-rep/\nCitation Context: We see that in the cosine-similarity based RoBERTa, the causal word pairs had a mean distance of 0.48, while in the quasi-pseudo metric based Faithful-RoBERTa, the mean distance between the causal word pairs reduced to 0.28. The distances are nor-malized between 0 and [URL_CITE] based on the maximum and minimum values of distances (cosine or d M ) in the sampled word-pairs.\nFootnote or Reference text (if exists): 1 https://github.com/ananthnyu/faithful-causal-rep/ and Antal van den Bosch. 2007. ILK: Machine learning of semantic relations with shallow features and almost no data. In Proceedings of the Fourth\nSection Titles (if exists):  ['5 Results', '5.3 Re-alignment towards causation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepmipt/ner\nCitation Context: We model the slot tagging task as sequence tag-ging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of re-spective tags (Lample et al., 2016). [URL_CITE] The output layer of the tagger network gives softmax proba-bility distributions over possible tags. To further increase recall, we add an inference-time rule \u2013 if slot candidate is split \u2013 it is just ranked for relevance multiple times (with respect to multiple contexts).\nFootnote or Reference text (if exists): 7 https://github.com/deepmipt/ner\nSection Titles (if exists):  ['3 Method', '3.3 Slot Tagger Model Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.darpa.mil/ipto/programs/gale/index.htm\nCitation Context: It is beneficial to design a framework that combines the decoding strategies of multiple systems as well as their out-puts and produces translations better than any single system output. More recently, within the GALE [URL_CITE] project, multiple MT systems have been developed in each consortium, thus system combination be-comes more important.\nFootnote or Reference text (if exists): 1 http://www.darpa.mil/ipto/programs/gale/index.htm\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.python.org/3/library/difflib.html\nCitation Context: To restrain the vocab-ulary size and correct the typos, we use a default vocabulary of fixed size 42K words from spaCy. Each word in the dataset is then compared with the vocabulary using the difflib library [URL_CITE] in Python (algorithm based on the Levenshtein distance), and mapped to the most similar word in the vocabu-lary. If no word with more than 90% of similarity is found, the word is considered a rare word or a typo, and is mapped to the out-of-vocabulary (OOV) word.\nFootnote or Reference text (if exists): 2 https://docs.python.org/3/library/difflib.html\nSection Titles (if exists):  ['4 Model Training', '4.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-06826\nCitation Context: Propensity score based matching aims to control for most confounding variables by using an ex-ternal method for estimating and controlling for the likelihood of outcomes (Olteanu et al., 2017). More recently, (Wang and Blei, 2018) [URL_CITE] showed that with multiple causal factors, it is possible to lever-age the correlation of those multiple causal fac-tors and deconfound using a latent variable model. This setting is similar to the one we consider, and is guaranteed to be truly causal if there is no con-founder which links a single cause and the out-come.\nFootnote or Reference text (if exists): Yixin Wang and David M. Blei. 2018. The Blessings of Multiple Causes. CoRR abs/1805.06826. http: //dblp.uni-trier.de/db/journals/ corr/corr1805.html#abs-1805-06826\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [URL_CITE] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nCitation Context: We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [URL_CITE] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison.\nFootnote or Reference text (if exists): 10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: For ACMLM, we build our model based on the BERT implementation from HuggingFace. [URL_CITE] We initialize our decoder using the pre-trained \u2018Bert-base\u2019 model and set the max sequence length to 30. We train the model for 5 epochs using Adam with learning rate 2e \u22125 .\nFootnote or Reference text (if exists): 7 https://github.com/huggingface/pytorch-pretrained- BERT\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert/blob/master/\nCitation Context: We experimented with two pre-trained BERT models: base (768 dimensions, layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). [URL_CITE] Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings.\nFootnote or Reference text (if exists): 9 https://github.com/google-research/bert/blob/master/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gucci-j/light-transformer-emnlp2021\nCitation Context: We implement our models using PyTorch (Paszke et al., 2019) and the transformers library (Wolf et al., 2020). We pretrain our models with two NVIDIA Tesla V100 (SXM2 - 32GB) and use one for fine-tuning. Our code is publicly available on GitHub: [URL_CITE] https://github.com/gucci-j/light-transformer-emnlp2021.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['C Experimental Setup', 'C.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nlpyang/PreSumm\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm [URL_CITE] . We train our model (with about 400M parameters) one day for 100,000 steps on GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps.\nFootnote or Reference text (if exists): 3 https://github.com/nlpyang/PreSumm\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.copyright.gov/title17/92chap1.html#107\nCitation Context: How-ever, you could claim the concept of fair use which allows the use of copyrighted material without per-mission from the copyright holder when it is used for research, scholarship, and teaching\u201d. We also consulted Section 107 [URL_CITE] of U.S. Copyright Act and ensured that our collection action fell under fair use category.\nFootnote or Reference text (if exists): 9 https://www.copyright.gov/title17/92chap1.html#107\nSection Titles (if exists):  ['8 Ethical Considerations', '8.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode\nCitation Context: This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, [URL_CITE] https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks. The data it-self can be found at http://www.cs.cmu.edu/ \u223c pmichel1/sated/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/\nCitation Context: This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks. The data it-self can be found at [URL_CITE] http://www.cs.cmu.edu/ \u223c pmichel1/sated/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/litbank\nCitation Context: In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at [URL_CITE] https://github.com/dbamman/litbank.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/VinAIResearch/BERTweet\nCitation Context: We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at: [URL_CITE] https://github.com/VinAIResearch/BERTweet.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://creativecommons.org/licenses/by-nc-sa/4.0/\nCitation Context: Copyright Act and ensured that our collection action fell under fair use category. We release our dataset under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [URL_CITE] .\nFootnote or Reference text (if exists): 10 https://creativecommons.org/licenses/by-nc-sa/4.0/\nSection Titles (if exists):  ['8 Ethical Considerations', '8.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://coretex.itc.it/\nCitation Context: Two other related FP5 IST projects are: C ORE - TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line. C ORETEX ( [URL_CITE] http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology. In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fb9-ti.uni-duisburg.de/alert\nCitation Context: The European project IST A LERT : Alert sys-tem for selective dissemination ( [URL_CITE] http://www.fb9-ti.uni-duisburg.de/alert) aims to associate state-of-the-art speech recognition with audio and video segmentation and automatic topic index-ing to develop an automatic media monitoring demonstrator and evaluate it in the context of real world applications. The targeted languages are French, German and Portuguese. Major media-monitoring companies in Europe are participating in this project.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lex4all.github.io/lex4all/\nCitation Context: This is the motivation behind lex4all, [URL_CITE] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.\nFootnote or Reference text (if exists): 1 http://lex4all.github.io/lex4all/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://i19pc5.ira.uka.de/rlat-dev\nCitation Context: CMUSphinx 3 ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit [URL_CITE] (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low-or zero-resource languages.\nFootnote or Reference text (if exists): 4 http://i19pc5.ira.uka.de/rlat-dev\nSection Titles (if exists):  ['2 Background and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cmusphinx.org\nCitation Context: Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx [URL_CITE] ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit 4 (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010).\nFootnote or Reference text (if exists): 3 http://www.cmusphinx.org\nSection Titles (if exists):  ['2 Background and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pc-erato2.iei.pi.cnr.it/echo\nCitation Context: In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions. The E CHO project ( [URL_CITE] http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives. The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://personalitycafe.com/forum\nCitation Context: We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurkovic\u0301 et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, [URL_CITE] where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts.\nFootnote or Reference text (if exists): 5 http://personalitycafe.com/forum\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/svcca\nCitation Context: Ecco is built on top of open source libraries including Scikit-Learn (Pedregosa et al., 2011), Matplotlib (Hunter, 2007), NumPy (Walt et al., 2011), PyTorch (Paszke et al., 2019) and Trans-formers (Wolf et al., 2020). Canonical Correla-tion Analysis is calculated using the code open-sourced [URL_CITE] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019).\nFootnote or Reference text (if exists): 4 https://github.com/google/svcca\nSection Titles (if exists):  ['6 System Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://archive.org/details/stackexchange\nCitation Context: StackExchange 2 is an online question answer-ing platform and has been used as a benchmark in previous work (dos Santos et al., 2015; Shah et al., 2018; Perkins and Yang, 2019). We took the June 2019 data dumps [URL_CITE] of the AskUbuntu and Su-perUser subdomains of the platform and combined them to form our dataset.\nFootnote or Reference text (if exists): 3 https://archive.org/details/ stackexchange\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/collaborations/past-projects/ace\nCitation Context: Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 2005 [URL_CITE] , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously over-looks the arguments-scattering challenge. In con-trast, EE on financial documents, such as ChFi-nAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event.\nFootnote or Reference text (if exists): 2 https://www.ldc.upenn.edu/collaborations/past-projects/ace\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf\nCitation Context: As an emerging research problem, text-based per-sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017 [URL_CITE] ; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).\nFootnote or Reference text (if exists): Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).\nSection Titles (if exists):  ['2 Related Work', '2.1 Personality Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/The_Big_Bang_Theory\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, [URL_CITE] available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 3 https://en.wikipedia.org/wiki/The_ Big_Bang_Theory\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.imsdb.com\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). [URL_CITE] We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 4 http://www.imsdb.com\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/\nCitation Context: We used the \u201cModApte\u201d split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987) [URL_CITE] . The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents D u ).\nFootnote or Reference text (if exists): Lewis, D. et al. (1987). Reuters-21578. http: //www.daviddlewis.com/resources/\nSection Titles (if exists):  ['5 Results', '5.1 Reuters-21578']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: We use the CQA-QL corpus from Subtask A of SemEval-2015 Task 3 on Answer Selection in CQA. The corpus contains data from the Qatar Living forum, [URL_CITE] and is publicly available on the task\u2019s website. The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads.\nFootnote or Reference text (if exists): 1 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Friends\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends [URL_CITE] and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 2 https://en.wikipedia.org/wiki/Friends\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We also extend the models in Kasai et al. (2017) [URL_CITE] and Lewis et al. (2016) by allowing highway con-nections between LSTM layers.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016) and Kasai et al. (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 512 units each.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016), Kasai et al. (2017) [URL_CITE] , and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017), and Fried-man et al. (2017) [URL_CITE] ; we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nCitation Context: We resolve spelling errors with a public spell checker [URL_CITE] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do.\nFootnote or Reference text (if exists): 4 https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/eladsegal/tag-based-multi-span-extraction\nCitation Context: Evaluation was performed with the official evaluation scripts of DROP and Q UOREF . Our full implementation can be found at [URL_CITE] https://github.com/eladsegal/tag-based-multi-span-extraction.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/SDLM-pytorch\nCitation Context: Experiments on language modeling and the downstream application of headline gener-ation demonstrate the significant effectiveness of SDLM. Source code and data used in the experiments can be accessed at [URL_CITE] https://github.com/thunlp/SDLM-pytorch.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/~aditya/g2p-tl-rr\nCitation Context: Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www.cs.toronto.edu/\u02dcaditya/ g2p-tl-rr\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://publish.illinois.edu/yirenwang/emnlp16source/\nCitation Context: All our implementations are based on Theano (Theano De-velopment Team, 2016) and run on one K40 GPU. All the source codes and datasets can be down-loaded at [URL_CITE] https://publish.illinois.edu/yirenwang/emnlp16source/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dolphin-zs/Doc2EDAG\nCitation Context: Ex-tensive experiments with comprehensive anal-yses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at [URL_CITE] https://github.com/dolphin-zs/Doc2EDAG.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [URL_CITE] ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 4 http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Vastlab/liblinear.git\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine [URL_CITE] , W-SVM and P I - SVM , and Exploratory learning ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 2 https://github.com/Vastlab/liblinear.git\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BorealisAI/de-simple\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE [URL_CITE] , TNTComplEx , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 5 https://github.com/BorealisAI/de-simple\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/CunchaoZ/CyGNet\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx , and CyGNet [URL_CITE] . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 7 https://github.com/CunchaoZ/CyGNet\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/tkbc\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx [URL_CITE] , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 6 https://github.com/facebookresearch/tkbc\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ljain2/libsvm-openset\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM [URL_CITE] , and Exploratory learning ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 3 https://github.com/ljain2/libsvm-openset\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/PepsiChallenge\nCitation Context: In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge [URL_CITE] , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola\u2019s branding has made it more popular. Even now, Microsoft uses similar blind taste tests to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing\u2019s results, Google\u2019s brand might have strengthened over the years.\nFootnote or Reference text (if exists): 1 http://en.wikipedia.org/wiki/Pepsi Challenge\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tinyurl.com/yblcb5q5\nCitation Context: From a fact-checkers\u2019 point of view, the abun-dance of previously fact-checked claims increases the likelihood that the next claim that needs to be checked would have been fact-checked already by some trusted organization. Indeed, viral claims of-ten come back after a while in social media, and politicians are known to repeat the same claims over and over again. [URL_CITE] Thus, before spending hours fact-checking a claim manually, it is worth first making sure that nobody has done it already.\nFootnote or Reference text (if exists): 7 President Trump has repeated one claim over 80 times: http://tinyurl.com/yblcb5q5.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/\nCitation Context: Social media platforms, particularly microblog-ging services such as Twitter, have become in-creasingly popular (Statista, 2019) [URL_CITE] as a means to express thoughts and opinions. Twitter users emit tweets about a wide variety of topics, which vary in the extent to which they reflect a user\u2019s person-ality, brand and interests. This observation mo-tivates the question we consider here, of how to quantify the degree to which tweets are character-istic of their author?\nFootnote or Reference text (if exists): Statista. 2019. Twitter: Num-ber 2010-2018.of active users (https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/).\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/\nCitation Context: Matching uses a binary index and is performed recursively starting at the top node in the query. TIGERSearch [URL_CITE] is associated with the German syntactic corpus TIGER. The tool is more typed than TGrep2 and allows search over discontinuous con-stituents that are common in German.\nFootnote or Reference text (if exists): 2 http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge [URL_CITE] , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bingiton.com/\nCitation Context: Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola\u2019s branding has made it more popular. Even now, Microsoft uses similar blind taste tests [URL_CITE] to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing\u2019s results, Google\u2019s brand might have strengthened over the years. These studies all suggest that brand and its associations play impor-tant roles in the customers\u2019 perceptions and deci-sions.\nFootnote or Reference text (if exists): 2 http://www.bingiton.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://git.io/JfV6J\nCitation Context: (2019) and parse test captions with Benepar (Kitaev and Klein, 2018). We use the same data preprocess-ing [URL_CITE] as in Shen et al. (2019) and Kim et al.\nFootnote or Reference text (if exists): 4 https://git.io/JfV6J.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/language/tree/master/language/realm\nCitation Context: All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM [URL_CITE] and the Routing Transformer. Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search).\nFootnote or Reference text (if exists): 19 https://github.com/google-research/language/tree/master/language/realm\nSection Titles (if exists):  ['7:453\u2013466.', 'A.1 Training & Model Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/svcca\nCitation Context: Ecco is built on top of open source libraries including Scikit-Learn (Pedregosa et al., 2011), Matplotlib (Hunter, 2007), NumPy (Walt et al., 2011), PyTorch (Paszke et al., 2019) and Trans-formers (Wolf et al., 2020). Canonical Correla-tion Analysis is calculated using the code open-sourced [URL_CITE] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019).\nFootnote or Reference text (if exists): 4 https://github.com/google/svcca\nSection Titles (if exists):  ['6 System Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sjcfr/CWVAE\nCitation Context: Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at [URL_CITE] https://github.com/sjcfr/CWVAE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bitbucket.org/tclup/alto\nCitation Context: Furthermore, chart constraints can be combined effectively with coarse-to-fine parsing for PCFGs (for an overall speedup of 70x) and supertagging for TAG (overall speedup of 124x), all while improving the accuracy over those of the baseline parsers. Our code is part of the Alto parser (Gontrum et al., 2017), available at [URL_CITE] http://bitbucket.org/tclup/alto.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NP-NET-research/Recursive-Semi-Markov-Model\nCitation Context: Our code is released at [URL_CITE] https://github.com/NP-NET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code and Tool)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/\nCitation Context: Our model is initialized using a pre-trained BERT model [URL_CITE] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.\nSection Titles (if exists):  ['3 Method', '3.6 Training Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.yelp.com/datasetchallenge\nCitation Context: Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs [URL_CITE] epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02.\nFootnote or Reference text (if exists): 4 http://www.yelp.com/dataset challenge\nSection Titles (if exists):  ['6 Experiment', '6.1 Dataset and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: \u2022 Lang, SeqAE: Pretrained with the language model and autoencoder model proposed in (Dai and Le, 2015) [URL_CITE] . The language model is the de-coder part of the encoder-decoder model using a zero vector as the initial hidden layer value, and the autoencoder model is the same structure of the encoder-decoder model, where input and output are the same.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['3 Experiments', '3.3 Compared Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/fnwkde/\nCitation Context: We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexical-semantic resources. The created resource is publicly available at [URL_CITE] http://www.ukp.tu-darmstadt.de/fnwkde/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isw3.naist.jp/IS/TechReport/report-list.html#2009\nCitation Context: For the description of features used in our adap-tation of the Shimbo-Hara model to Japanese, see (Okuma et al., 2009) [URL_CITE] . In this model, all features are defined as indicator functions asking whether one or more attributes (e.g., surface form, part-of-speech) take specific values at the neighbor of an arc. One example of a feature assigned to a diag-onal arc at row i and column j of the alignment graph is where POS [i] denotes the part-of-speech of the ith word in a sentence.\nFootnote or Reference text (if exists): H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.\nSection Titles (if exists):  ['2 Alignment-based coordinate structure analysis', '2.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://joshua-decoder.com/language-packs/paraphrase/\nCitation Context: This tool can be used to demonstrate and test a model or to hand-tune the model in order to de-termine the parameters for a configuration file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the config-uration file, are available at the language pack home page: [URL_CITE] http://joshua-decoder.com/language-packs/paraphrase/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Interactive tool']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://about.twitter.com/company\nCitation Context: With the rise in popularity of social media, mes-sage broadcasting sites such as Twitter and other microblogging services have become an important means of communication, with an estimated 500 million tweets being written every day [URL_CITE] . In addi-tion to individual users, various organizations and public figures such as newspapers, government officials and entertainers have established them-selves on social media in order to disseminate in-formation or promote their products.\nFootnote or Reference text (if exists): 1 https://about.twitter.com/company\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://slack.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack [URL_CITE] and Whatsapp , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 1 https://slack.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.whatsapp.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [URL_CITE] , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 2 https://www.whatsapp.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://mashable.com/article/snopes-stops-fact-checking-for-facebook/\nCitation Context: On the one hand, it is un-likely fact-checkers are able to check every social media post, due to limited resources and the sheer volume of data. [URL_CITE] On the other hand, a large num-ber of false claims, likely much more than those in traditional media, are being spread through so-cial media. This can be due to the compounded effect of several factors: social media platforms have become increasingly important to public fig-ures and organizations in engaging with voters and citizens; mobile devices have brought an age in which sharing and disseminating information is easy for anyone, including both malicious and un-intentional creators of falsehoods; the falsehoods are further replicated and amplified by social me-dia bots and clickbait articles.\nFootnote or Reference text (if exists): 1 https://mashable.com/article/snopes-stops-fact-checking-for-facebook/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Support"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/J07-4004\nCitation Context: In the CCG parsing literature, some work op-timizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007 [URL_CITE] ; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric.\nFootnote or Reference text (if exists): Stephen Clark and James R. Curran. 2007. Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Lin-guistics, Volume 33, Number 4, December 2007 http://aclweb.org/anthology/J07-4004.\nSection Titles (if exists):  ['7 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) [URL_CITE] and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016 [URL_CITE] ), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017) [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Next, we investigate the effect of initialization on the training of C -REALM. Unlike Jernite (2020) [URL_CITE] who initialize their model with BERT, before train-ing we initialize our retriever with a pretrained self-supervised retriever. As a baseline, we initial-ize our model with ICT, a weaker self-supervised retriever introduced in Lee et al.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/google-research/tree/master/routing_transformer\nCitation Context: Our imple-mentations are based on the open-source codebases of REALM and the Routing Transformer. [URL_CITE] Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search). Since our retriever is frozen, we do not use the document index refresher available in their codebase.\nFootnote or Reference text (if exists): 20 https://github.com/google-research/google-research/tree/master/routing_ transformer REALM predicted the correct retrieval. Notice very small differences in generation quality (R-L) as well as the fraction of n-grams (n-g) in the generation overlap-ping with retrievals predicted by C -REALM (vs pre-dicted retr.). To control for overlap due to stopwords, we also add n-gram overlaps with the randomly sam-pled retrievals. ble 13, notice very little difference with and without retrieval. no effect other than being a symbol of an economic entity or symbol of an era or location, and nothing more. For example look at Sears, Sears didn\u2019t care what went on inside, it was all about the _appearance_ of its location, the prestige of the location, the facilities and so on. It didn\u2019t care about how long it took it to operate, it was about how much people would pay to go see it. Sears was a landmark as a cultural movement and other big companies followed suit, so if you want to see a building you\u2019ve never seen before, you have to go see Sears, just like you have to see a Toyota Camry for Toyota Camry. They used to be all about building new factories, some of them if I recall, but now that they\u2019re bigger, that means that more factory jobs are coming to them. You\u2019ve probably seen them in stores as stores where people buy and sell stuff, so there aren\u2019t that many places for them to come from. Instead, it\u2019s just for show, a symbol of rich people.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.1 Training & Model Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/language/tree/master/language/realm\nCitation Context: All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM [URL_CITE] and the Routing Transformer. Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search).\nFootnote or Reference text (if exists): 19 https://github.com/google-research/language/tree/master/language/realm\nSection Titles (if exists):  ['7:453\u2013466.', 'A.1 Training & Model Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: We perform controlled exper-iments initializing our retrievers with REALM-CCNews (Guu et al., 2020) and varying batch size and keeping all other hyperparameters consistent. In Table 8, we notice a steady increase in perfor-mance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) [URL_CITE] from 512 to 1024. Finally, in pre-liminary experiments we saw no benefit of more intelligent negative sampling schemes.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [URL_CITE] . This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning (Chen et al., 2020) and dense retriever training (Karpukhin et al., 2020). Scaling up from Jernite (2020) [URL_CITE] , who used a mini-batch size of 512 and initialized their retriever with BERT, we use much large mini-batches of size 12,288 (and hence, many more negative examples) and initial-ize our retriever with a strong pretrained retriever, the REALM model (Guu et al., 2020) trained on the Common Crawl News (CC-News) corpus. These design decisions greatly improve retriever qual-ity, as we observe in an ablation study (see Ap-pendix A.2).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: \u2022 Lang, SeqAE: Pretrained with the language model and autoencoder model proposed in (Dai and Le, 2015) [URL_CITE] . The language model is the de-coder part of the encoder-decoder model using a zero vector as the initial hidden layer value, and the autoencoder model is the same structure of the encoder-decoder model, where input and output are the same.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['3 Experiments', '3.3 Compared Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cl-tohoku/bert-japanese\nCitation Context: We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [URL_CITE] We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set.\nFootnote or Reference text (if exists): 3 We adopted pretrained character-based BERT which is known to be suitable for processing Japanese texts. This is available at https://github.com/cl-tohoku/bert-japanese. for Low-Resource NLP, pages 175\u2013182.\nSection Titles (if exists):  ['5 Experiments', '5.2 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf\nCitation Context: Atten-tion distribution over the visual input is computed from the last convolutional layer of the network. The decoder is an RNN with 500 conditional GRU units (Firat and Cho, 2016) [URL_CITE] in the recurrent layer. We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.\nFootnote or Reference text (if exists): Orhan Firat and Kyunghyun Cho. 2016. Con-ditional gated recurrent unit with attention mechanism. https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf. Published online, version adbaeea.\nSection Titles (if exists):  ['4 Experiments', '4.1 Multimodal Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\nCitation Context: The encoder-decoder model is a conditional lan-guage model that predicts a correct output se-quence from an input sequence (Sutskever et al., 2014) [URL_CITE] . This model consists of two RNNs: an en-coder and decoder. The encoder extracts a context of the input sequence as a real-valued vector, and the decoder predicts the output sequences from the context individually.\nFootnote or Reference text (if exists): Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural In-formation Processing Systems 27 (NIPS 2014), Curran Associates, Inc., pages 3104\u20133112. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salesforce/cove\nCitation Context: Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings. We concatenate a pre-trained 600-dimensional CoVe vectors [URL_CITE] (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the fi-nal input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer. To reduce the parameter size, we use a maxout layer (Goodfellow et al., 2013) at each BiLSTM layer to shrink its dimension.\nFootnote or Reference text (if exists): 1 https://github.com/salesforce/cove\nSection Titles (if exists):  ['2 Proposed model: SAN']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-dl/dl4mt-tutorial\nCitation Context: \u2022 RNNSearchtentional NMT: a re-implementationsystem (Bahdanauof theet alat-., 2015) with slight changes taken from dl4mt tutorial. [URL_CITE] For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by (Cho et al., 2014b). It incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the lastly gen-erated word.\nFootnote or Reference text (if exists): 8 https://github.com/nyu-dl/ dl4mt-tutorial\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Document)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/\nCitation Context: A Replication Instructions Dataset The E2E dataset contains a training set of 42,061 pairs of meaning representations and hu-man authored utterances, 4,672 pairs in the devel-opment set, and 4,693 in the test set. Download the dataset from [URL_CITE] https://github.com/tuetschek/ e2e-dataset .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ckip.iis.sinica.edu.tw/DOMCAT/\nCitation Context: In this paper, we propose a web-based bilingual concordancer, DOMCAT [URL_CITE] , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents.\nFootnote or Reference text (if exists): 1 http://ckip.iis.sinica.edu.tw/DOMCAT/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.863data.org.cn/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation , and HTRDP Evaluation [URL_CITE] were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 3 http://www.863data.org.cn/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/index.htm\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation [URL_CITE] , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt/index.htm\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.slt.atr.jp/IWSLT2006/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation [URL_CITE] , NIST Machine Translation Evaluation , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 1 http://www.slt.atr.jp/IWSLT2006/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aka.ms/WikiQA\nCitation Context: We select WikiQA [URL_CITE] as the evaluation data, as it is precisely constructed based on natural language questions and Wikipedia documents, which con-tains 2,118 \u2018question-document\u2019 pairs in the train-ing set, 296 \u2018question-document\u2019 pairs in devel-opment set, and 633 \u2018question-document\u2019 pairs in testing set. Each sentence in the document of a given question is labeled as 1 or 0, where 1 de-notes the current sentence is a correct answer sen-tence, and 0 denotes the opposite meaning.\nFootnote or Reference text (if exists): 6 http://aka.ms/WikiQA\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/emoji\nCitation Context: We to-kenize those English Tweets using \u201cTweetTo-kenizer\u201d from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). [URL_CITE] We also normal-ize the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. We filter out retweeted Tweets and the ones shorter than 10 or longer than 64 word tokens.\nFootnote or Reference text (if exists): 2 https://pypi.org/project/emoji\nSection Titles (if exists):  ['2 BERTweet', 'Pre-training data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lxml/lxml\nCitation Context: 1. We use lxml [URL_CITE] to extract the plain text from the original eBook EPUB. We then break up paragraphs into sentences and create a map-ping between sentence identifiers and token indices where sentences start and end.\nFootnote or Reference text (if exists): 6 https://github.com/lxml/lxml\nSection Titles (if exists):  ['3 My Turn To Read App', '3.1 Read Aloud eBooks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://gitlab.freedesktop.org/poppler/poppler\nCitation Context: In terms of whether the field is rapidly growing, we note that there was a year-on-year increase of 42% between in 2017\u20132018 due to the increase in the number of papers published at NAACL and EMNLP, and a 34% increase between 2018\u20132019. Extracting citations To extract a list of refer-ences from an article, we first extract the text stream from the PDF file via pdftotext, [URL_CITE] then feed it into ParsCit (Councill et al., 2008) to obtain the references. For each reference in this list, we then extract and keep the parsed \u201cdate\u201d, \u201cauthor\u201d, and \u201ctitle\u201d entries.\nFootnote or Reference text (if exists): 2 https://gitlab.freedesktop.org/poppler/poppler\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/fact-check-ratings/\nCitation Context: We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [URL_CITE] ).\nFootnote or Reference text (if exists): 13 http://www.snopes.com/fact-check-ratings/\nSection Titles (if exists):  ['4 Datasets', '4.2 Snopes Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/newspaper\nCitation Context: We deduplicated the 16,000 extracted URLs into 6,003 unique addressed, then extracted and preprocessed their contents. The newspaper package [URL_CITE] was used to extract article text and the title from the web page. Since we are interested in text articles that can serve as the source text for summarization algorithms, we needed to re-move photos and video links such as those from Instagram and YouTube.\nFootnote or Reference text (if exists): 2 https://pypi.python.org/pypi/newspaper\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2017). For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) [URL_CITE] during the training phase, yielding the success in the analogy task. For example, a transitive verb su-pertag whose object has been filled by substitution should be treated by the parser in the same way as an intransitive verb supertag.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We next analyze the induced vector representa-tions in the output projection matrices of our su-pertagger and joint parsers using the syntactic analogy framework (Kasai et al., 2017) [URL_CITE] . Consider, for instance, the analogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a subject relative clause headed by a tran-sitive verb (t99) is to a subject relative headed by an intransitive verb (t109).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] . For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) during the training phase, yielding the success in the analogy task.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (both Supplement and Method)\nTYPE: Paper and Tool\nFUNCTION: Introduce, Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nCitation Context: All MT training experiments made use of an Arabic-English corpus of approximately 200 mil-lion tokens (English side). Most of the collections in this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly part of the resources specified for the constrained data track of the NIST MT evaluation [URL_CITE] .\nFootnote or Reference text (if exists): 4 For a list of the NIST MT09 constrained train-ing condition resources, see http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcg-www.uia.ac.be/conll2000/chunking\nCitation Context: A standard software program has been provided (which is available from [URL_CITE] http://lcg-www.uia.ac.be/conll2000/chunking) to compute the performance of each algorithm. For each chunk, three figures of merit are computed: precision (the percentage of detected phrases that are correct), recall (the percentage of phrases in the data that are found), and the \u201eL nL% metric the recall.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 CoNLL-2000 chunking task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bsalehi/wiktionary_MWE_compositionality\nCitation Context: We have proposed an unsupervised approach for predicting the compositionality of an MWE rel-ative to each of its components, based on lexi-cal overlap using Wiktionary, optionally incorpo-rating synonym and translation data. Our experi-ments showed that the various instantiations of our approach are superior to previous state-of-the-art supervised methods. All code to replicate the re-sults in this paper has been made publicly avail-able at [URL_CITE] https://github.com/bsalehi/wiktionary_MWE_compositionality.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/itl/iad/mig/openmt12.cfm\nCitation Context: We carried out experiments in two different settings, both involving data from NIST Open MT 2012. [URL_CITE] The first setting uses data from the Chinese to En-glish constrained track, comprising 283M English tokens. We manually identified 14 sub-corpora on the basis of genres and origins.\nFootnote or Reference text (if exists): 2 http://www.nist.gov/itl/iad/mig/openmt12.cfm\nSection Titles (if exists):  ['4 Experiments', '4.1 Data setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/multimodal-task.html\nCitation Context: To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [URL_CITE] . The best results of NMT model were those of Huang et al.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/multimodal-task.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lsi.upc.edu/~nlp/IQMT\nCitation Context: This approach has proven very effective in recent eval-uation campaigns. Metric computation has been carried out using the IQ MT Framework for Auto-matic MT Evaluation (Gime\u0301nez, 2007) [URL_CITE] . The sim-plicity of this approach (with no training of the metric weighting scheme) ensures that the poten-tial advantages detected in our experiments are not due to overfitting effects.\nFootnote or Reference text (if exists): 1 http://www.lsi.upc.edu/\u02dcnlp/IQMT\nSection Titles (if exists):  ['3 Metrics and Test Beds', '3.1 Metric Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service [URL_CITE] (with reported BLEU scores of 37.16 ( BR -\nFootnote or Reference text (if exists): 11 https://aws.amazon.com/translate\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mjpost/sacrebleu\nCitation Context: PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the [URL_CITE] ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b).\nFootnote or Reference text (if exists): 3 https://github.com/mjpost/sacrebleu\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kpu/kenlm\nCitation Context: PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). [URL_CITE] Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b).\nFootnote or Reference text (if exists): 9 https://github.com/kpu/kenlm\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/awslabs/mlm-scoring\nCitation Context: (2020) for computing pseudo-likelihood. [URL_CITE] PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects.\nFootnote or Reference text (if exists): 8 https://github.com/awslabs/ mlm-scoring\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt{11,12}/results.html\nCitation Context: In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [URL_CITE] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt{11,12}/results.html\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PhilipMay/stsb-multi-mt\nCitation Context: Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS [URL_CITE] and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -\nFootnote or Reference text (if exists): 10 https://github.com/PhilipMay/ stsb-multi-mt\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/glample/fastBPE\nCitation Context: Here we describe experimental details for un-supervised NMT setup. We use the English tokenizer from Moses (Koehn et al., 2007), and use FastBPE ( [URL_CITE] https://github.com/glample/fastBPE) to split into subword units, with a vocabulary size of 60000. The architec-ture uses a 4-layer transformer encoder and 4-layer transformer decoder, where one layer is language specific for both the encoder and decoder, the rest are shared.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: bert-base-uncased). [URL_CITE] We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4.\nFootnote or Reference text (if exists): 2 We use the BERT implementation from https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huashiyiqike/TMBP/tree/master/DocNADE\nCitation Context: \u2022 DocNADE (Larochelle and Lauly, 2012b). An unsupervised neural network topic model of documents and has shown that it is a com-petitive model both as a generative model and as a document representation learning algo-rithm [URL_CITE] . In DocNADE, the hidden size is 50, the learning rate is 0.0004 , the bath size is 64 and the max training number is 50000.\nFootnote or Reference text (if exists): 6 https://github.com/huashiyiqike/TMBP/tree/master/DocN ADE\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\nCitation Context: The encoder-decoder model is a conditional lan-guage model that predicts a correct output se-quence from an input sequence (Sutskever et al., 2014) [URL_CITE] . This model consists of two RNNs: an en-coder and decoder. The encoder extracts a context of the input sequence as a real-valued vector, and the decoder predicts the output sequences from the context individually.\nFootnote or Reference text (if exists): Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural In-formation Processing Systems 27 (NIPS 2014), Curran Associates, Inc., pages 3104\u20133112. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/redpony/cdec\nCitation Context: \u2022 cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://github.com/redpony/cdec\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marian-nmt/marian-examples/blob/master/transformer\nCitation Context: Transformer We used the same training, dev and test sets as in the big RNN model, and the exam-ple configuration for a transformer model from Marian [URL_CITE] adapted to our needs as shown in Table 1. This configuration utilises a six-layer deep encoder and decoder, learning rate warm-up and tied em-beddings for source, target and output layer.\nFootnote or Reference text (if exists): 2 https://github.com/marian-nmt/marian-examples/blob/master/transformer\nSection Titles (if exists):  ['4 Experiment setup', '4.3 Training setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dl.acm.org/citation.cfm?id=1557769.1557821\nCitation Context: Associa-tion for Computational Linguistics, Strouds-burg, PA, USA, ACL \u201907, pages 177\u2013 180. [URL_CITE] http://dl.acm.org/citation.cfm?id=1557769.1557821.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1073083.1073126\nCitation Context: Association for Compu-tational Linguistics, Philadelphia, Pennsylva-nia, USA, pages 255\u2013262. [URL_CITE] https://doi.org/10.3115/1073083.1073126.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1220175.1220321\nCitation Context: Association for Computational Linguistics, Sydney, Australia, pages 1161\u2013 1168. [URL_CITE] https://doi.org/10.3115/1220175.1220321.2016. Word alignment without null words.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P14-1006\nCitation Context: Association for Compu-tational Linguistics, Baltimore, Maryland, pages 58\u201368. [URL_CITE] http://www.aclweb.org/anthology/P14-1006.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1118905.1118906\nCitation Context: Computational Linguistics, Stroudsburg, PA, USA, HLT-NAACL-PARALLEL \u201903, pages 1\u201310. [URL_CITE] https://doi.org/10.3115/1118905.1118906.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crisisnlp.qcri.org/\nCitation Context: To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, [URL_CITE] to-gether with two resources from CrisisLex. Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex.\nFootnote or Reference text (if exists): 2 https://crisisnlp.qcri.org/\nSection Titles (if exists):  ['3 Aggregated Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/commonsense/conceptnet5/wiki/Downloads\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [URL_CITE] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 3 https://github.com/commonsense/conceptnet5/wiki/Downloads\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Websail-NU/CODAH\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [URL_CITE] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 4 We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the first word piece as the whole word representation.\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [URL_CITE] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 3 We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crisislex.org/data-collections.html\nCitation Context: To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, to-gether with two resources from CrisisLex. [URL_CITE] Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex. The original classes in each resource, together with the mapping to the new classes included in our data set, can be seen in Table 1.\nFootnote or Reference text (if exists): 3 https://crisislex.org/data-collections.html\nSection Titles (if exists):  ['3 Aggregated Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JiaweiSheng/FAAN\nCitation Context: Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at [URL_CITE] https://github.com/JiaweiSheng/FAAN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/contrast-sets\nCitation Context: We evaluate our model on four contrastive sets: IMDB, PERSPECTRUM, BoolQ and SNLI, which were provided by Contrast Sets [URL_CITE] (Gardner et al., 2020). We compare our approach with BERT and\nFootnote or Reference text (if exists): 3 https://github.com/allenai/ contrast-sets\nSection Titles (if exists):  ['4 Experiments', '4.3 Experiments on Contrastive Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2020-confidence-regularization\nCitation Context: We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/UKPLab/acl2020-confidence-regularization\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nCitation Context: This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article\u2019s text. [URL_CITE] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity.\nFootnote or Reference text (if exists): 2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nSection Titles (if exists):  ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/glosstag.shtml\nCitation Context: Let L p be the set containing p and all the Wikipedia pages hav-ing an outgoing link to p, and R s be the set con-sisting of s and all other synsets that are in its di-rect neighbourhood. We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [URL_CITE] . Let B be a function mapping each WordNet synset s 0 to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise.\nFootnote or Reference text (if exists): 2 http://wordnet.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['2 Semantic Representation of Concepts', '2.1 Collecting contextual information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TemporalKGTeam/TANGO\nCitation Context: We evaluate our model by performing future link prediction on five tKG datasets [URL_CITE] . We compare TANGO\u2019s performance with several existing meth-ods and evaluate its potential with inductive link prediction and long horizontal link forecasting.\nFootnote or Reference text (if exists): 1 Code and datasets are available at https://github.com/TemporalKGTeam/TANGO. filtered MRR (%) on ICEWS05-15 with regard to dif-ferent \u2206t. preprint arXiv:2012.08492.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Method and Material)\nTYPE: Mixed (Code and DataSource)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our SMT implementation is based on Moses [URL_CITE] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure.\nFootnote or Reference text (if exists): 10 http://www.statmt.org/moses/\nSection Titles (if exists):  ['5 Experiments and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Coverage measures the ratio of source words being aligned by any target words (Tu et al., 2016). Firstly, we trained an alignment model on the authentic parallel data by fast-align [URL_CITE] . Then we used the alignment model to force-align the mono-lingual sentences and the synthetic target sentences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/fast_align\nSection Titles (if exists):  ['A Appendix', 'A.2 Linguistic Properties']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq\nCitation Context: Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq [URL_CITE] for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018).\nFootnote or Reference text (if exists): 11 https://github.com/pytorch/fairseq\nSection Titles (if exists):  ['5 Experiments and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy [URL_CITE] library to annotate the English source sentences. We then use the fast_align tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data.\nFootnote or Reference text (if exists): 14 https://spacy.io/\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/ERNIE/tree/repro\nCitation Context: ERNIE baselines. E RNIE [URL_CITE] (Sun et al., 2019a,b) enhances B ERT through knowledge integration using a entity-level masked LM task and more raw text from the Web resources, which achieves the currently best results on Chinese NER. E RNIE +F UR +E NT is a stronger baseline, which uses the same entity dictionary as ours for entity-level masking and further pre-trains E RNIE on the same raw text as ours.\nFootnote or Reference text (if exists): 5 https://github.com/PaddlePaddle/ERNIE/tree/repro\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sjcfr/CWVAE\nCitation Context: Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at [URL_CITE] https://github.com/sjcfr/CWVAE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pku-wuwei/QCN\nCitation Context: \u2022 Our proposed Question Condensing Net-works (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effec-tiveness of our model. [URL_CITE]\nFootnote or Reference text (if exists): 3 An implementation of our model is available at https: //github.com/pku-wuwei/QCN.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ChenWu98/Point-Then-Operate\nCitation Context: Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/trangvu/alil-dream\nCitation Context: Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [URL_CITE] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/trangvu/alil-dream\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jiachenwestlake/Entity_BERT\nCitation Context: Results on the three datasets show that our method outperforms these methods and achieves the best results, which demonstrates the effec-tiveness of the proposed Char-Entity-Transformer structure for integrating entity information in LM pre-training for Chinese NER. To our knowl-edge, we are the first to investigate how to make use of the scale of the input document text for enhancing NER. Our code and NER datasets are released at [URL_CITE] https://github.com/jiachenwestlake/Entity_BERT.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Method and Material)\nTYPE: Code, Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/yamcha/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.cornell.edu/people/pabo/movie-review-data/\nCitation Context: Pang and Lee (2004) demonstrate that the subjec-tivity detection can be a useful input for a sen-timent classifier. They compose a publicly avail-able dataset [URL_CITE] of 5000 subjective and 5000 objec-tive sentences, classifying them with a reported accuracy of 90-92% and further show that predict-ing this information improves the end-level sen-timent classification on a movie review dataset. Kim (2014) and Wang and Manning (2013) fur-ther improve the performance through different machine learning methods.\nFootnote or Reference text (if exists): 9 https://www.cs.cornell.edu/people/pabo/movie-review-data/\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.3 Subjectivity Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2015/task3/\nCitation Context: The corpus contains data from the Qatar Living forum, and is publicly available on the task\u2019s website. [URL_CITE] The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads. Each question, and each answer, consist of a short title and a more de-tailed description.\nFootnote or Reference text (if exists): 2 http://alt.qcri.org/semeval2015/task3/\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xpqiu/fnlp/\nCitation Context: As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [URL_CITE] is used for Chinese word segmentation.\nFootnote or Reference text (if exists): 7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [URL_CITE] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence. The GVDB is the result of a large crowdsourced annotation effort. This an-notation is ongoing, and the GVDB will be regularly updated with new data and new layers of annotation, making it an interesting and challenging data set on which to evaluate state-of-the-art NLP tools.\nFootnote or Reference text (if exists): 2 http://gun-violence.org/\nSection Titles (if exists):  ['3 The Gun Violence Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/~tpederse/WSDTutorial.html\nCitation Context: Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005) [URL_CITE] . The hit pages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of a classifier. Features used in classification will be explained in the latter half of this subsection.\nFootnote or Reference text (if exists): Ted Pedersen and Rada. F. Mihalcea, \u201cAdvances in Word Sense Disambiguation,\u201d tutorial at ACL 2005. http://www.d.umn.edu/~tpederse/WSDTutorial.html.\nSection Titles (if exists):  ['2 The proposal']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~cyl/ROUGE\nCitation Context: For evaluation, we used the new automatic sum-mary evaluation metric, ROUGE [URL_CITE] , which was used for the first time in DUC 2004. ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence.\nFootnote or Reference text (if exists): 1 http://www.isi.edu/\u02dccyl/ROUGE\nSection Titles (if exists):  ['4 Experiments on DUC 2004 data', '4.1 DUC 2004 data and ROUGE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bioai.lab.asu.edu/logicgridpuzzles\nCitation Context: This is the first learning system that can solve logic grid puzzles described in natu-ral language in a fully automated manner. The code and the data will be made pub-licly available at [URL_CITE] http://bioai.lab.asu.edu/logicgridpuzzles.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~yww/data/WeiboTreebank.zip\nCitation Context: \u2022 We present a freely available Chinese Weibo dependency treebank [URL_CITE] , manually annotated with more than 18,000 tokens;\nFootnote or Reference text (if exists): 3 http://www.cs.cmu.edu/\u02dcyww/data/WeiboTreebank.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Eventually, we would like to expand our data set and relations to cover proper nouns as well. We are hopeful that our current dataset and re-lation definitions, which will be made available via [URL_CITE] http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cis.temple.edu/~edragut/DebugSL.webm\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [URL_CITE] https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/complementizer/news-tls\nCitation Context: For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets. The dataset will be made available at [URL_CITE] https://github.com/complementizer/news-tls.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://accent.gmu.edu\nCitation Context: The theoretical and practical value of studying human accented speech is of interest to language teachers, linguists, and computational linguists. It is also part of the research program behind the Speech Accent Archive ( [URL_CITE] http://accent.gmu.edu) housed at George Mason University. The Archive is a growing database of English speech varieties that contains more than 1,100 samples of native and non-native speakers reading from the same English paragraph.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bethard.info/data/fables-100-temporal-dependency.xml\nCitation Context: Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyre-Lapata collection and measured inter-annotator agree-ment by Krippendorff\u2019s Alpha for nominal data (Krip-pendorff, 2004; Hayes and Krippendorff, 2007). For the resulting annotated corpus annotators achieved Alpha of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering rela-tion labels. Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [URL_CITE] could be used to train a temporal dependency parsing model.\nFootnote or Reference text (if exists): 2 Available from http://www.bethard.info/data/fables-100-temporal-dependency.xml\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://voice.etri.re.kr/db/dbpop.asp?code=88\nCitation Context: We annotated predicate-argument structure of verbs in a corpus from the Electronics and Telecommunications Research Institute of Korea (ETRI). [URL_CITE] Our corpus was developed over two years using a specialized annotation tool (Song et al., 2012), resulting in more than 8,000 semanti-cally annotated sentences. As much as possible, annotations followed the PropBank guidelines for English (Bonial et al., 2010).\nFootnote or Reference text (if exists): 1 http://voice.etri.re.kr/db/dbpop.asp?code=88\nSection Titles (if exists):  ['2 A Semantically Annotated Korean Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu\nCitation Context: We evaluate the models using a corpus of ESL es-says, annotated [URL_CITE] by native English speakers (Ro-zovskaya and Roth, 2010a). For each preposition (article) used incorrectly, the annotator indicated the correct choice.\nFootnote or Reference text (if exists): 7 The annotation of the ESL corpus can be downloaded from http://cogcomp.cs.illinois.edu.\nSection Titles (if exists):  ['3 Comparison of Algorithms', '3.1 Evaluation Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [URL_CITE] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus [URL_CITE] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 2 http://wacky.sslmit.unibo.it/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia and the British National Corpus [URL_CITE] . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 4 http://www.natcorp.ox.ac.uk/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/datasets\nCitation Context: Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library [URL_CITE] (Wolf et al., 2020b).\nFootnote or Reference text (if exists): 3 https://github.com/huggingface/ datasets\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.umiacs.umd.edu/~jbg/static/downloads_and_media.html\nCitation Context: We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. [URL_CITE]\nFootnote or Reference text (if exists): 6 We followed Pang and Lee\u2019s method for creating a nu-merical score between 0 and 1 from a star rating. We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not as-sume integer values). The German movie review corpus is available at http://www.umiacs.umd.edu/\u02dcjbg/static/downloads_and_media.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Sentiment Prediction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.filmrezension.de\nCitation Context: We gathered 330 film reviews from a German film review site (Vetter et al., 2000) [URL_CITE] and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. 6\nFootnote or Reference text (if exists): Tobias Vetter, Manfred Sauer, and Philipp Wallutat. 2000. Filmrezension.de: Online-magazin fu\u0308r filmkritik. http://www.filmrezension.de.\nSection Titles (if exists):  ['4 Experiments', '4.3 Sentiment Prediction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.cornell.edu/~ainur/data.html\nCitation Context: Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) [URL_CITE] , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) .\nFootnote or Reference text (if exists): 3 http://www.cs.cornell.edu/~ainur/data. html\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Datasets\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/datasets\nCitation Context: Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library 3 (Wolf et al., 2020b) [URL_CITE] .\nFootnote or Reference text (if exists): Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, Fran\u00e7ois Lagunas, Lysandre Debut, Morgan Funtow-icz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Mu\u0161tar, Jeff Boudier, and Anna Tordjmann. 2020b. Datasets. GitHub. Note: https://github.com/huggingface/datasets.\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.sunysb.edu/~junkang/connotation_wordnet\nCitation Context: ConnotationWordNet, the final outcome of our study, is a new lexical resource that has conno-tation labels over both words and senses follow-ing the structure of WordNet. The lexicon is pub-licly available at: [URL_CITE] http://www.cs.sunysb.edu/\u02dcjunkang/connotation_wordnet.) In what follows, we will first describe the net-work of words and senses (Section 2), then intro-duce the representation of the network structure as pairwise Markov Random Fields, and a loopy be-lief propagation algorithm as collective inference (Section 3).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.mpi-inf.mpg.de/~corrogg/\nCitation Context: The score of a cluster is the sum of the indi-vidual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected. Our source code and the results of our evaluation are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 10 http://people.mpi-inf.mpg.de/\u02dccorrogg/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.neo4j.org/\nCitation Context: We imple-mented this algorithm using the Neo4j library. [URL_CITE] We used a fixed threshold of 1 and vary the search depth in range 1\u201320. We used the candidate senses of all nouns and verbs in a sentence as context.\nFootnote or Reference text (if exists): 9 http://www.neo4j.org/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/scalanlp/breeze\nCitation Context: Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. [URL_CITE] Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework.\nFootnote or Reference text (if exists): 6 https://github.com/scalanlp/breeze\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/sw2v\nCitation Context: For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V [URL_CITE] (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelev-ina et al.\nFootnote or Reference text (if exists): 13 http://lcl.uniroma1.it/sw2v\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/lexicons/effectlexicon/\nCitation Context: +E FFECT (x) and -E FFECT (x): We use the +/-effect sense-level lexicon (Choi and Wiebe, 2014) [URL_CITE] to extract the +/-effect events in each sen-tence. The score of +E FFECT (x) is the fraction of that word\u2019s senses that are +effect senses accord-ing to the lexicon, and the score of -E FFECT (x) is the fraction of that word\u2019s senses that are -effect senses according to the lexicon. If a word does not appear in the lexicon, we do not treat it as a +/-effect event, and thus assign 0 to both +E FFECT (x) and -E FFECT (x).\nFootnote or Reference text (if exists): 7 Available at: http://mpqa.cs.pitt.edu/lexicons/effectlexicon/\nSection Titles (if exists):  ['4 PSL for Sentiment Analysis', '4.4 PSL Augmented with +/-Effect Events (PSL3)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/S1351324909990209\nCitation Context: the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009) [URL_CITE] , the systems were provided with the sources to verify against, instead of having to retrieve them.\nFootnote or Reference text (if exists): Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i\u2013xvii. https://doi.org/10.1017/S1351324909990209.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/fact-check-ratings/\nCitation Context: We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [URL_CITE] ).\nFootnote or Reference text (if exists): 13 http://www.snopes.com/fact-check-ratings/\nSection Titles (if exists):  ['4 Datasets', '4.2 Snopes Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge [URL_CITE] , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tinyurl.com/yblcb5q5\nCitation Context: From a fact-checkers\u2019 point of view, the abun-dance of previously fact-checked claims increases the likelihood that the next claim that needs to be checked would have been fact-checked already by some trusted organization. Indeed, viral claims of-ten come back after a while in social media, and politicians are known to repeat the same claims over and over again. [URL_CITE] Thus, before spending hours fact-checking a claim manually, it is worth first making sure that nobody has done it already.\nFootnote or Reference text (if exists): 7 President Trump has repeated one claim over 80 times: http://tinyurl.com/yblcb5q5.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: Multi-Domain Text Classification The MDTC task was first examined by Li and Zong (2008), who proposed to fuse the training data from multi-ple domains either at the feature level or the classi-fier level. The prior art of MDTC (Wu and Huang, 2015) [URL_CITE] decomposes the text classifier into a gen-eral one and a set of domain-specific ones. How-ever, the general classifier is learned by param-eter sharing and domain-specific knowledge may sneak into it.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://towcenter.org/research/lies-damn-lies-and-viral-content/\nCitation Context: It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by jour-nalists in the context of the Emergent Project (Sil-verman, 2015) [URL_CITE] , and the dataset was first proposed by Ferreira and Vlachos (2016), who only classi-fied the claim w.r.t. the article headline instead of the whole article.\nFootnote or Reference text (if exists): Craig Silverman. 2015. Lies, Damn Lies and Viral Content. http: //towcenter.org/research/ lies-damn-lies-and-viral-content/.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nCitation Context: We implement our models using the jiant toolkit, [URL_CITE] which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 3 https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://beaker.org/\nCitation Context: We supervise the Sentence Relevance module with a binary cross entropy loss for predicting these relevance labels when available. We used PyTorch (Paszke et al., 2017) and AllenNLP to implement our models and ran them on Beaker [URL_CITE] . For pre-training we use the same hyper-parameters of ESIM(Chen et al., 2017) as available in implementation of AllenNLP (Gardner et al., 2017) and fine-tune the model pa-rameters.\nFootnote or Reference text (if exists): 7 https://beaker.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gucci-j/light-transformer-emnlp2021\nCitation Context: We implement our models using PyTorch (Paszke et al., 2019) and the transformers library (Wolf et al., 2020). We pretrain our models with two NVIDIA Tesla V100 (SXM2 - 32GB) and use one for fine-tuning. Our code is publicly available on GitHub: [URL_CITE] https://github.com/gucci-j/light-transformer-emnlp2021.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['C Experimental Setup', 'C.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepset-ai/FARM\nCitation Context: We retrieved the pretrained models, fine-tuned them separately on each downstream task using the training and development sets, and tested them on the test sets. We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [URL_CITE] libraries to implement the transformer-based models. A complete list of hyperparameter values is presented in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/deepset-ai/FARM\nSection Titles (if exists):  ['3 Language models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PyTorchLightning/pytorch-lightning,3\nCitation Context: Training details All of our experiments were done on a single Nvidia GeForceRTX 2080 Ti. We base our implementation on PyTorch (Paszke et al., 2017) and also use PyTorch Lightning (Fal-con, 2019) [URL_CITE] and Huggingface (Wolf et al., 2019). The gates and the experts in our MoE model were a single layer MLP.\nFootnote or Reference text (if exists): et al. Falcon, WA. 2019. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning,3.\nSection Titles (if exists):  ['C Hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We implement our models using the jiant toolkit, which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. [URL_CITE] Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 4 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [URL_CITE] are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: Please note that we remove the character embed-ding and position embedding in our experiments; in WD-Match(BERT), F is a pre-trained BERT-base [URL_CITE] model, M is an MLP. Please note that for easing of combining with WD-Match, BERT was only used to extract the sentence features separately in our experiments. The G module for four models are identical: a non-linear projection layer and a linear projection layer.\nFootnote or Reference text (if exists): 7 https://github.com/google-research/ bert\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.fb.com/fasttext/\nCitation Context: 1. We obtain representations of tweets as doc-ument embeddings. We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016) [URL_CITE] (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768).\nFootnote or Reference text (if exists): Facebook-Research. 2016. FastText. (https://research.fb.com/fasttext/).\nSection Titles (if exists):  ['4 Approaches to authorship verification', '4.4 Approach 4: Document embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.neo4j.org/\nCitation Context: We imple-mented this algorithm using the Neo4j library. [URL_CITE] We used a fixed threshold of 1 and vary the search depth in range 1\u201320. We used the candidate senses of all nouns and verbs in a sentence as context.\nFootnote or Reference text (if exists): 9 http://www.neo4j.org/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/easonnie/combine-FEVER-NSMN\nCitation Context: For fact checking, we consider NSMN (Nie et al., 2019), the highest scoring system from the FEVER shared task (Thorne et al., 2018b). We use the public model [URL_CITE] pre-trained on FEVER, and con-sider not enough information predictions as false. Moreover, we develop a fact checking baseline that combines a BERT-base classifier with passages re-turned from DPR where the claim and retrieved passage are input.\nFootnote or Reference text (if exists): 22 available at https://github.com/easonnie/ combine-FEVER-NSMN\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: As a text encoder, we employed BERT and a Hi-erarchical Attention Network (HAN) (Yang et al., 2016) for generating sentence and document rep-resentation, respectively. For BERT, we used the pre-trained BERT-base [URL_CITE] (d = 768). We imple-mented the HAN following Yang et al.\nFootnote or Reference text (if exists): 7 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 Experiments', '3.3 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/c/fake-news-pair-classification-challenge/\nCitation Context: Given a statement and a table as the input, the task is Row 0 2005 Arlandastad 272David Patrick Row 1 Matthew King2004 270Arlandastad Row 2 2003 Falsterbo Titch Moore 273 fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six fine-grained labels and further uses meta-data features. There is a fake news detection challenge [URL_CITE] hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the fac-tual accuracy of the generated summary in neural abstractive summarization systems (Goodrich et al., 2019; Krys\u0301cin\u0301ski et al., 2019), as well as the use of this factual accuracy as a reward to improve\nFootnote or Reference text (if exists): 5 https://www.kaggle.com/c/fake-news-pair-classification-challenge/\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://research.microsoft.com/users/silviu/WebAssistant/TestData\nCitation Context: We evaluated the system in two ways: on a set of Wikipedia articles, by comparing the system out-put with the references created by human contribu-tors, and on a set of news stories, by doing a post-hoc evaluation of the system output. The evalua-tion data can be downloaded from [URL_CITE] http://research.microsoft.com/users/silviu/WebAssistant/TestData. In both settings, we computed a disambiguation baseline in the following manner: for each surface form, if there was an entity page or redirect page whose title matches exactly the surface form then we chose the corresponding entity as the baseline disambiguation; otherwise, we chose the entity most frequently mentioned in Wikipedia using that surface form.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sheffieldnlp/fever-baselines\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [URL_CITE] to stimulate further research on verification.\nFootnote or Reference text (if exists): 3 https://github.com/sheffieldnlp/ fever-baselines\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fever.ai\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website [URL_CITE] , we also publish the annotation interfaces and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 1 http://fever.ai\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://gitlab.freedesktop.org/poppler/poppler\nCitation Context: In terms of whether the field is rapidly growing, we note that there was a year-on-year increase of 42% between in 2017\u20132018 due to the increase in the number of papers published at NAACL and EMNLP, and a 34% increase between 2018\u20132019. Extracting citations To extract a list of refer-ences from an article, we first extract the text stream from the PDF file via pdftotext, [URL_CITE] then feed it into ParsCit (Councill et al., 2008) to obtain the references. For each reference in this list, we then extract and keep the parsed \u201cdate\u201d, \u201cauthor\u201d, and \u201ctitle\u201d entries.\nFootnote or Reference text (if exists): 2 https://gitlab.freedesktop.org/poppler/poppler\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hltfbk.github.io/Excitement-Open-Platform/\nCitation Context: For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. [URL_CITE] (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015).\nFootnote or Reference text (if exists): 5 The BIUTEE system (Stern and Dagan, 2012) available under the Excitement Open Platform http://hltfbk.github.io/Excitement-Open-Platform/ was used for recognizing textual entailment.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/georgwiese/biomedical-qa\nCitation Context: We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [URL_CITE] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions.\nFootnote or Reference text (if exists): 5 https://github.com/georgwiese/ biomedical-qa\nSection Titles (if exists):  ['4 Experiments & Results', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11372/LRT-1632\nCitation Context: We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016 [URL_CITE] ), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.\nFootnote or Reference text (if exists): Marco Turchi, Rajen Chatterjee, and Matteo Negri. 2016. WMT16 APE shared task data. LIN-DAT/CLARIN digital library at the Institute of For-mal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.\nSection Titles (if exists):  ['4 Experiments', '4.2 Automatic MT Post-editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [URL_CITE] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description.\nFootnote or Reference text (if exists): 5 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/tkbc\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx [URL_CITE] , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 6 https://github.com/facebookresearch/tkbc\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BorealisAI/de-simple\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE [URL_CITE] , TNTComplEx , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 5 https://github.com/BorealisAI/de-simple\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.facebook.com/research/-babi/\nCitation Context: Bordes et al. (2015) released the SimpleQuestions data set [URL_CITE] , which consists of 108,442 English questions. Each ques-tion (e.g., What does Jimmy Neutron do?)\nFootnote or Reference text (if exists): 11 https://research.facebook.com/research/-babi/\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/commonsense/conceptnet5/wiki/Downloads\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [URL_CITE] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 3 https://github.com/commonsense/conceptnet5/wiki/Downloads\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Websail-NU/CODAH\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://xjtu-intsoft.github.io/chase\nCitation Context: To understand the characteristics of C HASE , we conduct a detailed data analysis and experi-ment with three state-of-the-art (SOTA) XDTS approaches, namely, EditSQL (Zhang et al., 2019), IGSQL (Cai and Wan, 2020), and our extension of RAT-SQL (Wang et al., 2020a). The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that C HASE presents significant chal-lenges for future research. The dataset, benchmark approaches, and our annotation tools are available at [URL_CITE] https://xjtu-intsoft.github.io/chase.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cl.cam.ac.uk/research/nl/bea2019st/\nCitation Context: A Experiment Details Table 5 describes the details of datasets used for English GEC. Except the sythetic data, all the data can be found at the website [URL_CITE] of the BEA-19 shared task. The synthetic data is generated from English Wikipedia , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption.\nFootnote or Reference text (if exists): 4 https://www.cl.cam.ac.uk/research/nl/bea2019st/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: We use 70 percent of stories in ROC (ROC_LM) and WP (WP_LM) for fine-tuning GPT2 (Radford et al., 2019) lan-guage model with batch size of 4. [URL_CITE] After 3 epochs of fine-tuning, the perplexity on the validation set of ROC and WP datasets are 8.28 and 25.04, re-spectively.\nFootnote or Reference text (if exists): 2 We fine-tune GPT2 language model using https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tau-nlp.org/commonsenseqa\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.answers.com\nCitation Context: Yahoo! An-swers, Answers.com, [URL_CITE] etc. ), which became very popular resources for question answering.\nFootnote or Reference text (if exists): 2 http://www.answers.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nCitation Context: Answers dataset from Yahoo! Labs Webscope [URL_CITE] . Each question in the dataset contains title, description, best an-swer, most voted answers and meta-data like categories, sub categories etc.\nFootnote or Reference text (if exists): 11 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nSection Titles (if exists):  ['7 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://openke.thunlp.org/\nCitation Context: Besides the toolkit, the embed-dings of some existing large-scale knowledge graphs pre-trained by OpenKE are also avail-able, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on [URL_CITE] http://openke.thunlp.org/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://answers.yahoo.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers [URL_CITE] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 1 https://answers.yahoo.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackoverflow.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora , StackOverflow [URL_CITE] etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 4 http://stackoverflow.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://answers.yahoo.com/\nCitation Context: Yahoo! An-swers, [URL_CITE] Answers.com, etc. ), which became very popular resources for question answering.\nFootnote or Reference text (if exists): 1 http://answers.yahoo.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.opensubtitles.org/\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, [URL_CITE] and even material such as the TED conference talks.\nFootnote or Reference text (if exists): 6 http://www.opensubtitles.org/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://l10n.kde.org/\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, [URL_CITE] movie subtitles, and even material such as the TED conference talks.\nFootnote or Reference text (if exists): 5 http://l10n.kde.org/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ted.com/translate/\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks. [URL_CITE]\nFootnote or Reference text (if exists): 7 http://www.ted.com/translate/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: During inference, each source token is also added with the corresponding language embed-ding in order to give the model a sense of the lan-guage it is currently processing. We decode with beam search and set beam size to 6 and length penalty \u03b1 = 1.1 for all the languages. We evaluate the translation quality by tokenized case sensitive BLEU (Papineni et al., 2002) with multi-bleu.pl [URL_CITE] .\nFootnote or Reference text (if exists): 7 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ted.com\nCitation Context: However, the work fo-cused on a single speaker in a limited domain. Of-fline speech translation of TED [URL_CITE] talks has been ad-dressed through the IWSLT 2011 and 2012 evalua-tion tracks. The talks are from a variety of speakers with varying dialects and cover a range of topics.\nFootnote or Reference text (if exists): 1 http://www.ted.com\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:Translation\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles [URL_CITE] (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks.\nFootnote or Reference text (if exists): 4 http://en.wikipedia.org/wiki/Wikipedia:Translation\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmdrr/testSignificanceNLP.git\nCitation Context: We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental re-sults, statistical significance testing is often ig-nored or misused. We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [URL_CITE] .\nFootnote or Reference text (if exists): 1 The code for all statistical tests detailed in this pa-per is found on: https://github.com/rtmdrr/testSignificanceNLP.git\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://eecs.berkeley.edu/~gdurrett\nCitation Context: We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language\u2019s in-flectional morphology. [URL_CITE]\nFootnote or Reference text (if exists): 2 See http://eecs.berkeley.edu/~gdurrett for our datasets and code.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ucrel.lancs.ac.uk/usas/\nCitation Context: In this paper, we have investigated the feasibility of rapidly bootstrapping semantic annotation tools for new target languages [URL_CITE] by mapping an existing semantic lexicon and software architecture. In par-ticular, we tested the possibility of automatically translating existing English semantic lexicons into other languages, Italian, Chinese and Brazilian Portuguese in this particular case.\nFootnote or Reference text (if exists): 2 The results are available at http://ucrel.lancs.ac.uk/usas/\nSection Titles (if exists):  ['6 Conclusion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/movie$-data\nCitation Context: We conclude that text features from pre-release re-views can substitute for and improve over a strong metadata-based first-weekend movie revenue pre-diction. The dataset used in this paper has been made available for research at [URL_CITE] http://www.ark.cs.cmu.edu/movie$-data.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://luululu.com/tweet/\nCitation Context: We use a \u201csoft\u201d normalization strategy to all of the experimental datasets by translating word to-kens of user mentions and web/url links into spe-cial tokens @USER and HTTPURL, respectively, and converting emotion icon tokens into corre-sponding strings. We also apply a \u201chard\u201d strategy by further applying lexical normalization dictio-naries (Aramaki, 2010 [URL_CITE] ; Liu et al., 2012; Han et al., 2012) to normalize word tokens in Tweets.\nFootnote or Reference text (if exists): Eiji Aramaki. 2010. TYPO CORPUS. http://luululu.com/tweet/.\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OnlpLab/morphodetection\nCitation Context: We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available on https://github.com/OnlpLab/morphodetection.\nSection Titles (if exists):  ['table, given a few forms of the same lexeme. 2']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/~aditya/g2p-tl-rr\nCitation Context: Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www.cs.toronto.edu/\u02dcaditya/ g2p-tl-rr\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/noa/naacl2021\nCitation Context: Our code, data splits, and scripts to repro-duce our experiments are available at [URL_CITE] http://github.com/noa/naacl2021.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/monoses\nCitation Context: Our experiments show the effective-ness of our approach, as we improve the previous state-of-the-art in unsupervised machine transla-tion by 5-7 BLEU points in French-English and German-English WMT 2014 and 2016. Our code is available as an open source project at [URL_CITE] https: //github.com/artetxem/monoses.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions and future work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: Our analyses show that four popular industrial MT systems and two recent state-of-the-art aca-demic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/longyuewangdcu/RLFW-NAT\nCitation Context: Encour-agingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, re-spectively. Our code, data, and trained mod-els are available at [URL_CITE] https://github.com/longyuewangdcu/RLFW-NAT.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Abstract']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/treelstm\nCitation Context: Our experiments show that Tree-LSTMs outper-form existing systems and sequential LSTM base-lines on both tasks. Implementations of our mod-els and experiments are available at [URL_CITE] https://github.com/stanfordnlp/treelstm.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nCitation Context: (4) By adding word-knowledge features and us-ing learning-based multi-sieve approach, we im-prove the performance of the state-of-the-art system of (Bengtson and Roth, 2008) by 3 MUC, [URL_CITE] B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset. We report our experimen-tal results in Sec.\nFootnote or Reference text (if exists): 2 The extracted attributes and the related re-sources are available for public download at http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mstparser\nCitation Context: The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [URL_CITE] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent.\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/mstparser\nSection Titles (if exists):  ['2 Unlabeled Dependency Parsing using Global Features', '2.3 Local Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/OpenBookQA\nCitation Context: For MultiRC, we also apply an off-the-shelf coreference resolution model 4 and re-place the mentions when they resolve to pronouns occurring in a different sentence . For Open-BookQA, we use the exact same retrieval as re-leased by the authors of OpenBookQA [URL_CITE] and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query.\nFootnote or Reference text (if exists): 6 https://github.com/allenai/OpenBookQA\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/page/softwareview/POS\nCitation Context: We built the temporal expression extraction module on top of the Heideltime system (Stro\u0308tgen and Gertz, 2010) to take advantage of a state-of-the-art tempo-ral extraction system in capturing basic expressions. We use the Illinois POS tagger [URL_CITE] (Roth and Zelenko, 1998) to provide part-of-speech tags for the input text before passing it to HeidelTime. Below is an example of the HeidelTime output of the example in the previous section:\nFootnote or Reference text (if exists): 1 http://cogcomp.cs.illinois.edu/page/softwareview/POS\nSection Titles (if exists):  ['2 The System', '2.1 Temporal Expression Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/software.php\nCitation Context: We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger [URL_CITE] ) to generate features. For edge the entailment and paraphrase identification tasks.\nFootnote or Reference text (if exists): 3 http://L2R.cs.uiuc.edu/\u02dccogcomp/software.php\nSection Titles (if exists):  ['5 Experiments', '5.2 Textual Entailment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt19/translation-task.html\nCitation Context: To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [URL_CITE] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experiment', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/YCOE/YcoeHome.htm\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003) [URL_CITE] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdf\nCitation Context: BBN\u2019s SERIF tool 8 (Boschee et al., 2005) provides a considerable suite of document annotations that are an excellent basis for building a knowledge base. The functions SERIF can provide are based largely on the NIST ACE specification, [URL_CITE] and include: (a) identifying named-entities and classifying them by type and subtype; (b) performing intra-document co-reference analysis, including named mentions, as well as co-referential nominal and pronominal mentions; (c) parsing sentences and extracting intra-sentential relations between entities; and, (d) detect-ing certain types of events.\nFootnote or Reference text (if exists): 9 The principal types of ACE named-entities are per-sons, organizations, and geo-political entities (GPEs). GPEs are inhabited locations with a government. See http://www.itl.nist.gov/iad/mig/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdf.\nSection Titles (if exists):  ['3 Pipeline Components', '3.1 SERIF']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opennlp.sourceforge.net/\nCitation Context: We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepo-sitional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and ar-gument head words. We use the OpenNLP [URL_CITE] coref-erence engine to resolve entity mentions.\nFootnote or Reference text (if exists): 1 http://opennlp.sourceforge.net/\nSection Titles (if exists):  ['6 Evaluation: Cloze', '6.2 Training and Test Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/P17-1161\nCitation Context: Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detec-tion, are often formulated as variations of se-quence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Ir-soy and Cardie, 2014), character-based represen-tations (Lample et al., 2016), and additional ex-ternal features (Peters et al., 2017) [URL_CITE] . Optimiza-tion of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language.\nFootnote or Reference text (if exists): Matthew E. Peters, Waleed Ammar, Chandra Bhaga-vatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language mod-els. In ACL 2017. https://doi.org/10.18653/v1/P17-1161.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/collaborations/past-projects/ace\nCitation Context: Besides employing active learning to create spe-cific data set with nested, overlapping, discontinu-ous entity mentions, we notice that there are some off-the-shelf corpora in biomedical domain that we can use to evaluate our proposed methods, al-though, to our knowledge, none of these data sets contains all three kinds of complex mentions, e.g., GENIA (Kim et al., 2003) only contains nested en-tity mentions, and CADEC (Karimi et al., 2015) and SemEval2014 (Pradhan et al., 2014) contain overlapping and discontinuous mentions. In ad-dition, ACE [URL_CITE] and NNE (Ringland, 2016) are newswire corpora with nested entity mentions.\nFootnote or Reference text (if exists): 1 https://www.ldc.upenn.edu/collaborations/past-projects/ace\nSection Titles (if exists):  ['4 Research Plan']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf\nCitation Context: There were 118 sentences, out of which 94 had at least one modification after the rewrite pro-cess. We selected 50 of these 94 sentences at ran-dom and asked 2 human judges to rate each sen-tence and its rewritten form on a scale of 1\u20135 for accuracy and fluency [URL_CITE] . We used 4 human judges, each judging 25 sentence pairs.\nFootnote or Reference text (if exists): 3 We followed the DARPA/LDC guidelines from http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf. For fluency, the scale was 5:Flawless, 4:Good, 3:Non-native, 2:Disfluent, 1:Incomprehensible. The accuracy scale for information covered (comparing with human translation) was 5:All, 4:Most, 3:Much, 2:Little, 1:None.\nSection Titles (if exists):  ['3 Arbitrary noun phrases', '3.5 Manual Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/\nCitation Context: An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [URL_CITE] This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time.\nFootnote or Reference text (if exists): 4 http://webscope.sandbox.yahoo.com/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://sites.google.com/site/trecliveqa2015/\nCitation Context: Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. [URL_CITE] The Yahoo! Answers man-ner question dataset prepared by Jansen et al.\nFootnote or Reference text (if exists): 5 https://sites.google.com/site/trecliveqa2015/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/moving-qatar/posts/can-i-obtain-driving-license-my-qid-written-employee\nCitation Context: A simplified example is shown in Figure 1, [URL_CITE] where answers 2 and 4 are good, answer 1 is po-tentially useful, and answer 3 is bad. In this paper, we focus on a 2-class variant of the above Sub-task A, which is closer to a real CQA application.\nFootnote or Reference text (if exists): 3 http://www.qatarliving.com/moving-qatar/posts/can-i- obtain-driving-license-my-qid-written-employee\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/\nCitation Context: We randomly select 100 candidates and compare the outputs of different systems. We use Ama-zon Turk [URL_CITE] for human evaluation. Each candidate is rated by three annotators and we report the av-erage scores here.\nFootnote or Reference text (if exists): 8 https://www.mturk.com/ tent preservation. Mean denotes the average of the met-\nSection Titles (if exists):  ['3 Experiments', '3.5 Human Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crowdflower.com\nCitation Context: To answer these questions, we also conduct a human judges study on Crowdflower [URL_CITE] . We se-lect 200 items at random (100 real reviews and 100 from the conditional model, half of each with meta-information), and ask annotators to rate them as real or fake.\nFootnote or Reference text (if exists): 6 https://crowdflower.com\nSection Titles (if exists):  ['4 Experiments', '4.2 Human Judges']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs\nCitation Context: Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance, and 3) extensive experimental results that outper-form state-of-the-art baselines. Finally, we release the code and datasets [URL_CITE] to enable future work on this problem.\nFootnote or Reference text (if exists): 3 Code and datasets available at https://github.com/jerbarnes/sentiment_graphs.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/iirc\nCitation Context: We fol-low recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at [URL_CITE] https://allennlp.org/iirc.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ucrel.lancs.ac.uk/usas/\nCitation Context: In this paper, we have investigated the feasibility of rapidly bootstrapping semantic annotation tools for new target languages [URL_CITE] by mapping an existing semantic lexicon and software architecture. In par-ticular, we tested the possibility of automatically translating existing English semantic lexicons into other languages, Italian, Chinese and Brazilian Portuguese in this particular case.\nFootnote or Reference text (if exists): 2 The results are available at http://ucrel.lancs.ac.uk/usas/\nSection Titles (if exists):  ['6 Conclusion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://iep.utm.edu/ethics/\nCitation Context: Our work bridges this gap by illustrating how a philosophical theory of ethics can be applied to NLP research. Ethics (or ethical theory), is a the-oretical and applied branch of philosophy which studies what is good and right, especially as it per-tains to how humans ought to behave in the most general sense (Fieser, 1995) [URL_CITE] . As NLP research qual-ifies as a human activity, it is within the purview of ethics.\nFootnote or Reference text (if exists): James Fieser. 1995. Ethics. https://iep.utm.edu/ethics/ (accessed: 11-03-2020).\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/levguy/talksumm\nCitation Context: Our main contributions are as follows: (1) we propose a new approach to automatically gener-ate summaries for scientific papers based on video talks; (2) we create a new dataset, that contains 1716 summaries for papers from several computer science conferences, that can be used as training data; (3) we show both automatic and human eval-uations for our approach. We make our dataset and related code publicly available [URL_CITE] . To our knowl-edge, this is the first approach to automatically cre-ate extractive summaries for scientific papers by utilizing the videos of conference talks.\nFootnote or Reference text (if exists): 3 https://github.com/levguy/talksumm\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lair.cse.msu.edu/lair/projects/actioneffect.html\nCitation Context: This opens up the possibility for humans to teach robots new tasks through language communication with a small number of examples. Third, we have cre-ated a dataset for this task, which is available to the community [URL_CITE] . Our bootstrapping approach can serve as a baseline for future work on this topic.\nFootnote or Reference text (if exists): 1 This dataset is available at http://lair.cse.msu.edu/lair/projects/actioneffect.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhcross/span-parser\nCitation Context: The Penn Treebank dataset and splits are available at [URL_CITE] https://github.com/jhcross/span-parser. The training, data, and test splits are the standard Penn Treebank splits (sections 2-21 for training, 22 for development, and 23 for test, containing 39832, 1700, and 2416 examples respectively).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.3 Syntactic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Our dataset is, to the best of our knowledge, the largest noun compound dataset yet produced. We will make it available via [URL_CITE] http://www.isi.edu.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Priberam/exconsumm\nCitation Context: We also make available a new dataset of oracle compressive summaries de-rived automatically from the CNN/DailyMail reference summaries. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our dataset and code is available at https://github.com/Priberam/exconsumm.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://turing.iimas.unam.mx/wix/MexSeg\nCitation Context: Final splits. In order to make follow-up work on minimal-resource settings for morphological segmentation easily comparable, we provide pre-defined splits of our datasets [URL_CITE] . 40% of the data constitute the test sets.\nFootnote or Reference text (if exists): 2 Our datasets can be found to-gether with the code of our models at http://turing.iimas.unam.mx/wix/MexSeg.\nSection Titles (if exists):  ['3 Morphological Segmentation Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://u.cs.biu.ac.il/~nlp/resources/downloads/\nCitation Context: We ob-serve similar trends to previously published results, and make the dataset splits available for replication. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://u.cs.biu.ac.il/~nlp/resources/downloads/\nSection Titles (if exists):  ['2 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/pointer-generator\nCitation Context: For S EQ 2S EQ , P T G EN and P T G EN -C OVG , we used the best settings reported on the CNN and DailyMail data (See et al., 2017). [URL_CITE] All three mod-els had 256 dimensional hidden states and 128 di-mensional word embeddings. They were trained using Adagrad (Duchi et al., 2011) with learning rate 0.15 and an initial accumulator value of 0.1.\nFootnote or Reference text (if exists): 2 We used the code available at https://github.com/abisee/pointer-generator.\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://rnnlm.org\nCitation Context: In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a) [URL_CITE] to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer.\nFootnote or Reference text (if exists): Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Unlabeled tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clarkkev/deep-coref\nCitation Context: The first resolver, the Stanford neural resolver (Clark and Manning, 2016) [URL_CITE] , takes as input a set of entity mentions identified for a given document by a rule-based MD system and trains using reinforce-ment learning a simple mention ranker consisting of three hidden layers of ReLU units and a final layer that is fully-connected.\nFootnote or Reference text (if exists): 3 https://github.com/clarkkev/deep-coref\nSection Titles (if exists):  ['2 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/dynet\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [URL_CITE] (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 5 https://github.com/clab/dynet\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Library\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Experiment Details We adopt the T5 base model from huggingface Transformer library [URL_CITE] for all experiments. T5 closely follows the original encoder-decoder architecture of the Transformer model, with some slight differences such as differ-ent position embedding schemes.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: For ACMLM, we build our model based on the BERT implementation from HuggingFace. [URL_CITE] We initialize our decoder using the pre-trained \u2018Bert-base\u2019 model and set the max sequence length to 30. We train the model for 5 epochs using Adam with learning rate 2e \u22125 .\nFootnote or Reference text (if exists): 7 https://github.com/huggingface/pytorch-pretrained- BERT\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~och/YASMET.html\nCitation Context: We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training. To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://www.isi.edu/\u02dcoch/YASMET.html\nSection Titles (if exists):  ['3 Adjacency Pairs', '3.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duncanka/lstm-causality-tagger\nCitation Context: The code for DeepCx is available on GitHub. [URL_CITE] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser.\nFootnote or Reference text (if exists): 3 https://github.com/duncanka/lstm-causality-tagger.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\nCitation Context: The encoder-decoder model is a conditional lan-guage model that predicts a correct output se-quence from an input sequence (Sutskever et al., 2014) [URL_CITE] . This model consists of two RNNs: an en-coder and decoder. The encoder extracts a context of the input sequence as a real-valued vector, and the decoder predicts the output sequences from the context individually.\nFootnote or Reference text (if exists): Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural In-formation Processing Systems 27 (NIPS 2014), Curran Associates, Inc., pages 3104\u20133112. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/flax\nCitation Context: We have used Flax, an open-source library [URL_CITE] to train neural networks, as the code base for the model training. Our H-Transformer-1D model uses the standard Transformer decoder implemen-tation in Flax as the backbone.\nFootnote or Reference text (if exists): 8 https://github.com/google/flax\nSection Titles (if exists):  ['8 Experiments And Results', '8.2 Language Models Trained on One-Billion Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Library\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/parsing/preprocessing.html\nCitation Context: As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words [URL_CITE] and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016).\nFootnote or Reference text (if exists): 10 https://radimrehurek.com/gensim/parsing/preprocessing.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dice-group/Palmetto\nCitation Context: In this section, we study whether jointly learning topic models and classification can be helpful in producing coherent and meaningful top-ics. We use the C V metric (Ro\u0308der et al., 2015) computed by Palmetto toolkit [URL_CITE] to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI (Bouma, 2009). Table 4 shows the compar-ison results of LDA, BTM, NTM, and TMN on the three English datasets.\nFootnote or Reference text (if exists): 12 https://github.com/dice-group/ Palmetto\nSection Titles (if exists):  ['4 Experimental Results', '4.2 Topic Coherence Comparison']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yftah89/PBLM-Cross-language-Cross-domain\nCitation Context: In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms pre-vious models even when it is trained in the lazy setup and previous models are trained in the full setup. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/yftah89/ PBLM-Cross-language-Cross-domain\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.jhu.edu/~jason/tutorials/variational.html\nCitation Context: Inference in this model class in intractable, so as has been done in previous work on topic modeling (Ng and Jordan, 2003) we rely on variational meth-ods to approximate the gradients needed during training as well as the posteriors over the topic bit vectors. Specifically, we will need the gradients of the normalizer and the sum of the energy function over the hidden variables which we refer to as the marginal energy. Follow-ing the approach described for undirected models by Eisner (2011) [URL_CITE] , we approximate these quantities and their gradients with respect to the model pa-rameters \u03b8 as we will now describe (thread-level embeddings are omitted in this section for clarity).\nFootnote or Reference text (if exists): Jason Eisner. 2011. High-level explanation of varia-tional inference. https://www.cs.jhu.edu/~jason/tutorials/variational.html. Accessed: 2018-02-22.\nSection Titles (if exists):  ['3 Learning and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/giza/GIZA++.html\nCitation Context: As seen from Table 2, the translation based methods outperforms the baseline signifi-cantly. The models are trained using GIZA++ [URL_CITE] tool with the question and best answer pair as the parallel corpus. For the topic based Q-A topic model and Q-A topic model(s), we implemented the models QATM -PR (Question-Answer Topic Model) Ji et al.\nFootnote or Reference text (if exists): 12 http://www.statmt.org/moses/giza/GIZA++.html\nSection Titles (if exists):  ['8 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: Multi-Domain Text Classification The MDTC task was first examined by Li and Zong (2008), who proposed to fuse the training data from multi-ple domains either at the feature level or the classi-fier level. The prior art of MDTC (Wu and Huang, 2015) [URL_CITE] decomposes the text classifier into a gen-eral one and a set of domain-specific ones. How-ever, the general classifier is learned by param-eter sharing and domain-specific knowledge may sneak into it.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf\nCitation Context: GPE. Geo-political entities are single units that contain a population, government, physical loca-tion, and political boundaries (LDC, 2005) [URL_CITE] . In literary data, this includes not only cities that have known geographical locations within the real world (London, New York), or nations (England, the United States), but also both named and com-mon imagined entities as well (the town, the vil-lage).\nFootnote or Reference text (if exists): LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.\nSection Titles (if exists):  ['3 Annotation', '3.1 Entity types']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://utexas.box.com/s/ekznoh08afi1kpkbf0hb\nCitation Context: Our account is also similar to that proposed by Erk (2015) [URL_CITE] . Erk suggests that distributional data influences semantic \u2018knowledge\u2019 : specifically, while a speaker may not know the extension of the word alligator, they maintain an information state which models properties of alligators (for instance, that they are animals).\nFootnote or Reference text (if exists): Katrin Erk. 2015. What do you know about an alli-gator when you know the company it keeps? Un-published draft. https://utexas.box.com/s/ekznoh08afi1kpkbf0hb.\nSection Titles (if exists):  ['2 Related Work', '2.1 Formal Distributional Semantics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.utexas.edu/users/ml/nldata/geoquery\nCitation Context: To test the coverage and precision of Locutus, I have customized it to answer questions from the G EOQUERY 250 corpus (Mooney, 1996) [URL_CITE] , which consists of a database of geographical information paired with 250 English sentences requesting in-formation from that database. 25 of these sentences are held out for the purposes of another study, and I have not examined the behavior of Locutus with respect to these sentences.\nFootnote or Reference text (if exists): Mooney, Raymond. Geoquery 1996.Data. http://www.cs.utexas.edu/users/ml/nldata/geoquery. html (accessed February 13, 2010).\nSection Titles (if exists):  ['4 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dianping.com\nCitation Context: Besides, experts\u2019 prior knowledge or de-velopers\u2019 ingenuity is not always reliable with the variations of domains and languages. For exam-ple, based on the datasets from Dianping site [URL_CITE] , Li et al. (2015) find that the real users tend to review the restaurants nearby, but the spammers are not re-stricted to the geographical location, they may come from anywhere.\nFootnote or Reference text (if exists): 4 http://www.dianping.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html\nCitation Context: We use the publicly available implementation of MSTParser [URL_CITE] (with modifications to the feature com-putation) and its default settings, so the feature weights of the projective and non-projective parsers are trained by the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2006).\nFootnote or Reference text (if exists): 7 http://www.seas.upenn.edu/\u02dcstrctlrn/MSTParser/MSTParser.html\nSection Titles (if exists):  ['6 Experiment', '6.2 Baseline Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clearnlp.com,v2.0.2\nCitation Context: The syntactic and semantic features were extracted using the ClearNLP parser. [URL_CITE] We used the default models and options for the parser. We treat this model as a strong baseline to which we will add reference-based features.\nFootnote or Reference text (if exists): 4 http://www.clearnlp.com,v2.0.2\nSection Titles (if exists):  ['3 Models for Short Answer Scoring', '3.1 Response-based']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mstparser\nCitation Context: The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [URL_CITE] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent.\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/mstparser\nSection Titles (if exists):  ['2 Unlabeled Dependency Parsing using Global Features', '2.3 Local Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nCitation Context: (4) By adding word-knowledge features and us-ing learning-based multi-sieve approach, we im-prove the performance of the state-of-the-art system of (Bengtson and Roth, 2008) by 3 MUC, [URL_CITE] B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset. We report our experimen-tal results in Sec.\nFootnote or Reference text (if exists): 2 The extracted attributes and the related re-sources are available for public download at http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr/\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [URL_CITE] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): 6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mstparser.sourceforge.net\nCitation Context: The parser is basically based on the MSTParser [URL_CITE] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser.\nFootnote or Reference text (if exists): 8 It\u2019s freely available at http://mstparser.sourceforge.net.\nSection Titles (if exists):  ['5 Evaluation Results', '5.1 Syntactic Dependency Parsers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/MiuLab/CLUSE\nCitation Context: The proposed approach shows the su-perior quality of sense embeddings evaluated in both monolingual and bilingual spaces. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and dataset are available at http://github.com/MiuLab/CLUSE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sjcfr/CWVAE\nCitation Context: Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at [URL_CITE] https://github.com/sjcfr/CWVAE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tagoyal/sow-reap-paraphrasing\nCitation Context: Our evaluation, both au-tomatic and human, shows that the proposed system retains the quality of the baseline ap-proaches while giving a substantial increase in the diversity of the generated paraphrases. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and code are available at https://github.com/tagoyal/sow-reap-paraphrasing\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wxjiao/UncSamp\nCitation Context: Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code is available at https://github.com/wxjiao/UncSamp\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://transmart.qq.com/index\nCitation Context: Further analyses demonstrate that our uncertainty-based sampling approach does improve the translation quality of high uncertainty sentences and also benefits the prediction of low-frequency words at the target side. The proposed technology has been applied to TranSmart [URL_CITE] (Huang et al., 2021), an interactive ma-chine translation system in Tencent, to improve the performance of its core translation engine. Future work includes the investigation on the confirmation bias issue of self-training and the effect of decoding strategies on self-training sampling.\nFootnote or Reference text (if exists): 6 https://transmart.qq.com/index\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuzhaopeng/NMT-Coverage\nCitation Context: Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: In prior work (Huenerfauth and Kacorri, 2015b), we investigated key methodological con-siderations in conducting a study to evaluate sign language animations with deaf users, including the use of appropriate baselines for comparison, appropriate presentation of questions and instruc-tions, demographic and technology experience factors influencing acceptance of signing avatars, and other factors that we have considered in the design of this current study. Our recent work (Kacorri et al., 2015) [URL_CITE] has established a set of de-mographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to eval-uate animation. Specifically, we screened for par-ticipants that identified themselves as \u201cdeaf/Deaf\u201d or \u201chard-of-hearing,\u201d who had grown up using ASL at home or had attended an ASL-based school as a young child, such as a residential or daytime school.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['3 Evaluation', '3.2 User Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: There were 8 men and 9 women of ages 19-29 (average age 22.8). In prior work, we (Kacorri et al., 2015) [URL_CITE] have advocated that participants in studies eval-uating sign language animation complete a two standardized surveys about their technology ex-perience (MediaSharing and AnimationAttitude) and that researchers report these values for partici-pants, to enable comparison across studies. In our study, participant scores for MediaSharing varied between 3 and 6, with a mean score of 4.3, and scores for AnimationAttitude varied from 2 to 6, with a mean score of 3.8.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['3 Evaluation', '3.2 User Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://openreview.net/pdf?id=rke2P1BFwS\nCitation Context: A tKG represents a temporal fact as a quadruple (s, r, o, t) by extending a static triple with time t, describing that this fact is valid at time t. In recent years, several sizable temporal knowledge graphs, such as ICEWS (Boschee et al., 2015), have been developed that provide widespread availability of such data and enable reasoning on temporal KGs. While lots of work (Garc\u00eda-Dur\u00e1n et al., 2018; Goel et al., 2020; Lacroix et al., 2020 [URL_CITE] ) focus on the tem-poral KG completion task and predict missing links at observed timestamps, recent work (Jin et al., 2019; Trivedi et al., 2017) paid attention to forecast future links of temporal KGs. In this work, we focus on the temporal KG forecasting task, which is more challenging than the completion task.\nFootnote or Reference text (if exists): Timothee Lacroix, Guillaume Obozinski, and Nico-las Usunier. 2020. Tensor decompositions for tem-poral knowledge base completion. ICLR preprint https://openreview.net/pdf?id=rke2P1BFwS.\nSection Titles (if exists):  ['\u2020 Corresponding author.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opennlp.sourceforge.net\nCitation Context: A number of systems have tried to address the question of coreference in RTE as a preprocessing step prior to inference proper, with most systems using off-the-shelf coreference resolvers such as JavaRap (Qiu et al., 2004) or OpenNLP [URL_CITE] . Gen-erally, anaphoric expressions were textually re-placed by their antecedents.\nFootnote or Reference text (if exists): 3 http://opennlp.sourceforge.net\nSection Titles (if exists):  ['2 Background', '2.2 Discourse in Textual Entailment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: A survey of recent work of several researchers on producing animations of sign language with fa-cial expressions appears in (Kacorri, 2015) [URL_CITE] . There is recent interest in data-driven approaches using facial motion-capture of human performances to generate sign language animations: For example, (Schmidt et al., 2013) used clustering techniques to select facial expressions that co-occur with indi-vidual lexical items, and (Gibet et al., 2011) stud-ied how to map facial motion-capture data to ani-mation controls.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['1 Introduction and Motivation', '1.2 Prior Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: Topics from 20 newsgroup We choose the five top-level classes in the 20 newsgroup dataset [URL_CITE] as topics; viz. : Computer, Recreation, Science, Pol-itics, Religion.\nFootnote or Reference text (if exists): 6 http://qwone.com/\u02dcjason/20Newsgroups/\nSection Titles (if exists):  ['Topics and tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nCitation Context: VN is or-ganised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class mem-bers. According to the official VerbNet guidelines, [URL_CITE] \u201cVerb Classes are numbered according to shared se-mantics and syntax, and classes which share a top-level number (9-109) have corresponding semantic relationships.\u201d For instance, all verbs from the top-level Class 9 are labelled \u201cVerbs of Putting\u201d, all verbs from Class 30 are labelled \u201cVerbs of Perception\u201d, while Class 39 contains \u201cVerbs of Ingesting\u201d.\nFootnote or Reference text (if exists): 5 http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhcross/span-parser\nCitation Context: The Penn Treebank dataset and splits are available at [URL_CITE] https://github.com/jhcross/span-parser. The training, data, and test splits are the standard Penn Treebank splits (sections 2-21 for training, 22 for development, and 23 for test, containing 39832, 1700, and 2416 examples respectively).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.3 Syntactic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\nCitation Context: Suppose p i , i = 1,..., n represent the training set of persons, and the classes for classifications are C ={c ,c } (for simplicity, we represent the 1 2 classes with C = { \u22121,1 } ). Then the classification task requires the solution of the following optimi-zation problem (Hsu et al., 2003) [URL_CITE] :\nFootnote or Reference text (if exists): Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-tical Guide to Support Vector Classification. Avail-able at: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.facebook.com/researchers/1543934539189348\nCitation Context: (2) The second dataset is a synthetic dataset released under the bAbI project [URL_CITE] (Weston et al., 2015). The dataset presents a set of 20 \u2018tasks\u2019, each testing a different aspect of text understand-ing and reasoning in the QA setting, and hence can be used to test and compare capabilities of learning models in a fine-grained manner.\nFootnote or Reference text (if exists): 2 https://research.facebook.com/researchers/1543934539189348 sider two evaluation metrics: accuracy (propor-\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm\nCitation Context: : Computer, Recreation, Science, Pol-itics, Religion. The corresponding five down-stream tasks are text classification over the 3\u2013 [URL_CITE] fine-grained classes under each top-level class. Train, test, dev splits were 50%, 40%, 10%.\nFootnote or Reference text (if exists): 5 https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm\nSection Titles (if exists):  ['Topics and tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Babylonpartners/corrsim\nCitation Context: Note that the dataset for the STS13 SMT subtask is no longer publicly available, so the mean Pear-son correlation for STS13 reported in our exper-iments has been re-calculated accordingly. The code for our experiments builds on the SentEval toolkit (Conneau and Kiela, 2018) and is available on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/Babylonpartners/ corrsim\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ted.com/talks\nCitation Context: The dataset consists of transcripts directly col-lected from [URL_CITE] https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks. We pre-process the data by removing sentences that don\u2019t have any translation or are longer than 60 words, lower-casing, and tokenizing (using the Moses tokenizer (Koehn et al., 2007)).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: We first pre-train an agreement classifier on a stance detection dataset provided by the Fake News Challenge [URL_CITE] . This dataset provides pairs of English sentences with their agreement annota-tions in \u201cagree\u201d, \u201cdisagree\u201d, \u201cdiscuss\u201d and \u201cunre-lated\u201d.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['5 Cross-lingual Cross-platform Features', '5.2 Agreement Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ai-systems/premise\nCitation Context: For the experiments using BERT and the pro-posed approach, we split the dataset using a 50/20/30 (train/dev/test) split. We run all exper-iments ten times, evaluating on the test set, and report the average Precision, Recall and F1-score. All evaluation data, as well as the experimental pipeline, can be found online [URL_CITE] for reproducibility purposes.\nFootnote or Reference text (if exists): 2 https://github.com/ai-systems/premise selection graph\nSection Titles (if exists):  ['6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/messense/jieba-rs\nCitation Context: All Chinese sentences are segmented by Jieba [URL_CITE] while all English and German sentences are tok-enized by Moses scripts (Koehn et al., 2007). For ZH-EN (EN-DE) translation, we merge the source and target sentences of the parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) opera-tions (Sennrich et al., 2016).\nFootnote or Reference text (if exists): 6 https://github.com/messense/jieba-rs\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt17/\nCitation Context: Following recent unconditional long text genera-tion work (Caccia et al., 2018), we perform exper-iments on EMNLP2017 WMT News dataset [URL_CITE] . All sentences are longer than 20, making the dataset appropriate for testing the exposure bias problem.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt17/\nSection Titles (if exists):  ['4 Experiments', '4.3 Neural Language Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://minorthird.sourceforge.net/\nCitation Context: For our supervised machine learning experiments on automatic classification of comments as qualified or bald, we used the Support Vector Machine classifier in the MinorThird toolkit (Cohen, 2004) [URL_CITE] with the de-fault linear kernel. We report average classification accuracy and average Cohen\u2019s Kappa using 10-fold cross-validation.\nFootnote or Reference text (if exists): William Cohen. 2004. Minorthird: Methods for Iden-tifying Names and Ontological Relations in Text us-ing Heuristics for Inducing Regularities from Data. http://minorthird.sourceforge.net/\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.procyclingstats.com/team/bahrain-merida-2019\nCitation Context: \u2022 Professional websites: To annotate the triple (Bahrain-Merida 2019, hasPart, Carlos Betancur), you may search the person in professional websites, such as [URL_CITE] https://www.procyclingstats.com/team/bahrain-merida-2019. Since there is no Carlos Betancur listed in that website, please choose false.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Literature Review']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/ajaech/querycompletion\nCitation Context: We use Adadelta (Zeiler, 2012) and tune the online learn-ing rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task. [URL_CITE]\nFootnote or Reference text (if exists): 2 Code at http://github.com/ajaech/query completion\nSection Titles (if exists):  ['4 Experiments', '4.1 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~cprose/SIDE.html\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\nCitation Context: \u2022 The threshold for CFCS as classifica-tion experiments (Section 6.2 (1)) we calculated by tuning for best balanced accuaracy [URL_CITE] https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['C Reproducibility Checklist', 'C.2 Neural Conversion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/English_grammar#Semantic_gradability\nCitation Context: \u2022 Notgood,quantifiablebetter, best etcgradable. usually [URL_CITE] wordsmake sucha claimas bald, as there is no qualified definition of being good or better.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org/wiki/English_ grammar#Semantic_gradability\nSection Titles (if exists):  ['3 Annotation Scheme']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Support"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dstc8-track2/NOESIS-II/tree/master/subtask4\nCitation Context: \u2022 Feed-Forward. This is the baseline model [URL_CITE] from DSTC-8 task organizers that has the best result for task 4 (Kummerfeld et al., 2019), which is trained by employing a two-layer feed-forward neural network on a set of 77 hand engineered features combined with word average embed-dings from pretrained Glove embeddings.\nFootnote or Reference text (if exists): 3 https://github.com/dstc8-track2/NOESIS-II/tree/master/subtask4\nSection Titles (if exists):  ['5 Experiments', '5.3 Experiment III: Disentanglement']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sarathchandar.in/bridge-corrnet\nCitation Context: In this paper, we propose Bridge Correlational Neu-ral Networks which can learn common representa-tions for multiple views even when parallel data is available only between these views and a pivot view. Our method performs better than the existing state of the art approaches on the cross language clas-sification task and gives very promising results on the cross modal access task. We also release a new multilingual image caption benchmark (MIC bench-mark) which will help in further research in this field [URL_CITE] .\nFootnote or Reference text (if exists): 5 Details about the MIC benchmark and performance of var-ious state-of-the-art models will be maintained at http://sarathchandar.in/bridge-corrnet\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryankiros/skip-thoughts/\nCitation Context: Skip-thought As a second baseline we use the sentence representations produced by the skip-thought architecture (Kiros et al., 2015). [URL_CITE] Skip-thought is a recently proposed method that learns sentence representations in a different way from ours, by using recurrent neural networks. This al-lows it to take word order into account.\nFootnote or Reference text (if exists): 3 The code and the trained models can be down-loaded from https://github.com/ryankiros/skip-thoughts/.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/x6cOpVSZcdo\nCitation Context: An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [URL_CITE] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files.\nFootnote or Reference text (if exists): 2 Demo: https://youtu.be/x6cOpVSZcdo\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Microsoft/NeuronBlocks\nCitation Context: An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [URL_CITE] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files.\nFootnote or Reference text (if exists): 1 Code: https://github.com/Microsoft/NeuronBlocks\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/YerevaNN/WARP\nCitation Context: In this paper, we introduce a novel technique to find optimal prompts. We call our method WARP: Word-level Adversarial RePrograming [URL_CITE] . The method is inspired by adversarial reprogramming (Elsayed et al., 2019) \u2014 a method of adding ad-versarial perturbations to an input image that re-programs a pretrained neural network to perform classification on a task other than the one it was originally trained for.\nFootnote or Reference text (if exists): 1 Our implementation is publicly available at: https: //github.com/YerevaNN/WARP\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sarathchandar.in/bridge-corrnet\nCitation Context: Even on this task we report promising results. Code and data used in this paper can be downloaded from [URL_CITE] http: //sarathchandar.in/bridge-corrnet.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dwadden/dygiepp\nCitation Context: For instance, propagat-ing span representations via predicted coref-erence links can enable the model to disam-biguate challenging entity mentions. Our code is publicly available at [URL_CITE] https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bioai.lab.asu.edu/logicgridpuzzles\nCitation Context: This is the first learning system that can solve logic grid puzzles described in natu-ral language in a fully automated manner. The code and the data will be made pub-licly available at [URL_CITE] http://bioai.lab.asu.edu/logicgridpuzzles.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nCitation Context: Suggestion extraction can also be employed for the summarisation of dedicated suggestion fo-rums [URL_CITE] . People often provide the context in such posts, which gets repetitive over a large number of posts. Suggestion mining methods can identify the exact textual unit in the post where a suggestion is conveyed.\nFootnote or Reference text (if exists): 1 https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/statnlp-framework\nCitation Context: In this tutorial, we will be discussing how such a wide spectrum of existing structured predic-tion models can all be implemented under a uni-fied framework [URL_CITE] that involves some basic building blocks. Based on such a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conve-niently and quickly.\nFootnote or Reference text (if exists): 1 http://statnlp.org/statnlp-framework\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/swift-coreml-transformers\nCitation Context: Models can use adapters to convert models to CoreML weights that are suit-able to be embedded inside a iOS application, to enable on-the-edge machine learning. Code is also made available [URL_CITE] . Similar methods can be used for Android devices.\nFootnote or Reference text (if exists): 3 https://github.com/huggingface/ swift-coreml-transformers\nSection Titles (if exists):  ['5 Deployment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.quest.dcs.shef.ac.uk/\nCitation Context: While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA and Q U E ST [URL_CITE] (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).\nFootnote or Reference text (if exists): 2 http://www.quest.dcs.shef.ac.uk/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: Each processing level is independent and can deal with the peculiarities of its type of feature. Machine learning Q U E ST ++ provides scripts to interface the Python toolkit scikit-learn [URL_CITE] (Pedregosa et al., ). This module is indepen-dent from the feature extraction code and uses the extracted feature sets to build and test QE models.\nFootnote or Reference text (if exists): 3 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/multimodal-task.html\nCitation Context: To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [URL_CITE] . The best results of NMT model were those of Huang et al.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/multimodal-task.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/metricsmatr/\nCitation Context: Our choice of metrics was based on their popular-ity in the MT community, their performance in open competitions such as the NIST MetricsMATR chal-lenge (NIST, 2008) [URL_CITE] and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity.\nFootnote or Reference text (if exists): NIST. 2008. NIST MetricsMATR Challenge. Informa-tion Access Division. http://www.itl.nist.gov/iad/mig/tests/metricsmatr/.\nSection Titles (if exists):  ['3 Classifying with MT Metrics', '3.2 MT metrics used']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://sites.google.com/site/xianchaowu2012\nCitation Context: However, few tree/forest-to-string systems have been made open source and this makes it diffi-cult and time-consuming to testify and follow exist-ing proposals involved in recently published papers. The Akamon system [URL_CITE] , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding (Liu et al., 2006; Mi et al., 2008). We hope this system will help re-lated researchers to catch up with the achievements of tree/forest-based translations in the past several years without re-implementing the systems or gen-eral algorithms from scratch.\nFootnote or Reference text (if exists): 2 Code available at https://sites.google.com/site/xianchaowu2012\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.lsi.upc.edu/asiya/\nCitation Context: While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA [URL_CITE] and Q U E ST (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).\nFootnote or Reference text (if exists): 1 http://nlp.lsi.upc.edu/asiya/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html\nCitation Context: We use the publicly available implementation of MSTParser [URL_CITE] (with modifications to the feature com-putation) and its default settings, so the feature weights of the projective and non-projective parsers are trained by the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2006).\nFootnote or Reference text (if exists): 7 http://www.seas.upenn.edu/\u02dcstrctlrn/MSTParser/MSTParser.html\nSection Titles (if exists):  ['6 Experiment', '6.2 Baseline Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/\nCitation Context: Following previous work, we measure the style transfer accuracy using a FastText [URL_CITE] (Joulin et al., 2017) style classifier trained on the respective train-ing set of each dataset. To measure content preser-vation, we use SIM and BLEU as metrics where self-SIM and self-BLEU are computed between the source sentences and system outputs, while ref-SIM and ref-BLEU are computed between the system outputs and human references when avail-able.\nFootnote or Reference text (if exists): 5 https://fasttext.cc/\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lsi.upc.es/srlconll/home.html\nCitation Context: We evaluated our system using the setup of the Conll 2005 semantic role labeling task. [URL_CITE] Thus, we trained on Sections 2-21 of PropBank and used Section 24 as development data. Our test data includes both the selected portion of Section 23 of PropBank, plus the extra data on the Brown corpus.\nFootnote or Reference text (if exists): 2 http://www.lsi.upc.es/srlconll/home.html\nSection Titles (if exists):  ['8 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.umiacs.umd.edu/~hal/megam/\nCitation Context: The optimal weights for each feature are estimated using the Pairwise Ranking Op-timization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM [URL_CITE] . We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM.\nFootnote or Reference text (if exists): 4 http://www.umiacs.umd.edu/~hal/megam/\nSection Titles (if exists):  ['5 Features and Tuning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~ravichan/YASMET.html\nCitation Context: Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smooth-ing and using incremental feature selection as de-scribed in (Berger et al., 1996) and implemented in the yasmetFS package. [URL_CITE] Note that restricting our-selves to only backward looking features decreases the performance significantly, as we can see in Ta-ble 2.\nFootnote or Reference text (if exists): 6 http://www.isi.edu/\u02dcravichan/YASMET.html\nSection Titles (if exists):  ['3 Adjacency Pairs', '3.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.waikato.ac.nz/ml/weka/\nCitation Context: Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights. To this end, we use SVM-Regression [URL_CITE] (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system. We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown cor-pus (Section 5).\nFootnote or Reference text (if exists): 2 Weka software (http://www.cs.waikato.ac.nz/ml/weka/)\nSection Titles (if exists):  ['2 Related Work and Experimental Framework']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/docs/streaming-api/methods\nCitation Context: All tweets were col-lected from September 2010 to January 2011 via the Twitter API. [URL_CITE] From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O\u2019Connor et al. (2010)).\nFootnote or Reference text (if exists): 1 https://dev.twitter.com/docs/streaming-api/methods\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Coverage measures the ratio of source words being aligned by any target words (Tu et al., 2016). Firstly, we trained an alignment model on the authentic parallel data by fast-align [URL_CITE] . Then we used the alignment model to force-align the mono-lingual sentences and the synthetic target sentences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/fast_align\nSection Titles (if exists):  ['A Appendix', 'A.2 Linguistic Properties']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/word2vec/\nCitation Context: We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [URL_CITE] proposed in (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep\nSection Titles (if exists):  ['3 Experiments', '3.2 Training Strategy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nCitation Context: For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. [URL_CITE] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/saffsd/langid.py\nCitation Context: We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py [URL_CITE] (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source-target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2.\nFootnote or Reference text (if exists): 1 https://github.com/saffsd/langid.py\nSection Titles (if exists):  ['6 Data and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/go-through#go-through\nCitation Context: In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [URL_CITE]\nFootnote or Reference text (if exists): 4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60\nSection Titles (if exists):  ['3 Corpus', '3.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/penumbra\nCitation Context: In contrast, the use of penumbra in (2) is more creative and novel. Its literal meaning is \u201can area covered by the outer part of a shadow.\u201d [URL_CITE] Its metaphoric meaning is seldom encountered: Shadows follow objects that cast them, and espe-cially penumbras can be perceived as having fuzzy outlines; attributes which are picked up by the metaphorical sense of a rather unspecified group of people following someone in differing vicinity.\nFootnote or Reference text (if exists): 1 https://www.macmillandictionary.com/dictionary/american/penumbra\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/retriever.html\nCitation Context: Lycos Retriever [URL_CITE] is something new on the Web: a patent-pending information fusion engine. That is, unlike a search engine, rather than returning ranked documents links in response to a query, Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia article. These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching.\nFootnote or Reference text (if exists): 1 http://www.lycos.com/retriever.html. Work on Retriever was done while author was employed at Lycos.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.org\nCitation Context: Lycos Retriever is something new on the Web: a patent-pending information fusion engine. That is, unlike a search engine, rather than returning ranked documents links in response to a query, Lycos Re-triever categorizes and disambiguates topics, col-lects documents on the Web relevant to the disambiguated sense of that topic, extracts para-graphs and images from these documents and ar-ranges these into a coherent summary report or background briefing on the topic at something like the level of the first draft of a Wikipedia [URL_CITE] article. These topical pages are then arranged into a browsable hierarchy that allows users to find re-lated topics by browsing as well as searching.\nFootnote or Reference text (if exists): 2 http://www.wikipedia.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/licensed-sw/RSTTool/index.html\nCitation Context: This response includes three disfluencies (EDUs 3, 6, and 9), which are satellites of the corresponding repair nuclei. In addition, the re-sponse also includes an awkward Comment-Topic relation between EDU 2 and the node combin-ing EDUs 3-11, indicated by awkward-Comment-Topic-2; in this multinuclear relation, the annota-tor judged that the second branch of the relation was awkward, which is indicated by the [URL_CITE] that was appended to the relation label.\nFootnote or Reference text (if exists): 2 Downloaded from http://www.isi.edu/licensed-sw/RSTTool/index.html\nSection Titles (if exists):  ['3 Annotation', '3.1 Guidelines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/\nCitation Context: In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [URL_CITE]\nFootnote or Reference text (if exists): 4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60\nSection Titles (if exists):  ['3 Corpus', '3.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [URL_CITE] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html\nCitation Context: In the second experiment, we use the reverse order of the two corpora and denote it by \u201cB-to-WSJ\u201d. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool [URL_CITE] with the stan-dard head rules (Yamada and Matsumoto, 2003).\nFootnote or Reference text (if exists): 3 http://w3.msi.vxu.se/\u02dcnivre/research/Penn2Malt.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/tapas\nCitation Context: For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset. For T A P AS we used the trained BASE (reset) models [URL_CITE] for Wik-iSQL and applied to the lookup subsets of the dev and test sets.\nFootnote or Reference text (if exists): 4 https://github.com/google-research/tapas\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/robertostling/eflomal/\nCitation Context: Dictionary Extraction As our data augmenta-tion technique requires a dictionary, we propose to extract the dictionary from parallel corpora using a statistical word aligner, eflomal [URL_CITE] (\u00d6stling and Tiedemann, 2016). Once we produce word align-ments between tokens in our parallel corpora, we simply take word alignments that appear over 20 times to produce our bilingual dictionary.\nFootnote or Reference text (if exists): 7 https://github.com/robertostling/eflomal/\nSection Titles (if exists):  ['3 AfroBART', '3.2 Dictionary Augmentation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt15/\nCitation Context: (2016) demonstrated that bpe boosts NMT, so sim-ilar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora [URL_CITE] to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English\u2013Turkish (En\u2013Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt15/\nSection Titles (if exists):  ['4 Experimental Study', '4.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://w3.usf.edu/FreeAssociation/\nCitation Context: To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set [URL_CITE] (Nelson et al., 2004), and (2) the VerbNet verb lexicon (Kipper et al., 2004; Kipper et al., 2008).\nFootnote or Reference text (if exists): 3 http://w3.usf.edu/FreeAssociation/\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/verb-index/\nCitation Context: To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set (Nelson et al., 2004), and (2) the VerbNet verb lexicon [URL_CITE] (Kipper et al., 2004; Kipper et al., 2008).\nFootnote or Reference text (if exists): 4 http://verbs.colorado.edu/verb-index/\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1002/9781444395037.ch5\nCitation Context: Semantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002). OntoNotes (Hovy et al., 2006), lambda- DCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and Baldridge, 2011) [URL_CITE] (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations. A comparison of semantic representations for natural language se-mantics is described in Abend and Rappoport.\nFootnote or Reference text (if exists): Mark Steedman and Jason Baldridge. 2011. Combina-tory Categorial Grammar, Wiley-Blackwell, pages 181\u2013224. https://doi.org/10.1002/9781444395037.ch5.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.rochester.edu/~gildea/Verbs/\nCitation Context: Slot structures are similar to verb frames, which are manually created for the PropBank annota-tion (Palmer et al., 2005). [URL_CITE] An example of the PropBank frame for the verb to kill is:\nFootnote or Reference text (if exists): 6 http://www.cs.rochester.edu/\u02dcgildea/Verbs/\nSection Titles (if exists):  ['5 Creating Templates']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes\nCitation Context: Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score [URL_CITE] , , i.e., the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes.\nFootnote or Reference text (if exists): 3 Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://verbs.colorado.edu/semlink/\nCitation Context: The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenom-ena. Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007). [URL_CITE]\nFootnote or Reference text (if exists): 7 https://verbs.colorado.edu/semlink/\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017) [URL_CITE] . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test.\nFootnote or Reference text (if exists): 3 Scripts publicly available at https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [URL_CITE] and fairseq .\nFootnote or Reference text (if exists): 1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.stanford.edu/%7ejurafsky/ws97/\nCitation Context: (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [URL_CITE] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).\nFootnote or Reference text (if exists): 7 http://web.stanford.edu/%7ejurafsky/ws97/\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.github.com/festvox/datasets/CMUDoG\nCitation Context: In this paper we introduce a crowd-sourced con-versations dataset that is grounded in a predefined set of documents which is available for download [URL_CITE] . We perform multiple automatic and human judgment based analysis to understand the value the information from the document provides to the generation of responses.\nFootnote or Reference text (if exists): 4 https://www.github.com/festvox/datasets/CMU DoG\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2012T21\nCitation Context: To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [URL_CITE] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well.\nFootnote or Reference text (if exists): 4 https://catalog.ldc.upenn.edu/LDC2012T21\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/\nCitation Context: (2004)). Our biomedi-cal data comes from the GENIA treebank [URL_CITE] (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. We downloaded additional sentences from Medline for our self-trained MEDLINE corpus.\nFootnote or Reference text (if exists): 8 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/PubMed/\nCitation Context: Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. [URL_CITE] We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE .\nFootnote or Reference text (if exists): 9 http://www.ncbi.nlm.nih.gov/PubMed/\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://archive.org/details/twitterstream\nCitation Context: Social Media Corpora Our English Twitter corpus is obtained from Archive Team\u2019s Twitter stream grab [URL_CITE] . The Chinese Weibo corpus comes from Open Weiboscope Data Access 5 (Fu et al., 2013).\nFootnote or Reference text (if exists): 4 https://archive.org/details/twitterstream\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academic.research.microsoft.com\nCitation Context: Our second keyphrase dataset comes from the Mi-crosoft Academic (MSRA) search corpus (Mi-crosoft, 2015) [URL_CITE] . While particularly focused on\nFootnote or Reference text (if exists): Microsoft. 2015. Microsoft academic search. http://academic.research.microsoft.com. Accessed: 2015-2-26.\nSection Titles (if exists):  ['4 Overlap with Keyphrase Corpora', '4.2 Microsoft Academic (MSRAC ORPUS )']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msci.com/gics\nCitation Context: Breakdown of results by industry. We ana-lyze errors on the validation data by segmenting earnings calls by each company\u2019s Global Industry Classification Standard (GICS) sector [URL_CITE] . See Fig-ure 2 for the breakdown results.\nFootnote or Reference text (if exists): 21 See https://www.msci.com/gics. There are 11 broad industry sectors.\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.4 Results.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://casmacat.eu/corpus/\nCitation Context: Our data comes primarily from WikiMatrix (Schwenk et al., 2019), Global Voices, [URL_CITE] EuroParl\nFootnote or Reference text (if exists): 5 http://casmacat.eu/corpus/ Bootstrap resampling (Koehn, 2004; Graham et al., global-voices.html 2014) is used to estimate confidence intervals for\nSection Titles (if exists):  ['1 Introduction', '4.2 Model Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cordova.apache.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova [URL_CITE] \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 3 https://cordova.apache.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.iptc.org\nCitation Context: We suggest the we adopt text categorization on top of topic detection and tracking, similar to Figure 3. There has been good results in text categorization (see, e.g., (Yang and Liu, 1999; Sebastiani, 2002)) The pre-defined categories would form the static hierarchy \u2013 the IPTC Subject Reference System [URL_CITE] , for example \u2013 on top of all event-based information organization, and the mod-els for the categories could be built on the basis of the test set.\nFootnote or Reference text (if exists): 2 International Press Telecommunications Council, http://www.iptc.org\nSection Titles (if exists):  ['5 Dynamic Hierarchies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/page/demoview/TempSys\nCitation Context: We have implemented our system in a web-based demo [URL_CITE] . Figure 2 shows a screenshot of the input panel of the system.\nFootnote or Reference text (if exists): 4 http://cogcomp.cs.illinois.edu/page/demoview/TempSys\nSection Titles (if exists):  ['4 The Demonstration', '4.1 Visualization']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlpviz.bpodgursky.com\nCitation Context: Figure 1 was generated from (Podgursky, 2015) [URL_CITE] , which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom).\nFootnote or Reference text (if exists): Ben Podgursky. 2015. Nlpviz. http://nlpviz.bpodgursky.com. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nationsreportcard.gov/reading_2017/nation/achievement?grade=4\nCitation Context: According to the results of the 2017 National As-sessment of Educational Progress (NAEP) [URL_CITE] , 32% of U.S. 4 th graders read below the Basic level. Most such students lack foundational skills of oral reading fluency \u2013 accuracy, reading rate, and prosody. Furthermore, more than a million stu-dents at the Basic level are also relatively slow readers, have poor prosody, and make more errors than skilled readers (Sabatini et al., 2018).\nFootnote or Reference text (if exists): 1 https://www.nationsreportcard.gov/reading_2017/nation/achievement?grade=4\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://readium.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium [URL_CITE] , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 4 https://readium.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sodawater/SentenceOrdering\nCitation Context: SIND, ROCStory: SIND is a visual storytelling dataset (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset [URL_CITE] (Wang and Wan, 2019; Mostafazadeh et al., 2016). It is randomly split by 8:1:1 for the training/validation/test sets.\nFootnote or Reference text (if exists): 4 https://github.com/sodawater/SentenceOrdering\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingyaohsu/VIST-Edit\nCitation Context: We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [URL_CITE] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.\nFootnote or Reference text (if exists): 1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jflanigan/jamr/tree/Generator\nCitation Context: The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMR-gen [URL_CITE] (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).\nFootnote or Reference text (if exists): 5 https://github.com/jflanigan/jamr/tree/Generator\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tombosc/cpae\nCitation Context: For the evaluation of definition embeddings, we used the modified Word Embedding Benchmarks projects [URL_CITE] , following Bosc and Vincent (2018). These benchmarks include SimLex999 (SL999), SimLex333 (SL333) (Hill et al., 2015), SimVerb (SV) (Gerz et al., 2016), MEN (Bruni et al., 2014), RG (Rubenstein and Goodenough, 1965), WS353 (Finkelstein et al., 2002), SCWS (Huang et al., 2012), and MTurk (Radinsky et al., 2011; Halawi et al., 2012).\nFootnote or Reference text (if exists): 3 https://github.com/tombosc/cpae\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Definition Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://research.microsoft.com/mct\nCitation Context: Datasets: We use two datasets for our evaluation. (1) First is the MCTest-500 dataset [URL_CITE] , a freely available set of 500 stories (split into 300 train, 50 dev and 150 test) and associated questions (Richardson et al., 2013). The stories are fictional so the answers can be found only in the story it-self.\nFootnote or Reference text (if exists): 1 http://research.microsoft.com/mct\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://visionandlanguage.net/VIST/dataset.html\nCitation Context: SIND, ROCStory: SIND is a visual storytelling dataset [URL_CITE] (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset (Wang and Wan, 2019; Mostafazadeh et al., 2016).\nFootnote or Reference text (if exists): 3 http://visionandlanguage.net/VIST/dataset.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: In Universal Dependencies, Dozat et al. (2017) [URL_CITE] empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ivri/dmorph\nCitation Context: (2017) against the reported performance of their encoder-decoder model. [URL_CITE] Notice Vylomova et al. (2017) show that providing the LSTM with the POS of the deriva-tive increases performance.\nFootnote or Reference text (if exists): 8 The dataset is available at https://github.com/ivri/dmorph. While Vylomova et al. (2017) take morpho-orthographic changes into account, we only predict affixes, not the accompanying changes in orthography (Section 4.1).\nSection Titles (if exists):  ['4 Experiments', '4.3 Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Indeed, Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.donga.com\nCitation Context: For latent morpheme representations, we used the Donga news article corpus. [URL_CITE] The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others.\nFootnote or Reference text (if exists): 3 http://www.donga.com\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://catalog.ldc.upenn.edu/LDC2006T03\nCitation Context: We view our work as building on the efforts of the Penn Korean PropBank (PKPB). [URL_CITE] Our corpus is roughly similar in size to the PKPB, and taken together, the two Korean corpora now total about half the size of the Penn English PropBank. One advantage of our corpus is that it is built on top of the ETRI Korean corpus, which uses a richer Ko-rean morphological tagging scheme than the Penn Korean Treebank.\nFootnote or Reference text (if exists): 2 http://catalog.ldc.upenn.edu/LDC2006T03\nSection Titles (if exists):  ['2 A Semantically Annotated Korean Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html\nCitation Context: In order to understand the task of lexicon acquisi-tion, we briefly describe the Japanese morpholog-ical analyzer JUMAN. [URL_CITE] We explain Japanese mor-phemes in Section 2.1, morphological constraints in Section 2.2, and unknown morpheme processing in Section 2.3.\nFootnote or Reference text (if exists): 1 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html\nSection Titles (if exists):  ['2 Japanese Morphology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csc.fi/kielipankki/\nCitation Context: Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001) [URL_CITE] . We obtained a lexicon of 25k morphs by feeding the learning algorithm with the word list containing the 160k most common words.\nFootnote or Reference text (if exists): CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other. http://www.csc.fi/kielipankki/.\nSection Titles (if exists):  ['4 Speech recognition experiments', '4.2 Finnish']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.naist.jp/hiki/ChaSen/\nCitation Context: We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen [URL_CITE] 2.3.3 and then labeled the NEs. Unreadable to-kens such as parentheses were removed in to-kenization.\nFootnote or Reference text (if exists): 1 http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://kkma.snu.ac.kr/\nCitation Context: The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer [URL_CITE] on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions.\nFootnote or Reference text (if exists): 4 http://kkma.snu.ac.kr/\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lancers.jp/\nCitation Context: We use a crowdsourcing application, Lancers, [URL_CITE] to perform substitute extraction, substitute evalua-tion, and substitute ranking. In each task, we re-quested the annotators to complete at least 95% of their previous assignments correctly. They were native Japanese speakers.\nFootnote or Reference text (if exists): 3 http://www.lancers.jp/\nSection Titles (if exists):  ['4 Balanced dataset for evaluation of Japanese lexical simplification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com/\nCitation Context: We conducted user study via crowdsourcing using the FigureEight [URL_CITE] platform. To ensure the quality of annotators, we used a dedicated expert-worker pool provided by FigureEight. We present the detailed setup of our user studies as follows.\nFootnote or Reference text (if exists): 6 https://www.figure-eight.com/\nSection Titles (if exists):  ['B Human Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clinc/oos-eval\nCitation Context: We also thank Jin Qu, Tian Xie, Xinyi Yang, and Yingbo Zhou for their support in the deployment of DNNC into the internal system. Appendix A Training Details Dataset preparation To use the CLINC150 dataset (Larson et al., 2019) [URL_CITE] in our ways, espe-cially for the single-domain experiments, we pro-vide preprocessing scrips accompanied with our code.\nFootnote or Reference text (if exists): 9 https://github.com/clinc/oos-eval.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: At the time of writing, the GVDB contains 7,366 fully annotated articles (Table 1) coming from 1,512 US cities, and the database is continuing to grow. The latest version of the database will be main-tained and available for download at [URL_CITE] http://gun-violence.org/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 The Gun Violence Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mindspore.cn/\nCitation Context: We thank Mindspore [URL_CITE] which is a new deep learning computing framework for the partial support of this work.\nFootnote or Reference text (if exists): 3 https://www.mindspore.cn/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: We also thank the students of the University of Pennsylvania\u2019s crowdsourcing class (NETS 213) for their involvement in building and testing a useful crowdsourcing pipeline for in-formation extraction. Finally, we thank the devel-opers at 10clouds for the excellent engineering and design of [URL_CITE] http://gun-violence.org/. Amit Singhal.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (acknowledgment of developers)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [URL_CITE] and optimized with Adam (Kingma and Ba, 2014) .\nFootnote or Reference text (if exists): 20 https://pytorch.org/\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/nni\nCitation Context: To make a fair compari-son, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform. [URL_CITE] Due to the limited number of test data in each domain, we run each approach five times and take the average number. Section A.2 in the supplementary material provides the search space of hyper-parameters for each approach and the pre-processing procedures of logical forms.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/nni\nSection Titles (if exists):  ['4 Experimental Setup', '4.2 Implementations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/diegma/neural-dep-srl\nCitation Context: When we stack GCNs on top of LSTM layers, we obtain a substantial improve-ment over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for En-glish and Chinese. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/diegma/neural-dep-srl.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/ajaech/querycompletion\nCitation Context: We use Adadelta (Zeiler, 2012) and tune the online learn-ing rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task. [URL_CITE]\nFootnote or Reference text (if exists): 2 Code at http://github.com/ajaech/query completion\nSection Titles (if exists):  ['4 Experiments', '4.1 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jzhou316/Unsupervised-Sentence-Summarization\nCitation Context: We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [URL_CITE] The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/jzhou316/Unsupervised-Sentence-Summarization.\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/andy194673/nlg-sclstm-multiwoz\nCitation Context: The best model for both datasets was found through a grid search over a set of hyper-parameters such as the size of embeddings, learn-ing rate, and number of LSTM layers. [URL_CITE]\nFootnote or Reference text (if exists): 6 The model is publicly available at https://github.com/andy194673/ nlg-sclstm-multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.3 Dialogue-Act-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cogcomp.cs.illinois.edu/page/publication_view/790\nCitation Context: Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space re-quired by a factor of 16. The truncated word embeddings are available from the papers web page at [URL_CITE] https://cogcomp.cs.illinois.edu/page/publication_view/790.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://labs.google.com/sets\nCitation Context: Even for state of the art methods, expansion er-rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art systems such as GoogleSets [URL_CITE] , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed instances for the target set Roman Gods:\nFootnote or Reference text (if exists): 1 http://labs.google.com/sets the North American Chapter of the ACL, pages 290\u2013298,\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.everytrail.com/best/\nCitation Context: In our setting, we take as input a natural language query and extract entities from a single web page. trails near Baltimore) and a web page (e.g., [URL_CITE] http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.). Figure 1 summarizes the task setup.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.epinions.com\nCitation Context: The files retrieved from the web are converted in xml format, which allows preserving the metadata information. As an example, Figure 1 shows the organization of this xml file from a review retrieved from the website epinions.com ( [URL_CITE] http://www.epinions.com).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 The System and its Interface']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jknack/handlebars.java\nCitation Context: The output component employs a tem-plate engine [URL_CITE] to convert the data collected by the processing component into HTML and JavaScript for handling display and user interac-tion in the web browser.\nFootnote or Reference text (if exists): 7 https://github.com/jknack/handlebars. java\nSection Titles (if exists):  ['4 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.psychpark.org\nCitation Context: Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark ( [URL_CITE] http://www.psychpark.org) (Bai, 2001) and John Tung Foundation (http://www.jtf.org.tw).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [URL_CITE] and fairseq .\nFootnote or Reference text (if exists): 1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.elsevier.com\nCitation Context: Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website [URL_CITE] . The documents were taken from the medical domain within the sub-domain of \u201cbreast cancer\u201d.\nFootnote or Reference text (if exists): 2 http://www.elsevier.com\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/~jrennie/20Newsgroups/\nCitation Context: In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [URL_CITE] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).\nFootnote or Reference text (if exists): 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dbpedia.org/\nCitation Context: To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia , or DBpedia. [URL_CITE] Only facts that are as-serted in and gleaned from the source documents are posited.\nFootnote or Reference text (if exists): 3 http://www.dbpedia.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sogou.com/labs/dl/c.html\nCitation Context: Data preparation The data is composed of two parts: source documents and background knowl-edge. For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news [URL_CITE] including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Mil-itary. We then randomly but equally select 600 articles as the set of source documents from 9 top-ics without data bias.\nFootnote or Reference text (if exists): 2 http://www.sogou.com/labs/dl/c.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Evaluation as a ranking problem']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) [URL_CITE] and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P15-2041\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015 [URL_CITE] ; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Wenduan Xu, Michael Auli, and Stephen Clark. 2015. CCG supertagging with a recurrent neural network. In ACL. Association for Computational Linguistics, Beijing, China, pages 250\u2013255. http://www.aclweb.org/anthology/P15-2041.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1027\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016 [URL_CITE] ; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. 2016. Supertagging with LSTMs. In NAACL. Association for Computational Linguistics, San Diego, California, pages 232\u2013237. http://www.aclweb.org/anthology/N16-1027.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016 [URL_CITE] ), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017 [URL_CITE] ) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: This concatenation af-ter each layer differs from Kasai et al. (2017) [URL_CITE] and Lewis et al. (2016), where concatenation happens only after the final BiLSTM layer.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/F1score\nCitation Context: On an average, 40% of replies in each thread and 77% of first replies were seen to be solutions, leading to an F-measure of 53% for our initializa-tion heuristic. We use the F-measure [URL_CITE] for solu-tion identification, as the primary evaluation mea-sure. While we vary the various parameters sep-arately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set \u03bb = 0.5 and \u03c4 = 0.4 unless other-wise mentioned.\nFootnote or Reference text (if exists): 12 http://en.wikipedia.org/wiki/F1 score\nSection Titles (if exists):  ['5 Experimental Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://tizirinagh.github.io/acl2019/\nCitation Context: We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [URL_CITE] . We then evaluated the generative model and the generative + discriminative model with the Snorkel architecture on the test set with the results in Table 2.\nFootnote or Reference text (if exists): 3 https://tizirinagh.github.io/acl2019/\nSection Titles (if exists):  ['5 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [URL_CITE] . Three sets of NLP tasks are used for evaluation, with the statistics of all the seven public datasets reported in Table 1:\nFootnote or Reference text (if exists): 8 We use Google\u2019s official base models. See: https://github.com/google-research/bert and https://github.com/google-research/albert.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/albert\nCitation Context: We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [URL_CITE] . Three sets of NLP tasks are used for evaluation, with the statistics of all the seven public datasets reported in Table 1:\nFootnote or Reference text (if exists): 8 We use Google\u2019s official base models. See: https://github.com/google-research/bert and https://github.com/google-research/albert.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/tpederse/namedata.html\nCitation Context: We create our dataset using name-conflate [URL_CITE] to extract instances containing the conflate words from the 2005 Medline Baseline. Table 4 shows our cur-rent set of conflated words with their corresponding number of test (test) and training (train) instances. We refer to the conflated words as their pseudowords throughout the paper.\nFootnote or Reference text (if exists): 2 http://www.d.umn.edu/tpederse/namedata.html\nSection Titles (if exists):  ['5 Data', '5.3 Conflate Test Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/andreasvc/disco-dop\nCitation Context: We evaluate our parser on the test sets of the three treebanks. We report F-measure and discontinuous F-measure as computed using the disco-dop tool [URL_CITE] with standard parameters in Table 2. ants of our parsers produced exactly the same re-sults in all settings. This may be expected as their cover of the original treebanks are almost similar.\nFootnote or Reference text (if exists): 11 https://github.com/andreasvc/ disco-dop\nSection Titles (if exists):  ['4 Experiments', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.cornell.edu/~ainur/data.html\nCitation Context: Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) [URL_CITE] , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) .\nFootnote or Reference text (if exists): 3 http://www.cs.cornell.edu/~ainur/data. html\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Datasets\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~mdredze/datasets/sentiment/\nCitation Context: And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD [URL_CITE] (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews.\nFootnote or Reference text (if exists): 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cornell.edu/People/pabo/movie-review-data/\nCitation Context: And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset [URL_CITE] (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews.\nFootnote or Reference text (if exists): 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/~jrennie/20Newsgroups/\nCitation Context: In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [URL_CITE] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).\nFootnote or Reference text (if exists): 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.umiacs.umd.edu/~jbg/static/downloads_and_media.html\nCitation Context: We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. [URL_CITE]\nFootnote or Reference text (if exists): 6 We followed Pang and Lee\u2019s method for creating a nu-merical score between 0 and 1 from a star rating. We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not as-sume integer values). The German movie review corpus is available at http://www.umiacs.umd.edu/\u02dcjbg/static/downloads_and_media.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Sentiment Prediction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.jhu.edu/~mdredze/datasets/sentiment/\nCitation Context: Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Datasets\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList. Among the word-level perturbations, Deletion, Repetition, Singular/plural verbs, Word order, and Verb tense were not already used to test the robustness. However, Negation was included in CheckList, and Replacement with Synonyms was used for adversarial attack (Dong et al., 2020; Ren et al., 2019).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['4 Perturbation methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Some types of perturbation utilized in this work were already tested in previous work on adversarial attacks on NLP systems (Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ). However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kandorm/CLINE\nCitation Context: To be more spe-cific, our model achieves +1.6% absolute improve-ment on 4 contrastive test sets and +0.5% absolute improvement on 4 adversarial test sets compared to RoBERTa model (Liu et al., 2019). That is, with the training on the proposed objectives, CLINE si-multaneously gains the robustness of adversarial attacks and sensitivity of semantic changes [URL_CITE] .\nFootnote or Reference text (if exists): 1 The source code of CLINE will be publicly available at https://github.com/kandorm/CLINE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: CheckList enabled generating new (perturbed) test samples through abstracting different test types aimed at testing linguistic capabilities. Other studies focused on evaluating robustness to perturbed inputs for machine translation (Belinkov and Bisk, 2018; Niu et al., 2020), perturbation sensitivity analysis for detecting unintended model biases (Prabhakaran et al., 2019), or robustness to adversarial perturbations (Alshemali and Kalita, 2020 [URL_CITE] ; Ebrahimi et al., 2018; Liang et al., 2018). However, a comprehensive methodology for evaluating the performance of NLP models under real-world conditions is still missing.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/google-research/tree/master/routing_transformer\nCitation Context: Our imple-mentations are based on the open-source codebases of REALM and the Routing Transformer. [URL_CITE] Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search). Since our retriever is frozen, we do not use the document index refresher available in their codebase.\nFootnote or Reference text (if exists): 20 https://github.com/google-research/google-research/tree/master/routing_ transformer REALM predicted the correct retrieval. Notice very small differences in generation quality (R-L) as well as the fraction of n-grams (n-g) in the generation overlap-ping with retrievals predicted by C -REALM (vs pre-dicted retr.). To control for overlap due to stopwords, we also add n-gram overlaps with the randomly sam-pled retrievals. ble 13, notice very little difference with and without retrieval. no effect other than being a symbol of an economic entity or symbol of an era or location, and nothing more. For example look at Sears, Sears didn\u2019t care what went on inside, it was all about the _appearance_ of its location, the prestige of the location, the facilities and so on. It didn\u2019t care about how long it took it to operate, it was about how much people would pay to go see it. Sears was a landmark as a cultural movement and other big companies followed suit, so if you want to see a building you\u2019ve never seen before, you have to go see Sears, just like you have to see a Toyota Camry for Toyota Camry. They used to be all about building new factories, some of them if I recall, but now that they\u2019re bigger, that means that more factory jobs are coming to them. You\u2019ve probably seen them in stores as stores where people buy and sell stuff, so there aren\u2019t that many places for them to come from. Instead, it\u2019s just for show, a symbol of rich people.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.1 Training & Model Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com/\nCitation Context: Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT sys-tem. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system [URL_CITE] are also listed in this table.\nFootnote or Reference text (if exists): 4 http://translate.google.com/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/understanding-mbr\nCitation Context: We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and documentation available at https://github.com/ZurichNLP/understanding-mbr\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nCitation Context: We perform 5-shot KG completion task for all the methods. Our implementation for KG embedding baselines is based on OpenKE [URL_CITE] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature. During training, all triples in back-ground KG G 0 and training set, as well as few-shot reference triples of validation and testing set are used to train models.\nFootnote or Reference text (if exists): 2 https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nSection Titles (if exists):  ['5 Experiments', '5.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: We implement our TrigNet in Pytorch [URL_CITE] and train it on four NVIDIA RTX 2080Ti GPUs. Adam (Kingma and Ba, 2014) is utilized as the optimizer, with the learning rate of BERT set to 2e-5 and of other components set to 1e-3.\nFootnote or Reference text (if exists): 8 https://pytorch.org/\nSection Titles (if exists):  ['4 Experiments', '4.3 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/domluna/memn2n\nCitation Context: For MemN2N, we use the publicly available im-plementation [URL_CITE] and train it exactly as all other mod-els (same optimizer, total epochs, and early stop-ping criteria) for fairness. While the reported best result for MemN2N is on the version with posi-tion encoding, linear start training, and random-injection of time index noise (Sukhbaatar et al., 2015), the version we use has only position encod-ing. Note that the comparison is still meaningful because linear start training and time index noise are not used in DMN+ (and as a result, neither in our proposed DSMN).\nFootnote or Reference text (if exists): 2 https://github.com/domluna/memn2n\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duncanka/lstm-causality-tagger\nCitation Context: The code for DeepCx is available on GitHub. [URL_CITE] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser.\nFootnote or Reference text (if exists): 3 https://github.com/duncanka/lstm-causality-tagger.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tensorflow.org/\nCitation Context: The delay in supertagging can be attributed to sev-eral factors, in particular the differences in net-work architectures including the number of bi- LSTM layers (4 vs. 2) and the use of bilin-ear transformation instead of linear one. There are also many implementation differences in our parser (C++ A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ Tensor-Flow (Abadi et al., 2015) [URL_CITE] supertagger and recur-sive neural model in C++ DyNet (Neubig et al., 2017)).\nFootnote or Reference text (if exists): Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane\u0301, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Vie\u0301gas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Sys-tems. Software available from tensorflow.org. http://tensorflow.org/.\nSection Titles (if exists):  ['6 Experiments', '6.3 English Parsing Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html\nCitation Context: We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010) [URL_CITE] , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct com-parison problematic. Some first steps at analysis of the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.\nFootnote or Reference text (if exists): Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser/tree/easy-to-use\nCitation Context: DeepCx is implemented using a refactored version of the LSTM parser codebase that performs iden-tically to the original. [URL_CITE] The neural network frame-work, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02\u201321; development: 22).\nFootnote or Reference text (if exists): 2 https://github.com/clab/lstm-parser/tree/easy-to-use.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryankiros/neural-storyteller\nCitation Context: Finally, it discusses how the automated corpus-based eval-uations correlate with human judgements. In fu-ture, this work aims to be applied for synthesizing images from text, exploring complementary archi-tectures to projects like neural-storyteller [URL_CITE] where skip-thought embeddings are already used to per-form image captioning with story-style transfer.\nFootnote or Reference text (if exists): 3 https://github.com/ryankiros/ neural-storyteller\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Yes,_and\nCitation Context: Looking at the entire conversation, utterance (4) turns out to be a bad action to take because it offers no way of continuing the conversation. [URL_CITE] two agents using a 4-layer LSTM encoder-decoder trained on the OpenSubtitles dataset. The first turn (index 1) is input by the authors.\nFootnote or Reference text (if exists): 1 A similar rule is often suggested in improvisational comedy: https://en.wikipedia.org/wiki/Yes,_and...\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/iirc\nCitation Context: We fol-low recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at [URL_CITE] https://allennlp.org/iirc.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Liang-Qiu/SVRNN-dialogues\nCitation Context: While on multi-party dia-logue datasets, our model learns an interac-tive structure demonstrating its capability of distinguishing speakers or addresses, automat-ically disentangling dialogues without explicit human annotation. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is released at https://github.com/Liang-Qiu/SVRNN-dialogues.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bitbucket.org/chloebt/discourse/\nCitation Context: We also showed that scores are lowered when considering a realistic setting, relying on predicted tokenization and not assuming gold sen-tences. We make our code available at [URL_CITE] https://bitbucket.org/chloebt/discourse/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['9 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/anjalief/unsupervised_gender_bias\nCitation Context: Ulti-mately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and pre-trained models are available at https: //github.com/anjalief/unsupervised_ gender_bias\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.opensubtitles.org\nCitation Context: Models are trained and tested on fan-produced parallel subtitles from OpenSubtitles2016 [URL_CITE] (Lison and Tiedemann, 2016). The data is first corrected using heuristics (e.g.\nFootnote or Reference text (if exists): 6 http://www.opensubtitles.org\nSection Titles (if exists):  ['4 Experiments', '4.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yftah89/PBLM-Cross-language-Cross-domain\nCitation Context: In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms pre-vious models even when it is trained in the lazy setup and previous models are trained in the full setup. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/yftah89/ PBLM-Cross-language-Cross-domain\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ted.com\nCitation Context: In order to evaluate the effectiveness of our proposed methods, we construct a new dataset, Speaker Annotated TED (SATED) based on TED talks, [URL_CITE] with three language pairs, English-French (en-fr), English-German (en-de) and English-Spanish (en-es) and speaker annotation.\nFootnote or Reference text (if exists): 4 https://www.ted.com\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['7 Experiments', '7.1 Hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\nCitation Context: Fully fine-tuned RoBERTa and the current state-of-the-art method (DeBERT) score higher by 6.5 and 9.2 points, respectively. Linear Classifier 64.2 78.1 74.9 59.2 88.4 WARP 0 70.9 78.8 77.1 72.2 89.8 WARP 1 83.9 87.6 81.6 72.6 93.8 WARP 2 85.4 88.0 81.5 69.7 94.3 WARP [URL_CITE] 86.9 92.4 83.1 68.2 95.9 WARP 8 87.6 93.0 83.8 72.9 95.4 WARP init 86.8 90.4 83.6 80.1 96.0 WARP 20 88.2 93.5 84.5 75.8 96.0 WARP MNLI 86.3\nFootnote or Reference text (if exists): 4 https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\nSection Titles (if exists):  ['4 Experiments on GLUE', '4.1 Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/quankiquanki/\nCitation Context: Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), CLCD English-German S-T D-B M-B B-D M-D B-M D-M English-French All D-B M-B B-D M-D B-M D-M All PBLM Models P+BE 78.7 78.6 80.6 79.2 81.7 78.5 PBLM 70.9 62.9 74.5 66.5 75.0 75.5 Lazy 74.8 74.0 75.1 72.8 73.3 73.7 79.5 81.1 74.7 76.3 75.0 75.1 76.8 76.5 71.0 76.0 67.9 70.3 69.9 67.3 70.4 70.3 73.9 74.2 73.1 75.3 74.4 74.1 72.4 73.9 Autoencoder+pivot Models A-S-SR 68.3 62.5 69.4 69.9 70.2 69 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015) [URL_CITE] ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target A-B A-D A-M All A-B A-D A-M All PBLM Models P+BE 64 6. 65.167.9 62.5 63.6 63.5 66.9 64.8 PBLM 60.9 59.6 60.1 .60 2 60.9 61.9 58.9 60.5 Lazy 66.3 65.0 66.6 66 0.\nFootnote or Reference text (if exists): Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Dataset)\nFUNCTION: Introduce, Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/snowblink14/smatch\nCitation Context: We evaluate AMR and cross-lingual AMR parsing by using the Smatch score [URL_CITE] (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems\u2019 performances, we also re-port the fine-grained scores as computed by the evaluation toolkit 14 of Damonte et al.\nFootnote or Reference text (if exists): 13 https://github.com/snowblink14/smatch\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mdtux89/amr-evaluation\nCitation Context: We evaluate AMR and cross-lingual AMR parsing by using the Smatch score 13 (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems\u2019 performances, we also re-port the fine-grained scores as computed by the evaluation toolkit [URL_CITE] of Damonte et al. (2017).\nFootnote or Reference text (if exists): 14 https://github.com/mdtux89/ amr-evaluation\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/luheng/lsgn\nCitation Context: Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and models: https://github.com/luheng/lsgn\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebastianruder/emnlp2021-multiqa-tutorial\nCitation Context: Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don\u2019t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kakaoenterprise/KorAdvMRSTestData\nCitation Context: Statistics and examples are described in Table 1. We release this data set at [URL_CITE] https: //github.com/kakaoenterprise/ KorAdvMRSTestData.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Adversarial Test Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SapienzaNLP/sgl\nCitation Context: Furthermore, we find that mod-els trained under this configuration scale re-markably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its com-petitors by a large margin without even explic-itly seeing non-English to AMR examples at training time and, once these examples are in-cluded as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at [URL_CITE] https: //github.com/SapienzaNLP/sgl.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt11/\nCitation Context: Next we evaluated our method on a word transla-tion task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were ex-tracted from the publicly available WMT11 [URL_CITE] cor-pus. The experiments were done for two sets of translation: English to Spanish and Spanish to En-glish.\nFootnote or Reference text (if exists): 2 http://www.statmt.org/wmt11/\nSection Titles (if exists):  ['5 Experiments', '5.2 P@k Word Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://rnnlm.org\nCitation Context: In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a) [URL_CITE] to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer.\nFootnote or Reference text (if exists): Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Unlabeled tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: For training, our Our+FULL model is first ini-tialized by pre-training using Our+AVG model, and we set the iteration number over all the train-ing data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe [URL_CITE] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data.\nFootnote or Reference text (if exists): 6 http://nlp.stanford.edu/data/glove.840B.300d.zip\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [URL_CITE] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 4 We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the first word piece as the whole word representation.\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We initialized our word representations using publicly available 300-dimensional Glove vec-tors [URL_CITE] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned.\nFootnote or Reference text (if exists): 5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.\nSection Titles (if exists):  ['5 Experiments', '5.3 Hyperparameters and Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~treebank/tokenizer.sed\nCitation Context: Content was lowercased, then processed by regu-lar expression to collapse the following into respec-tive single symbols: emoticons; URLs; usernames (@mentions); and hashtags. Any content deemed to be a retweet (following the characters RT) was removed. Text was then tokenized according to a modified version of the Penn TreeBank tokenization standard [URL_CITE] that was less English-centric.\nFootnote or Reference text (if exists): 6 Such as codified in http://www.cis.upenn.edu/\u02dctreebank/tokenizer.sed\nSection Titles (if exists):  ['5 Real World Stream: Twitter', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tedlab.mit.edu/\u223cdr/Tgrep2/\nCitation Context: (2005) for an overview and critical comparisons.) TGrep2 [URL_CITE] is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts. It allows Boolean expressions over nodes and regular expressions in-side nodes.\nFootnote or Reference text (if exists): 1 http://tedlab.mit.edu/\u223cdr/Tgrep2/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UppsalaNLP/uuparser\nCitation Context: We use UUParser [URL_CITE] (de Lhoneux et al., 2017a), which is based on the transition-based parser of Kiperwasser and Goldberg (2016), and adapted to UD. It uses the arc-hybrid transition system from Kuhlmann et al.\nFootnote or Reference text (if exists): 2 https://github.com/UppsalaNLP/ uuparser\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 The Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/\nCitation Context: A Replication Instructions Dataset The E2E dataset contains a training set of 42,061 pairs of meaning representations and hu-man authored utterances, 4,672 pairs in the devel-opment set, and 4,693 in the test set. Download the dataset from [URL_CITE] https://github.com/tuetschek/ e2e-dataset .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ir.dlut.edu.cn/EmotionOntologyDownload.aspx\nCitation Context: In this paper, both Chinese [URL_CITE] and English sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002).\nFootnote or Reference text (if exists): 5 DUTIR Sentiment Lexicon is adopt-ed as the Chinese sentiment lexicon: http://ir.dlut.edu.cn/EmotionOntologyDownload.aspx\nSection Titles (if exists):  ['4 Emotion Detection via Bilingual and Sentiment Information', '4.2 Sentimental Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-cleaning\nCitation Context: It uses modified regu-lar expressions from the e2e-cleaning repos-itory. [URL_CITE] https://github.com/tuetschek/ e2e-cleaning\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/facebookresearch/Zero-Shot-DST\nCitation Context: , R t\u22121 , U t }, we add the \"user:\" and \"system:\" prefixes to the user and system utterance respectively. Then all the utterances and slot names s i are concatenated into a single sequence, i.e., user:U [URL_CITE] . .\nFootnote or Reference text (if exists): 1 Source code is available in https://github.com/facebookresearch/Zero-Shot-DST\nSection Titles (if exists): ['3 Methodology', '3.1 T5DST']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasperGuo/Unimer\nCitation Context: Also, pro-gram alias and grammar rules heavily impact the performance of different meaning repre-sentations. Our benchmark, execution engines and implementation can be found on: [URL_CITE] https: //github.com/JasperGuo/Unimer .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: Each processing level is independent and can deal with the peculiarities of its type of feature. Machine learning Q U E ST ++ provides scripts to interface the Python toolkit scikit-learn [URL_CITE] (Pedregosa et al., ). This module is indepen-dent from the feature extraction code and uses the extracted feature sets to build and test QE models.\nFootnote or Reference text (if exists): 3 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.chokkan.org/software/crfsuite/\nCitation Context: For word-level prediction, Q U E ST ++ provides an interface for CRFSuite (Okazaki, 2007) [URL_CITE] , a se-quence labelling C++ library for Conditional Ran-dom Fields (CRF). One can configure CRFSuite training settings, produce models and test them.\nFootnote or Reference text (if exists): N. Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields. http://www.chokkan.org/software/crfsuite/.\nSection Titles (if exists):  ['2 Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://files.pushshift.io/reddit/\nCitation Context: We consider publicly-available Reddit submissions and comments [URL_CITE] from 2006 to 2018 as users\u2019 ut-terances. Given a Reddit user having a set of ut-terances U = u 0 ..u N , we aim to label the user with a set of profession and hobby values, based on explicit personal assertions (e.g., \u201cI work as a doctor\u201d) found in the user\u2019s posts.\nFootnote or Reference text (if exists): 4 https://files.pushshift.io/reddit/\nSection Titles (if exists):  ['4 Dataset', '4.1 Users\u2019 utterances']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasperGuo/Unimer\nCitation Context: We plan to cover more domains and more MRs in U NIMER . We have made U NIMER along with the execution engines publicly available. [URL_CITE] We be-lieve that U NIMER can provide fertile soil for ex-ploring MRs and addressing challenges in semantic parsing.\nFootnote or Reference text (if exists): 3 Our benchmark, execution engines and and our im-plementation can be found on: https://github.com/JasperGuo/Unimer\nSection Titles (if exists):  ['3 Benchmark']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com\nCitation Context: We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries to extract etymology-related information and Google Trans-late [URL_CITE] to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well.\nFootnote or Reference text (if exists): 5 http://translate.google.com\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.2 Relationships Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dexonline.ro\nCitation Context: We use lemmas for identifying words\u2019 definitions in dictionaries and for computing adequate distances between words and their cognates or etymons. We use the Dex-online [URL_CITE] machine-readable dictionary to lemmatize Romanian words.\nFootnote or Reference text (if exists): 3 http://dexonline.ro\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.1 Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.unibuc.ro/resources/rosim.pdf\nCitation Context: In this subsection we present and analyze the main results drawn from our research. In Table 3 we list the output of our method for each corpus, with and without diacritics [URL_CITE] . We report the similarity between Romanian and related languages, provid-ing the average value of the three metrics used.\nFootnote or Reference text (if exists): 6 The complete ranking of similarity is available online at http://nlp.unibuc.ro/resources/rosim.pdf.\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Romanian Evolution', '3.1.2 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/mt-para\nCitation Context: We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [URL_CITE]\nFootnote or Reference text (if exists): 4 The data is available at http://bit.ly/mt-para.\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cambridgeltl/adversarial-postspec\nCitation Context: We also plan to extend the method to asymmet-ric relations (e.g., hypernymy) and to more target (resource-lean) languages. The code is available at [URL_CITE] https://github.com/cambridgeltl/adversarial-postspec.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sapere.it/sapere/dizionari\nCitation Context: We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries [URL_CITE] to extract etymology-related information and Google Trans-late to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well.\nFootnote or Reference text (if exists): 4 Italian: http://www.sapere.it/sapere/dizionari\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.2 Relationships Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fortune.com/2015/10/19/amazon-fake-reviews/\nCitation Context: Writing fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detec-tors (Roberts, 2012) [URL_CITE] . What if fake review writ-ers become aware of the ways to game a detection algorithm?\nFootnote or Reference text (if exists): Jeff John Roberts. 2012. Amazon sues people who charge $5 for fake reviews. Fortune, October 19. http://fortune.com/2015/10/19/amazon-fake-reviews/ Retrieved Feb 27, 2016.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nytimes.com/2012/08/26/business/book-reviewers-for-hire-meet-a-demand-for-online-raves.html\nCitation Context: Writing fake reviews has become a lucrative business (Streitfeld, 2012) [URL_CITE] , and so there is now an arms race going on between producers and detec-tors (Roberts, 2012). What if fake review writ-ers become aware of the ways to game a detection algorithm?\nFootnote or Reference text (if exists): David Streitfeld. 2012. The Best Book Reviews Money Can Buy. The New York Times, August 25. http://www.nytimes.com/2012/08/26/business/book-reviewers-for-hire-meet-a-demand-for-online-raves.html Retrieved Feb 27, 2016.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com\nCitation Context: Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation. We adopt Google Translate [URL_CITE] for review transla-tion. After review translation, the training re-views and the test reviews are now in the same language, and any regression algorithm (e.g.\nFootnote or Reference text (if exists): 1 http://translate.google.com\nSection Titles (if exists):  ['3 Problem Definition and Baseline Ap-proaches']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://snap.stanford.edu/data,June\nCitation Context: For Movies dataset, we used the Amazon review data set (Leskovec and Krevl, 2014) [URL_CITE] . It has over 7.9 million reviews for 250,000 movies.\nFootnote or Reference text (if exists): Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data,June.\nSection Titles (if exists):  ['4 Human Subject Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.brandeis.edu/j\u0303amesp/arda/time/documentation/TimeML-use-in-qa-v1.0.pdf\nCitation Context: For the evaluation a corpus of questions containing as many simple as complex questions is required. Due to the fact that question corpora used in TREC (TREC, ) and CLEF (CLEF, ) do not contain com-plex questions, the TERQAS question corpus has been chosen (Radev and Sundheim, 2002 [URL_CITE] ; Puste-jovsky, 2002). It consists of 123 temporal questions.\nFootnote or Reference text (if exists): D. Radev and B. Sundheim. 2002. Us-ing timeml in question answering. http://www.cs.brandeis.edu/j\u0303amesp/arda/time/documentation/TimeML-use-in-qa-v1.0.pdf.\nSection Titles (if exists):  ['4 Question Decomposition Unit', '4.3.3 Example']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bbc.com/news/technology-24299742\nCitation Context: However, the question remains: is every online review trustful? It has been reported that up to 25% of the reviews on Yelp.com could be fraudu-lent [URL_CITE] . Due to the great profit or reputation, impostors or spammers energetically post fake reviews on the web to promote or defame targeted products (Jindal and Liu, 2008).\nFootnote or Reference text (if exists): 1 http://www.bbc.com/news/technology-24299742\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Support"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) \u2208 T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [URL_CITE] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 \u00b7 10 \u22127 , following Joshi et al. (2019).\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/data/glove.6B.zip\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Obtaining Word-Pair Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/martin-arvidsson/InterpretableWordEmbeddings\nCitation Context: We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling [URL_CITE] . We set the size of the embeddings K = 100, use a context window size of 8 and \u03c3 = 1 throughout all experiments.\nFootnote or Reference text (if exists): 1 Code is available at: https://github.com/martin-arvidsson/InterpretableWordEmbeddings\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [URL_CITE] are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\nCitation Context: Suppose p i , i = 1,..., n represent the training set of persons, and the classes for classifications are C ={c ,c } (for simplicity, we represent the 1 2 classes with C = { \u22121,1 } ). Then the classification task requires the solution of the following optimi-zation problem (Hsu et al., 2003) [URL_CITE] :\nFootnote or Reference text (if exists): Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-tical Guide to Support Vector Classification. Avail-able at: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nmrksic/attract-repel\nCitation Context: The constraints are initially injected into the dis-tributional vector space (see Figure 1 again) us-ing ATTRACT - REPEL , a state-of-the-art specializa-tion model, for which we adopt the original sug-gested model setup (Mrk\u0161ic\u0301 et al., 2017). [URL_CITE] Hyper-parameter values are set to: \u03b4 A = 0.6, \u03b4 R = 0.0, \u03bb P = 10 \u22129 . The models are trained for 5 epochs with Adagrad (Duchi et al., 2011), with batch sizes set to k A = k R = 50, again as in the original work.\nFootnote or Reference text (if exists): 6 https://github.com/nmrksic/ attract-repel\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ai.mit.edu/people/jrennie/20_newsgroups/\nCitation Context: All methods are initialized by a ran-dom choice of 10 document labels and 10 word la-bels. For simplicity, we follow the widely used cost model (Raghavan and Allan, 2007; Druck et al., 2008; Sindhwani et al., 2009) where features are roughly 5 times cheaper to label than examples, so we assume the cost is [URL_CITE] for a word query and is 5 for a document query. We set \u03b1 = \u03b2 = 5, \u03b3 = 1 for all the following experiments 2 .\nFootnote or Reference text (if exists): 1 http://www.ai.mit.edu/people/jrennie/20_newsgroups/\nSection Titles (if exists):  ['6 Experiments', '6.1 Experiments Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/MiuLab/TC-Bot\nCitation Context: The MLP has a single hidden layer of size 128. We use the simulator [URL_CITE] as in Li et al. (2016c) to generate user utterances, and the threshold interval is set to a range between 0.45 and 0.55.\nFootnote or Reference text (if exists): 2 https://github.com/MiuLab/TC-Bot\nSection Titles (if exists):  ['4 Experiments', '4.3 Task-Oriented Dialogue Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/isi-nlp/ZophRNN\nCitation Context: Model Architecture. For all experiments [URL_CITE] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.\nFootnote or Reference text (if exists): 2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/monologg/KoBERT-Transformers\nCitation Context: The pre-training language model used in BPM was KoBERT [URL_CITE] , and ReLU was used as the activa-tion function for each hidden layer; the dropout rate was 0.3. The batch size was 64, and the number of epochs was 60.\nFootnote or Reference text (if exists): 2 https://github.com/monologg/KoBERT-Transformers\nSection Titles (if exists):  ['3 Backchannel Prediction Model', '3.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Transformer-DyNet\nCitation Context: The dropout rate for the output layer is set to 0.2. For the Transformer, we use Transformer-DyNet [URL_CITE] implementation and extend it for our context-aware NMT model. The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively.\nFootnote or Reference text (if exists): 7 https://github.com/duyvuleo/Transformer-DyNet\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/GloVe\nCitation Context: We use the validation set (SE7) to find the optimal settings of our framework: the hidden state size n, the number of passes |T M |, the optimizer, etc. We use pre-trained word embeddings with 300 di-mensions [URL_CITE] , and keep them fixed during the train-ing process. We employ 256 hidden units in both the gloss module and the context module, which means n=256.\nFootnote or Reference text (if exists): 9 We download the pre-trained word embeddings from https://github.com/stanfordnlp/GloVe, and we select the smaller Wikipedia 2014 + Gigaword 5.\nSection Titles (if exists):  ['4 Experiments and Evaluation', '4.2 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rsennrich/nematus/\nCitation Context: We use the Nematus toolkit (Sennrich et al., 2017). [URL_CITE] We exactly follow the recipe in Kann and Schu\u0308tze (2016), the winning submission on the 2016 SIGMORPHON shared task for inflectional morphology. Accordingly, we use a character em-bedding size of 300, 100 hidden units in both the encoder and decoder, Adadelta (Zeiler, 2012) with a minibatch size of 20, and a beam size of 12.\nFootnote or Reference text (if exists): 6 https://github.com/rsennrich/nematus/\nSection Titles (if exists):  ['3 Task and Models', '3.4 RNN Encoder-Decoder']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jzhou316/Unsupervised-Sentence-Summarization\nCitation Context: We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [URL_CITE] The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/jzhou316/Unsupervised-Sentence-Summarization.\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use the third-party tool jieba [URL_CITE] for word seg-mentation and POS tagging; both steps are cus-tomized in order to achieve a better performance on domain- and task-specific data. Specifically, the dictionary provided by the tool is inter-sected with a user-specified dictionary.\nFootnote or Reference text (if exists): 3 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Processing steps', '5.1 Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used through-out our model. For word embedding, we use pre-trained case-sensitive GloVe embeddings [URL_CITE] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com-pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas-sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment.\nFootnote or Reference text (if exists): 2 Downloaded from http://nlp.stanford.edu/data/glove.840B.300d.zip.\nSection Titles (if exists):  ['4 Experiment', '4.1 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: Implementation details: The spaCy tool [URL_CITE] is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encod-ing.\nFootnote or Reference text (if exists): 2 https://spacy.io\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/pubmed/\nCitation Context: We used equal numbers of sentences from three different genres [URL_CITE] :\nFootnote or Reference text (if exists): 1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['3 Linguistic acceptability study', '3.1 Dataset creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk\nCitation Context: We used equal numbers of sentences from three different genres [URL_CITE] :\nFootnote or Reference text (if exists): 1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['3 Linguistic acceptability study', '3.1 Dataset creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.uco.es/kdis/mllresources/\nCitation Context: Datasets: Figure 3 and 4 shows the distribution of the instance numbers of the categories in the datasets RCV1 and Delicious [URL_CITE] . In both datasets, the instance numbers of the categories have long-tail distribution.\nFootnote or Reference text (if exists): 2 http://www.uco.es/kdis/mllresources/\nSection Titles (if exists):  ['5:361\u2013397. Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vikas95/AutoROCC\nCitation Context: The ROCC system and the codes for generat-ing all the analysis are provided here - [URL_CITE] https: //github.com/vikas95/AutoROCC .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948/\nCitation Context: Reproducibility. All code, data, and ex-periments for this paper are available on the CodaLab platform at [URL_CITE] https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Related Work and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.delph-in.net/crawler/\nCitation Context: All numbers reported here reflect this frozen system. [URL_CITE]\nFootnote or Reference text (if exists): 10 The code and data are available from http://www.delph-in.net/crawler/ , for replicability (Fokkens et al., 2013).\nSection Titles (if exists):  ['4 Experiments', '4.1 Data Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sheffieldnlp/fever-baselines\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [URL_CITE] to stimulate further research on verification.\nFootnote or Reference text (if exists): 3 https://github.com/sheffieldnlp/ fever-baselines\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fever.ai\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website [URL_CITE] , we also publish the annotation interfaces and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 1 http://fever.ai\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csc.fi/kielipankki/\nCitation Context: Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001) [URL_CITE] . We obtained a lexicon of 25k morphs by feeding the learning algorithm with the word list containing the 160k most common words.\nFootnote or Reference text (if exists): CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other. http://www.csc.fi/kielipankki/.\nSection Titles (if exists):  ['4 Speech recognition experiments', '4.2 Finnish']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://timesofindia.indiatimes.com/archive.cms\nCitation Context: In order to qualitatively validate that the latent inter-topic edges learnt from the news stream is also humanly interpretable, we constructed PCG from the online archives of Times of India (TOI) [URL_CITE] , the most circulated Indian English newspaper. We used this dataset as, unlike the previous dataset which provided just the time series of words, we also have the raw text of the articles, which al-lowed us to perform manual causal signature ver-ification. This dataset contains all the articles published in their online edition between Jan-uary 1, 2006 and December 31, 2015 containing 1,538,932 articles.\nFootnote or Reference text (if exists): 3 https://timesofindia.indiatimes.com/archive.cms\nSection Titles (if exists):  ['6 Interpretation of Predictive Causal Factors']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/awslabs/fever\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces [URL_CITE] and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 2 https://github.com/awslabs/fever\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salesforce/TopicBERT\nCitation Context: Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selec-tion and topic disentanglement tasks outper-forming existing methods by a good margin. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code will be available at https://github.com/salesforce/TopicBERT.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/~aditya/g2p-tl-rr\nCitation Context: Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www.cs.toronto.edu/\u02dcaditya/ g2p-tl-rr\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/1xEEfsMwL0k\nCitation Context: Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at [URL_CITE] https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at https://textessence.github.io.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/coder352/HAHSum\nCitation Context: The contributions of this paper are as below: 1) We propose a hierarchical attentive heteroge-neous graph based model(HAHSum) to guide the redundancy information propagating between sen-tences and learn redundancy-aware sentence rep-resentation; 2) Our architecture is able to extract flexible quantity of sentences with a threshold, in-stead of top-k strategy; 3) We evaluate HAHSum on three popular benchmarks (CNN/DM, NYT, NEWSROOM) and experimental results show that HAHSum outperforms the existing state-of-the-art approaches. Our source code will be available on Github [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://github.com/coder352/HAHSum\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.mpi-inf.mpg.de/~corrogg/\nCitation Context: The score of a cluster is the sum of the indi-vidual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected. Our source code and the results of our evaluation are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 10 http://people.mpi-inf.mpg.de/\u02dccorrogg/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cosmozhang/Modular_Neural_CRF\nCitation Context: All of our models beat the state-of-the-art results by a large margin. The source code and experimental setup are avail-able online [URL_CITE] .\nFootnote or Reference text (if exists): 4 https://github.com/cosmozhang/ Modular_Neural_CRF\nSection Titles (if exists):  ['6 Experimental Evaluation', '6.2 Q1: Monolithic vs. Modular Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.bing.com/translator\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [URL_CITE] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 2 https://www.bing.com/translator\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://translate.google.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [URL_CITE] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 1 https://translate.google.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [URL_CITE] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 3 https://aws.amazon.com/translate\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences). As creating manual alignment data is costly, we also created two training sets that con-sisted of 1/4 and 1/16 of the total data to test if we can achieve an effect with smaller amounts of manually annotated data. The details of data size and alignment accuracy are shown in Table 3.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://diamt.limsi.fr/eval.html\nCitation Context: We created two contrastive test sets to help com-pare how well different contextual MT mod-els handle (i) anaphoric pronoun translation and (ii) coherence and cohesion. [URL_CITE] For each test set, models are assessed on their ability to rank the cor-rect translation of an ambiguous sentence higher than the incorrect translation, using the disam-biguating context provided in the previous source and/or target sentence. All examples in the test sets are hand-crafted but inspired by real examples from OpenSubtitles2016 (Lison and Tiedemann, 2016) to ensure that they are credible and that vo-cabulary and syntactic structures are varied.\nFootnote or Reference text (if exists): 2 The test sets are freely available at https://diamt.limsi.fr/eval.html.\nSection Titles (if exists):  ['2 Evaluating contextual phenomena', '2.1 Our contrastive discursive test sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mrlyk423/relation_extraction\nCitation Context: Experimental results on real-world datasets show that, as com-pared with baselines, our model achieves significant and consistent improvements on knowledge base completion and re-lation extraction from text. The source code of this paper can be obtained from [URL_CITE] https://github.com/mrlyk423/relation_extraction.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JiaweiSheng/FAAN\nCitation Context: Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at [URL_CITE] https://github.com/JiaweiSheng/FAAN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenQA\nCitation Context: Experimen-tal results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from [URL_CITE] https: //github.com/thunlp/OpenQA\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/venkatasg/Advice-EMNLP2020\nCitation Context: Our results also show that r/AskParents is more challenging for advice identification, despite the fact that r/needadvice has a wider range of topics. We make all of our data and code available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/venkatasg/ Advice-EMNLP2020\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/isobs\nCitation Context: We evaluate different character recognition mod-els on self-created dataset. The results show that our implementation of prototypical network can achieve stable and competitive results. The datasets and source code can be found from [URL_CITE] https: //github.com/thunlp/isobs .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiment and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neukg/GRTE\nCitation Context: Ex-perimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: [URL_CITE] https://github.com/neukg/GRTE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.winemag.com/\nCitation Context: The website [URL_CITE] http://www.winemag.com/, owned by Wine Enthusiast Companies, hosts a substantial catalog of wine descriptions. We downloaded the available reviews 4 and gathered a total of 76,585 wine reviews.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.createdebate.com\nCitation Context: The second dataset (createdebate, henceforth) comes from an online debating site [URL_CITE] . It consists of 30 debates containing a total of 2,712 posts.\nFootnote or Reference text (if exists): 3 http://www.createdebate.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki\nCitation Context: http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List [URL_CITE] http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html\nCitation Context: http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. ( [URL_CITE] http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ehealthmd.com/library/acltears\nCitation Context: Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. [URL_CITE] http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.englishbaby.com/lessons/4349/slang/fruitcake\nCitation Context: She is a total fruitcake. [URL_CITE]\nFootnote or Reference text (if exists): 12 http://www.englishbaby.com/lessons/4349/slang/ fruitcake\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jemmryx/EQG-RACE\nCitation Context: We carry out the training and inference on EQG-RACE dataset [URL_CITE] proposed by Jia et al. (2020). The passage numbers of training set, validation set and test set are respectively 11457, 642, 609.\nFootnote or Reference text (if exists): 1 https://github.com/jemmryx/EQG-RACE\nSection Titles (if exists):  ['4 Experiment', '4.1 Experiment Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md#optimizers\nCitation Context: We discard 3.87% of the training examples of DROP and 5.05% of the training example of Q UOREF , which are cases when the answer cannot be outputted by the model (due to a dataset error, or trunca-tion of the correct answer span). For training, the BertAdam [URL_CITE] optimizer is used with default parame-ters and learning rates of either 5 \u00d7 10 \u22126 or 10 \u22125 . Hyperparameter search was not performed.\nFootnote or Reference text (if exists): 4 https://github.com/huggingface/transformers/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md#optimizers\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.umiacs.umd.edu/~hal/megam/\nCitation Context: The optimal weights for each feature are estimated using the Pairwise Ranking Op-timization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM [URL_CITE] . We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM.\nFootnote or Reference text (if exists): 4 http://www.umiacs.umd.edu/~hal/megam/\nSection Titles (if exists):  ['5 Features and Tuning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.yelp.com/datasetchallenge\nCitation Context: Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs [URL_CITE] epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02.\nFootnote or Reference text (if exists): 4 http://www.yelp.com/dataset challenge\nSection Titles (if exists):  ['6 Experiment', '6.1 Dataset and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gnu.org/s/wdiff/\nCitation Context: The HOO test data consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff [URL_CITE] to extract system edits. We obtain each system\u2019s output and segment it at the sentence level accord-ing to the gold standard sentence segmentation.\nFootnote or Reference text (if exists): 4 http://www.gnu.org/s/wdiff/\nSection Titles (if exists):  ['3 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr\nCitation Context: De-spite the reduced set of features, training error rate at both token and sentence level is under 1%. We didn\u2019t carry out optimization for parameters \u03c1 1 and \u03c1 [URL_CITE] of the elastic net (see section 3.1), default values lead in most cases to very accurate models.\nFootnote or Reference text (if exists): 2 available at http://wapiti.limsi.fr\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html\nCitation Context: In the second experiment, we use the reverse order of the two corpora and denote it by \u201cB-to-WSJ\u201d. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool [URL_CITE] with the stan-dard head rules (Yamada and Matsumoto, 2003).\nFootnote or Reference text (if exists): 3 http://w3.msi.vxu.se/\u02dcnivre/research/Penn2Malt.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neu-big et al., 2011) [URL_CITE] for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/egret-parser\nCitation Context: In previous reports, it has been noted (Kummerfeld et al., 2012) that the la-tent variable model of the Berkeley parser tends to have the higher accuracy of the two, so if the accu-racy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret, [URL_CITE] which achieves nearly iden-tical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other el-ement in the phrase.\nFootnote or Reference text (if exists): 4 http://code.google.com/p/egret-parser\nSection Titles (if exists):  ['3 Parsing', '3.1 Parsing Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jaist.ac.jp/~h-yamada/\nCitation Context: We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [URL_CITE] . During the training phase, we used intact POS and chunk tags .\nFootnote or Reference text (if exists): 2 http://www.jaist.ac.jp/\u02dch-yamada/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://plata.ar.media.kyoto-u.ac.jp/tool/EDA\nCitation Context: (2011), as implemented in the Eda toolkit. [URL_CITE] In order to convert dependencies into phrase-structure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing.\nFootnote or Reference text (if exists): 5 http://plata.ar.media.kyoto-u.ac.jp/tool/EDA\nSection Titles (if exists):  ['3 Parsing', '3.1 Parsing Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/document-qa\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Hence, we train and evaluate two SCRIPT mod-els \u201csmall\u201d and \u201cbase\u201d with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/\nCitation Context: These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) [URL_CITE] and law, medical OPUS datasets. For this setting, we con-sider two language pairs, namely Czech and Ger-man to English.\nFootnote or Reference text (if exists): Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/.\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tdil-dc.in/index.php?lang=en\nCitation Context: Datasets: For training English-Hindi NMT sys-tems, we use the IITB English-Hindi parallel cor-pus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010) [URL_CITE] spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB par-allel corpus for validation.\nFootnote or Reference text (if exists): 1 The corpus is available on request from http://tdil-dc.in/index.php?lang=en\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sinantie/NeuralAmr\nCitation Context: We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [URL_CITE] We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017).\nFootnote or Reference text (if exists): 1 We used the evaluation script available at https://github.com/sinantie/NeuralAmr.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Website)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.openchannelfoundation.org/projects/Qanda/\nCitation Context: On a different line of research, several Textual-based Question Answering (QA) systems (Qanda [URL_CITE] , QANUS , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources.\nFootnote or Reference text (if exists): 5 http://www.openchannelfoundation.org/projects/Qanda/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qanus.com/\nCitation Context: On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS [URL_CITE] , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources.\nFootnote or Reference text (if exists): 6 http://www.qanus.com/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://technologies.kmi.open.ac.uk/aqualog/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog [URL_CITE] , NLBean etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 8 http://technologies.kmi.open.ac.uk/aqualog/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.markwatson.com/opensource/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog , NLBean [URL_CITE] etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 9 http://www.markwatson.com/opensource/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://answers.yahoo.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers [URL_CITE] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 1 https://answers.yahoo.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dzonesoftware.com/products/open-source-question-answer-software/\nCitation Context: On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS , QSQA [URL_CITE] etc.) are developed that retrieve answers from the Web and other tex-tual sources.\nFootnote or Reference text (if exists): 7 http://www.dzonesoftware.com/products/open-source-question-answer-software/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/LRLS2MhPRE#Ebola\nCitation Context: Tweet @PNHP: 6. Renounce punitive and counterproductive measures such as sealing the borders, [URL_CITE] http://t.co/LRLS2MhPRE #Ebola\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Analysis', '4.1 Exact Match Calculations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/zrRIBxMUYJ\nCitation Context: Tweet Officer Wilson will be returned to active duty if no indictment, says #Ferguson Police Chief [URL_CITE] http://t.co/zrRIBxMUYJ\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/K542wvSNVz\u2019\nCitation Context: Tweet \u2018#RiggsReport: #CA as the #Election-Night exception. Voters rewarded #GOP nationally, but not in the #GoldenState. [URL_CITE] http://t.co/K542wvSNVz\u2019\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/VinAIResearch/BERTweet\nCitation Context: We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at: [URL_CITE] https://github.com/VinAIResearch/BERTweet.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://glaros.dtc.umn.edu/gkhome/views/cluto\nCitation Context: The Clusterer generates individual levels of the taxonomy by using text clustering. We used CLUTO package [URL_CITE] for doing text clustering. We experimented with all the available clustering functions in CLUTO but no one clustering al-gorithm consistently outperformed others.\nFootnote or Reference text (if exists): 2 http://glaros.dtc.umn.edu/gkhome/views/cluto\nSection Titles (if exists):  ['4 Generation of Domain Model', '4.2 Taxonomy Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/XExWwxmjnk#travel\nCitation Context: Tweet Are #Airlines doing enough with #Ebola? [URL_CITE] http://t.co/XExWwxmjnk #travel\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepset-ai/FARM\nCitation Context: We retrieved the pretrained models, fine-tuned them separately on each downstream task using the training and development sets, and tested them on the test sets. We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [URL_CITE] libraries to implement the transformer-based models. A complete list of hyperparameter values is presented in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/deepset-ai/FARM\nSection Titles (if exists):  ['3 Language models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/GloVe\nCitation Context: We use the validation set (SE7) to find the optimal settings of our framework: the hidden state size n, the number of passes |T M |, the optimizer, etc. We use pre-trained word embeddings with 300 di-mensions [URL_CITE] , and keep them fixed during the train-ing process. We employ 256 hidden units in both the gloss module and the context module, which means n=256.\nFootnote or Reference text (if exists): 9 We download the pre-trained word embeddings from https://github.com/stanfordnlp/GloVe, and we select the smaller Wikipedia 2014 + Gigaword 5.\nSection Titles (if exists):  ['4 Experiments and Evaluation', '4.2 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/lda\nCitation Context: A classical proba-bilistic topic model. We use the LDA pack-age [URL_CITE] for its implementation. We use the set-tings with iteration number n = 2000, the Dirichlet parameter for distribution over top-ics \u03b1 = 0.1 and the Dirichlet parameter for distribution over words \u03b7 = 0.01.\nFootnote or Reference text (if exists): 4 https://pypi.python.org/pypi/lda\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/\nCitation Context: (2009) we use asymmetric \u03b1 and symmetric \u03b2; rather than using fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method. [URL_CITE] We use standard set-tings for the number of training iterations (1000), the length of the burnin period before hyperparameter estimation begins (200 iterations) and the frequency of hyperparameter estimation (50 iterations).\nFootnote or Reference text (if exists): 2 We use the estimation methods provided by the MAL-LET toolkit, available from http://mallet.cs.umass.edu/.\nSection Titles (if exists):  ['3 Models', '3.1 Latent variable context models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1146/annurev.neuro.26.041002.131047\nCitation Context: In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015) [URL_CITE] ; in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015). In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value.\nFootnote or Reference text (if exists): Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Rep-resentations. https://doi.org/10.1146/annurev.neuro.26.041002.131047.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/72.410363\nCitation Context: In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015); in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015) [URL_CITE] . In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value.\nFootnote or Reference text (if exists): Karl Moritz Hermann, Toma\u0301s\u030c Koc\u030cisky\u0301, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom. 2015. Teaching Ma-chines to Read and Comprehend. In Advances in Neural Information Processing Systems (NIPS 2015). pages 1\u201314. https://doi.org/10.1109/72.410363.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ASRU.2015.7404872\nCitation Context: The objective of this task is to re-rank a list of candidate answers according to a non-factoid question, where the best-ranked candidate is selected as an answer. Models usually learn to generate dense vector representations for questions and candidates, where representations of a question and an associated correct answer should lie closely together within the vector space (Feng et al., 2015) [URL_CITE] . Accordingly, the ranking score can be determined with a simple similarity met-ric.\nFootnote or Reference text (if exists): Minwei Feng, Bing Xiang, Michael R. Glass, Li-dan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding. pages 813\u2013 820. https://doi.org/10.1109/ASRU.2015.7404872.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.scalanlp.org\nCitation Context: First, the text is processed with a part-of-speech and a named entity taggers. [URL_CITE] Next, each detected noun or entity is disambiguated in the same way as in the single word disambiguation mode described above, yet the disambiguation re-sults are represented as annotations of a running text. The best matching sense is represented by a hypernym and an image as depicted in Figure 3.\nFootnote or Reference text (if exists): 10 http://www.scalanlp.org\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org\nCitation Context: There are custom solutions that can easily simplify the slot filling task. For instance, subject entities can be used for lookups by title in Wikipedia to retrieve knowledge (this heuristic will always work for zsRE), and structured human-curated resources (such as Wikidata [URL_CITE] ) could be used to get all an-swers right. Nevertheless, we are interested in test-ing if a general model can extract attributes about specific entities from a large body of text.\nFootnote or Reference text (if exists): 13 https://www.wikidata.org\nSection Titles (if exists):  ['8 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freebase.com/\nCitation Context: Given a structured knowledge base, such as Free-base [URL_CITE] , a single relation question Q (in natural language) with its answer can be first parsed into a fact formatted as he sbj ,rel,e obj i, where e sbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, e obj denotes an object entity found from the knowledge base based on e sbj and rel. Then we can get hQ,reli pairs.\nFootnote or Reference text (if exists): 5 http://www.freebase.com/\nSection Titles (if exists):  ['4 Response Ranking', '4.5 Relation-level Feature']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Word_Association\nCitation Context: Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being pre-sented with a word (the cue) [URL_CITE] . Table 1 lists some examples of the test.\nFootnote or Reference text (if exists): 2 https://en.wikipedia.org/wiki/Word_ Association\nSection Titles (if exists):  ['2 Word Association Test']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea\nCitation Context: If a review contains a token that doesn\u2019t occur in at least 20 other reviews, then the review is excluded from the dataset. We used Jieba [URL_CITE] for Chi-nese and KyTea 2 for Japanese word segmentation. The segmenters were only used during the filtering process, and the text provided in the dataset is not segmented or tokenized.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba 2 http://www.phontron.com/kytea\nSection Titles (if exists):  ['2 Data preparation', '2.2 Data Processing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: If a review contains a token that doesn\u2019t occur in at least 20 other reviews, then the review is excluded from the dataset. We used Jieba [URL_CITE] for Chi-nese and KyTea 2 for Japanese word segmentation. The segmenters were only used during the filtering process, and the text provided in the dataset is not segmented or tokenized.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba 2 http://www.phontron.com/kytea\nSection Titles (if exists):  ['2 Data preparation', '2.2 Data Processing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/18814\nCitation Context: But, the ground truth evidence and labels of the test set are not publicly available. Instead, once extracted evidence sets/sentences and predicted labels of the test set by a model are sub-mitted to the online evaluation system [URL_CITE] , its perfor-mance is measured and displayed at the system. In this work, we train and tune our hyper-parameters on training and development sets, respectively.\nFootnote or Reference text (if exists): 1 https://competitions.codalab.org/competitions/18814\nSection Titles (if exists):  ['5 Experiments', '5.1 Experiment Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation system [URL_CITE] over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word.\nFootnote or Reference text (if exists): 7 We use Jieba for segmentation: https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['3 Embeddings for Chinese Text']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alpage.inria.fr/~sagot/lefff-en.html\nCitation Context: The continuous word representations are used to train a logis-tic regression classifier 2 for each morphological feature: gender and number for noun and adjec-tives; tense for verbs (with labels: present, fu-ture, imperfect, or simple past). Word labels are taken from the Lefff French morphological lexicon (Sagot, 2010) [URL_CITE] . To ensure a fair comparison be-tween context-independent and context-dependent embedding classification, words that are ambigu-ous with respect to a given feature are excluded from the respective classifier\u2019s training and test data.\nFootnote or Reference text (if exists): 3 Lexique des Formes Fle\u0301chies du Franc\u0327ais: http://alpage.inria.fr/\u02dcsagot/lefff-en.html\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm\nCitation Context: We conduct our experiments on the public Short Text Conversation (STC) dataset [URL_CITE] released in NTCIR-13. STC maintains a large reposit-ory of post-comment pairs from the Sina Weibo which is one of the popular Chinese social sites.\nFootnote or Reference text (if exists): 1 http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm\nSection Titles (if exists):  ['4 Experiment', '4.1 Dataset Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03\nCitation Context: The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC [URL_CITE] , containing about 28 millions of Chinese words. Since punc-tuations are rarely used to construct collocations, they were removed from the corpora. To auto-matically estimate the precision of extracted col-locations on the Chinese corpus, we built a gold set by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations.\nFootnote or Reference text (if exists): 2 Available at: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03\nSection Titles (if exists):  ['3 Initial Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://twentyone.tpd.tno.nl/olive/\nCitation Context: The European project LE-4 O LIVE : A Multilingual Indexing Tool for Broadcast Material Based on Speech Recognition ( [URL_CITE] http://twentyone.tpd.tno.nl/olive/) addressed methods to automate the disclosure of the infor-mation content of broadcast data thus allowing content-based indexation. Speech recognition was used to produce a time-linked transcript of the audio channel of a broadcast, which was then used to produce a concept index for retrieval.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lex4all.github.io/lex4all/\nCitation Context: This is the motivation behind lex4all, [URL_CITE] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.\nFootnote or Reference text (if exists): 1 http://lex4all.github.io/lex4all/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pemistahl/lingua\nCitation Context: Currently, A CTIVE A NNO has three inbuilt an-notation generator implementations. The first one automatically detects the language of the generator input using the language detection library Lingua [URL_CITE] . This is an example of a statistical and rule-based annotation generator as compared to a machine learning-based generator.\nFootnote or Reference text (if exists): 6 https://github.com/pemistahl/lingua\nSection Titles (if exists):  ['4 Annotation Generator']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://rnnlm.org\nCitation Context: In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a) [URL_CITE] to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer.\nFootnote or Reference text (if exists): Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Unlabeled tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/nmt/\nCitation Context: \u2022 Back-translation (BT): For neural machine trans-lation, we employ a fixed target-to-source transla-tion model trained on the original dataset. For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [URL_CITE] . We again augment one sam-ple from each original training sample.\nFootnote or Reference text (if exists): 2 Datasets used in this work can be found at https://nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://naudio.codeplex.com/\nCitation Context: The recorder, built using the open-source library NAudio, [URL_CITE] takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1).\nFootnote or Reference text (if exists): 8 http://naudio.codeplex.com/\nSection Titles (if exists):  ['5 User interface', '5.1 Audio input and recording']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf\nCitation Context: Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data 3 . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005) [URL_CITE] . Thus, human as-sessments are in a 2-10 scale.\nFootnote or Reference text (if exists): LDC. 2005. Linguistic Data Annotation Spec-ification: Assessment of Adequacy and Flu-ency in Translations. Revision 1.5. Tech-nical report, Linguistic Data Consortium. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf.\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.1 Data sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf\nCitation Context: There were 118 sentences, out of which 94 had at least one modification after the rewrite pro-cess. We selected 50 of these 94 sentences at ran-dom and asked 2 human judges to rate each sen-tence and its rewritten form on a scale of 1\u20135 for accuracy and fluency [URL_CITE] . We used 4 human judges, each judging 25 sentence pairs.\nFootnote or Reference text (if exists): 3 We followed the DARPA/LDC guidelines from http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf. For fluency, the scale was 5:Flawless, 4:Good, 3:Non-native, 2:Disfluent, 1:Incomprehensible. The accuracy scale for information covered (comparing with human translation) was 5:All, 4:Most, 3:Much, 2:Little, 1:None.\nSection Titles (if exists):  ['3 Arbitrary noun phrases', '3.5 Manual Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/seraphinatarrant/plan-write-revise\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cwc-story.isi.edu\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dmoz.com\nCitation Context: After a topic was input to the system, the Retriever system assigned it a category using a na\u00efve Bayes classifier built on a spidered DMOZ [URL_CITE] hierarchy. Various heuristics were implemented to make the returned set of categories uniform in length and depth, up-to-date, and readable.\nFootnote or Reference text (if exists): 5 http://www.dmoz.com\nSection Titles (if exists):  ['4 Topic Selection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/natashamjaques/neural_chat/tree/master/BatchRL\nCitation Context: After chatting, users were asked to provide a Likert scale rating of the bot\u2019s conversation quality, fluency, diversity, contingency/relatedness, and empathy. The code for the RL models is available in open-source at [URL_CITE] https://github.com/natashamjaques/neural_chat/tree/master/BatchRL. Using the server, we collected a batch of human interaction data containing 46,061 pairs of user input and agent response.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Learning from talking to humans', '4.2 Hosting real-time conversations online']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: Table 4: Syntactic analogy test results on the 300 most frequent supertags. Avg. rank is the average position of the correct choice in the ranked list of the closest neighbors; the top line indicates the result of using su-pertag embeddings that are trained jointly with a tran-sition based parser (Friedman et al., 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['5 Downstream Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kde.cs.uni-kassel.de/bibsonomy/dumps\nCitation Context: douban.com, which contains the descriptions of books and the tags collaboratively annotated by users. The second dataset, denoted as BIBTEX, is obtained from an English online bibliography web-site www.bibsonomy.org [URL_CITE] . The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users.\nFootnote or Reference text (if exists): 2 The dataset can be obtained from http://www.kde.cs.uni-kassel.de/bibsonomy/dumps\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TalLinzen/rnn_agreement\nCitation Context: (2016). [URL_CITE] Word types beyond the most frequent 10,000 are converted to their respective POS tags. We summarize the corpus statistics of the dataset, along with the test set distribution of the num-ber of attractors, in Table 1.\nFootnote or Reference text (if exists): 1 The dataset and scripts are obtained from https://github.com/TalLinzen/rnn_agreement.\nSection Titles (if exists):  ['2 Number Agreement with LSTM Language Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/page/softwareview/POS\nCitation Context: We built the temporal expression extraction module on top of the Heideltime system (Stro\u0308tgen and Gertz, 2010) to take advantage of a state-of-the-art tempo-ral extraction system in capturing basic expressions. We use the Illinois POS tagger [URL_CITE] (Roth and Zelenko, 1998) to provide part-of-speech tags for the input text before passing it to HeidelTime. Below is an example of the HeidelTime output of the example in the previous section:\nFootnote or Reference text (if exists): 1 http://cogcomp.cs.illinois.edu/page/softwareview/POS\nSection Titles (if exists):  ['2 The System', '2.1 Temporal Expression Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://compprag.christopherpotts.net/swda.html\nCitation Context: We use the Switch-board Dialogue Act (SwDA) corpus (Calhoun et al., 2010) in our experiments [URL_CITE] . SwDA contains the transcripts of several people discussing a given topic on the telephone. There are 42 dialogue act tags in SwDA, and we list the 10 most frequent tags in Table 3.\nFootnote or Reference text (if exists): 5 http://compprag.christopherpotts.net/swda.html.\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cogcomp.cs.illinois.edu/page/publication_view/790\nCitation Context: Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space re-quired by a factor of 16. The truncated word embeddings are available from the papers web page at [URL_CITE] https://cogcomp.cs.illinois.edu/page/publication_view/790.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://labs.google.com/sets\nCitation Context: Even for state of the art methods, expansion er-rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art systems such as GoogleSets [URL_CITE] , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed instances for the target set Roman Gods:\nFootnote or Reference text (if exists): 1 http://labs.google.com/sets the North American Chapter of the ACL, pages 290\u2013298,\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.everytrail.com/best/\nCitation Context: In our setting, we take as input a natural language query and extract entities from a single web page. trails near Baltimore) and a web page (e.g., [URL_CITE] http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.). Figure 1 summarizes the task setup.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.epinions.com\nCitation Context: The files retrieved from the web are converted in xml format, which allows preserving the metadata information. As an example, Figure 1 shows the organization of this xml file from a review retrieved from the website epinions.com ( [URL_CITE] http://www.epinions.com).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 The System and its Interface']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jknack/handlebars.java\nCitation Context: The output component employs a tem-plate engine [URL_CITE] to convert the data collected by the processing component into HTML and JavaScript for handling display and user interac-tion in the web browser.\nFootnote or Reference text (if exists): 7 https://github.com/jknack/handlebars. java\nSection Titles (if exists):  ['4 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jtf.org.tw\nCitation Context: Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark (http://www.psychpark.org) (Bai, 2001) and John Tung Foundation ( [URL_CITE] http://www.jtf.org.tw).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yixinL7/Direct-Style-Transfer\nCitation Context: The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and data are available at: https://github.com/yixinL7/Direct-Style-Transfer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wing.comp.nus.edu.sg/~linzihen/summeval/\nCitation Context: Experimental results on AESOP 2011 show that DICOMER significantly outperforms all sub-mitted metrics on both initial and update tasks with large gaps, while TESLA-S and CREMER signifi-cantly outperform all metrics on the initial task. [URL_CITE]\nFootnote or Reference text (if exists): 3 Our metrics are publicly available at http://wing.comp.nus.edu.sg/\u02dclinzihen/summeval/.\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/MuGNN\nCitation Context: Extensive experiments on five pub-licly available datasets demonstrate our su-perior performance (5% Hits@1 up on av-erage). Source code and data used in the experiments can be accessed at [URL_CITE] https://github.com/thunlp/MuGNN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/luyaojie/text2event\nCitation Context: We conducted experiments [URL_CITE] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings. In summary, the contri-butions are as follows:\nFootnote or Reference text (if exists): 1 Our source codes are openly available at https://github.com/luyaojie/text2event\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mozilla/DeepSpeech\nCitation Context: Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [URL_CITE] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/mozilla/DeepSpeech\nSection Titles (if exists):  ['4 Experiments', '4.2.2 Automatic Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/lipiji/SongNet\nCitation Context: Extensive experiments conducted on two collected corpora demonstrate that our pro-posed framework generates significantly better results in terms of both automatic metrics and the human evaluation. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code: http://github.com/lipiji/SongNet\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nCitation Context: The dataset and baseline models will be freely available online. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz\nCitation Context: Simple PPDB, along with the hu-man judgements collected as part of its creation, is freely available with the publication of this paper. [URL_CITE] rial is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.\nFootnote or Reference text (if exists): 6 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.uni-weimar.de/medien/webis/corpora\nCitation Context: The Webis-Debate-16 corpus will be made freely available online. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://www.uni-weimar.de/medien/webis/corpora\nSection Titles (if exists):  ['3 Mining Argumentative Text through Distant Supervision', '3.2 The Webis-Debate-16 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sunlab-osu/covid-faq\nCitation Context: We analyze COUGH by testing different FAQ re-trieval models built on top of BM25 and BERT, among which the best model achieves 48.8 under P@5, indicating a great challenge pre-sented by COUGH and encouraging future re-search for further improvement. Our COUGH dataset is available at [URL_CITE] https://github.com/sunlab-osu/covid-faq.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Appendix B Aggregation Schemes']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wanqiulong0923/TED-CDB\nCitation Context: The dataset and our Chinese annotation guidelines has been made freely available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wanqiulong0923/TED-CDB manifestation of intelligence, but more about our na-ture as a living, breathable organism] 1 . [The difference between consciousness and intelligence is very large. You will feel pain even if you are not smart, but only if you have to live] 2 . [(Therefore) RESULT+SPEECHACT\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~yww/data/emnlp2013.zip\nCitation Context: The coffee shop dataset is freely available [URL_CITE] for re-search purposes.\nFootnote or Reference text (if exists): 6 http://www.cs.cmu.edu/\u02dcyww/data/emnlp2013.zip\nSection Titles (if exists):  ['Coffee Shops Reviews']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asmadotgh/neural_chat_web\nCitation Context: For further information and instructions on server configuration, please read the server doc-umentation available at [URL_CITE] https://github.com/asmadotgh/neural_chat_web. We hope that this platform will allow others to host their own bots and evaluate them in an interactive setting.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['F Interactive bot platform details', 'F.1 Website server setup and configuration']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mponza/SalIE\nCitation Context: The source code and the processed datasets are publicly available [URL_CITE] to encourage further develop-ments of the fact salience task.\nFootnote or Reference text (if exists): 1 https://github.com/mponza/SalIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/msgs\nCitation Context: Second, since models learn useful features early, there is hope that fu-ture advances could accelerate by reducing the amount of data needed to learn which features mat-ter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: [URL_CITE] https://github.com/nyu-mll/msgs.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZhangShiyue/ChrEn\nCitation Context: Our best re-sults are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/En-Chr translations, respectively, and we hope that our dataset and systems will encourage fu-ture work by the community for Cherokee lan-guage revitalization. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our data, code, and demo will be publicly available at https://github.com/ZhangShiyue/ChrEn.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.5281/zenodo.4064408\nCitation Context: We hope that our public release of data will encourage others to further improve models for this task. [URL_CITE]\nFootnote or Reference text (if exists): 6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses [URL_CITE] to translate the linearized AMR into a sen-tence. To traverse the children of an AMR con-cept, we use the original order in the text file.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/moses/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/textblob/\nCitation Context: We use PoS (Part of Speech) information as a way of capturing error types of current word-level QE systems. First, the MT output from the training set of the WMT 2019 QE shared task is PoS tagged using the TextBlob library [URL_CITE] , which provides a sim-ple API for common natural language processing tasks . The PoS-tagged MT is then fed into the \u201cQEBrain\u201d model (Wang et al., 2018) to generate quality predictions.\nFootnote or Reference text (if exists): 1 https://pypi.org/project/textblob/\nSection Titles (if exists):  ['3 Concept and Implementation', '3.1.1 Pre-Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lexvo.org/uwn/\nCitation Context: the number of senses existing as entries in a WordNet for a given target word t i or a source word s i . We employ the Universal WordNet, [URL_CITE] which provides access to WordNets of various languages. Pseudo-reference This binary feature explores the similarity between the target sentence and a translation for the source sentence produced by an-other MT system.\nFootnote or Reference text (if exists): 5 http://www.lexvo.org/uwn/\nSection Titles (if exists):  ['3 Features', '3.1 Word level']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.lsi.upc.edu/asiya/\nCitation Context: Metrics from A SIYA . We used the freely avail-able version of the A SIYA toolkit [URL_CITE] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation.\nFootnote or Reference text (if exists): 4 http://nlp.lsi.upc.edu/asiya/\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 MT Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/\nCitation Context: On the other hand, the test-on-trans(lation) method trains an English sequence tagger, and utilizes MT to translate the test set of each target language into English in order to make predictions. In this work, we adopt the Mi-crosoft Translator [URL_CITE] , a strong commercial MT sys-tem. Note that for a MT system to work for se-quence tagging tasks, word alignment informa-tion must be available, in order to project word-level annotations across languages.\nFootnote or Reference text (if exists): 4 https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/\nSection Titles (if exists):  ['4 Experiments', '4.1 Cross-Lingual Semantic Slot Filling', '4.1.1 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~dbikel/software.html\nCitation Context: In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples [URL_CITE] . The statistical significance between two MWE identification experiments was established by using the McNemar-s test (Gillick and Cox, 1989). The results of the two experiments are considered statistically significant with the com-puted value p < 0.01.\nFootnote or Reference text (if exists): 12 Dan Bikel\u2019s tool available at http://www.cis.upenn.edu/\u02dcdbikel/software.html.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/MT/\nCitation Context: We also performed the paired bootstrap re-sampling test (Koehn, 2004). [URL_CITE] We sampled 2000 samples for each significance test.\nFootnote or Reference text (if exists): 3 We used the code available at http://www.ark.cs.cmu.edu/MT/.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/compare-mt\nCitation Context: For the En\u21d2Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improve-ment with paired bootstrap resampling (Koehn, 2004) using compare-mt [URL_CITE] (Neubig et al., 2019).\nFootnote or Reference text (if exists): 3 https://github.com/neulab/compare-mt\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nCitation Context: We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [URL_CITE] . Model and Hyper-parameters.\nFootnote or Reference text (if exists): 6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [URL_CITE] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm\nCitation Context: We calculated statistical significance for the main results on the development section using bootstrap random sampling. [URL_CITE] After re-sampling 1000 times, significance was calculated using a paired t-test (999 d.f.). The results indicated that lp-only exceeded the baseline, lp-ngram and lp-syn exceeded lp-only, and the full model exceeded lp-syn, with p < 0.0001 in each case.\nFootnote or Reference text (if exists): 6 Scripts for running these tests are available at http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm\nSection Titles (if exists):  ['4 Evaluation', '4.2 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://business.twitter.com\nCitation Context: Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy (https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day ( [URL_CITE] https://business.twitter.com). Thus there is an ex-treme class imbalance between user and non-user tweets.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://twitter.com/about\nCitation Context: As September 2010, Twitter reports that its users publish nearly 95 million tweets per day [URL_CITE] . This makes Twitter an excellent case to analyze misin-formation in social media.\nFootnote or Reference text (if exists): 1 http://twitter.com/about\nSection Titles (if exists):  ['4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tweetcongress.org/members/\nCitation Context: It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011). More than two thirds of U.S. congress members have created a Twitter account and many are actively using Twitter to reach their constituents (Lassen & Brown, 2010; TweetCongress, 2012 [URL_CITE] ). Since October 12, 2012, we have gathered over 36 million tweets about the 2012 U.S. presidential candidates, a quarter million per day on average.\nFootnote or Reference text (if exists): TweetCongress. (2012). Congress Members on Twitter Retrieved Mar 18, 2012, from http://tweetcongress.org/members/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://about.twitter.com/company\nCitation Context: With the rise in popularity of social media, mes-sage broadcasting sites such as Twitter and other microblogging services have become an important means of communication, with an estimated 500 million tweets being written every day [URL_CITE] . In addi-tion to individual users, various organizations and public figures such as newspapers, government officials and entertainers have established them-selves on social media in order to disseminate in-formation or promote their products.\nFootnote or Reference text (if exists): 1 https://about.twitter.com/company\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://about.Twitter.com/company\nCitation Context: In recent years, short texts are increasingly preva-lent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter [URL_CITE] , one of the most popular online social networking services. Proba-bilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, s-ince the low-dimensional semantic representation is crucial for many applications, such as prod-uct recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis (Si et al., 2013).\nFootnote or Reference text (if exists): 1 See https://about.Twitter.com/company\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://help.twitter.com/en/rules-and-policies/twitter-limits\nCitation Context: User tweets are simply all the tweets from that user\u2019s Twitter account, and measure perhaps in the thousands. Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy ( [URL_CITE] https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day (https://business.twitter.com).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flask.pocoo.org\nCitation Context: This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask [URL_CITE] micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters.\nFootnote or Reference text (if exists): 4 http://flask.pocoo.org\nSection Titles (if exists):  ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/\nCitation Context: So the co-occurrence statistics can be used to measure the social relatedness between named entities. In this paper, given a NE Co-occurrence Corpus D, the social relatedness scr between two named entities ne 1 and ne 2 is measured using the Google Similarity Distance (Cilibrasi and Vitanyi, 2007): where D 1 and D 2 are the document sets corres-pondingly containing ne 1 and ne [URL_CITE] . An example of social relatedness is shown in Table 3, which is computed using the Web corpus through Google.\nFootnote or Reference text (if exists): 2 http://wordnet.princeton.edu/\nSection Titles (if exists):  ['2 The Structural Semantic Relatedness Measure', '2.1 Knowledge Sources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.brenbarn.net/werewolf/\nCitation Context: There are other important roles in Were-wolf, such as seer, vigilante, etc, the goals and avail-able actions of which are not the focus of this paper. (Barnwell, 2012) [URL_CITE] provides a broader description of the game and roles.\nFootnote or Reference text (if exists): Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www/eml-research.de/nlp/download/wikirelations.php\nCitation Context: To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [URL_CITE] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links.\nFootnote or Reference text (if exists): 1 http://www/eml-research.de/nlp/download/wikirelations.php\nSection Titles (if exists):  ['4 Representation', '4.2 Selectional preference features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~nbach/papers/\nCitation Context: Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), se-mantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and re-lation finding (Bach and Badaskar, 2007) [URL_CITE] .\nFootnote or Reference text (if exists): Nguyen Bach and Sameer Badaskar. 2007. A Review of Relation Extraction. Unpublished. http: //www.cs.cmu.edu/\u02dcnbach/papers/ A-survey-on-Relation-Extraction. pdf.\nSection Titles (if exists):  ['9 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.metaversum.com/\nCitation Context: The KomParse system, described in this paper, provides NPCs for a virtual online world named Twinity, a product of the Berlin startup company Metaversum [URL_CITE] . The KomParse NPCs offer vari-ous services through conversation with game users using question-answering and dialog functional-ity. The utilization of Semantic Web technology with RDF-encoded generic and domain-specific ontologies furthermore enables semantic search and inference.\nFootnote or Reference text (if exists): 1 http://www.metaversum.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website and Company)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/resources/scoring.htm\nCitation Context: There were 210 generated references in the 18 testing sets. The table also shows the popular BLEU (Papineni et al., 2002) and NIST [URL_CITE] MT metrics. We also provide two base-lines - most frequent initial reference to the person in the input (Base1) and a randomly selected initial reference to the person (Base2).\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt/resources/scoring.htm\nSection Titles (if exists):  ['2 References to people', '2.6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.csai.tsinghua.edu.cn/~ly/systems/TsinghuaAligner/TsinghuaAligner.html\nCitation Context: For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. [URL_CITE] The evalu-ation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets.\nFootnote or Reference text (if exists): 1 http://nlp.csai.tsinghua.edu.cn/\u02dcly/systems/TsinghuaAligner/TsinghuaAligner.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [URL_CITE] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/optimization/\nCitation Context: We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://developers.google.com/optimization/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ictclas.org/\nCitation Context: (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) [URL_CITE] with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees.\nFootnote or Reference text (if exists): 1 http://ictclas.org/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/\nCitation Context: We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. [URL_CITE] To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.\nFootnote or Reference text (if exists): 5 http://www.itl.nist.gov/iad/mig/tests/mt/\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [URL_CITE] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nCitation Context: We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [URL_CITE] . Model and Hyper-parameters.\nFootnote or Reference text (if exists): 6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.csai.tsinghua.edu.cn/~ly/systems/TsinghuaAligner/TsinghuaAligner.html\nCitation Context: For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. [URL_CITE] The evalu-ation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets.\nFootnote or Reference text (if exists): 1 http://nlp.csai.tsinghua.edu.cn/\u02dcly/systems/TsinghuaAligner/TsinghuaAligner.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanford-qa.com\nCitation Context: F1 measures the overlap between the prediction and ground truth answers which takes the max-imum F1 over all of the ground truth answers. The scores on dev set are evaluated by the offi-cial script [URL_CITE] . Since the test set is hidden, we are re-quired to submit the model to Stanford NLP group to obtain the test scores.\nFootnote or Reference text (if exists): 3 Downloaded from http://stanford-qa.com\nSection Titles (if exists):  ['4 Experiment', '4.2 Main Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [URL_CITE] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/boots\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [URL_CITE] for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 5 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance. pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [URL_CITE] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus [URL_CITE] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 2 http://wacky.sslmit.unibo.it/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: (2013a) learns vec-tor representations using a neural network archi-tecture by trying to predict a target word given the words surrounding it. We use the word2vec soft-ware [URL_CITE] to build vectors of size 300 and using a con-text window of 5 words to either side of the target. We set the sub-sampling option to 1e-05 and esti-mate the probability of a target word with the neg-ative sampling method, drawing 10 samples from the noise distribution (see Mikolov et al.\nFootnote or Reference text (if exists): 3 Available at https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['4 Evaluation setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) \u2208 T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [URL_CITE] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 \u00b7 10 \u22127 , following Joshi et al. (2019).\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/data/glove.6B.zip\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Obtaining Word-Pair Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/futurulus/colors-in-context\nCitation Context: Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public with our code and trained model parameters. [URL_CITE] setting up the data collection platform, and mem-bers of the Stanford NLP group\u2014particularly Reid Pryzant, Sebastian Schuster, and Reuben Cohn-Gordon\u2014for valuable feedback on earlier drafts. This material is based in part upon work supported by the Stanford Data Science Initiative and by the NSF under Grant Nos.\nFootnote or Reference text (if exists): 8 https://github.com/futurulus/ colors-in-context\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [URL_CITE] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence. The GVDB is the result of a large crowdsourced annotation effort. This an-notation is ongoing, and the GVDB will be regularly updated with new data and new layers of annotation, making it an interesting and challenging data set on which to evaluate state-of-the-art NLP tools.\nFootnote or Reference text (if exists): 2 http://gun-violence.org/\nSection Titles (if exists):  ['3 The Gun Violence Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/tuvuumass/task-transferability\nCitation Context: We publicly release our task library, which con-sists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability. [URL_CITE]\nFootnote or Reference text (if exists): 2 Library and code available at http://github.com/tuvuumass/task-transferability.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cocolab.stanford.edu/datasets/colors.html\nCitation Context: Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public [URL_CITE] with our code and trained model parameters. setting up the data collection platform, and mem-bers of the Stanford NLP group\u2014particularly Reid Pryzant, Sebastian Schuster, and Reuben Cohn-Gordon\u2014for valuable feedback on earlier drafts.\nFootnote or Reference text (if exists): 7 https://cocolab.stanford.edu/datasets/colors.html\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.netlib.org/ode/vode.f\nCitation Context: The CSLS scheme with a neighbourhood size of 10 (CSLS-10) is adopted to build synthetic dictio-naries via the input CLWEs. A variable-coefficient ordinary differential equation (VODE) solver [URL_CITE] was implemented for the system described in Eq. (7).\nFootnote or Reference text (if exists): 3 http://www.netlib.org/ode/vode.f\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation Details of Algorithm 1']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nCitation Context: Suggestion extraction can also be employed for the summarisation of dedicated suggestion fo-rums [URL_CITE] . People often provide the context in such posts, which gets repetitive over a large number of posts. Suggestion mining methods can identify the exact textual unit in the post where a suggestion is conveyed.\nFootnote or Reference text (if exists): 1 https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/decompositional-semantics-initiative/improved-ParaBank-rewriter\nCitation Context: Here we\u2019re showing examples of perturbations generated via a recent machine paraphraser sys-tem. [URL_CITE]\nFootnote or Reference text (if exists): 6 https://github.com/decompositional-semantics-initiative/improved-ParaBank-rewriter\nSection Titles (if exists):  ['E Rule-Based Perturbations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: We implemented our model in Tensorflow [URL_CITE] . We tuned the hyper-parameters via the development set.\nFootnote or Reference text (if exists): 3 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiment', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/9rs6pa\nCitation Context: (rumor) \u201cRT @johnnyA99 Ann Coulter Tells Larry King Why People Think Obama Is A Muslim [URL_CITE] http://bit.ly/9rs6pa #Hussein via @NewsBusters #tcot ..\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Tweet, Video)\nFUNCTION: Other (Citing a tweet that contains a video link)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Xu et al. (2017) [URL_CITE] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: We roughly follow the method presented in Xu et al. (2017) [URL_CITE] with extensions. Under the TAG analysis, VP coordination involves a VP-recursive auxiliary tree headed by the coordinator that in-cludes a VP substitution node (for the second con-junct) with label 1.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['A.2 Coordination']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://godel.iis.sinica.edu.tw/ROCLING\nCitation Context: Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): 2 See http://godel.iis.sinica.edu.tw/ROCLING.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ivri/dmorph\nCitation Context: (2017) against the reported performance of their encoder-decoder model. [URL_CITE] Notice Vylomova et al. (2017) show that providing the LSTM with the POS of the deriva-tive increases performance.\nFootnote or Reference text (if exists): 8 The dataset is available at https://github.com/ivri/dmorph. While Vylomova et al. (2017) take morpho-orthographic changes into account, we only predict affixes, not the accompanying changes in orthography (Section 4.1).\nSection Titles (if exists):  ['4 Experiments', '4.3 Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: We apply three types of transformation from Xu et al. (2017) [URL_CITE] to interpret the TAG parses.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['A.1 Transformations from PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285\nCitation Context: To get a coarse-grained representation of the polysemy of the word, we consider the number of different POS present. For a finer-grained repre-sentation, we use the number of different Word-Net senses associated with the word (Miller, 1995; Fellbaum, 1998 [URL_CITE] ).\nFootnote or Reference text (if exists): Christiane Fellbaum. 1998. WordNet. Wiley Online Library. https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285.\nSection Titles (if exists):  ['4 Factors Influencing Stability', '4.2 Word Properties']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LIAAD/KeywordExtractor-Datasets\nCitation Context: To evaluate the keyword extraction methods, we consider 15 different public datasets in English. [URL_CITE] Each entry in a dataset consists of a source document and a set of gold keyphrases, where the source document is processed through the pipeline described in Section 3 and the gold keyphrase set is filtered to include only phrases which appear in its candidate set. Table 1 pro-vides high-level statistics of each dataset, including length and number of keyphrases 7 (both average and standard deviation).\nFootnote or Reference text (if exists): 6 All the datasets were fetched from a public data reposi-tory for keyword extraction data: https://github.com/LIAAD/KeywordExtractor-Datasets: KPCrowd (Marujo et al., 2013), Inspec (Hulth, 2003), Krapivin2009 (Krapivin et al., 2009), SemEval2017 (Augenstein et al., 2017), kdd (Gollapalli and Caragea, 2014), www (Gollapalli and Caragea, 2014), wiki20 (Medelyan and Witten, 2008), PubMed (Schutz et al., 2008), Schutz2008 (Schutz et al., 2008), citeulike180 (Medelyan et al., 2009), fao30 and fao780 (Medelyan and Witten, 2008), guyen2007 (Nguyen and Kan, 2007), and SemEval2010 (Kim et al., 2010).\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/budzianowski/multiwoz\nCitation Context: In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [URL_CITE]\nFootnote or Reference text (if exists): 5 The model is publicly available at https://github.com/budzianowski/multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.2 Dialogue-Context-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://universaldependencies.github.io\nCitation Context: \u2022 We convert the representation of coordination structures into a format inspired by Universal Dependencies. [URL_CITE] In the original Treex anno-tation style, the conjunction heads both con-juncts, whereas in our modification, the first conjunct is at the top, heading the coordina-tion and the second conjunct (see Figure 4).\nFootnote or Reference text (if exists): 8 http://universaldependencies.github.io\nSection Titles (if exists):  ['5 Experimental Setup', '5.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sanketvmehta/efficient-meta-lifelong-learning\nCitation Context: For our framework, Meta-MbPA [URL_CITE] , unless stated otherwise, we set the number of neighbors K = 32 and control the memory size through a write rate r M = 1%. We use L = 30 local adaptation steps and perform local adaptation for whole testing set.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/sanketvmehta/efficient-meta-lifelong-learning.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/c/asap-aes/\nCitation Context: At the outset, our goal is to develop a framework that strengthens the validity of state-of-the-art neu-ral AES approaches with respect to adversarial in-put related to local aspects of coherence. For our experiments, we use the Automated Student As-sessment Prize (ASAP) dataset, [URL_CITE] which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4).\nFootnote or Reference text (if exists): 3 https://www.kaggle.com/c/asap-aes/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum\nCitation Context: WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. [URL_CITE] We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al.\nFootnote or Reference text (if exists): 3 We take the processed Wikipedia articles from https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum released on April 25th 2018.\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://glaros.dtc.umn.edu/gkhome/views/cluto\nCitation Context: We use NEs to construct feature vectors for the documents, rather than considering all the words as in most previous work. We use the CLUTO [URL_CITE] toolkit to perform clustering. It finds a predefined number of clusters based on a specific criterion, for which we chose the following func-tion: where K is the desired number of clusters, S i is the set of documents belonging to the i th cluster, v and u represent two documents, and sim(v, u) is the similarity between them.\nFootnote or Reference text (if exists): 1 Available at http://glaros.dtc.umn.edu/gkhome/views/cluto\nSection Titles (if exists):  ['2 Related Work', '3.2 NE-based Clustering for LM Adaptation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/scalanlp/breeze\nCitation Context: Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. [URL_CITE] Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework.\nFootnote or Reference text (if exists): 6 https://github.com/scalanlp/breeze\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mappings.dbpedia.org/server/ontology/classes/\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/downloads-2016-10\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://demo.patrickpantel.com/\nCitation Context: Consider the set of seed terms S and an errone-ous expanded instance e. In the SIM method of Section 3.1 all set elements that have a feature vec-tor (i.e., context vector) similar to e are removed. The Feature Modification Method (FMM) instead tries to identify the subset of features of the error e which represent the unintended sense of the seed terms S. For example, let S = {Minerva, Neptune, Baccus, Juno, Apollo}. Looking at the contexts of these words in a large corpus, we construct a cen-troid context vector for S by taking a weighted av-erage of the contexts of the seeds in S. In Wikipedia articles we see contexts (i.e., features) such as [URL_CITE] :\nFootnote or Reference text (if exists): 4 The full feature vector for these and all other terms in Wiki-pedia can be found at http://demo.patrickpantel.com/..\nSection Titles (if exists):  ['3 Dynamic Similarity Modeling', '3.2 Feature Modification Method (FMM)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cis.temple.edu/~edragut/DebugSL.webm\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [URL_CITE] https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bioai.lab.asu.edu/logicgridpuzzles\nCitation Context: This is the first learning system that can solve logic grid puzzles described in natu-ral language in a fully automated manner. The code and the data will be made pub-licly available at [URL_CITE] http://bioai.lab.asu.edu/logicgridpuzzles.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: The three-minute video demonstrating the interac-tion capabilities of the system can be viewed at [URL_CITE] https://youtu.be/-hGd2399dnA. (Same video as linked in the paper footnote).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://api.syntagnet.org\nCitation Context: In this paper we presented and described the architecture of SyntagRank, our state-of-the-art knowledge-based system for multilingual Word Sense Disambiguation using syntagmatic informa-tion. We also provided details concerning the use of SyntagRank\u2019s Web interface and RESTful API, accessible at http://syntagnet.org/ and [URL_CITE] http://api.syntagnet.org , respectively.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://syntagnet.org/\nCitation Context: In this paper we presented and described the architecture of SyntagRank, our state-of-the-art knowledge-based system for multilingual Word Sense Disambiguation using syntagmatic informa-tion. We also provided details concerning the use of SyntagRank\u2019s Web interface and RESTful API, accessible at [URL_CITE] http://syntagnet.org/ and http://api.syntagnet.org , respectively.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/atschneid/DebugSL\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at [URL_CITE] https://github.com/atschneid/DebugSL and a screencast can be viewed at https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://manikvarma.org/downloads/XC/XMLRepository.html\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [URL_CITE] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 3 http://manikvarma.org/downloads/XC/XMLRepository.html\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: 2. IMDB movie review dataset [URL_CITE] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).\nFootnote or Reference text (if exists): 2 http://ai.stanford.edu/~amaas/data/sentiment/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ltrc.iiit.ac.in/download.php\nCitation Context: Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words [URL_CITE] and most frequent Telugu words . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists.\nFootnote or Reference text (if exists): 2 https://ltrc.iiit.ac.in/download.php\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Dataset details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wiktionary.org/Frequencylists/Telugu\nCitation Context: Initially, we create data lists for Hindi and Telugu. For this, we have extracted a corpus of most frequent Hindi words and most frequent Telugu words [URL_CITE] . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists.\nFootnote or Reference text (if exists): 3 https://en.wiktionary.org/Frequencylists/Telugu\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Dataset details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.imdb.com/\nCitation Context: Thus, our models include movie genre as an additional feature. Genres for each movie were obtained from IMDb [URL_CITE] and transformed into a multi-hot encoding.\nFootnote or Reference text (if exists): 3 https://www.imdb.com/\nSection Titles (if exists):  ['3 Method', '3.3 Role of Movie Genre']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.summarization.com\nCitation Context: MEAD [URL_CITE] is a publicly available toolkit for extractive multi-document summarization. Although it comes as a centroid-based summarization system by de-fault, its feature set can be extended to implement other methods.\nFootnote or Reference text (if exists): 2 http://www.summarization.com\nSection Titles (if exists):  ['4 Experiments on DUC 2004 data', '4.2 MEAD summarization toolkit']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/najoungkim/pdtb3\nCitation Context: We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [URL_CITE] In addition to accounting for the variation previously discussed, we take Shi and Demberg (2017)\u2019s concerns into consideration.\nFootnote or Reference text (if exists): 4 https://github.com/najoungkim/pdtb3\nSection Titles (if exists):  ['3 Proposed Evaluation Protocol']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.gsk.or.jp/en/catalog//\nCitation Context: 6. We have released a part of it to the public on the web [URL_CITE] .\nFootnote or Reference text (if exists): 4 The corpus data are available at https://www.gsk.or.jp/en/catalog//.\nSection Titles (if exists):  ['4 Data Development']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryankiros/skip-thoughts/\nCitation Context: Skip-thought As a second baseline we use the sentence representations produced by the skip-thought architecture (Kiros et al., 2015). [URL_CITE] Skip-thought is a recently proposed method that learns sentence representations in a different way from ours, by using recurrent neural networks. This al-lows it to take word order into account.\nFootnote or Reference text (if exists): 3 The code and the trained models can be down-loaded from https://github.com/ryankiros/skip-thoughts/.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lancopku/SGM\nCitation Context: We include it as a baseline by using its publicly released code. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/lancopku/SGM\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mfaruqui/crosslingual-cca\nCitation Context: For all experiments, we use the Word2Vec imple-mentation in Gensim to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) [URL_CITE] in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).\nFootnote or Reference text (if exists): 3 https://github.com/mfaruqui/ crosslingual-cca proaches on various languages. Since about one-third of the test mentions are non-trivial, a baseline is 66.67 for all languages, if we pick the most common title given the mention. Bold signi-fies highest score for each column.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/attardi/wikiextractor\nCitation Context: More concretely, we use WikiExtractor [URL_CITE] to ex-tract plain text from Wikipedia dumps, and pre-process the resulting corpus using standard Moses tools (Koehn et al., 2007) by applying sentence splitting, punctuation normalization, tokenization with aggressive hyphen splitting, and lowercasing. We then train word embeddings for each language using the skip-gram implementation of fastText (Bojanowski et al., 2017) with default hyperpa-rameters, restricting the vocabulary to the 200,000 most frequent tokens. The official embeddings in the MUSE dataset were trained using these exact same settings, so our embeddings only differ in the Wikipedia dump used to extract the training cor-pus and the pre-processing applied to it, which is not documented in the original dataset.\nFootnote or Reference text (if exists): 4 https://github.com/attardi/ wikiextractor\nSection Titles (if exists):  ['3 Experimental settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: We implemented the CPAE (Section 2.1) as a baseline and compared it to CPAE with the word-pair embeddings (Section 3.1), which is our pro-posed method. The word embeddings in the def-inition encoder were initialized by the Google Word2Vec vectors [URL_CITE] . Google Word2Vec vectors and GloVe were used as the other baselines.\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Definition Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://vectors.nlpl.eu/repository/\nCitation Context: All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [URL_CITE] (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set (Targeted F 1 for the baselines).\nFootnote or Reference text (if exists): 8 Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58).\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: For all experiments, we use the Word2Vec imple-mentation in Gensim [URL_CITE] to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).\nFootnote or Reference text (if exists): 2 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/seraphinatarrant/plan-write-revise\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cwc-story.isi.edu\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/J11-2003\nCitation Context: Our architecture is similar to the pipelined struc-tures of earlier work in question answering that rely on a retrieval step followed by a more expen-sive supervised ranking approach (Surdeanu et al., 2011 [URL_CITE] ; Higashinaka and Isozaki, 2008). We primar-ily chose this architecture because it allows the user to directly relate the results of the system to the an-swer selection model.\nFootnote or Reference text (if exists): Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank an-swers to non-factoid questions from web collections. Computational Linguistics 37(2). http://aclweb.org/anthology/J11-2003.\nSection Titles (if exists):  ['2 System Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N/N16/N16-1148.pdf\nCitation Context: Pick-Revise is an interaction cycle for MT Post-Editing proposed by Cheng et al. (2016) [URL_CITE] . Starting with the original translation hypothesis, a (sim-ulated) user first picks a part of the hypothesis which is incorrect, and then provides the correct translation for that portion of the output.\nFootnote or Reference text (if exists): Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240\u20131249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.\nSection Titles (if exists):  ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.acm.org/crossroads/xrds3-1/aied.html\nCitation Context: The architecture of iList includes a problem model, a constraint evaluator, a feedback manager, and a graphical user interface. Student model and pedagogical module, important components of a complete ITS (Beck et al., 1996) [URL_CITE] , have not been implemented yet, and will be included in a future version. Currently, the system provides only simple negative feedback in response to students\u2019 mistakes, as customary in constraint-based ITSs.\nFootnote or Reference text (if exists): Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996. Applications of AI in education. ACM crossroads. http://www.acm.org/crossroads/xrds3-1/aied.html.\nSection Titles (if exists):  ['4 A tutoring system for linked lists']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/RSN\nCitation Context: Experiment results on two real-world datasets show that our frame-work can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at [URL_CITE] https://github.com/thunlp/RSN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/venkatasg/Advice-EMNLP2020\nCitation Context: Our results also show that r/AskParents is more challenging for advice identification, despite the fact that r/needadvice has a wider range of topics. We make all of our data and code available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/venkatasg/ Advice-EMNLP2020\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/monoses\nCitation Context: Our experiments show the effective-ness of our approach, as we improve the previous state-of-the-art in unsupervised machine transla-tion by 5-7 BLEU points in French-English and German-English WMT 2014 and 2016. Our code is available as an open source project at [URL_CITE] https: //github.com/artetxem/monoses.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions and future work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: Our analyses show that four popular industrial MT systems and two recent state-of-the-art aca-demic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sjcfr/CWVAE\nCitation Context: Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at [URL_CITE] https://github.com/sjcfr/CWVAE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ShyamSubramanian/HESM\nCitation Context: Our ex-perimental results show that HESM outper-forms 7 state-of-the-art methods for fact ex-traction and claim verification. Our source code is available at [URL_CITE] https://github.com/ShyamSubramanian/HESM.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pallets/flask\nCitation Context: It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) [URL_CITE] framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002).\nFootnote or Reference text (if exists): Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.\nSection Titles (if exists):  ['3 System', '3.3 Technical implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepage.fudan.edu.cn/zhengxq/deeplearning/\nCitation Context: only a few changes above and beyond the dominant first-order scores). We call this proposed model an incremental neural de-pendency parsing (INDP) [URL_CITE] .\nFootnote or Reference text (if exists): 1 The source code is available at http://homepage.fudan.edu.cn/zhengxq/deeplearning/\nSection Titles (if exists):  ['1 Introduction and Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.everyzing.com/\nCitation Context: We\u2019ve taken the rather nar-row view that systems might only expose the list of scores they assigned to retrieved documents, a plau-sible scenario considering the many systems now emerging which are effectively doing this already. Some examples include EveryZing, [URL_CITE] the MIT Lec-ture Browser, 2 and Comcast\u2019s video search. 3 This trend is likely to continue as the underlying repre-sentations of the content are themselves becoming increasingly complex (e.g., word and subword level lattices or confusion networks).\nFootnote or Reference text (if exists): 1 http://www.everyzing.com/\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lemurproject.org/clueweb12\nCitation Context: WNED-CWEB (Guo and Barbosa, 2018) is a dataset created with the same strategy as WNED-WIKI, but sampling from the ClueWeb 2012 cor-pora annotated with the FACC1 system. [URL_CITE] Similarly, we randomly split into dev and test.\nFootnote or Reference text (if exists): 7 http://lemurproject.org/clueweb12\nSection Titles (if exists):  ['3 Tasks', '3.2 Entity Linking']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lsi.upc.edu/~nlp/Asiya\nCitation Context: As for evaluation measures, for MT we have used a rich set of 64 measures provided within the ASIYA Toolkit (Gime\u0301nez and Ma\u0300rquez, 2010) [URL_CITE] . This in-cludes measures operating at different linguistic lev-els: lexical, syntactic, and semantic.\nFootnote or Reference text (if exists): 5 http://www.lsi.upc.edu/\u02dcnlp/Asiya\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.2 Measures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.python.org/\nCitation Context: \u2022 Minimal requirements on the underlying server structure: Sisyphus only requires Python 3 [URL_CITE] with a few basic packages and a Unix-type operating system.\nFootnote or Reference text (if exists): 1 https://www.python.org/\nSection Titles (if exists):  ['1 Introduction', '1.2 Design Goals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://corpus.korean.go.kr\nCitation Context: We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [URL_CITE] . We split these corpora into three, and each is for training, validation, and test.\nFootnote or Reference text (if exists): 1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rikdz/GraphWriter\nCitation Context: We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings [URL_CITE] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer.\nFootnote or Reference text (if exists): 12 https://github.com/rikdz/GraphWriter\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11372/LRT-1632\nCitation Context: We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016 [URL_CITE] ), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.\nFootnote or Reference text (if exists): Marco Turchi, Rajen Chatterjee, and Matteo Negri. 2016. WMT16 APE shared task data. LIN-DAT/CLARIN digital library at the Institute of For-mal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.\nSection Titles (if exists):  ['4 Experiments', '4.2 Automatic MT Post-editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Alex-Fabbri/\nCitation Context: Datasets can be obtained by running the data scripts at [URL_CITE] https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch. We use Dong and Lapata (2016)\u2019s training, development (for ATIS), and test sets. ATIS, JOBS, and GEO contain 5410, 640, and 880 examples respectively.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.2 Semantic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/The_Big_Bang_Theory\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, [URL_CITE] available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 3 https://en.wikipedia.org/wiki/The_ Big_Bang_Theory\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhanghainan/TailoredSeq2Seq2DifferentConversationScenarios\nCitation Context: We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/zhanghainan/TailoredSeq2Seq2 DifferentConversationScenarios\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings', '5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pan.webis.de/clef16/pan16-web/author-profiling.html\nCitation Context: Dataset Three tasks are conducted on three pub-lic sentence classification datasets. Author profiling (Age dataset [URL_CITE] ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset ) is to predict the number of stars the user assigned to by analysis their reviews.\nFootnote or Reference text (if exists): 1 https://pan.webis.de/clef16/pan16-web/author-profiling.html\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.1 Self-attentive Sentence Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:User_pages\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; [URL_CITE] re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/ Wikipedia:User_pages\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/about\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. [URL_CITE] On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 1 http://stackexchange.com/about\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge [URL_CITE] , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nmrksic/attract-repel\nCitation Context: cialisation framework (Mrk\u0161ic\u0301 et al., 2017). [URL_CITE] The key idea of LEAR , illustrated by Figure 1, is to pull desirable ( ATTRACT ) examples described by the constraints closer together, while at the same time pushing undesirable ( REPEL ) word pairs away from each other. Concurrently, LEAR (re-)arranges vector norms so that norm values in the Euclidean space reflect the hierarchical organisation of con-cepts according to the given LE constraints: put simply, higher-level concepts are assigned larger norms.\nFootnote or Reference text (if exists): 2 https://github.com/nmrksic/attract-repel\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset/download\nCitation Context: Author profiling (Age dataset ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset [URL_CITE] ) is to predict the number of stars the user assigned to by analysis their reviews. Tex-tual entailment (SNLI dataset 3 ) is to tell whether the semantics in the two sentences are entailment or contradiction or neutral.\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset/download\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.1 Self-attentive Sentence Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt\nCitation Context: Some concepts (such as \u201chave-rel-role-91\u201d) in an AMR graph do not contribute to the final translation, and we skip them when generating concept rules. Besides that, we use a verbalization list [URL_CITE] for concept rule generation. For rule \u201cVERBALIZE peacekeep-ing TO keep-01 :ARG1 peace\u201d, we will create a con-cept rule \u201c(k/keep-01 :ARG1 (p/peace)) ||| peace-keeping\u201d if the left-hand-side fragment appears in the target graph.\nFootnote or Reference text (if exists): 2 http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt\nSection Titles (if exists):  ['2 Method', '2.3 Rule Acquisition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/airsplay/R2R-EnvDrop\nCitation Context: Empirically, we show that our agent is substantially better at generalizabil-ity when fine-tuned with these triplets, outper-forming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code, data, and models publicly available at: https://github.com/airsplay/R2R-EnvDrop\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://badaa.mngl.net/docs.php?p=trans_table\nCitation Context: We romanize all the Katakana words in the dictionary and index them using consecutive N 1 [URL_CITE] http://badaa.mngl.net/docs.php?p=trans_table (May, 2006) characters. We also romanize each candidate loanword when use as a query. We experimentally set N = 2, and use the Okapi BM25 (Robertson et al., 1995) for the retrieval model.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Methodology', '3.6 N-gram retrieval']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://kheafield.com/code/kenlm/\nCitation Context: For DWF, we ranked the monolingual data by word rarity (Platanios et al., 2019) of sentences and also selected the top 80M monolingual data for self-training. For S RC LM, we trained an N -gram language model (Heafield, 2011) [URL_CITE] on the source sentences in the bitext and measured the distance between each monolingual sentence to the bitext source sentences by cross-entropy. Similarly, we se-lected 8M monolingual data with the lowest cross-entropy for self-training.\nFootnote or Reference text (if exists): 9 https://kheafield.com/code/kenlm/\nSection Titles (if exists):  ['A Appendix', 'A.3 Comparison with Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xiaochang13/AMR-generation\nCitation Context: Since incorporating unaligned words will introduce noise, we rank the translation candi-dates for each AMR fragment by their counts in the training data, and select the top N candidates. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code for grammar induction can be downloaded from https://github.com/xiaochang13/AMR-generation\nSection Titles (if exists):  ['2 Method', '2.3 Rule Acquisition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://odur.let.rug.nl/\u223cvannoord/TextCat/\nCitation Context: Each webpage is stripped of HTML tags, tokenized, and sentence segmented. Each docu-ment is language identified by the software TextCat [URL_CITE] which implements the paper by Cavnar and Trenkle (1994). We retain only English documents.\nFootnote or Reference text (if exists): 4 http://odur.let.rug.nl/\u223cvannoord/TextCat/\nSection Titles (if exists):  ['4 Building Noun Similarity Lists', '4.1 Web Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://deepgraphlearning.github.io/project/wikidata5m\nCitation Context: For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [URL_CITE] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning.\nFootnote or Reference text (if exists): 4 https://deepgraphlearning.github.io/project/wikidata5m\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014\nCitation Context: To experiment with a diverse set of queries and web pages, we created a new dataset, O PEN W EB , using web pages from Google search results. [URL_CITE] We use the method from Berant et al. (2013) to gen-erate search queries by performing a breadth-first search over the query space.\nFootnote or Reference text (if exists): 1 The O PEN W EB dataset and our code base are available for download at http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014.\nSection Titles (if exists):  ['2 Problem statement', '2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/freebase\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dmoz.org/\nCitation Context: We set up a spider to download roughly 70 million web pages from the Internet. Initially, we use the links from Open Directory project [URL_CITE] as seed links for our spider. Each webpage is stripped of HTML tags, tokenized, and sentence segmented.\nFootnote or Reference text (if exists): 3 http://www.dmoz.org/\nSection Titles (if exists):  ['4 Building Noun Similarity Lists', '4.1 Web Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FudanNLP/fnlp\nCitation Context: We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [URL_CITE] for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times.\nFootnote or Reference text (if exists): 9 https://github.com/FudanNLP/fnlp\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/parsing/preprocessing.html\nCitation Context: As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words [URL_CITE] and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016).\nFootnote or Reference text (if exists): 10 https://radimrehurek.com/gensim/parsing/preprocessing.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~ark/TweetNLP/\nCitation Context: We preprocessed Twitter dataset with Twitter NLP tool [URL_CITE] (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization. For Weibo dataset, we used NLPIR tool 10 (Zhang et al., 2003) for Chi-nese word segmentation.\nFootnote or Reference text (if exists): 9 http://www.cs.cmu.edu/\u02dcark/TweetNLP/\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xpqiu/fnlp/\nCitation Context: As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [URL_CITE] is used for Chinese word segmentation.\nFootnote or Reference text (if exists): 7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nCitation Context: We randomly split each of the two datasets into two portions, where 90% of the ut-terances are used for training the punctuation predic-tion models, and the remaining 10% for evaluating the prediction performance. For all the experiments, we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with the Penn Treebank tokenizer [URL_CITE] . We list the statistics of the two datasets after processing in Table 3.\nFootnote or Reference text (if exists): 2 http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/utils.html\nCitation Context: 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer [URL_CITE] for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation.\nFootnote or Reference text (if exists): 8 https://radimrehurek.com/gensim/utils.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clg.ox.ac.uk/tedcorpus\nCitation Context: To assess the ability of our framework to han-dle multiple target languages, we ran a second set of experiments on the TED corpus (Cettolo et al., 2012), using the training and test parti-tions created by Hermann and Blunsom (2014), downloaded from [URL_CITE] http://www.clg.ox.ac.uk/tedcorpus. The corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference in 12 different languages, with 12,078 parallel docu-ments in the training partition (totalling 1,641,985 parallel sentences).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 TED Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://chywang.github.io/data/emnlp17.zip\nCitation Context: In the experiments, given only 0.6M entities and their respective 2.4M categories in Chinese Wikipedia, our method extracts 1.52M relations with an overall accuracy of 93.6%. The exper-iments also show that our approach outperforms previous methods for both is-a and non-taxonomic relation extraction from Chinese UGCs. The ex-tracted relations and the labeled test set are pub-licly available [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://chywang.github.io/data/emnlp17.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.whlib.ac.cn/sjk/bkqs.htm\nCitation Context: We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great Encyclopedia [URL_CITE] Chinese corpus as unclassified data in Chinese. We defined, for each translation, a seed word in English as a classified example (cf., Table 5).\nFootnote or Reference text (if exists): 5 http://www.whlib.ac.cn/sjk/bkqs.htm\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/luyaojie/text2event\nCitation Context: We conducted experiments [URL_CITE] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings. In summary, the contri-butions are as follows:\nFootnote or Reference text (if exists): 1 Our source codes are openly available at https://github.com/luyaojie/text2event\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlpado.de/~sebastian/data.shtml\nCitation Context: Our projection on the English side results in 53K V and 35K T sentences, of which 731 are labeled as both T and V. Finally, from the English labeled sentences we ex-tracted a training set with 72 novels (63K sentences) and a test set with 21 novels (15K sentences). [URL_CITE]\nFootnote or Reference text (if exists): 4 The corpus can be downloaded for research purposes from http://www.nlpado.de/~sebastian/data.shtml.\nSection Titles (if exists):  ['2 A Parallel Corpus of Literary Texts', '2.3 T/V Gold Labels for English Utterances']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://contrib.scikit-learn.org/lightning/\nCitation Context: 16 We cross-validate regularization strength 10\u02c6(-100,-5,- 4,-3,-2,-1,0,1), model type (SVM vs. Logistic L1 vs. Logistic L2 vs. Logistic L1/L2), and whether or not to apply feature standardization for each feature set and cross-validation split separately. These are trained using lightning ( [URL_CITE] http: //contrib.scikit-learn.org/lightning/). W2V.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Early Prediction of Controversy', '4.1 Comparing Text Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jstatsoft.org/v18/i02/\nCitation Context: The pls package: Principal component and partial least squares regression in R. Journal of Statistical Soft-ware, 18(2). Published online: [URL_CITE] http://www.jstatsoft.org/v18/i02/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: We train a multi-class logistic regression model [URL_CITE] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output.\nFootnote or Reference text (if exists): 4 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Identifying Simplification Rules', '2.2 Lexical Simplification Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/\nCitation Context: \u2022 TED talks: data-split from Duh (2018) [URL_CITE] .\nFootnote or Reference text (if exists): Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/.\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Data and Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://search.cpan.org/~snowhare/Lingua-Stem-0.83\nCitation Context: \u2022 All above features and their stems. [URL_CITE] (All-Words-Stemmed)\nFootnote or Reference text (if exists): 7 To obtain stemmed words, we use the CPAN package: http://search.cpan.org/~snowhare/Lingua-Stem-0.83.\nSection Titles (if exists):  ['6 Local Lexical Context']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/stable/\nCitation Context: \u2022 Logistic regression classifier: [URL_CITE] http://scikit-learn.org/stable/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt16/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) [URL_CITE] , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 2 https://www.statmt.org/wmt16/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wit3.fbk.eu/\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) [URL_CITE] , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 1 https://wit3.fbk.eu/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011) [URL_CITE] , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible. Details are in Tab.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt19/translation-task.html\nCitation Context: To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [URL_CITE] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experiment', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11372/LRT-1632\nCitation Context: We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016 [URL_CITE] ), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.\nFootnote or Reference text (if exists): Marco Turchi, Rajen Chatterjee, and Matteo Negri. 2016. WMT16 APE shared task data. LIN-DAT/CLARIN digital library at the Institute of For-mal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.\nSection Titles (if exists):  ['4 Experiments', '4.2 Automatic MT Post-editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt14/translation-task.html\nCitation Context: 4 We use WMT14 EN-DE translation dataset as the EN-DE sentence-level parallel dataset which consists of 4.4M sentence pairs. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://www.statmt.org/wmt14/translation-task.html\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge [URL_CITE] , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fakenewschallenge.org/\nCitation Context: Progress on the aforementioned tasks has bene-fited from the availability of large-scale datasets (Bowman et al., 2015; Rajpurkar et al., 2016). However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims. Indicatively, the recently con-ducted Fake News Challenge (Pomerleau and Rao, 2017) [URL_CITE] with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 as-sociated news articles which is orders of magni-tude smaller than those used for TE and QA.\nFootnote or Reference text (if exists): Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/fact-check-ratings/\nCitation Context: We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [URL_CITE] ).\nFootnote or Reference text (if exists): 13 http://www.snopes.com/fact-check-ratings/\nSection Titles (if exists):  ['4 Datasets', '4.2 Snopes Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: We first pre-train an agreement classifier on a stance detection dataset provided by the Fake News Challenge [URL_CITE] . This dataset provides pairs of English sentences with their agreement annota-tions in \u201cagree\u201d, \u201cdisagree\u201d, \u201cdiscuss\u201d and \u201cunre-lated\u201d.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['5 Cross-lingual Cross-platform Features', '5.2 Agreement Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-2016 Task 6 (Mohammad et al., 2016) and from the Fake News Challenge (FNC). [URL_CITE] De-spite its name, the latter has no annotations for fac-tuality, but consists of article-claim pairs labeled for stance: agrees, disagrees, discusses, and unre-lated. In contrast, we retrieve documents for each claim, which yields an arguably more natural dis-tribution of stance labels compared to FNC.\nFootnote or Reference text (if exists): 3 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fakenewschallenge.org/\nCitation Context: The Fake News challenge (Pomerleau and Rao, 2017) [URL_CITE] modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles.\nFootnote or Reference text (if exists): Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908\nCitation Context: The recently proposed KILT benchmark (Petroni et al., 2020), which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA models by the quality of both generated an-swers (ROUGE-L against reference answers) and retrieved documents (R-precision against human-annotated relevant documents). In this paper, we build a state-of-the-art system [URL_CITE] for ELI5 by using a sparse Transformer variant (Roy et al., 2020) to condition over Wikipedia paragraphs returned by a REALM-style retriever (Guu et al., 2020).\nFootnote or Reference text (if exists): 2 State-of-the-art as of April 3, 2021 \u2014 the \u201cGoogle Research & UMass Amherst\u201d team entry on https: //evalai.cloudcv.org/web/challenges/ challenge-page/689/leaderboard/1908\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by Jernite (2020) [URL_CITE] that uses gold answers for distant supervision. The key idea is to push the encoded vector for a ques-tion close to a vector representation of its ground-truth answer(s), but away from all other answer vectors in the mini-batch (negative examples).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [URL_CITE] . This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Next, we investigate the effect of initialization on the training of C -REALM. Unlike Jernite (2020) [URL_CITE] who initialize their model with BERT, before train-ing we initialize our retriever with a pretrained self-supervised retriever. As a baseline, we initial-ize our model with ICT, a weaker self-supervised retriever introduced in Lee et al.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: In this section, we describe our proposed LFQA system, which conditions answer generation on Wikipedia articles identified by a pretrained re-triever. We use a dense retriever trained by scaling up a distantly supervised algorithm from Jernite (2020) [URL_CITE] . Since retrieved articles can be quite long and often exceed the maximum sequence length of pretrained models like BERT (Devlin et al., 2019), we use a sparse-attention variant of the Transformer to allow modeling over longer sequences.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Mixed (Paper and Tool)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2008/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [URL_CITE] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description.\nFootnote or Reference text (if exists): 5 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://stackexchange.com/sites\nCitation Context: MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset where [URL_CITE] to 10 news articles share a single summary. We consider every pair of articles that share a summary to be a similar document pair. Table 2 shows summary statistics of the two document ranking datasets.\nFootnote or Reference text (if exists): 2 https://stackexchange.com/sites\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nCitation Context: The Text Analysis Conference (TAC) Knowledge Base Population (KBP) Cold Start task [URL_CITE] requires systems to take set of documents and produce a comprehensive set of triples that encode relationships between and at-tributes of the named-entities that are mentioned in the corpus. Systems are evaluated based on the fi-delity of the constructed knowledge base. For the 2012 evaluation, a fixed schema of 42 relations (or slots), and their logical inverses was provided, for example:\nFootnote or Reference text (if exists): 1 See details at http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipediadatabase\nCitation Context: Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [URL_CITE] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/Wikipedia database\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2009/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/manuyavuz/TurkishMDSDataSet_alpha\nCitation Context: One of the greatest challenges for MDS studies in Turk-ish is that there does not exist a manually annotated data set. In this study, we have collected and manually annotated a Turkish MDS data set, which is publicly available for future studies [URL_CITE] .\nFootnote or Reference text (if exists): 1 The data set can be retrieved from the following github repository: https://github.com/manuyavuz/ TurkishMDSDataSet_alpha\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Data Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/18814\nCitation Context: But, the ground truth evidence and labels of the test set are not publicly available. Instead, once extracted evidence sets/sentences and predicted labels of the test set by a model are sub-mitted to the online evaluation system [URL_CITE] , its perfor-mance is measured and displayed at the system. In this work, we train and tune our hyper-parameters on training and development sets, respectively.\nFootnote or Reference text (if exists): 1 https://competitions.codalab.org/competitions/18814\nSection Titles (if exists):  ['5 Experiments', '5.1 Experiment Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ihtsdo.org/snomed-ct/\nCitation Context: After finding concept boundaries, we determine the probability distribution for each concept over 4 possible types (TEST, TRE, PROB or NULL). These probability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011). Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) and SNOMED CT (Url5, 2013) [URL_CITE] , MeSH and SNOMED CT descriptors.\nFootnote or Reference text (if exists): Url5. 2013. Snomed ct: Snomed clinical terms (http://www.ihtsdo.org/snomed-ct/) (accessed july 1, 2013).\nSection Titles (if exists):  ['2 Identifying Medical Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/allennlp-semparse\nCitation Context: IS-SP is a semantic parsing based model trained on WikiTa-blesQuestions (Pasupat and Liang, 2015) dataset (See Section 2 for the details of this work). For building their model we used the code provided in (Gardner et al., 2020) [URL_CITE] . For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset.\nFootnote or Reference text (if exists): Matt Gardner, Dirk Groeneveld, Brendan Roof, Pradeep Dasigi, Michael Schmitz, and Nitish Gupta. 2020. allennlp-semparse. https://github.com/allenai/allennlp-semparse.\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://archive.ics.uci.edu/ml\nCitation Context: Since the constraint type is determined by syntactic and dependency parse features, this component does not need to be retrained for new domains. In this work, we trained this classifier based on a manually annotated set of 80 sentences describing classes in the small UCI Zoo dataset (Lichman, 2013) [URL_CITE] , and used this model for all experiments.\nFootnote or Reference text (if exists): M. Lichman. 2013. UCI machine learning repository. http://archive.ics.uci.edu/ml.\nSection Titles (if exists):  ['3 Learning Classifiers from Language', '3.1 Mapping language to constraints', '3.1.1 Semantic Parser components']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://archive.org/details/stackexchange\nCitation Context: StackExchange 2 is an online question answer-ing platform and has been used as a benchmark in previous work (dos Santos et al., 2015; Shah et al., 2018; Perkins and Yang, 2019). We took the June 2019 data dumps [URL_CITE] of the AskUbuntu and Su-perUser subdomains of the platform and combined them to form our dataset.\nFootnote or Reference text (if exists): 3 https://archive.org/details/ stackexchange\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/brmson/question-classification\nCitation Context: This analysis is done on the 10% split of the SQuAD training set. We consider two classifications of each ques-tion \u2013 one determined on the first word (usually a wh-word) of the question (Figure 2 (bottom)) and one based on the output of a separate ques-tion type classifier [URL_CITE] adapted from (Li and Roth, 2002). We use the coarse grain labels namely Abbreviation (ABBR), Entity (ENTY), Descrip-tion (DESC), Human (HUM), Location (LOC), Numeric (NUM) trained on a Logistic Regres-sion classification system .\nFootnote or Reference text (if exists): 7 https://github.com/brmson/question-classification\nSection Titles (if exists):  ['4 Experiments & Results', '4.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UniversalDependencies/UD_Japanese-BCCWJ\nCitation Context: For the Japanese dataset, we follow the training/development/test split of Universal Depen-dencies (UD) Japanese-BCCWJ. (Asahara et al., 2018) [URL_CITE] Table 1 shows the statistics of the dataset.\nFootnote or Reference text (if exists): 4 https://github.com/UniversalDependencies/UD_Japanese-BCCWJ\nSection Titles (if exists):  ['3 Experiments', '3.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [URL_CITE] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description.\nFootnote or Reference text (if exists): 5 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.microsoft.com/en-us/download/details.aspx?id=52419\nCitation Context: WikiQA [URL_CITE] is a retrieval-based question answer-ing dataset based on Wikipedia. We follow the data split of original paper.\nFootnote or Reference text (if exists): 6 https://www.microsoft.com/en-us/download/details.aspx?id=52419\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our SMT implementation is based on Moses [URL_CITE] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure.\nFootnote or Reference text (if exists): 10 http://www.statmt.org/moses/\nSection Titles (if exists):  ['5 Experiments and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://phontron.com/lader\nCitation Context: For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types of pre-ordering: original order with F 0 \u2190 F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3-step), and the proposed model with latent derivations (lader). [URL_CITE] Except when stated oth-erwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10 \u22123 (chosen through cross-validation).\nFootnote or Reference text (if exists): 7 Available open-source: http://phontron.com/lader\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nltk.org/\nCitation Context: The third set of features that we use are stylis-tic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK [URL_CITE] (Loper and Bird, 2002).\nFootnote or Reference text (if exists): 7 http://www.nltk.org/\nSection Titles (if exists):  ['4 Features', '4.1 Text-based Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/messense/jieba-rs\nCitation Context: All Chinese sentences are segmented by Jieba [URL_CITE] while all English and German sentences are tok-enized by Moses scripts (Koehn et al., 2007). For ZH-EN (EN-DE) translation, we merge the source and target sentences of the parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) opera-tions (Sennrich et al., 2016).\nFootnote or Reference text (if exists): 6 https://github.com/messense/jieba-rs\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://kheafield.com/code/kenlm/\nCitation Context: This pruning is done efficiently through a varia-tion of cube-pruning (Chiang, 2007). We use KenLM [URL_CITE] (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estima-tions(Heafield et al., 2012).\nFootnote or Reference text (if exists): 3 http://kheafield.com/code/kenlm/\nSection Titles (if exists):  ['4 Decoding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Next, we investigate the effect of initialization on the training of C -REALM. Unlike Jernite (2020) [URL_CITE] who initialize their model with BERT, before train-ing we initialize our retriever with a pretrained self-supervised retriever. As a baseline, we initial-ize our model with ICT, a weaker self-supervised retriever introduced in Lee et al.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/MUSE\nCitation Context: We also propose a joint-learning model with multilingual word embedding. We take advantage of the availability of mul-tilingual word embeddings [URL_CITE] to build a joint-learning model. Figure 1 summarize how the data is transformed and learned in this model.\nFootnote or Reference text (if exists): 7 https://github.com/facebookresearch/ MUSE\nSection Titles (if exists):  ['5 Cross-lingual Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt\nCitation Context: We monitor the training process on the develop-ment set and report the final result on the test set. One layer CNN with a filter of size [URL_CITE] is utilized to generate 128-dimension word embeddings by max pooling. The cased, 300d Glove is adapted to initialize word embeddings, which is frozen in all models.\nFootnote or Reference text (if exists): 3 https://www.clips.uantwerpen.be/conll2000/chunking/conlleval.txt\nSection Titles (if exists):  ['4 Experiments', '4.2 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/word2vec/\nCitation Context: We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [URL_CITE] proposed in (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep\nSection Titles (if exists):  ['3 Experiments', '3.2 Training Strategy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: P cg (s,a) is 1 or 0, indicating whether the correct answer is c. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent. We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.9M [URL_CITE] . We randomize other pa-rameters with uniform distribution U(\u22120.01, 0.01), and set the learning rate as 0.01.\nFootnote or Reference text (if exists): 4 Available at: http://nlp.stanford.edu/projects/glove/.\nSection Titles (if exists):  ['3 Deep Memory Network for Aspect Level Sentiment Classification', '3.6 Aspect Level Sentiment Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mmihaltz/word2vec-GoogleNews-vectors\nCitation Context: We use the open-domain word embeddings [URL_CITE] for the initialization of word vectors. We initialize other model parameters from a uniform distribu-tion U(-0.05, 0.05).\nFootnote or Reference text (if exists): 1 https://github.com/mmihaltz/word2vec-GoogleNews-vectors\nSection Titles (if exists):  ['6 Experiments', '6.3 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We release our open domain event schemas and the Rel-grams database [URL_CITE] for further use by the NLP community.\nFootnote or Reference text (if exists): 3 Available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['1 Introduction', '1.1 Contributions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fever.ai\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website [URL_CITE] , we also publish the annotation interfaces and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 1 http://fever.ai\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sheffieldnlp/fever-baselines\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [URL_CITE] to stimulate further research on verification.\nFootnote or Reference text (if exists): 3 https://github.com/sheffieldnlp/ fever-baselines\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Elbria/utdsm_naacl2018\nCitation Context: We further per-form experiments on two NLP downstream tasks: text classification and paraphrase identification and demonstrate that our learned word represen-tations consistently provide higher performance than single-prototype word embedding models. The code of the present work is publicly avail-able [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/Elbria/utdsm_ naacl2018\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/liushulinle/event\nCitation Context: We publish our code for further study by the NLP community. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/liushulinle/event detection without triggers\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [URL_CITE] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 3 https://aws.amazon.com/translate\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.bing.com/translator\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [URL_CITE] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 2 https://www.bing.com/translator\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://translate.google.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [URL_CITE] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 1 https://translate.google.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0\nCitation Context: Named entity counts and concreteness ratio. For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [URL_CITE] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn.\nFootnote or Reference text (if exists): 6 Version 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0. pdf Section 2.6.\nSection Titles (if exists):  ['4 Pragmatic correlations with analysts\u2019 pre-call judgments', '4.1 Pragmatic lexical features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/multimodal-task.html\nCitation Context: To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [URL_CITE] . The best results of NMT model were those of Huang et al.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/multimodal-task.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.systransoft.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, [URL_CITE] (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 4 http://www.systransoft.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paraphrase.org\nCitation Context: For instance, the word foolishness is simpler than its meaning-preserving substitution folly even though foolish-ness is longer and less frequent in the Google 1T Ngram corpus (Brants and Franz, 2006). In fact, we found that 21% of the 2272 meaning-equivalent word pairs randomly sampled from PPDB [URL_CITE] (Ganitkevitch et al., 2013) had the simpler word longer than the complex word, while 14% had the simpler word less frequent.\nFootnote or Reference text (if exists): 2 PPDB is a large paraphrase database derived from static bilingual translation data available at: http://paraphrase.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lifengjin/dimi_emnlp18\nCitation Context: Specifically, \u03c4each tree is a set {\u03c4 , \u03c4 , \u03c4 [URL_CITE] , \u03c4 11 , \u03c4 12 , \u03c4 21 , ...} of category node labels \u03c4 \u03b7 where \u03b7 \u2208 {1,2} \u2217 defines a path of left or right branches from the root to that node. Category labels for every pair of left and right children \u03c4 \u03b71 ,\u03c4 \u03b72 are drawn from a multinomial distribution defined by the grammar G and the category of the parent \u03c4 \u03b7 : where \u03b4 x is a Kronecker delta function equal to 1 at value x and 0 elsewhere, and terminals have null expansions P G (a b | w) = P G (a b | \u22a5) = ~a, b=\u22a5, \u22a5\u007f for w \u2208 W. 4\nFootnote or Reference text (if exists): 2 https://github.com/lifengjin/dimi_emnlp18.\nSection Titles (if exists):  ['5 Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/fact-check-ratings/\nCitation Context: We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [URL_CITE] ).\nFootnote or Reference text (if exists): 13 http://www.snopes.com/fact-check-ratings/\nSection Titles (if exists):  ['4 Datasets', '4.2 Snopes Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Shakespeare%27ssonnets\nCitation Context: The example in the third row of Figure 1 comes from Shakespeare\u2019s \u201cSonnet 116\u201d (Shake-speare, 2000), the first four sentences. Usually, the rhyming schemes of Shakespeare\u2019s Sonnets is \u201cABAB CDCD EFEF GG\u201d [URL_CITE] . In the example, the rhyming words in scheme \u201cABAB\u201d are \u201cminds\u201d, \u201clove\u201d, \u201cfinds\u201d, and \u201cremove\u201d.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org/wiki/Shakespeare%27s sonnets\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/mozillazg/python-pinyin\nCitation Context: As the example shown in Table 1, the pronunciation of the red rhyming words are \u201czhu\u201d, \u201cyu\u0308\u201d, \u201cdu\u201d, and \u201cgu\u201d respectively, and the rhyming phoneme is \u201cu\u201d. For the generated samples, we first use the tool pinyin [URL_CITE] to get the pronunciations (PinYin) of the words in the rhyming positions, and then conduct the evaluation. For Shakespeare\u2019s Sonnets corpus, the rhyming rule is clear \u201cABAB CDCD EFEF GG\u201d and there are 7 groups of rhyming tokens.\nFootnote or Reference text (if exists): 4 http://github.com/mozillazg/python-pinyin\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh/meshhome.html\nCitation Context: These nodes have children, for example, Abdomen (A01.047) and Back (A01.176) are level [URL_CITE] children of Body Regions. The longer the ID of the MeSH term, the longer the path from the root and the more precise the description. For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous\nFootnote or Reference text (if exists): 1 http://www.nlm.nih.gov/mesh/meshhome.html; the work reported in this paper uses MeSH 2001.\nSection Titles (if exists):  ['3 The Lexical Hierarchy: MeSH']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/litbank\nCitation Context: In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at [URL_CITE] https://github.com/dbamman/litbank.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sarathchandar.in/bridge-corrnet\nCitation Context: In this paper, we propose Bridge Correlational Neu-ral Networks which can learn common representa-tions for multiple views even when parallel data is available only between these views and a pivot view. Our method performs better than the existing state of the art approaches on the cross language clas-sification task and gives very promising results on the cross modal access task. We also release a new multilingual image caption benchmark (MIC bench-mark) which will help in further research in this field [URL_CITE] .\nFootnote or Reference text (if exists): 5 Details about the MIC benchmark and performance of var-ious state-of-the-art models will be maintained at http://sarathchandar.in/bridge-corrnet\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea/\nCitation Context: The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) 7 ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea [URL_CITE] ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.\nFootnote or Reference text (if exists): 8 http://www.phontron.com/kytea/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/segmenter.shtml\nCitation Context: The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) [URL_CITE] ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.\nFootnote or Reference text (if exists): 7 http://nlp.stanford.edu/software/segmenter.shtml\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf\nCitation Context: We present in this work a new dataset of en-tity annotations for a wide sample of 210,532 to-kens from 100 literary texts to help address these issues and help advance computational work on literature. These annotations follow the guide-lines set forth by the ACE 2005 entity tagging task (LDC, 2005) [URL_CITE] in labeling all nominal entities (named and common alike), including those with nested structure. In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them.\nFootnote or Reference text (if exists): LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tinyurl.com/joccfqy\nCitation Context: This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 [URL_CITE] to solve the ILP described in Sec. 2.\nFootnote or Reference text (if exists): 11 http://tinyurl.com/joccfqy\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gurobi.com/\nCitation Context: After incorporating all the constraints mentioned above, the final optimization problem (an IQP) is shown in Figure 2. We used Gurobi toolkit (Url6, 2013) [URL_CITE] to solve such IQPs. In our case, it solves 76 IQPs per second on a quad-core server with In-tel Xeon X5650 @ 2.67 GHz processors and 50 GB RAM.\nFootnote or Reference text (if exists): Url6. 2013. Gurobi optimization toolkit (http://www.gurobi.com/) (accessed july 1, 2013).\nSection Titles (if exists):  ['3 Modeling Global Inference', '3.2 Final Optimization Problem - An IQP']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Xu et al. (2017) [URL_CITE] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/wordprobs/\nCitation Context: This paper studies the task of learning to automatically solve such problems given only the natural language. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data for this work are available at http://groups.csail.mit.edu/rbg/code/wordprobs/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/optimization/\nCitation Context: We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://developers.google.com/optimization/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/\nCitation Context: We used lp solve [URL_CITE] to solve our ILP optimization problems. We ran experiments on two datasets.\nFootnote or Reference text (if exists): 3 From http://lpsolve.sourceforge.net/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: Our classifier forms an encoder-labeler struc-ture, which consists of the above encoder and a la-beler that predicts a sentiment label from the con-text. Note that the encoder of the classifier is fine-tuned with labeled data, as in (Dai and Le, 2015) [URL_CITE] . The main difference between their approach and ours is that we examine paired (dialog) data for pretraining, while they only showed the usefulness of pretraining with unpaired data.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/KB2E\nCitation Context: In these tables, the difference between our implementations and the paper reported results are listed in the parentheses. To demonstrate the efficiency of OpenKE, we select TransE as a rep-resentative and implement it with both OpenKE and KB2E [URL_CITE] , and then compare their training time. KB2E is a widely-used toolkit for KE models on GitHub.\nFootnote or Reference text (if exists): 4 https://github.com/thunlp/KB2E\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/aclwiki/index.php?title=Textual_Entailment_Resource_Pool\nCitation Context: To date, the primary sources of annotated NLI cor-pora have been the Recognizing Textual Entail-ment (RTE) challenge tasks. [URL_CITE] These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical mod-els of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed rep-resentations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowl-edge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al.\nFootnote or Reference text (if exists): 1 http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool\nSection Titles (if exists):  ['2 A new corpus for NLI']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/egret-parser\nCitation Context: In previous reports, it has been noted (Kummerfeld et al., 2012) that the la-tent variable model of the Berkeley parser tends to have the higher accuracy of the two, so if the accu-racy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret, [URL_CITE] which achieves nearly iden-tical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other el-ement in the phrase.\nFootnote or Reference text (if exists): 4 http://code.google.com/p/egret-parser\nSection Titles (if exists):  ['3 Parsing', '3.1 Parsing Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatik.uni-trier.de/\u223cley/db/\nCitation Context: We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP [URL_CITE] -record/citation-text pairs and global GE criteria of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008).\nFootnote or Reference text (if exists): 2 http://www.informatik.uni-trier.de/\u223cley/db/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/2700406\nCitation Context: It seems unclear that this is a good simulation of the kind of shift in distribution that one is likely to en-counter in practice. An exception to this is Esuli and Sebastiani (2015) [URL_CITE] , who test their method on the RCV1-v2 corpus, also splitting by time. They perform a large number of experiments, but un-fortunately, nearly all of their experiments involve only a very small difference in label proportions between the source and target (with the vast ma-jority < 0.01), which limits the generalizability of their findings.\nFootnote or Reference text (if exists): Andrea Esuli and Fabrizio Sebastiani. 2015. Optimiz-ing text quantifiers for multivariate loss functions. ACM Trans. Knowl. Discov. Data 9(4). https: //doi.org/10.1145/2700406.\nSection Titles (if exists):  ['5 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/m-amr2text\nCitation Context: Our pretrained mod-els are available for download. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://github.com/facebookresearch/ m-amr2text\nSection Titles (if exists):  ['4 Experimental Setting', '4.2 Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/demos.php\nCitation Context: For the Named Entity information, we used a publicly available NE Recognizer capable of recognizing a range of cat-egories including Person, Location and Organiza-tion. On the CoNLL-03 shared task, its f-score is about 90% [URL_CITE] . We evaluate our system from different points of view, as described below.\nFootnote or Reference text (if exists): 4 A web demo of the NER is at http://L2R.cs.uiuc.edu/\u02dccogcomp/demos.php.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/fnwkde/\nCitation Context: The approach is in theory applicable to any language, but there are some obstacles: first, it relies on the availability of the target sense in the target language Wiktionary. For many of the top 30 languages in FNWKxx, the Wiktionary edi-tions seem sufficiently large to provide targets for translation disambiguation, [URL_CITE] and they are contin-uously extended. Second, our approach requires access to the target language Wiktionary, but the data format across Wiktionary language editions is not standardized.\nFootnote or Reference text (if exists): 4 see overview table at http://www.ukp.tu-darmstadt.de/fnwkde/.\nSection Titles (if exists):  ['9 Discussion: a Multilingual FrameNet']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mdbg.net/chindict/chindict.php?page=cedict\nCitation Context: For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon [URL_CITE] , and is the language model, and p \u03b8 LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary and PMI based word correlation are used to enhance the language mod-el for machine translation.\nFootnote or Reference text (if exists): 1 MDBG CC-CEDICT is adopted as the bilingual lexicon: http://www.mdbg.net/chindict/chindict.php?page=cedict\nSection Titles (if exists):  ['4 Emotion Detection via Bilingual and Sentiment Information', '4.1 Bilingual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/universal-tagger/\nCitation Context: We made our code are available for download. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://code.google.com/p/universal-tagger/\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://joshua-decoder.com/language-packs/paraphrase/\nCitation Context: Several different size language packs are available for download. [URL_CITE] The components of the language packs are described below. Grammar Our approach to sentential paraphras-ing is analogous to machine translation.\nFootnote or Reference text (if exists): 1 http://joshua-decoder.com/language-packs/paraphrase/\nSection Titles (if exists):  ['2 Language pack description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/metricsmatr/\nCitation Context: Our choice of metrics was based on their popular-ity in the MT community, their performance in open competitions such as the NIST MetricsMATR chal-lenge (NIST, 2008) [URL_CITE] and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity.\nFootnote or Reference text (if exists): NIST. 2008. NIST MetricsMATR Challenge. Informa-tion Access Division. http://www.itl.nist.gov/iad/mig/tests/metricsmatr/.\nSection Titles (if exists):  ['3 Classifying with MT Metrics', '3.2 MT metrics used']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://toeflpractice.ets.org/\nCitation Context: The manual annotations were provided by two ex-perts with prior experience in various types of data annotation on both text and speech. First, a pi-lot annotation was conducted to train and cali-brate the annotators based on 48 training samples drawn from the TOEFL R Practice Online (TPO) product [URL_CITE] , which offers practice tests simulating the TOEFL iBT testing experience with authentic test questions. The training samples were selected from a TPO test form with 6 test questions and were balanced according to test question and pro-ficiency score, i.e., 2 responses from each score level for each question.\nFootnote or Reference text (if exists): 3 https://toeflpractice.ets.org/\nSection Titles (if exists):  ['3 Annotation', '3.2 Pilot Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.slt.atr.jp/IWSLT2006/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation [URL_CITE] , NIST Machine Translation Evaluation , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 1 http://www.slt.atr.jp/IWSLT2006/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.863data.org.cn/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation , and HTRDP Evaluation [URL_CITE] were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 3 http://www.863data.org.cn/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/index.htm\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation [URL_CITE] , and HTRDP Evaluation were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt/index.htm\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf\nCitation Context: The 769 sentences were translated by using five commercial MT systems to investigate the relation-ship between subjective evaluation based on yes/no questions and conventional subjective evaluation based on fluency and adequacy. The instruction for the subjective evaluation based on fluency and ad-equacy followed that given in the TIDES specifi-cation (TIDES, 2002) [URL_CITE] . The subjective evaluation based on yes/no questions was done by manually answering each question for each translation.\nFootnote or Reference text (if exists): TIDES. 2002. Linguistic Data Annotation Specifi-cation: Assessment of Fluency and Adequacy in Arabic-English and Chinese-English Translations. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf.\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/\nCitation Context: These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) [URL_CITE] and law, medical OPUS datasets. For this setting, we con-sider two language pairs, namely Czech and Ger-man to English.\nFootnote or Reference text (if exists): Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/.\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: The depths of all MLPs are all 1, and the MLPs for unlabeled attachment and those for labeling contain 500 (d arc ) and 100 (d rel ) units re-spectively. For character-level CNNs, we use the hyperparameters from Ma and Hovy (2016) [URL_CITE] .\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html\nCitation Context: We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010) [URL_CITE] , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct com-parison problematic. Some first steps at analysis of the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.\nFootnote or Reference text (if exists): Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/XinnuoXu/\nCitation Context: For further analysis and studies, we re-domains. However, GPT-2 is pre-trained on Open- lease the data from all intermediate steps for WebText and the language style and topics thereof each domain at [URL_CITE] https://github.com/XinnuoXu/ are quite different from those of daily conversa- AugNLG/tree/master/augmented_data .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code and DataSource)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/yamcha/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: We used the multilingual BERT implementation from Huggingface. [URL_CITE] The design for these exper-iments is making a comparison between a cross validation within the TED-MDB and a zero-shot transfer learning from TED-CDB to TED-MDB. Due to the unbalanced distribution of senses in TED-MDB, using the method of Easy Ensemble (Liu, 2009), we divided the Expansion data of every language in the TED-MDB into 4 parts and then each part was added into the data of other types to become the training set.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['6 Transfer Learning via TED-CDB', '6.2 Same-Domain Cross-Language Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.yelp.com/datasetchallenge\nCitation Context: Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009). We implement mini-batch stochastic gradient descent (SGD) with a batch size of 25, and an adaptive learning rate (AdaGrad) initialized at 0.02 for pretraining of DT-RNN, which runs [URL_CITE] epochs for the restaurant domain and 5 epochs for the laptop domain. For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02.\nFootnote or Reference text (if exists): 4 http://www.yelp.com/dataset challenge\nSection Titles (if exists):  ['6 Experiment', '6.1 Dataset and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/postagger/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger [URL_CITE] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 4 http://www-tsujii.is.s.u-tokyo.ac.jp/\u02dctsuruoka/postagger/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/TinySVM/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/moving-qatar/posts/can-i-obtain-driving-license-my-qid-written-employee\nCitation Context: A simplified example is shown in Figure 1, [URL_CITE] where answers 2 and 4 are good, answer 1 is po-tentially useful, and answer 3 is bad. In this paper, we focus on a 2-class variant of the above Sub-task A, which is closer to a real CQA application.\nFootnote or Reference text (if exists): 3 http://www.qatarliving.com/moving-qatar/posts/can-i- obtain-driving-license-my-qid-written-employee\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/page/demoview/TempSys\nCitation Context: We have implemented our system in a web-based demo [URL_CITE] . Figure 2 shows a screenshot of the input panel of the system.\nFootnote or Reference text (if exists): 4 http://cogcomp.cs.illinois.edu/page/demoview/TempSys\nSection Titles (if exists):  ['4 The Demonstration', '4.1 Visualization']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: , p K independently selects a pair of answer positions (i k ,j k ), which are the begin and end of the answer from paragraph p k . In the case that p k does not support answering the question q, special NULL positions are selected (follow-ing the SQuAD 2.0 BERT implementation [URL_CITE] ). Thus, the set of possible outcomes \u2126 in the paragraph-level probability space is the set of lists of begin/end position pairs, one from each paragraph: {[(i 1 , j 1 ), .\nFootnote or Reference text (if exists): 3 https://github.com/google-research/bert\nSection Titles (if exists):  ['2 Probability Space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nCitation Context: TrecQA [URL_CITE] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included.\nFootnote or Reference text (if exists): 5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cqa.iyas.qcri.org/cQA-Arabic-Demo\nCitation Context: A second example is a cQA demo [URL_CITE] in Arabic, which retrieves data from multiple medical forums from middle-east. In this case physicians answer to patients\u2019 questions: the left panel shows a ques-tion from a user and the right panel similar ques-tions with the answers from the expert. In general there is only one (good) answer from the doctor, so this is mostly a question re-ranking task.\nFootnote or Reference text (if exists): 4 http://cqa.iyas.qcri.org/ cQA-Arabic-Demo\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mdbg.net/chindict/chindict.php?page=cedict\nCitation Context: For using bilingual information, a word-by-word statistical machine translation strategy is adopt-ed to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon [URL_CITE] , and is the language model, and p \u03b8 LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary and PMI based word correlation are used to enhance the language mod-el for machine translation.\nFootnote or Reference text (if exists): 1 MDBG CC-CEDICT is adopted as the bilingual lexicon: http://www.mdbg.net/chindict/chindict.php?page=cedict\nSection Titles (if exists):  ['4 Emotion Detection via Bilingual and Sentiment Information', '4.1 Bilingual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/cgibin/cmudict\nCitation Context: The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998) [URL_CITE] . This dictionary has an inventory of 39 phonemes.\nFootnote or Reference text (if exists): Robert L Weide. 1998. The cmu pronouncing dictionary. URL: http://www.speech.cs.cmu.edu/cgibin/cmudict.\nSection Titles (if exists):  ['3 Target Recovery', '3.3 Phonetic Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ltp-cloud.com/\nCitation Context: For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: where is the translation model, which is converted from the bilingual lexicon , and is the language model, and p \u03b8 LM (c) is the bigram language model which is trained from a large s-cale Weibo data set . As text in micro-blogs is in-formal, synonym dictionary [URL_CITE] and PMI based word correlation are used to enhance the language mod-el for machine translation. p \u03b8 SYN (c) denotes the synonym similarity between translated words and the contexts.\nFootnote or Reference text (if exists): 3 TongYiCiLin is adopted as the Chinese synonym dictio-nary: http://www.ltp-cloud.com/\nSection Titles (if exists):  ['4 Emotion Detection via Bilingual and Sentiment Information', '4.1 Bilingual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chdw.de\nCitation Context: Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005) [URL_CITE] . As with WordNet, the words in entries for English and German were stemmed to improve coverage.\nFootnote or Reference text (if exists): Jan Hefti. 2005. HanDeDict. http://chdw.de.\nSection Titles (if exists):  ['3 Bridges Across Languages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-user.tu-chemnitz.de/fri/ding/\nCitation Context: Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) [URL_CITE] a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005). As with WordNet, the words in entries for English and German were stemmed to improve coverage.\nFootnote or Reference text (if exists): Frank Richter. 2008. Dictionary nice grep. http://www-user.tu-chemnitz.de/fri/ding/.\nSection Titles (if exists):  ['3 Bridges Across Languages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.elra.info/\nCitation Context: The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M0033 available from the ELRA catalogue [URL_CITE] . This re-source is a general language dictionary which con-tains only a few terms related to the medical do-main.\nFootnote or Reference text (if exists): 5 http://www.elra.info/\nSection Titles (if exists):  ['3 Linguistic Resources', '3.2 Bilingual Dictionary']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mlg.ucd.ie/datasets/bbc.html\nCitation Context: We use the BBC corpus prepared by Greene and Cunningham (2006). [URL_CITE] Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro-posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages.\nFootnote or Reference text (if exists): 2 http://mlg.ucd.ie/datasets/bbc.html\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [URL_CITE] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.2 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html\nCitation Context: We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010) [URL_CITE] , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct com-parison problematic. Some first steps at analysis of the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.\nFootnote or Reference text (if exists): Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: \u2022 Lang, SeqAE: Pretrained with the language model and autoencoder model proposed in (Dai and Le, 2015) [URL_CITE] . The language model is the de-coder part of the encoder-decoder model using a zero vector as the initial hidden layer value, and the autoencoder model is the same structure of the encoder-decoder model, where input and output are the same.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['3 Experiments', '3.3 Compared Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mattmahoney.net/dc/textdata\nCitation Context: Fil9 Fil9 is a cleaned Wikipedia corpus, consist-ing of approximately 140M tokens, and is pro-vided on Matthew Mahoney\u2019s website. [URL_CITE] We tok-enized the corpus and used the 44k most frequent words in the training corpus for recurrent language models.\nFootnote or Reference text (if exists): 4 http://mattmahoney.net/dc/textdata\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl\nCitation Context: Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization. [URL_CITE] Penn Treebank We evaluate a normal recurrent language model, count-based n-gram language model as well as the proposed RLM-BoW-EF-n and RLM-BoW-LF-n with varying n = 1, 2, 4, 8 on the Penn Treebank Corpus. We preprocess the corpus according to (Mikolov et al., 2011) and use a vocabulary of 10k words from the training cor-pus.\nFootnote or Reference text (if exists): 3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/freesunshine0316/neural-graph-to-seq-mp\nCitation Context: We release our code and models at [URL_CITE] https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DancingSoul/NQ_BERT-DM\nCitation Context: We will release our code and models at [URL_CITE] https: //github.com/DancingSoul/NQ_BERT-DM . Figure 2: System overview.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/SDLM-pytorch\nCitation Context: Experiments on language modeling and the downstream application of headline gener-ation demonstrate the significant effectiveness of SDLM. Source code and data used in the experiments can be accessed at [URL_CITE] https://github.com/thunlp/SDLM-pytorch.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-relations-guidelines-v6.2.pdf\nCitation Context: Our first dataset is the ACE relation extrac-tion task. [URL_CITE] We hypothesize that this task can add knowledge of different domains and of the con-cept of linking two spans in text given a taxonomy of relations. While this is not directly related to events and our farthest task in terms of similarity, the pairwise span classification is the reason we include this.\nFootnote or Reference text (if exists): 5 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-relations-guidelines-v6.2.pdf\nSection Titles (if exists):  ['3 Multi-task Learning', '3.2 Auxiliary Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Human eval-uation shows significant advantages of CLARE in terms of textual similarity, fluency and gram-maticality. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2016/task13/index.php\nCitation Context: Proposed methods are evaluated on the data of SemEval2016 TExEval (Bordea et al., 2016) for submitted systems that created taxonomies for all domains of the task [URL_CITE] , namely the task-winning system TAXI (Panchenko et al., 2016) as well as the systems USAAR (Tan et al., 2016) and JUNLP (Maitra and Das, 2016). TAXI harvests hypernyms with substring inclusion and lexical-syntactic patterns by obtaining domain-specific texts via focused web crawling.\nFootnote or Reference text (if exists): 4 http://alt.qcri.org/semeval2016/task13/index.php\nSection Titles (if exists):  ['4 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/messense/jieba-rs\nCitation Context: All Chinese sentences are segmented by Jieba [URL_CITE] while all English and German sentences are tok-enized by Moses scripts (Koehn et al., 2007). For ZH-EN (EN-DE) translation, we merge the source and target sentences of the parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) opera-tions (Sennrich et al., 2016).\nFootnote or Reference text (if exists): 6 https://github.com/messense/jieba-rs\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/YCOE/YcoeHome.htm\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003) [URL_CITE] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.linguist.is/icelandic_treebank\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011) [URL_CITE] , French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Joel Wallenberg, Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson. 2011. Icelandic Parsed Historical Corpus (IcePaHC) version 0.4. http://www.linguist.is/icelandic_treebank.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.tycho.iel.unicamp.br/~tycho/corpus/en/index.html\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010) [URL_CITE] , totaling 4.5 million words.\nFootnote or Reference text (if exists): Charlotte Galves and Pabol Faria. 2010. Tycho Brahe Parsed Corpus of Historical Portuguese. http://www.tycho.iel.unicamp.br/\u02dctycho/corpus/en/index.html.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org\nCitation Context: The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [URL_CITE] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).\nFootnote or Reference text (if exists): 2 http://jobimtext.org: The PattaMaika compo-nent is based on UIMA RUTA (Kluegl et al., 2016).\nSection Titles (if exists):  ['3 Taxonomy Refinement using Hyperbolic Word Embeddings', '3.1 Domain-specific Poincare\u0301 Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://manikvarma.org/downloads/XC/XMLRepository.html\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [URL_CITE] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 3 http://manikvarma.org/downloads/XC/XMLRepository.html\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nCitation Context: We randomly split each of the two datasets into two portions, where 90% of the ut-terances are used for training the punctuation predic-tion models, and the remaining 10% for evaluating the prediction performance. For all the experiments, we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with the Penn Treebank tokenizer [URL_CITE] . We list the statistics of the two datasets after processing in Table 3.\nFootnote or Reference text (if exists): 2 http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: All datasets used in the experiments are available at [URL_CITE] https://github.com/Anna146/CHARM. We pro-vide IDs and texts of the posts used as training and test data for CHARM.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opus.nlpl.eu/bible-uedin.php\nCitation Context: For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS [URL_CITE] , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary.\nFootnote or Reference text (if exists): 20 http://opus.nlpl.eu/bible-uedin.php\nSection Titles (if exists):  ['B Experimental Details', 'B.1 Data and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt18/index.html\nCitation Context: For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 [URL_CITE] or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary.\nFootnote or Reference text (if exists): 19 http://www.statmt.org/wmt18/index.html\nSection Titles (if exists):  ['B Experimental Details', 'B.1 Data and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.imdb.com/\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database [URL_CITE] and the rest five datasets are downloaded from the extreme classification repository . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 2 https://www.imdb.com/\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gorokoba560/norm-analysis-of-transformer\nCitation Context: The codes of our experiments are pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/gorokoba560/ norm-analysis-of-transformer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nCitation Context: The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nSection Titles (if exists):  ['5 Evaluation', '5.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/AIPHES/ACL20-Reference-Free-MT-Evaluation\nCitation Context: We make our MT evaluation code available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/adhigunasurya/distillation_parser.git\nCitation Context: The code to reproduce our results is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/adhigunasurya/distillation_parser.git\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.google.com\nCitation Context: In this section we will describe the various similarity features we use in our model. We utilize page counts and snippets returned by the Google [URL_CITE] search engine for simple text queries to define various similarity scores.\nFootnote or Reference text (if exists): 3 http://www.google.com\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://translate.google.com/\nCitation Context: To leverage this align-ment flexibility, we apply MASSAlign to English articles and Spanish articles machine translated into English by Google translate. [URL_CITE] An important property of Google translated articles is that they are aligned 1-1 at the sentence level. This lets us deterministically find the Spanish replacement for the aligned Google translated English version re-turned by MASSAlign.\nFootnote or Reference text (if exists): 2 https://translate.google.com/\nSection Titles (if exists):  ['4 The Newsela Cross-Lingual Simplification Dataset', '4.1 Cross-Lingual Segment Alignment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jwebpro.sourceforge.net/data-web-snippets.tar.gz\nCitation Context: \u2022 Web Snippet: includes 12340 Web search snippets with 8 categories, we remove the words with fewer than 3 characters and with document frequency less than 3 in the dataset [URL_CITE] .\nFootnote or Reference text (if exists): 3 http://jwebpro.sourceforge.net/data-web-snippets.tar.gz\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014\nCitation Context: To experiment with a diverse set of queries and web pages, we created a new dataset, O PEN W EB , using web pages from Google search results. [URL_CITE] We use the method from Berant et al. (2013) to gen-erate search queries by performing a breadth-first search over the query space.\nFootnote or Reference text (if exists): 1 The O PEN W EB dataset and our code base are available for download at http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014.\nSection Titles (if exists):  ['2 Problem statement', '2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com/\nCitation Context: Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT sys-tem. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system [URL_CITE] are also listed in this table.\nFootnote or Reference text (if exists): 4 http://translate.google.com/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.google.com/apis\nCitation Context: Here, we give an example illustrating the prin-ciple of clustering web search results in the FGS. In submitting TREC 2004 test question 1.1 \u201dwhen was the first Crip gang started?\u201d to Google ( [URL_CITE] http://www.google.com/apis), we extract n(= 8) different candidates from the top m(= 30) Google snippets. The Google snippets containing the same candidates are aligned snippets, and thus the 12 re-tained snippets are grouped into 8 clusters, as listed in Table 2.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 The U-SVM', '3.1 Clustering Web Search Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mcs.anl.gov/research/projects/tao/\nCitation Context: The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [URL_CITE] as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~mconstan/research/software/\nCitation Context: The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [URL_CITE] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 7 Available at http://igm.univ-mlv.fr/\u02dcmconstan/research/software/.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) [URL_CITE] to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/\u02dcunitex.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr/\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [URL_CITE] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): 6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mcs.anl.gov/petsc/\nCitation Context: The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [URL_CITE] as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo\nCitation Context: The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [URL_CITE] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures.\nFootnote or Reference text (if exists): 5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Evaluation setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aka.ms/WikiQA\nCitation Context: In SIN-CONV, we first use a convolution layer to obtain phrase representations for the two sen-tences s 1 and s 2 , and the SIN interaction proce-dure is then applied to these phrase representations as before to model phrase interactions. The aver-age of all hidden states are treated as sentence vec-tors v scnn 1 and v scnn [URL_CITE] . Thus, SIN-CONV is SIN with word vectors substituted by phrase vectors.\nFootnote or Reference text (if exists): 2 http://aka.ms/WikiQA\nSection Titles (if exists):  ['3 Method', '3.3 SIN with Convolution (SIN-CONV)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: A i and B j denote the word vector of the i th word in A and j th word in B respec-tively. The embeddings used in ACS computations are pre-trained GloVe word vectors [URL_CITE] and thus the computation is fair among different methods.\nFootnote or Reference text (if exists): 13 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.3 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: We implemented the CPAE (Section 2.1) as a baseline and compared it to CPAE with the word-pair embeddings (Section 3.1), which is our pro-posed method. The word embeddings in the def-inition encoder were initialized by the Google Word2Vec vectors [URL_CITE] . Google Word2Vec vectors and GloVe were used as the other baselines.\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Definition Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk\nCitation Context: The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [URL_CITE] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures.\nFootnote or Reference text (if exists): 5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Evaluation setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09\nCitation Context: However, we identify several limitations in the schemas produced by their system. [URL_CITE] Their schemas often lack coherence: mixing unrelated events and having actors whose entities do not play the same role in the schema. Table 2 shows an event schema from Chambers that mixes the events of fire spread-ing and disease spreading.\nFootnote or Reference text (if exists): 2 Available at http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We have created a Rel-grams database with 1.1M entries and a set of over 2K event schemas from a corpus of 1.8M New York Times articles. Both are freely available to the research community [URL_CITE] and may prove useful for a wide range of NLP applications.\nFootnote or Reference text (if exists): 9 available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['7 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We release our open domain event schemas and the Rel-grams database [URL_CITE] for further use by the NLP community.\nFootnote or Reference text (if exists): 3 Available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['1 Introduction', '1.1 Contributions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nCitation Context: The Text Analysis Conference (TAC) Knowledge Base Population (KBP) Cold Start task [URL_CITE] requires systems to take set of documents and produce a comprehensive set of triples that encode relationships between and at-tributes of the named-entities that are mentioned in the corpus. Systems are evaluated based on the fi-delity of the constructed knowledge base. For the 2012 evaluation, a fixed schema of 42 relations (or slots), and their logical inverses was provided, for example:\nFootnote or Reference text (if exists): 1 See details at http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0\nCitation Context: Named entity counts and concreteness ratio. For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [URL_CITE] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn.\nFootnote or Reference text (if exists): 6 Version 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0. pdf Section 2.6.\nSection Titles (if exists):  ['4 Pragmatic correlations with analysts\u2019 pre-call judgments', '4.1 Pragmatic lexical features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.usna.edu/Users/cs/\nCitation Context: We compare Rel-grams schemas against the state-of-the-art narrative schemas released by Cham-bers (Chambers and Jurafsky, 2009). [URL_CITE] Chambers\u2019 schemas are less expressive than ours \u2013 they do not associate types with actors and each schema has a constant pre-specified number of relations. For a fair comparison we use a similarly expressive ver-sion of our schemas that strips off argument types and has the same number of relations per schema (six) as their highest quality output set.\nFootnote or Reference text (if exists): 8 Available at http://www.usna.edu/Users/cs/\nSection Titles (if exists):  ['5 Evaluation', '5.2 Schemas Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [URL_CITE] , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SAG-KeLP\nCitation Context: The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [URL_CITE] . Furthermore it can be imported via Maven.\nFootnote or Reference text (if exists): 4 https://github.com/SAG-KeLP\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Sentiment Analysis in Twitter, Text Categorization and Question Clas-sification, providing examples of kernel-based and linear learning algorithms. Further examples are available on the K E LP website [URL_CITE] where it is shown how to instantiate each algorithm or kernel via JSON and how to add new algorithms, represen-tations and kernels.\nFootnote or Reference text (if exists): 7 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['3 Case Studies in NLP']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/\nCitation Context: Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub . Detailed documentation and tutorials are available on Hugging Face\u2019s website [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://huggingface.co/transformers/ in small details, but need to be in sync with pretraining. (Left) Transformer architectures specialized for different tasks, e.g. understanding versus generation, or for specific use-cases, e.g. speed, image+text. (Top) heads allow a Transformer to be used for different tasks. Here we assume the input token sequence is x 1:N from a vocabulary V, and y represents different possible outputs, possibly from a class set C. Example datasets represent a small subset of example code distributed with the library.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene [URL_CITE] . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://goo.gl/yHWeJp\nCitation Context: In practice, exact values of percent-upvoted are not directly available; the site adds \u201cvote fuzzing\u201d to fight vote manipulation. [URL_CITE] To begin with, we first dis-card posts with fewer than 30 comments. Then, we query for the noisy percent-upvoted from each post ten times using the Reddit API, and take a mean to produce a final estimate.\nFootnote or Reference text (if exists): 6 Prior to Dec. 2016, vote information was fuzzed accord-ing to a different algorithm; however, vote statistics for all posts were recomputed according to a new algorithm that, according to a reddit moderator, can \u201cactually be trusted;\u201d https://goo.gl/yHWeJp\nSection Titles (if exists):  ['2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Our first group of experiments is to investigate whether our proposed label propagation model with both bilingual and sentimental information can improve emotion detection in code-switching texts. Figure 3 shows the experimental results of different models, where ME is the basic Maximum Entropy (ME) classification model [URL_CITE] in which all Chinese and English words of each post function as a feature, ME-CN and ME-EN in which only the Chinese or English text of each post function as features, and BLP-BS, our proposed LP-based ap-proach which incorporates both bilingual and sen-timental information. We adopt F1-Measure (F1.)\nFootnote or Reference text (if exists): 8 ME algorithm is implemented with the MALLET Toolkit, http://mallet.cs.umass.edu\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cadillacforums.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles [URL_CITE] to gadgets such as those of Mac or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 3 http://www.cadillacforums.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://discussions.apple.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac [URL_CITE] or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 4 https://discussions.apple.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://discussions.apple.com\nCitation Context: We use a crawl of 140k threads from Apple Dis-cussion forums [URL_CITE] . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement of 0.71.\nFootnote or Reference text (if exists): 10 http://discussions.apple.com\nSection Titles (if exists):  ['5 Experimental Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.reddit.com\nCitation Context: Our experiments are based on Reddit [URL_CITE] , one of the world\u2019s largest public discussion forums. On Red-dit, registered users initiate a post and people re-spond with comments, either to the original post or one of its associated comments.\nFootnote or Reference text (if exists): 2 http://www.reddit.com\nSection Titles (if exists):  ['2 Popularity Prediction and Tracking']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: bert-base-uncased). [URL_CITE] We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4.\nFootnote or Reference text (if exists): 2 We use the BERT implementation from https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/martin-arvidsson/InterpretableWordEmbeddings\nCitation Context: We follow Rudolph et al. (2016), obtaining maximum a posteriori estimates of the parameters using TensorFlow (Abadi et al., 2015) with the Adam optimizer (Kingma and Ba, 2015) and negative sampling [URL_CITE] . We set the size of the embeddings K = 100, use a context window size of 8 and \u03c3 = 1 throughout all experiments.\nFootnote or Reference text (if exists): 1 Code is available at: https://github.com/martin-arvidsson/InterpretableWordEmbeddings\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/jieba\nCitation Context: We apply the Adam algorithm (Kingma and Ba, 2015) for optimization, where the parameters of Adam are set as in (Kingma and Ba, 2015). The variance \u03c3 [URL_CITE] of the Gaussian Kernel layer is set as 1, and all other trainable parameters are randomly initialized by uniform distribution within [-0.08,0.08]. The mini-batch size for the update is set as 128.\nFootnote or Reference text (if exists): 2 https://pypi.python.org/pypi/jieba\nSection Titles (if exists):  ['4 Experiment', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [URL_CITE] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs.\nFootnote or Reference text (if exists): 8 Our implementation was based on https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 The Contextual Classifier Using BERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dblp.uni-trier.de/db/journals/corr/corr1412.html\nCitation Context: Network parameters for CNN, LSTM & GRU: In the fully connected layers we use 50 and 10 neurons , respectively for the two hidden layers. We use Relu activations (Glorot et al., 2011) for intermediate layers and tanh activation in the final layer. We employ 20% Dropout (Srivastava et al., 2014) in the fully connected layers as a measure of regularization and Adam optimizer (Kingma and Ba, 2014) [URL_CITE] for optimization.\nFootnote or Reference text (if exists): Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimiza-tion. CoRR abs/1412.6980. http://dblp.uni-trier.de/db/journals/corr/corr1412.html.\nSection Titles (if exists):  ['2 Proposed Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: \u2022 Genericembeddingswordusedembeddingsare GloVe: Generic from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings [URL_CITE] . These generic embeddings will be denoted as Glv, GlvCC and w2v.\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: \u2022 Genericembeddingswordusedembeddingsare GloVe: Generic [URL_CITE] from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings . These generic embeddings will be denoted as Glv, GlvCC and w2v.\nFootnote or Reference text (if exists): 1 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T13\nCitation Context: We take the same character un-igram and bigram embeddings as Zhang et al. (2016), who pretrain embeddings using word2vec (Mikolov et al., 2013) on Chinese Gigaword [URL_CITE] . The vocabulary of subword is constructed with 200000 merge operations and the subword embeddings are also trained using word2vec (Heinzerling and Strube, 2018).\nFootnote or Reference text (if exists): 3 https://catalog.ldc.upenn.edu/LDC2011T13.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://reddit.com\nCitation Context: \u2022 Reddit - Reddit [URL_CITE] is a highly popular social forum and community. Similar to Tweets, sarcastic posts are obtained via the tag \u2018/s\u2019 which are marked by the authors themselves.\nFootnote or Reference text (if exists): 3 https://reddit.com\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: Previous work found that the performance can be improved by pre-training the word embeddings on large unla-beled data and using the obtained embeddings to make initialization. With the result in Table 2, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec [URL_CITE] on Chinese Gigaword Corpus for word pre-training). However, as shown in Table 2, com-pared to standard pre-training, the influence of het-erogenous data is more evident.\nFootnote or Reference text (if exists): 3 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Experiments', '3.2 Chinese SRL Performance']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: \u2022 DSobtainedwordviaembeddingsLatent Semantic: DS embeddingsAnalysis (LSAare) and via retraining word2vec on the test data sets using the implementation in gensim [URL_CITE] . DS embeddings via LSA are denoted by LSA and DS embeddings via word2vec are de-noted by DSw2v.\nFootnote or Reference text (if exists): 3 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html\nCitation Context: We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010) [URL_CITE] , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct com-parison problematic. Some first steps at analysis of the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.\nFootnote or Reference text (if exists): Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: We used the English and Chinese parsers in Petrov and Klein (2007) [URL_CITE] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).\nFootnote or Reference text (if exists): 5 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cogsci.princeton.edu/~wn\nCitation Context: The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001), the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998) [URL_CITE] , and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4\nFootnote or Reference text (if exists): Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/\u02dcwn [2000, Septem-ber 7].\nSection Titles (if exists):  ['3 Building the CatVar']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2013T19\nCitation Context: It is worth noting that to attain our best perfor-mance, we require the use of both an NER system, indirectly using labelled data from OntoNotes 5, and a constituency parser for extracting sub-clauses, trained on the Penn Treebank (Marcus et al., 1994). [URL_CITE] Moreover, a language-specific wh* heuristic was used for training the best perform-ing NMT models. This limits the applicability and flexibility of our best-performing approach to do-mains and languages that already enjoy extensive linguistic resources (named entity recognition and treebank datasets), as well as requiring some hu-man engineering to define new heuristics.\nFootnote or Reference text (if exists): 7 Ontonotes 5: https://catalog.ldc.upenn.edu/LDC2013T19\nSection Titles (if exists):  ['5 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://parl.ai/docs/zoo.html\nCitation Context: Poly-encoders extend Bi-encoders compute S(x a , x b ) = P Ti=1 \u03b1 i \u00b7 f(x a ) i> f pool (x b ) where \u03b1 i is the weight from attention and f(x a ) i is the ith token\u2019s contextualized representation. We use the published models [URL_CITE] for evaluation.\nFootnote or Reference text (if exists): 4 https://parl.ai/docs/zoo.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Tasks and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/ahonkela/dippa/dippa.html\nCitation Context: The expectation and variance of Dirichlet(\u03b8|v) are given as follows. [URL_CITE] where v 0 = i v i . Therefore the variance of a scaled Dirichlet is:\nFootnote or Reference text (if exists): 3 http://www.cis.hut.fi/ahonkela/dippa/dippa.html\nSection Titles (if exists):  ['2 Bayesian Model of Summaries']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org/\nCitation Context: The sense of word based similarity is infused to SCQA by using BM25 ranking algorithm. Lucene [URL_CITE] is used to cal-culate the BM25 scores for question pairs. The score from similarity metric of SCQA is com-bined with the BM25 score.\nFootnote or Reference text (if exists): 10 https://lucene.apache.org/\nSection Titles (if exists):  ['6 Siamese Neural Network with Textual Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nCitation Context: An input pair (A i ,A s ) is then associated with a grade g(A i , A s ) = u T \u03c8(A i , A s ) computed as a lin-ear combination of features. The weight vector u is trained to optimize performance in two scenarios: Regression: An SVM model for regression (SVR) is trained using as target function the grades as-signed by the instructors. We use the libSVM [URL_CITE] im-plementation of SVR, with tuned parameters.\nFootnote or Reference text (if exists): 7 http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/\nSection Titles (if exists):  ['6 http://Infomap-nlp.sourceforge.net/']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T08\nCitation Context: A subsequence span i is defined by start i , end i \u2208 [0,n). Subsequences span 1 and span [URL_CITE] represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2.\nFootnote or Reference text (if exists): 2 https://catalog.ldc.upenn.edu/ LDC2006T08\nSection Titles (if exists):  ['2 Our Baseline Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/birch\nCitation Context: The final score used by Birch for re-reranking is: \u03b3\u00b7S BM25 +(1\u2212\u03b3)\u00b7P i \u03ba i \u00b7S(x q , x i ) where S BM25 is the baseline BM25 score and \u03b3, \u03ba i are weight coefficients. We use the published mod-els [URL_CITE] and coefficient values for evaluation.\nFootnote or Reference text (if exists): 3 https://github.com/castorini/birch\nSection Titles (if exists):  ['5 Experiments', '5.1 Tasks and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: For parsing, since the derivation tree in a lexicalized TAG is a type of dependency tree (Rambow and Joshi, 1994), we can directly apply dependency parsing models. In particular, we use the biaffine graph-based parser proposed by Dozat and Manning (2017) [URL_CITE] together with our novel techniques for supertagging.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1231\nCitation Context: Andor et al. (2016) [URL_CITE] developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search. The globally normalized objective ad-dresses the label bias problem and makes global training effective in the transition-based parsing setting.\nFootnote or Reference text (if exists): Daniel Andor, Chris Alberti, David Weiss, Aliak-sei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Glob-ally normalized transition-based neural networks. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 2442\u20132452. http://www.aclweb.org/anthology/P16-1231.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N/N16/N16-1148.pdf\nCitation Context: Pick-Revise is an interaction cycle for MT Post-Editing proposed by Cheng et al. (2016) [URL_CITE] . Starting with the original translation hypothesis, a (sim-ulated) user first picks a part of the hypothesis which is incorrect, and then provides the correct translation for that portion of the output.\nFootnote or Reference text (if exists): Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240\u20131249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.\nSection Titles (if exists):  ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nCitation Context: Replication Our code (part of the TextGrounder system), our processed version of Wikipedia, and in-structions for replicating our experiments are avail-able on the TextGrounder website. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://goo.gl/mKqvro\nCitation Context: We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy. Code and data are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://goo.gl/mKqvro\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.shigeku.com/\nCitation Context: One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website [URL_CITE] and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training.\nFootnote or Reference text (if exists): 2 http://www.shigeku.com/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Setups']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/UnsupervisedQA\nCitation Context: To summarize, this paper makes the follow-ing contributions: i) The first approach for unsu-pervised QA, reducing the problem to unsuper-vised cloze translation, using methods from unsu-pervised machine translation ii) Extensive experi-ments testing the impact of various cloze question translation algorithms and assumptions iii) Ex-periments demonstrating the application of our method for few-shot learning in EQA. [URL_CITE]\nFootnote or Reference text (if exists): 1 Synthetic EQA training data and models that generate it will be made publicly available at https://github.com/facebookresearch/UnsupervisedQA\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/e-bug/nmt-difficulty\nCitation Context: We then present the first systematic and con-trolled study of cross-lingual translation dif-ficulties using modern neural translation sys-tems. Code for replicating our experiments is available online at [URL_CITE] https://github.com/e-bug/nmt-difficulty.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://discussions.apple.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac [URL_CITE] or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 4 https://discussions.apple.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cadillacforums.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles [URL_CITE] to gadgets such as those of Mac or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 3 http://www.cadillacforums.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow [URL_CITE] , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 1 http://www.stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.alexa.com/siteinfo/stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [URL_CITE] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 2 http://www.alexa.com/siteinfo/stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.com\nCitation Context: The third dataset (wikipedia, henceforth) comes from the Wikipedia [URL_CITE] discussion section. When a topic on Wikipedia is disputed, the editors of that topic start a discussion about it.\nFootnote or Reference text (if exists): 4 http://www.wikipedia.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.galaxyforums.net/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac or Samsung [URL_CITE] . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 5 http://www.galaxyforums.net/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right y-axis).\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['3 Defining Stability']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 1: Stability of word2vec as a property of fre-quency in the PTB. Stability is measured across ten randomized embedding spaces trained on the training portion of the PTB (determined using language model-ing splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (x-axis), and each column (fre-quency bucket) is normalized.\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 3: Stability of both word2vec and GloVe as properties of the starting word position in the training data of the PTB. Stability is measured across ten ran-domized embedding spaces trained on the training data of the PTB (determined using language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Boxes with more than 0.02% of the total vocabulary mass are outlined.\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['5 Lessons Learned: What Contributes to the Stability of an Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/cc-100/\nCitation Context: We set the maximum se-quence length to be 512, using a batch size of 1024 for 100K iterations with 32 NVIDIA V100 GPUs for one day. When we continue training us-ing pseudo-monolingual data, we use a learning rate of 7 \u00d7 [URL_CITE] \u22125 and warm up over 5K iterations and train for 35K iterations.\nFootnote or Reference text (if exists): 10 http://data.statmt.org/cc-100/\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface\nCitation Context: We build SentiBERT on the HuggingFace li-brary [URL_CITE] and initialize the model parameters using pre-trained BERT-base and RoBERTa-base models whose maximum length is 128, layer number is 12, and embedding dimension is 768. For the train-ing on SST-phrase, the learning rate is 2 \u00d7 10 \u22125 , batch size is 32 and the number of training epochs is 3.\nFootnote or Reference text (if exists): 1 https://github.com/huggingface\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: approx 32 0.6 ( Neighbors ( 1024 9998 Corpus 100 2 Particular 32 9998 Nearest 1024 0.4 Training 200 in 2 Words 32 of in 1024 # 0.2 % Frequency 800 99982 32 1024 max 9998 0 20 40 60 80 100 % Stability Figure 6: Stability of word2vec on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['5 Lessons Learned: What Contributes to the Stability of an Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) \u2208 T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [URL_CITE] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 \u00b7 10 \u22127 , following Joshi et al. (2019).\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/data/glove.6B.zip\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Obtaining Word-Pair Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/7KG2U\nCitation Context: For German, there are several resources with derivational information. We use version 1.3 of DE RIV B ASE (Zeller et al., 2013), [URL_CITE] a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 non-singleton derivational families. It has a precision of 84% and a recall of 71%.\nFootnote or Reference text (if exists): 1 Downloadable from: http://goo.gl/7KG2U\nSection Titles (if exists):  ['3 A Resource for German Derivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/ttc-project\nCitation Context: The French and English documents were then normalised through the following linguistic pre-processing steps: tokenisation, part-of-speech tag-ging, and lemmatisation. These steps were car-ried out using the TTC TermSuite [URL_CITE] that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded.\nFootnote or Reference text (if exists): 4 http://code.google.com/p/ttc-project\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.collocations.de/temp/kappa_example.zip\nCitation Context: (1969)), observed agreement 0.96. [URL_CITE] There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. The dataset as used here consists of 239 intersective and 130 subsective ANs.\nFootnote or Reference text (if exists): 7 Code for the computation of inter-annotator agreement by Stefan Evert, available at http://www.collocations.de/temp/kappa_example.zip.\nSection Titles (if exists):  ['p = Bv (5)', '3.3 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nCitation Context: Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [URL_CITE] . This is similar to the removal of stop words widely used in information retrieval.\nFootnote or Reference text (if exists): 2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nSection Titles (if exists):  ['2 Related Work', '4.3 Clustering vs. LDA Based LM Adaptation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/CBO9780511809071.007\nCitation Context: Only a small percentage of the words appearing in news can be used for meaningful information extraction and analysis (Manning et al., 1999; Ho-vold, 2005). Specifically, we eliminated too fre-quent (at least once in more than 50% of the days) or too rare (appearing in less than 100 articles) (Manning et al., 2008) [URL_CITE] . Many common English nouns, adjectives and verbs, whose contribution to semantics is minimal (Forman, 2003) were also removed from the vocabulary.\nFootnote or Reference text (if exists): Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Scoring, term weighting, and the vector space model. Cambridge University Press, 100123. https://doi.org/10.1017/CBO9780511809071.007\nSection Titles (if exists):  ['3 Predictive Causal Graph', '3.1 Selecting Informative Words:']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Hence, we train and evaluate two SCRIPT mod-els \u201csmall\u201d and \u201cbase\u201d with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mmautner/readability\nCitation Context: (2008) use it to evaluate the performance of read-ing difficulty prediction. Here we estimate the reading grade level complexity of MT outputs and reference translations using the Automatic Read-ability Index (ARI) [URL_CITE] score, which combines evi-dence from the number of characters per word and number of words per sentence using hand-tuned weights (Senter and Smith, 1967):\nFootnote or Reference text (if exists): 4 https://github.com/mmautner/ readability\nSection Titles (if exists):  ['5 Experiment Settings', '5.1 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://saifmohammad.com/WebPages/BestWorst.html\nCitation Context: For creation of the best\u2013worst scaling tuples, and for aggregation of the annotations, we use the scripts provided by Kiritchenko and Mohammad (2016). [URL_CITE] We use a best-worst scaling factor of 1.5 and four items per tuple. Thus, each metaphor appears in six differ-ent best-worst scaling comparisons.\nFootnote or Reference text (if exists): 3 http://saifmohammad.com/WebPages/BestWorst.html\nSection Titles (if exists):  ['3 Corpus', '3.2 Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/OpenBookQA\nCitation Context: For MultiRC, we also apply an off-the-shelf coreference resolution model 4 and re-place the mentions when they resolve to pronouns occurring in a different sentence . For Open-BookQA, we use the exact same retrieval as re-leased by the authors of OpenBookQA [URL_CITE] and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query.\nFootnote or Reference text (if exists): 6 https://github.com/allenai/OpenBookQA\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/electra\nCitation Context: We compare the number of train-ing steps per second. For direct comparison, we modify the ELECTRA reference code [URL_CITE] . For TPU v3 with 8 TPU cores, ELECTRA and SCRIPT achieve 31.3 and 22.7 training iterations per sec-ond with a mean MXU utilization of 14.93% and 17.91% for small models, respectively.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ electra\nSection Titles (if exists):  ['3 Experiments', '3.3 Computational Efficiency']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mat-annotation.sourceforge.net/\nCitation Context: Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) [URL_CITE] is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system.\nFootnote or Reference text (if exists): Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mat-annotation.sourceforge.net/\nCitation Context: Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT [URL_CITE] (Bayer, 2015) is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system.\nFootnote or Reference text (if exists): 1 http://mat-annotation.sourceforge.net/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bioai.lab.asu.edu/logicgridpuzzles\nCitation Context: This is the first learning system that can solve logic grid puzzles described in natu-ral language in a fully automated manner. The code and the data will be made pub-licly available at [URL_CITE] http://bioai.lab.asu.edu/logicgridpuzzles.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.2200/S00429ED1V01Y201207AIM018\nCitation Context: Finally, this work also relates to the problem of active learning, where the goal is to interac-tively choose instances to be labeled, in a way that maximizes accuracy while minimizing the to-tal cost of annotation (Beygelzimer et al., 2009; Baldridge and Osborne, 2004; Rai et al., 2010; Settles, 2012 [URL_CITE] ). This is an interesting area that might be productively combined with the ideas in this paper. In general, however, the use of active learning involves additional logistical complica-tions and does not always work better than ran-dom sampling in practice (Attenberg and Provost, 2011).\nFootnote or Reference text (if exists): Burr Settles. 2012. Active Learning. Morgan & Claypool. https://doi.org/10.2200/S00429ED1V01Y201207AIM018.\nSection Titles (if exists):  ['5 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IsakZhang/Generative-ABSA\nCitation Context: This also validates the strong generality of the pro-posed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design. [URL_CITE]\nFootnote or Reference text (if exists): 1 The data and code can be found at https://github.com/IsakZhang/Generative-ABSA\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mat-annotation.sourceforge.net/\nCitation Context: This process can then be repeated until the ma-chine learning model is performing well enough to be partly or fully automated. This is similar to the \u201ctag-a-little, learn-a-little loop\u201d from MAT (Bayer, 2015) [URL_CITE] . If combined with enabling pre-annotations, it is also very similar to the annotation process of RapTAT (Gobbel et al., 2014).\nFootnote or Reference text (if exists): Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.\nSection Titles (if exists):  ['4 Annotation Generator', '4.2 Active Learning Process']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/\nCitation Context: We further augment this data with 20 participants who took the TOEIC Listening and Reading test within the same four years range, resulting in a total of 53 external proficiency scores. The TOEIC scores were converted to the TOEFL scale by fitting a third degree polynomial on an unofficial score conversion table [URL_CITE] between the tests. The converted scores were then divided by two.\nFootnote or Reference text (if exists): 2 http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/ Although both TOEFL and TOEIC are administered by the same company (ETS), to the best of our knowledge there is no publicly available official conversion table between the two tests.\nSection Titles (if exists):  ['2 Experimental Setup', '2.3 Standardized English Tests']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://davidmlane.com/hyperstat/A62436.html\nCitation Context: Spearman\u2019s correlation coefficient [URL_CITE] is also a measure of correlation between two variables. It is a non-parametric measure and is a special case of the Pearson\u2019s correlation coefficient when the val-ues of data are converted into ranks before comput-ing the coefficient.\nFootnote or Reference text (if exists): 6 For a quick overview of the Spearman\u2019s coefficient, see: http://davidmlane.com/hyperstat/A62436.html.\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://davidmlane.com/hyperstat/A34739.html\nCitation Context: The Pearson\u2019s correlation coefficient [URL_CITE] measures the strength and direction of a linear relationship be-tween any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.\nFootnote or Reference text (if exists): 5 For a quick overview of the Pearson\u2019s coefficient, see: http://davidmlane.com/hyperstat/A34739.html.\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nCitation Context: Relevance (R) We use the Lucene implementa-tion [URL_CITE] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer. In particular, we form a query that concate-nates the question and candidate answer, and use as underlying document collection (necessary to com-pute document statistics such as inverse document frequencies (IDF)) either: sentences in the entire KB (for ARC), or all sentences in the correspond-ing passage in the case of reading comprehension (MultiRC). The arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set.\nFootnote or Reference text (if exists): 4 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nSection Titles (if exists):  ['3 Approach', '3.1 Ranking of Candidate Justification Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://toeflpractice.ets.org/\nCitation Context: The manual annotations were provided by two ex-perts with prior experience in various types of data annotation on both text and speech. First, a pi-lot annotation was conducted to train and cali-brate the annotators based on 48 training samples drawn from the TOEFL R Practice Online (TPO) product [URL_CITE] , which offers practice tests simulating the TOEFL iBT testing experience with authentic test questions. The training samples were selected from a TPO test form with 6 test questions and were balanced according to test question and pro-ficiency score, i.e., 2 responses from each score level for each question.\nFootnote or Reference text (if exists): 3 https://toeflpractice.ets.org/\nSection Titles (if exists):  ['3 Annotation', '3.2 Pilot Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.efset.org/research/~/media/centralefcom/efset/pdf/\nCitation Context: First, as mentioned previously, in our dataset the MET and reported TOEFL scores have a Pearson\u2019s r correlation of 0.74. We further note an external study conducted by the testing com-pany Education First (EF) which measured the correlation of their flagship standardized English proficiency test EFSET-PLUS with TOEFL-iBT (Luecht, 2015) [URL_CITE] . Using 384 participants who took both tests, the study found a Pearson\u2019s r of 0.63 for the reading comprehension and 0.69 for the listen-ing comprehension sections of these tests.\nFootnote or Reference text (if exists): Richard M Luecht. 2015. EFSET PLUS - TOEFL iBT correlation study report. https://www.efset.org/research/\u02dc/media/centralefcom/efset/pdf/ EFSET_TOEFL_correlational_report_ Sep_v1.pdf.\nSection Titles (if exists):  ['4 English Proficiency Scoring Based on Eye Movements in Reading', '4.1 Correlation with MET and TOEFL']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Paper, DataSource)\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hlt.utdallas.edu/\u223cady\nCitation Context: However, the utilization of the ACE corpus for the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using a restricted set of event types such as LIFE , BUSI - NESS , CONFLICT , and JUSTICE . Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus [URL_CITE] to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution. One important step in the cre-ation process of this corpus consists in finding sets of related documents that describe the same semi-nal event such that the annotation of coreferential event mentions across documents is possible.\nFootnote or Reference text (if exists): 6 ECB is available at http://www.hlt.utdallas.edu/\u223cady.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bethard.info/data/fables-100-temporal-dependency.xml\nCitation Context: Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyre-Lapata collection and measured inter-annotator agree-ment by Krippendorff\u2019s Alpha for nominal data (Krip-pendorff, 2004; Hayes and Krippendorff, 2007). For the resulting annotated corpus annotators achieved Alpha of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering rela-tion labels. Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [URL_CITE] could be used to train a temporal dependency parsing model.\nFootnote or Reference text (if exists): 2 Available from http://www.bethard.info/data/fables-100-temporal-dependency.xml\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt\nCitation Context: We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP -record/citation-text pairs and global GE criteria [URL_CITE] of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008).\nFootnote or Reference text (if exists): 3 Expectation criteria used in our experiments are listed at http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatik.uni-trier.de/\u223cley/db/\nCitation Context: We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP [URL_CITE] -record/citation-text pairs and global GE criteria of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008).\nFootnote or Reference text (if exists): 2 http://www.informatik.uni-trier.de/\u223cley/db/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.jhu.edu/rams/\nCitation Context: Dataset. We conduct experiments on the RAMS [URL_CITE] dataset, which is annotated with 139 event types and 65 corresponding argument roles. Each in-stance consists of a 5-sentences context around the typed event trigger, and there are several typed ar-guments to be extracted.\nFootnote or Reference text (if exists): 1 https://nlp.jhu.edu/rams/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/\nCitation Context: Sil et al. (2010) extract sequences of verb constructions from webpages to learn common 8 See [URL_CITE] https://en.wikipedia.org/wiki/ preconditions of actions and events, and Cham- Wikipedia:Notability_(events) be transferable to the cross-document case. on Empirical Methods in Natural Language Process-To the best of our knowledge, we are the first to ing: System Demonstrations, pages 205\u2013215, On-propose a cheap and high-quality data extraction approach specifically for cross-document event ex-traction and coreference which does not depend on pre-existing resources, combining past work on event extraction from raw newswire text and min-ing of hyperlinks.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Discussion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ChenWu98/Point-Then-Operate\nCitation Context: Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nanguoshun/LSR\nCitation Context: Experiments show that our model significantly outperforms the existing approaches on DocRED, a large-scale document-level relation extraction dataset with a large number of entities and re-lations, and also yields new state-of-the-art re-sults on two popular document-level relation ex-traction datasets in the biomedical domain. The code and pretrained model are available at [URL_CITE] https: //github.com/nanguoshun/LSR 1 . els in various settings.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) \u2208 T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [URL_CITE] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 \u00b7 10 \u22127 , following Joshi et al. (2019).\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/data/glove.6B.zip\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Obtaining Word-Pair Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hiaoxui/D2T-Grounding\nCitation Context: Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset\nCitation Context: The proposed model learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized de-pendency tree, which has the benefits of re-ducing the complexity of the whole model. Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art ap-proaches on Chinese literature text [URL_CITE] .\nFootnote or Reference text (if exists): 1 The Chinese literature text corpus for relation classi-fication, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/AD3\nCitation Context: We trained a struc-tured SVM with stochastic subgradient descent; the cost-augmented inference problems are re-laxed and solved with AD 3 , as described in \u00a73.3. [URL_CITE] We followed the procedure described in Berg-Kirkpatrick et al. (2011) to reduce the number of candidate sentences: scores were defined for each sentence (the sum of the scores of the concepts they cover), and the best-scored sentences were greedily selected up to a limit of 1,000 words.\nFootnote or Reference text (if exists): 8 We use the AD 3 implementation in http://www.ark.cs.cmu.edu/AD3, setting the maximum number of iterations to 200 at training time and 1000 at test time. We extended the code to handle the knapsack and budget factors; the modified code will be part of the next release (AD 3 2.1).\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isw3.naist.jp/IS/TechReport/report-list.html#2009\nCitation Context: A coordination is indicated by a specific relation of type \u201cand\u201d in the semantic frame. The scope of conjuncts (where a conjunct may be a word, or a series of words) can be obtained by combining this information with that of the syntactic tree. The detail of this procedure can be found in (Okuma et al., 2009) [URL_CITE] .\nFootnote or Reference text (if exists): H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.\nSection Titles (if exists):  ['4 Experimental setup', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ncbi.nlm.nih.gov/pubmed/\nCitation Context: For each sentence, the annotation indi-cates whether adverse effects of a drug are be-ing described (\u2018Positive\u2019) or not (\u2018Negative\u2019). The original release of the data does not contain the document context, which we reconstructed from PubMed [URL_CITE] . Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129.\nFootnote or Reference text (if exists): 4 https://www.ncbi.nlm.nih.gov/pubmed/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com\nCitation Context: We take advantage of the annotation platform in Figure Eight. [URL_CITE] The annotators are randomly eval-uated through a pre-defined set of test questions, and they have to maintain an accuracy threshold to be able to continue their annotations. If an an-notator\u2019s accuracy drops below a threshold, their previous annotations are labeled as untrusted and will be added to the pool of annotations again.\nFootnote or Reference text (if exists): 5 https://www.figure-eight.com\nSection Titles (if exists):  ['4 Dataset', '4.1 Annotation using Crowd Workers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Category:Clinical_medicine\nCitation Context: The medical literature dataset consists of passages from textbooks, guidelines, and a subset of med-ically relevant Wikipedia articles [URL_CITE] . The number of sentences per source type is shown in Table 1. In total there are four sources in this dataset: Wikipedia and licensed content from DynaMed, Elsevier, and Wiley publishers.\nFootnote or Reference text (if exists): 2 Articles under the \u2018Clinical Medicine\u2019 category (https://en.wikipedia.org/wiki/Category:Clinical_medicine).\nSection Titles (if exists):  ['3 Data', '3.1 Medical Literature (MedLit)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pstadler1990/nmt_paper21_appendix\nCitation Context: We created 10 sentences per phe-nomenon, resulting in a total of 585 sentences, ex-amining 49 phenomena organised in 13 categories. The raw test items, as well as the translations eval-uated can be found in our repository [URL_CITE] . The phe-nomena selected for this experiment are a subset of the ones of German\u2192English MT, as described in Macketanz et al.\nFootnote or Reference text (if exists): 1 https://github.com/pstadler1990/nmt_ paper21_appendix\nSection Titles (if exists):  ['4 Experiment setup', '4.1 Test suite setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/\nCitation Context: The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [URL_CITE] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences.\nFootnote or Reference text (if exists): 3 http://data.statmt.org/news-crawl/\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [URL_CITE] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description.\nFootnote or Reference text (if exists): 5 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://manikvarma.org/downloads/XC/XMLRepository.html\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [URL_CITE] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 3 http://manikvarma.org/downloads/XC/XMLRepository.html\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2008/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://archive.ics.uci.edu/ml/\nCitation Context: Two multiclass document classification datasets were used: Reuters Transcribed and Mini News-groups, distributed at [URL_CITE] http://archive.ics.uci.edu/ml/. The properties of the datasets are summarized in Ta-ble 2.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Experiments', '7.2 Document classification', '7.2.1 Task and dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yixinL7/Direct-Style-Transfer\nCitation Context: The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and data are available at: https://github.com/yixinL7/Direct-Style-Transfer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~dbikel/software.html\nCitation Context: In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples [URL_CITE] . The statistical significance between two MWE identification experiments was established by using the McNemar-s test (Gillick and Cox, 1989). The results of the two experiments are considered statistically significant with the com-puted value p < 0.01.\nFootnote or Reference text (if exists): 12 Dan Bikel\u2019s tool available at http://www.cis.upenn.edu/\u02dcdbikel/software.html.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/XinnuoXu/AugNLG\nCitation Context: We fur-ther confirm improved results on the F EW - S HOT SGD data and provide comprehensive analysis results on key components of our sys-tem. Our code and data are available at [URL_CITE] https: //github.com/XinnuoXu/AugNLG .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcg-www.uia.ac.be/conll2000/chunking\nCitation Context: A standard software program has been provided (which is available from [URL_CITE] http://lcg-www.uia.ac.be/conll2000/chunking) to compute the performance of each algorithm. For each chunk, three figures of merit are computed: precision (the percentage of detected phrases that are correct), recall (the percentage of phrases in the data that are found), and the \u201eL nL% metric the recall.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 CoNLL-2000 chunking task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nCitation Context: (4) By adding word-knowledge features and us-ing learning-based multi-sieve approach, we im-prove the performance of the state-of-the-art system of (Bengtson and Roth, 2008) by 3 MUC, [URL_CITE] B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset. We report our experimen-tal results in Sec.\nFootnote or Reference text (if exists): 2 The extracted attributes and the related re-sources are available for public download at http://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/dbikel/software.html\nCitation Context: The sec-ond largest gain was on PPs, where we improved by 1.7% over 3,775 constituents. We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel\u2019s randomized parsing evaluation com-parator [URL_CITE] and found that both the precision and recall gains were significant at p \u2264 0.01.\nFootnote or Reference text (if exists): 6 Available at http://www.cis.upenn.edu/dbikel/software.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nCitation Context: We conduct experiments on the ACE2005 corpus. [URL_CITE] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al.\nFootnote or Reference text (if exists): 6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nclt.computing.dcu.ie/~jfoster/resources/\nCitation Context: Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [URL_CITE] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.\nFootnote or Reference text (if exists): 10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.npm.gov.tw\nCitation Context: We use the Chinese/English web pages of the National Palace Museum [URL_CITE] as our underlying parallel corpus. It contains about 30,000 sentences in each language.\nFootnote or Reference text (if exists): 2 http://www.npm.gov.tw\nSection Titles (if exists):  ['3 Experimental Results', '3.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dx.doi.org/10.3765/sp.11.10\nCitation Context: While earlier versions only included linguistic moves by players, the latest version of STAC is a multi-modal corpus of multi-party chats between players of an online game (Asher et al., 2016; Hunter et al., 2018 [URL_CITE] ). It includes 2,593 dialogues (each with a weakly connected DAG discourse structure), 12,588 \u201clinguistic\u201d DUs, 31,811 \u201cnon-linguistic\u201d DUs and 31,251 semantic relations.\nFootnote or Reference text (if exists): Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.\nSection Titles (if exists):  ['3 The STAC Annotated Corpus', '3.1 Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nCitation Context: In the third experiment denoted by \u2019\u201cWSJ-to- G\u201d, we used WSJ corpus as the source domain and Genia corpus (G) [URL_CITE] as the target domain. Following Plank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007).\nFootnote or Reference text (if exists): 4 Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.imsdb.com\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). [URL_CITE] We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 4 http://www.imsdb.com\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/The_Big_Bang_Theory\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, [URL_CITE] available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 3 https://en.wikipedia.org/wiki/The_ Big_Bang_Theory\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/\nCitation Context: These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) [URL_CITE] and law, medical OPUS datasets. For this setting, we con-sider two language pairs, namely Czech and Ger-man to English.\nFootnote or Reference text (if exists): Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/.\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dx.doi.org/10.3765/sp.11.10\nCitation Context: These player ut-terances are the \u201clinguistic\u201d turns. In addition the corpus contains information given visually in the game interface but transcribed in the corpus into Server or interface messages, \u201cnon-linguistic\u201d turns (Hunter et al., 2018) [URL_CITE] . All turns are seg-mented into DUs, and these units are then con-nected by semantic relations.\nFootnote or Reference text (if exists): Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.\nSection Titles (if exists):  ['3 The STAC Annotated Corpus', '3.1 Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, [URL_CITE] for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/ (Wikipedia+Gigaword 5, 300d)\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fromhuggingface/transformers\nCitation Context: We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. [URL_CITE] We then normalize by the total expected cosine similarity A\u0302(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer.\nFootnote or Reference text (if exists): 2 All models https://github.com/fromhuggingface/transformers\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jmlr.org/proceedings/papers/v32/santos14.pdf\nCitation Context: The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014 [URL_CITE] ; Chiu and Nichols, 2016; Ma and Hovy, 2016). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word.\nFootnote or Reference text (if exists): Cicero D. Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Tony Jebara and Eric P. Xing, editors, ICML. JMLR Workshop and Conference Proceedings, pages 1818\u2013 1826. http://jmlr.org/proceedings/papers/v32/santos14.pdf.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016 [URL_CITE] ). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word.\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zenodo.org/record/4421380\nCitation Context: All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec [URL_CITE] and GloVe, for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia.\nFootnote or Reference text (if exists): 3 https://zenodo.org/record/4421380\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Dataset\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea/\nCitation Context: To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the [URL_CITE] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data.\nFootnote or Reference text (if exists): 2 http://www.phontron.com/kytea/\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nCitation Context: For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. [URL_CITE] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/\nCitation Context: Under the circumstances that the average length of negation sentences is almost as long as that of speculation ones (29.28 vs 29.77), shorter negation scopes mean that more tokens do not belong to the scopes, indicating more negative instances. The imbalance between positive and negative instances has negative effects on both the baseline and the 5 [URL_CITE] http://mallet.cs.umass.edu/ CNN-based models for negation scope detection.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experimentation', '4.3 Experimental Results on Abstracts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow\nCitation Context: It is also worth noting that, building the VAE language model (including data reading, model construction, and optimization) on Texar uses only 70 lines of code (with the length of each line < 80 chars). As a (rough) reference, a popular public TensorFlow code (Li, 2017) [URL_CITE] of the same model has used around 400 lines of code for the same part (without line length limit).\nFootnote or Reference text (if exists): Zhong-Yi Li. 2017. https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow.\nSection Titles (if exists):  ['4 Case Study: Transformer on Different Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html\nCitation Context: The maximum sequence length is truncated to 512 tokens. We enable mixed preci-sion and pad all sequence lengths to the multiples of 8 for the effective usage of TensorCores [URL_CITE] .\nFootnote or Reference text (if exists): 8 https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.htmlels. [MASK] means the prompts are intialized with the word embedding of same token, and MNLI means the prompt is initialized with the prompts of out best MNLI run.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NLPIR-team/NLPIR\nCitation Context: If use the maximum length of all conversations as the input length for en-coders, padding the inputs will lead to a sparse ma-trix. Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of [URL_CITE] messages and then encode the pro-duced summaries. In contrast, we do not sum-marize Twitter conversations because their length range is much narrower (from 4 to 1,035 words).\nFootnote or Reference text (if exists): 10 https://github.com/NLPIR-team/NLPIR\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: The statistics of these datasets are listed in Table 2. The corpora contain data from the QatarLiving forum [URL_CITE] , and are pub-licly available on the task website. Each dataset consists of questions and a list of answers for each question, and each question consists of a short ti-tle and a more detailed description.\nFootnote or Reference text (if exists): 5 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/\nCitation Context: The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [URL_CITE] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences.\nFootnote or Reference text (if exists): 3 http://data.statmt.org/news-crawl/\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.twitter.com/2012/03/twitter-turns-six.html\nCitation Context: Twitter is perhaps the most popular such service with over 140 million active users as of 2012. [URL_CITE] Twitter enables users to send and read text-based posts of up to 140 characters, known as tweets. Twitter users follow others or are followed.\nFootnote or Reference text (if exists): 1 For details see http://blog.twitter.com/2012/03/twitter-turns-six.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/twitter-customer-care-document-prediction\nCitation Context: Two of these datasets are internal: one from an internal customer support service on Mac devices (Mac-Support) and another from an external client in the telecommunication domain (Telco-Support). We also release a new Twitter dataset, containing conversations between users and CC agents in 25 organizations on the Twitter platform [URL_CITE] . We summarize the statistics of the three datasets in Table 2.\nFootnote or Reference text (if exists): 3 The Twitter dataset is available at: https://github.com/IBM/ twitter-customer-care-document-prediction based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, tion set of Telco-Support dataset for dialog context se-quence input handling.\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr\nCitation Context: One is described in (Hahn et al., 2009) and uses a margin based criterion for probabilities estimation. The other is described in (Lavergne et al., 2010) and has been implemented in the software wapiti [URL_CITE] . The latter solution in partic-ular trains the model using two different regulariza-tion factors at the same time:\nFootnote or Reference text (if exists): 1 available at http://wapiti.limsi.fr\nSection Titles (if exists):  ['3 Reranking Framework', '3.1 Conditional Random Fields']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/\nCitation Context: The supplemental material is available at our web site [URL_CITE] that provides proofs of the theorems, pseudo-codes of the algorithms, and more experiment re-sults with examples.\nFootnote or Reference text (if exists): 9 http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/\nSection Titles (if exists):  ['Supplemental Material']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: CRF++ [URL_CITE] , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 7 http://crfpp.sourceforge.net/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/5.5/\nCitation Context: CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve [URL_CITE] , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 8 http://lpsolve.sourceforge.net/5.5/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/page/softwareview/\nCitation Context: Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) [URL_CITE] of words in a window of size 3. We also use conjunctions of the features.\nFootnote or Reference text (if exists): Url3. 2013. Illinois part-of-speech tagger (http://cogcomp.cs.illinois.edu/page/softwareview/ pos) (accessed july 1, 2013).\nSection Titles (if exists):  ['2 Identifying Medical Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: The CRF model we compare with was trained with the CRF++ tool, available at [URL_CITE] http://crfpp.sourceforge.net/. The model is equiva-lent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic cat-egories in a window of [-2, +2] with respect to the current token, plus bigrams of concept tags (see (Hahn et al., 2008) and the CRF++ web site for more details).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://psy.takelab.fer.hr/datasets/\nCitation Context: There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora [URL_CITE] is another dataset collected from Reddit, where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nFootnote or Reference text (if exists): 6 https://psy.takelab.fer.hr/datasets/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://personalitycafe.com/forum\nCitation Context: We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurkovic\u0301 et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, [URL_CITE] where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts.\nFootnote or Reference text (if exists): 5 http://personalitycafe.com/forum\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/about\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. [URL_CITE] On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 1 http://stackexchange.com/about\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:User_pages\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; [URL_CITE] re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/ Wikipedia:User_pages\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.twitter.com/2012/03/twitter-turns-six.html\nCitation Context: Twitter is perhaps the most popular such service with over 140 million active users as of 2012. [URL_CITE] Twitter enables users to send and read text-based posts of up to 140 characters, known as tweets. Twitter users follow others or are followed.\nFootnote or Reference text (if exists): 1 For details see http://blog.twitter.com/2012/03/twitter-turns-six.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/\nCitation Context: There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora is another dataset collected from Reddit, [URL_CITE] where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nFootnote or Reference text (if exists): 7 https://www.reddit.com/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SAG-KeLP\nCitation Context: The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [URL_CITE] . Furthermore it can be imported via Maven.\nFootnote or Reference text (if exists): 4 https://github.com/SAG-KeLP\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duncanka/lstm-causality-tagger\nCitation Context: The code for DeepCx is available on GitHub. [URL_CITE] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser.\nFootnote or Reference text (if exists): 3 https://github.com/duncanka/lstm-causality-tagger.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cis.temple.edu/~edragut/DebugSL.webm\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [URL_CITE] https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.bbc.com/news/technology-35890188\nCitation Context: For instance, in 2016, Microsoft released an AI chatbot named Tay [URL_CITE] , which is claimed to be able to improve itself through communicating with social media users. Nevertheless, within just 24 hours after Microsoft released the chatbot, it started to generate misogynistic and racist words.\nFootnote or Reference text (if exists): 1 https://www.bbc.com/news/technology-35890188\nSection Titles (if exists):  ['1 Introductions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [URL_CITE] , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/lex4all/lex4all\nCitation Context: We have developed lex4all as a desktop applica-tion for Microsoft Windows, since it relies on the Microsoft Speech Platform (MSP) as explained in Section 4.1. The application and its source code are freely available via GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://github.com/lex4all/lex4all\nSection Titles (if exists):  ['3 System overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] [URL_CITE] , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence 10 .\nFootnote or Reference text (if exists): 9 Evalb tool available at http://nlp.cs.nyu.edu/evalb/. We also used the evaluation by category implemented in the class EvalbByCat in the Stanford Parser.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Adaxry/GCDT\nCitation Context: Experiments on two standard se-quence labeling tasks show that, given only training data and the ubiquitous word embed-dings (Glove), our GCDT achieves 91.96 F 1 on the CoNLL03 NER task and 95.43 F 1 on the CoNLL2000 Chunking task, which outper-forms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F 1 on NER and 97.30 F 1 on Chunking [URL_CITE] .\nFootnote or Reference text (if exists): 1 Code is available at: https://github.com/Adaxry/GCDT.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\nCitation Context: We report performance in F 0.5 -measure, as calculated by the m2scorer\u2014 the official implementation of the scoring metric in the shared task. [URL_CITE] Given system outputs and gold-standard edits, m2scorer computes the F 0.5 measure of a set of system edits against a set of gold-standard edits.\nFootnote or Reference text (if exists): 1 http://www.comp.nus.edu.sg/\u02dcnlp/sw/m2scorer.tar.gz\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/20717\nCitation Context: F 1 scores for intra- and inter-sentence entity pairs are also reported. Evaluation on the test set is done through CodaLab [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://competitions.codalab.org/competitions/20717\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.mpi-inf.mpg.de/~corrogg/\nCitation Context: The score of a cluster is the sum of the indi-vidual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected. Our source code and the results of our evaluation are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 10 http://people.mpi-inf.mpg.de/\u02dccorrogg/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/sempre/\nCitation Context: Because there can be more than one answer to a question, precision, recall and F 1 are computed based on the system output for each individual question. The average F 1 score is reported as the main evaluation metric [URL_CITE] .\nFootnote or Reference text (if exists): 6 We used the official evaluation script from http://www-nlp.stanford.edu/software/sempre/.\nSection Titles (if exists):  ['4 Experiments', '4.1 Data & evaluation metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asmadotgh/neural_chat_web\nCitation Context: For further information and instructions on server configuration, please read the server doc-umentation available at [URL_CITE] https://github.com/asmadotgh/neural_chat_web. We hope that this platform will allow others to host their own bots and evaluate them in an interactive setting.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['F Interactive bot platform details', 'F.1 Website server setup and configuration']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mponza/SalIE\nCitation Context: The source code and the processed datasets are publicly available [URL_CITE] to encourage further develop-ments of the fact salience task.\nFootnote or Reference text (if exists): 1 https://github.com/mponza/SalIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/msgs\nCitation Context: Second, since models learn useful features early, there is hope that fu-ture advances could accelerate by reducing the amount of data needed to learn which features mat-ter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: [URL_CITE] https://github.com/nyu-mll/msgs.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZhangShiyue/ChrEn\nCitation Context: Our best re-sults are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/En-Chr translations, respectively, and we hope that our dataset and systems will encourage fu-ture work by the community for Cherokee lan-guage revitalization. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our data, code, and demo will be publicly available at https://github.com/ZhangShiyue/ChrEn.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dgaddy/silent_speech\nCitation Context: We hope that our public release of data will encourage others to further improve models for this task. [URL_CITE]\nFootnote or Reference text (if exists): 6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asahi417/kex\nCitation Context: In this section, we explain our keyword extrac-tion experimental setting. All our experiments are run on a 16-core Ubuntu computer equipped with 3.8GHz i7 core and 64GiB memory. [URL_CITE]\nFootnote or Reference text (if exists): 5 All the details to reproduce our experiments are available at https://github.com/asahi417/kex\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/isobs\nCitation Context: We evaluate different character recognition mod-els on self-created dataset. The results show that our implementation of prototypical network can achieve stable and competitive results. The datasets and source code can be found from [URL_CITE] https: //github.com/thunlp/isobs .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiment and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/recsysjustification.git\nCitation Context: Our data and code are available online. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/nijianmo/recsysjustification.git\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mponza/SalIE\nCitation Context: The source code and the processed datasets are publicly available [URL_CITE] to encourage further develop-ments of the fact salience task.\nFootnote or Reference text (if exists): 1 https://github.com/mponza/SalIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nju-websoft/GLRE\nCitation Context: We implemented our GLRE with PyTorch 1.5. The source code and datasets are available online. [URL_CITE] In this section, we report our experimental results.\nFootnote or Reference text (if exists): 1 https://github.com/nju-websoft/GLRE\nSection Titles (if exists):  ['4 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://publish.illinois.edu/yirenwang/emnlp16source/\nCitation Context: All our implementations are based on Theano (Theano De-velopment Team, 2016) and run on one K40 GPU. All the source codes and datasets can be down-loaded at [URL_CITE] https://publish.illinois.edu/yirenwang/emnlp16source/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://deeplearning.net/software/theano\nCitation Context: We implement NNJM in Python using the deep learning library Theano [URL_CITE] (Bergstra et al., 2010) in order to use the massively parallel processing power of GPUs for training. We first train an NNJM (NNJM B ASELINE ) with complete training data for 10 epochs.\nFootnote or Reference text (if exists): 2 http://deeplearning.net/software/theano\nSection Titles (if exists):  ['7 Experiments and Results', '7.2 NNJM Adaptation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jungokasai/graph_parser\nCitation Context: We implement all of our models in TensorFlow (Abadi et al., 2016). [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available online for easy replication of our results at https://github.com/jungokasai/graph_parser.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://deeplearning.net/software/theano/\nCitation Context: We implement our models and DeClarE with Theano [URL_CITE] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation.\nFootnote or Reference text (if exists): 3 http://deeplearning.net/software/theano/\nSection Titles (if exists):  ['5 Experiments and Results', '5.2 Experiments on Veracity-based Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We ran each model for 50 epochs\u2014enough for the training loss to converge. For our BERT ex-periments we use a PyTorch implementation [URL_CITE] with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam op-timizer, a batch size of 32, dropout of 0.1, and em-bedding size 768.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Method and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bitbucket.org/TomKenter/siamese-cbow\nCitation Context: We use Theano (Theano Development Team, 2016) to implement our network. [URL_CITE] We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al., 2016).\nFootnote or Reference text (if exists): 4 The code for Siamese CBOW is available under an open-source license at https://bitbucket.org/TomKenter/siamese-cbow.\nSection Titles (if exists):  ['3 Experimental Setup', '3.4 Network']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\nCitation Context: This subset of word pairs was chosen using an adaptation of the Symmetric Delete spelling correction algorithm described by Garbe (2012) [URL_CITE] , which we denote as symDelete(\u00b7,\u00b7,\u00b7). This al-gorithm takes as arguments the target vocabulary, source vocabulary, and a constant k, and identifies all source-target word pairs that are identical af-ter k or fewer deletions from each word; that is, all pairs where each is reachable from the other with no more than k insertions and k deletions. For example, the Italian-English pair moderno-modern will be identified with k = 1, and the pair tollerante-tolerant will be identified with k = 2.\nFootnote or Reference text (if exists): Wolf Garbe. 2012. 1000x faster spelling correc-tion algorithm. http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/. Accessed: 2018-02-12.\nSection Titles (if exists):  ['4 Orthographic Similarity Adjustment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: We implemented our method as an extension to the Alchemy system (Kok et al., 2007) [URL_CITE] . Since our learn-ing uses sampling, all results are the average of five runs using different random seeds.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['6 Experiments', '6.1 System']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nltk.org/book/ch02.html\nCitation Context: For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [URL_CITE] random seeds.\nFootnote or Reference text (if exists): 5 http://qwone.com/\u02dcjason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html\nSection Titles (if exists):  ['4 Computational Complexity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [URL_CITE] random seeds.\nFootnote or Reference text (if exists): 5 http://qwone.com/\u02dcjason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html\nSection Titles (if exists):  ['4 Computational Complexity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sanketvmehta/efficient-meta-lifelong-learning\nCitation Context: For our framework, Meta-MbPA [URL_CITE] , unless stated otherwise, we set the number of neighbors K = 32 and control the memory size through a write rate r M = 1%. We use L = 30 local adaptation steps and perform local adaptation for whole testing set.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/sanketvmehta/efficient-meta-lifelong-learning.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/GloVe\nCitation Context: For all other hyper-parameters, we use the default values of previous implementations. [URL_CITE] For each corpus-algorithm pair, we train k = 32 word embedding models using different random seeds.\nFootnote or Reference text (if exists): 6 For SGNS, we use Gensim 3.8.3 (R\u030cehu\u030ar\u030cek and Sojka, 2010) with a window size of 5, a minimum word count of 5 and 5 iterations. For GloVe, we use the official implemen-tation https://github.com/stanfordnlp/GloVe, with a window size of 15, a minimum word count of 5 and 15 iterations. Because we do not fine-tune hyper-parameters, our results do not necessarily indicate which algorithm itself is of better reliability. To investigate the potential impact of this decision, we also include an explorative study on the influence of hyper-parameters in Appendix E.\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) [URL_CITE] and the 2016 International Workshop on Spoken Language Translation (IWSLT16) . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 4 WMT16: http://www.statmt.org/wmt16/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ucsb.edu/~william/\nCitation Context: He has published more than 50 papers at leading con-ferences and journals including ACL, EMNLP, NAACL, CVPR, COLING, IJCAI, CIKM, ICWSM, SIGDIAL, IJCNLP, INTERSPEECH, ICASSP, ASRU, SLT, Ma-chine Learning, and Computer Speech & Language, and he has received paper awards and honors from CIKM, ASRU, and EMNLP. Website: [URL_CITE] http://www.cs.ucsb.edu/\u02dcwilliam/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Information About the Presenters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://web.stanford.edu/~jiweil/\nCitation Context: He is the recipient of a Facebook Fellowship in 2015. Web-site: [URL_CITE] https://web.stanford.edu/\u02dcjiweil/ Xiaodong He is the Deputy Managing Director of JD AI Research and Head of the Deep learning, NLP and Speech Lab, and a Technical Vice President of JD.com. He is also an Affiliate Professor at the Uni-versity of Washington (Seattle), serves in doctoral su-pervisory committees.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Information About the Presenters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://accent.gmu.edu\nCitation Context: The theoretical and practical value of studying human accented speech is of interest to language teachers, linguists, and computational linguists. It is also part of the research program behind the Speech Accent Archive ( [URL_CITE] http://accent.gmu.edu) housed at George Mason University. The Archive is a growing database of English speech varieties that contains more than 1,100 samples of native and non-native speakers reading from the same English paragraph.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 4 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.clsp.jhu.edu/workshops/18-workshop/\nCitation Context: This work was mostly conducted at the 2018 Fred-erick Jelinek Memorial Summer Workshop on Speech and Language Technologies, [URL_CITE] hosted and sponsored by Johns Hopkins University. Shruti Palaskar received funding from Facebook and Ama-zon grants.\nFootnote or Reference text (if exists): 1 https://www.clsp.jhu.edu/workshops/18-workshop/\nSection Titles (if exists):  ['Acknowledgements']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT mod-els are significantly prone to translate based on gender stereotypes rather than more mean-ingful context. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.bing.com/translator\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [URL_CITE] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 2 https://www.bing.com/translator\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://translate.google.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [URL_CITE] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 1 https://translate.google.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [URL_CITE] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 3 https://aws.amazon.com/translate\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq\nCitation Context: We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://github.com/pytorch/fairseq\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/emoji\nCitation Context: We to-kenize those English Tweets using \u201cTweetTo-kenizer\u201d from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token). [URL_CITE] We also normal-ize the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. We filter out retweeted Tweets and the ones shorter than 10 or longer than 64 word tokens.\nFootnote or Reference text (if exists): 2 https://pypi.org/project/emoji\nSection Titles (if exists):  ['2 BERTweet', 'Pre-training data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: For the ease of reference, we call the corpus so produced \u2018NICOM\u2019 for the rest of the paper. A part of our sys-tem makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006 [URL_CITE] ). Throughout the experiments, we call our approach \u2018Generic Sen-tence Trimmer\u2019 or GST.\nFootnote or Reference text (if exists): Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['5 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nCitation Context: We randomly split each of the two datasets into two portions, where 90% of the ut-terances are used for training the punctuation predic-tion models, and the remaining 10% for evaluating the prediction performance. For all the experiments, we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with the Penn Treebank tokenizer [URL_CITE] . We list the statistics of the two datasets after processing in Table 3.\nFootnote or Reference text (if exists): 2 http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://meka.sourceforge.net\nCitation Context: We extract features with the help of ClearTK (Ogren et al., 2008). For the machine learning part, we use Weka (Hall et al., 2009) with the Meka [URL_CITE] and Mu-lan (Tsoumakas et al., 2010) extensions for multi-label classification. We use DKPro Lab (Eckart de Castilho et al., 2011) to test different parameter com-binations.\nFootnote or Reference text (if exists): 6 http://meka.sourceforge.net\nSection Titles (if exists):  ['3 Experiments', '3.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sarathchandar.in/bridge-corrnet\nCitation Context: Since none of the approaches described in this work rely on syn-tax, we decided to use all the 5000 translations as test data. This multilingual image caption test data (MIC test data) will be made publicly available [URL_CITE] and will hopefully assist further research in this area.\nFootnote or Reference text (if exists): 3 http://sarathchandar.in/bridge-corrnet\nSection Titles (if exists):  ['4 Datasets', '4.2 Multilingual Image Caption dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://spacy.io/\nSection Titles (if exists):  ['3 Experimental Framework', '3.3 Tools and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/glosstag.shtml\nCitation Context: Let L p be the set containing p and all the Wikipedia pages hav-ing an outgoing link to p, and R s be the set con-sisting of s and all other synsets that are in its di-rect neighbourhood. We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [URL_CITE] . Let B be a function mapping each WordNet synset s 0 to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise.\nFootnote or Reference text (if exists): 2 http://wordnet.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['2 Semantic Representation of Concepts', '2.1 Collecting contextual information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nCitation Context: This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article\u2019s text. [URL_CITE] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity.\nFootnote or Reference text (if exists): 2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nSection Titles (if exists):  ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.org/\nCitation Context: 1. Wikipedia [URL_CITE] , a large-scale online encyc-lopedia, its English version includes more than 3,000,000 concepts and new articles are added quickly and up-to-date. Wikipedia contains rich semantic knowledge in the form of hyperlinks between Wikipedia articles, such as Polysemy (disambiguation pages), Synonym (redirect pages) and Associative relation (hyperlinks between Wikipedia articles).\nFootnote or Reference text (if exists): 1 http://www.wikipedia.org/\nSection Titles (if exists):  ['2 The Structural Semantic Relatedness Measure', '2.1 Knowledge Sources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www/eml-research.de/nlp/download/wikirelations.php\nCitation Context: To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [URL_CITE] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links.\nFootnote or Reference text (if exists): 1 http://www/eml-research.de/nlp/download/wikirelations.php\nSection Titles (if exists):  ['4 Representation', '4.2 Selectional preference features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/nasari/\nCitation Context: N ASARI attains state-of-the-art performance on multiple standard benchmarks in word similarity as well as Wikipedia sense clustering. We release the representations obtained for all the Wikipedia pages and WordNet synsets in [URL_CITE] http://lcl.uniroma1.it/nasari/. As future work we plan to integrate N ASARI into BabelNet and apply our representation to a mul-tilingual setting, enabling the comparison of pairs of concepts across languages.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://babelnet.org\nCitation Context: To cope with these issues, we exploit Babel-Net, [URL_CITE] a huge multilingual semantic network ob-tained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Nav-igli and Ponzetto, 2012), and create the Babel-Net subgraph induced by the WordNet vertices. The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to Word-Net (such as hyperlinks in a Wikipedia article con-necting it to other articles). The greatest contribu-tion of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related ar-ticles (e.g., the English Wikipedia Bus article is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more).\nFootnote or Reference text (if exists): 2 http://babelnet.org\nSection Titles (if exists):  ['3 Creating a Denser and Multilingual Semantic Network']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/changzhisun/AntNRE\nCitation Context: Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary rela-tion classification, which makes the proposed ad-jacency matrix more explanatory. To summarize, the main contributions of this work are [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementation is available at https://github.com/changzhisun/AntNRE.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/GuyAllard/markov_clustering\nCitation Context: Further-more, it reduces the number of similarity compu-tations between pairs of articles considerably. We run MCL on this graph and obtain clusters by iden-tifying the connected components in the resulting connectivity matrix [URL_CITE] .\nFootnote or Reference text (if exists): 4 We use the implementation and default parameters from https://github.com/GuyAllard/markov_ clustering\nSection Titles (if exists):  ['3 Strategies for Timeline Summarization', 'Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm\nCitation Context: In our experiments, we used the LIBLINEAR (Fan et al., 2008) package for the linear kernel and the LIBSVM (Chang and Lin, 2001) [URL_CITE] package for the RBF and sigmoid kernels. We discarded any peri-ods and parentheses, but kept apostrophes and hy-phens, and we converted all letters to lower case.\nFootnote or Reference text (if exists): C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a li-brary for support vector machines. Software available at http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm.\nSection Titles (if exists):  ['3 Language identification with SVMs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.codeplex.com/NodeXL\nCitation Context: We can think of these connections as relationships of contrast in meaning and seman-tic orientation, not just between the two words but also between the two categories. To better aid our understanding of the automatically deter-mined category relationships we visualized this network using the Fruchterman-Reingold force-directed graph layout algorithm (Fruchterman and Reingold, 1991) and the NodeXL network analy-sis tool (Smith et al., 2009) [URL_CITE] .\nFootnote or Reference text (if exists): 5 Available from http://www.codeplex.com/NodeXL\nSection Titles (if exists):  ['6 Visualizing the semantic orientation of thesaurus categories']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/\nCitation Context: Following previous work, we measure the style transfer accuracy using a FastText [URL_CITE] (Joulin et al., 2017) style classifier trained on the respective train-ing set of each dataset. To measure content preser-vation, we use SIM and BLEU as metrics where self-SIM and self-BLEU are computed between the source sentences and system outputs, while ref-SIM and ref-BLEU are computed between the system outputs and human references when avail-able.\nFootnote or Reference text (if exists): 5 https://fasttext.cc/\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ofai.at/\nCitation Context: For comparison, we also tested two recently pro-posed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012) [URL_CITE] , and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the co-sine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, we computed both the regularized Laplacian ker-nel (Chebotarev and Shamis, 1997; Smola and Kon-dor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels.\nFootnote or Reference text (if exists): 2 We used the Matlab script downloaded from http://www.ofai.at/ \u223c dominik.schnitzer/mp/.\nSection Titles (if exists):  ['7 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum\nCitation Context: WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. [URL_CITE] We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al.\nFootnote or Reference text (if exists): 3 We take the processed Wikipedia articles from https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum released on April 25th 2018.\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.gdeltproject.org/events/index.html\nCitation Context: We crawled and parsed the GDELT Even-t Database [URL_CITE] containing news articles published in May 2014. We manually annotated one-week da-ta containing 101,654 documents and identified 77 storylines for evaluation.\nFootnote or Reference text (if exists): 1 http://data.gdeltproject.org/events/index.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.di.unipi.it/~gulli/AG\nCitation Context: 1. AG\u2019s news corpus [URL_CITE] ,a news article corpus with categorized articles from more than 2,000 news sources. We use the dataset with 4 largest classes constructed in (Zhang et al., 2015).\nFootnote or Reference text (if exists): 1 http://www.di.unipi.it/~gulli/AG corpus of news articles.html\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://knowledgeworks.cn:20313/cndbpedia/api/entityAVP\nCitation Context: Up till February 2017, it contains 41M explicit semantic relations of 9M entities, excluding entity summaries, syn-onyms, etc. We use the CN-DBpedia API [URL_CITE] to ob-tain relations for each entity and report the cover-age of relation r as:\nFootnote or Reference text (if exists): 10 http://knowledgeworks.cn:20313/cndbpedia/api/entityAVP\nSection Titles (if exists):  ['6 Experiments', '6.3 Non-taxonomic Relation Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/downloads-2016-10\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://open-platform.theguardian.com/\nCitation Context: Input Articles: For each entity from the ground-truth timelines, we search for news articles using TheGuardian API [URL_CITE] . We use this source because it provides access to all published articles starting from 1999.\nFootnote or Reference text (if exists): 7 http://open-platform.theguardian.com/\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). It contains 3,361 articles that have been downloaded from the BBC News website [URL_CITE] . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 3 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: It contains 3,361 documents that have been down-loaded from the BBC News website. [URL_CITE] Each doc-ument comes with an image that depicts some of its content. The images are usually 203 pixels wide and 152 pixels high.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mlg.ucd.ie/datasets/bbc.html\nCitation Context: We use the BBC corpus prepared by Greene and Cunningham (2006). [URL_CITE] Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro-posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages.\nFootnote or Reference text (if exists): 2 http://mlg.ucd.ie/datasets/bbc.html\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nikkei.co.jp\nCitation Context: We created a corpus of sentence summaries based on email news bulletins we had received over five to six months from an on-line news provider called Nikkei Net, which mostly deals with finance and politics. [URL_CITE] Each bulletin consists of six to seven news briefs, each with a few sentences. Since a news brief contains nothing to indicate what its longer version might look like, we manually searched the news site for a full-length article that might reasonably be con-sidered a long version of that brief.\nFootnote or Reference text (if exists): 9 http://www.nikkei.co.jp\nSection Titles (if exists):  ['5 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0677528/data.html\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). [URL_CITE] It contains 3,361 articles that have been downloaded from the BBC News website . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 2 Available from http://homepages.inf.ed.ac.uk/s0677528/data.html.\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/resources/scoring.htm\nCitation Context: There were 210 generated references in the 18 testing sets. The table also shows the popular BLEU (Papineni et al., 2002) and NIST [URL_CITE] MT metrics. We also provide two base-lines - most frequent initial reference to the person in the input (Base1) and a randomly selected initial reference to the person (Base2).\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt/resources/scoring.htm\nSection Titles (if exists):  ['2 References to people', '2.6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.csai.tsinghua.edu.cn/~ly/systems/TsinghuaAligner/TsinghuaAligner.html\nCitation Context: For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. [URL_CITE] The evalu-ation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets.\nFootnote or Reference text (if exists): 1 http://nlp.csai.tsinghua.edu.cn/\u02dcly/systems/TsinghuaAligner/TsinghuaAligner.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [URL_CITE] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/optimization/\nCitation Context: We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://developers.google.com/optimization/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ictclas.org/\nCitation Context: (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) [URL_CITE] with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees.\nFootnote or Reference text (if exists): 1 http://ictclas.org/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/slavpetrov/berkeleyparser\nCitation Context: We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser [URL_CITE] (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.\nFootnote or Reference text (if exists): 6 https://github.com/slavpetrov/ berkeleyparser\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1146/annurev.neuro.26.041002.131047\nCitation Context: We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015) [URL_CITE] , sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rockta\u0308schel et al., 2016), and error cor-rection (Ji et al., 2017). In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.\nFootnote or Reference text (if exists): Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Rep-resentations. https://doi.org/10.1146/annurev.neuro.26.041002.131047.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1.1.56.7752\nCitation Context: The main system takes as input a sentence, sep-arated into tokens, and outputs a binary predic-tion as the label of the sentence. We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) [URL_CITE] architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015), sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rockta\u0308schel et al., 2016), and error cor-rection (Ji et al., 2017).\nFootnote or Reference text (if exists): Sepp Hochreiter and Ju\u0308rgen Schmidhuber. 1997. Long Short-term Memory. Neural Computation 9. https://doi.org/10.1.1.56.7752.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: Models Compared: We experiment with Glove (Pennington et al., 2014) [URL_CITE] and ELMo (Peters et al., 2018) embeddings for Multee and compare with following three types of systems: (A) Baselines using entailment as a black-box We use the pre-trained entailment model as a black-box in two ways: concatenate premises (Concat) and aggregate sentence level decisions with a max operation (Max). Both models were also pre-trained on SNLI and MultiNLI datasets and fine-tuned on the target QA datasets with same pre-processing.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ckip.iis.sinica.edu.tw/DOMCAT/\nCitation Context: In this paper, we propose a web-based bilingual concordancer, DOMCAT [URL_CITE] , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents.\nFootnote or Reference text (if exists): 1 http://ckip.iis.sinica.edu.tw/DOMCAT/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/chrisjbryant/errant\nCitation Context: To identify incorrect spans, we use a binary se-quence tagging model in which tag 0 means the token is in a correct span; while tag 1 means the to-ken is in a grammatically incorrect span that needs to be edited, as shown in Figure 1(a). In order to train the tagging model, we align [URL_CITE] tokens across the source and target sentence in training data. With token alignment, we can identify the text spans that are edited and thus can annotate the edited text spans in the original sentences as erroneous spans.\nFootnote or Reference text (if exists): 1 Alignment can be solved by dynamic programming like Levenshtein distance. We here use ERRANT (https://github.com/chrisjbryant/errant) for alignment.\nSection Titles (if exists):  ['3 Erroneous Span Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/P17-1070\nCitation Context: We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015), sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rockta\u0308schel et al., 2016), and error cor-rection (Ji et al., 2017) [URL_CITE] . In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.\nFootnote or Reference text (if exists): Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen Gong, Steven Truong, and Jianfeng Gao. 2017. A Nested Attention Neural Hybrid Model for Gram-matical Error Correction. In ACL 2017. pages 753\u2013762. https://doi.org/10.18653/v1/P17-1070.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://deepgraphlearning.github.io/project/wikidata5m\nCitation Context: For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [URL_CITE] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning.\nFootnote or Reference text (if exists): 4 https://deepgraphlearning.github.io/project/wikidata5m\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: The Amazon dataset contains 2000 samples for each of the four domains: book, DVD, electron-ics, and kitchen, with binary labels (positive, neg-ative). Following Wu and Huang (2015) [URL_CITE] , we con-duct 5-way cross validation. Three out of the five folds are treated as the training set, one serves as the validation set, while the remaining is the test set.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['4 Experiments', '4.1 Multi-Domain Text Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wndomains.itc.it\nCitation Context: To form a lexicon required for the identification of this feature, we extract from W ORD N ET D OMAINS [URL_CITE] all the synsets labeled with the domain S EXUALITY . The list is further processed by removing all words with high polysemy (\u2265 4).\nFootnote or Reference text (if exists): 3 W ORD N ET D OMAINS assigns each synset in W ORD N ET with one or more \u201cdomain\u201d labels, such as S PORT , M EDICINE , E CONOMY . See http://wndomains.itc.it.\nSection Titles (if exists):  ['3 Automatic Humor Recognition', '3.1 Humor-Specific Stylistic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nCitation Context: We perform 5-shot KG completion task for all the methods. Our implementation for KG embedding baselines is based on OpenKE [URL_CITE] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature. During training, all triples in back-ground KG G 0 and training set, as well as few-shot reference triples of validation and testing set are used to train models.\nFootnote or Reference text (if exists): 2 https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nSection Titles (if exists):  ['5 Experiments', '5.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/wiki/Wikidata:List_of_properties\nCitation Context: For Freebase we select 5, 000 triples as the validation set, while for Wikidata we use the validation set provided in the transductive setting (5, 136 triples). A.4 W IKI D ISTANT Relation Categories Since W IKI D ISTANT contains 454 relations, their labels are used directly from the WikiData proper-ties [URL_CITE] . Here, we add explanations about the top 10 most frequent categories used in Figures 4c, 4d.\nFootnote or Reference text (if exists): 9 https://www.wikidata.org/wiki/Wikidata: List_of_properties\nSection Titles (if exists):  ['A Appendix', 'A.3 Hyper-parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Knowledge\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList. Among the word-level perturbations, Deletion, Repetition, Singular/plural verbs, Word order, and Verb tense were not already used to test the robustness. However, Negation was included in CheckList, and Replacement with Synonyms was used for adversarial attack (Dong et al., 2020; Ren et al., 2019).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['4 Perturbation methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016). In order to generate effective adversarial examples, most attack methods need to have access to the NLP model structure, internal weights, and hyperparameters, which may not be possible in every testing scenario (Zhang et al., 2020) [URL_CITE] . Furthermore, adversarial perturbations should not be perceived by humans (Liang et al., 2018).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.engappai.2020.103641\nCitation Context: Since it has been proven that sentences that contain few typos, misspellings, or minor character-level errors can be still fully understandable to humans (Belinkov and Bisk, 2018; Xu and Du, 2020 [URL_CITE] ), character-level perturbations are not expected to change the text\u2019s meaning in most cases. Therefore, they can be automatically produced and used for testing the robustness of NLP systems.\nFootnote or Reference text (if exists): Jincheng Xu, and Qingfeng Du. 2020. TextTricker: Loss-based and gradient-based adversarial attacks on text classification models. Engineering Applications of Artificial Intelligence, 92: 103641. https://doi.org/10.1016/j.engappai.2020.103641.\nSection Titles (if exists):  ['5 Experimental results', '5.1 Performance on perturbed inputs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [URL_CITE] , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SAG-KeLP\nCitation Context: The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [URL_CITE] . Furthermore it can be imported via Maven.\nFootnote or Reference text (if exists): 4 https://github.com/SAG-KeLP\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Sentiment Analysis in Twitter, Text Categorization and Question Clas-sification, providing examples of kernel-based and linear learning algorithms. Further examples are available on the K E LP website [URL_CITE] where it is shown how to instantiate each algorithm or kernel via JSON and how to add new algorithms, represen-tations and kernels.\nFootnote or Reference text (if exists): 7 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['3 Case Studies in NLP']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/\nCitation Context: Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub . Detailed documentation and tutorials are available on Hugging Face\u2019s website [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://huggingface.co/transformers/ in small details, but need to be in sync with pretraining. (Left) Transformer architectures specialized for different tasks, e.g. understanding versus generation, or for specific use-cases, e.g. speed, image+text. (Top) heads allow a Transformer to be used for different tasks. Here we assume the input token sequence is x 1:N from a vocabulary V, and y represents different possible outputs, possibly from a class set C. Example datasets represent a small subset of example code distributed with the library.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://opennlp.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP [URL_CITE] or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 2 https://opennlp.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N/N16/N16-1148.pdf\nCitation Context: Pick-Revise is an interaction cycle for MT Post-Editing proposed by Cheng et al. (2016) [URL_CITE] . Starting with the original translation hypothesis, a (sim-ulated) user first picks a part of the hypothesis which is incorrect, and then provides the correct translation for that portion of the output.\nFootnote or Reference text (if exists): Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240\u20131249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.\nSection Titles (if exists):  ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com/\nCitation Context: We conducted user study via crowdsourcing using the FigureEight [URL_CITE] platform. To ensure the quality of annotators, we used a dedicated expert-worker pool provided by FigureEight. We present the detailed setup of our user studies as follows.\nFootnote or Reference text (if exists): 6 https://www.figure-eight.com/\nSection Titles (if exists):  ['B Human Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/multimodal-task.html\nCitation Context: To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [URL_CITE] . The best results of NMT model were those of Huang et al.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/multimodal-task.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: 5. CorrNet + MT: Here, we train an En-Image Cor-rNet using Z 1 and an Fr/De-En MT system [URL_CITE] using Z 2 . For the task of retrieving images given a French\nFootnote or Reference text (if exists): 4 http://www.statmt.org/moses/\nSection Titles (if exists):  ['6 Experiment 2: Cross modal access using a pivot language']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses [URL_CITE] to translate the linearized AMR into a sen-tence. To traverse the children of an AMR con-cept, we use the original order in the text file.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/moses/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hub.microsofttranslator.com\nCitation Context: To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [URL_CITE] to train a cus-tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida-tion process, first using crowd-sourced evaluations with Amazon\u2019s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior.\nFootnote or Reference text (if exists): 2 http://hub.microsofttranslator.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatik.uni-trier.de/\u223cley/db/\nCitation Context: We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP [URL_CITE] -record/citation-text pairs and global GE criteria of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008).\nFootnote or Reference text (if exists): 2 http://www.informatik.uni-trier.de/\u223cley/db/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt\nCitation Context: We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP -record/citation-text pairs and global GE criteria [URL_CITE] of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008).\nFootnote or Reference text (if exists): 3 Expectation criteria used in our experiments are listed at http://www.cs.umass.edu/\u223ckedarb/dbieexpts.txt.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com\nCitation Context: Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation. We adopt Google Translate [URL_CITE] for review transla-tion. After review translation, the training re-views and the test reviews are now in the same language, and any regression algorithm (e.g.\nFootnote or Reference text (if exists): 1 http://translate.google.com\nSection Titles (if exists):  ['3 Problem Definition and Baseline Ap-proaches']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Eventually, we would like to expand our data set and relations to cover proper nouns as well. We are hopeful that our current dataset and re-lation definitions, which will be made available via [URL_CITE] http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dcs.shef.ac.uk/nlp/meter\nCitation Context: A prototype browser-based demo of both the GST algorithm and alignment program, allow-ing users to test arbitrary text pairs for simi-larity, is now available [URL_CITE] and will continue to be enhanced. helpful comments on earlier drafts.\nFootnote or Reference text (if exists): 4 See http://www.dcs.shef.ac.uk/nlp/meter.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.pitt.edu/mpqa\nCitation Context: The MPQA corpus contains news articles man-ually annotated for opinions and private states. [URL_CITE] Notably, it also has polarity annotations (posi-tive/negative) at the phrase-level. We conducted an extrinsic evaluation of the manually-generated and automatically-generated lexicons by using them to determine the polarity of phrases in the MPQA version 1.1 collection of positive and neg-ative phrases (1,726 positive and 4,485 negative).\nFootnote or Reference text (if exists): 4 http://www.cs.pitt.edu/mpqa\nSection Titles (if exists):  ['4 Evaluation', '4.2 Extrinsic: Identifying phrase polarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html\nCitation Context: We use hierarchical clustering (Rokach and Maimon, 2005) [URL_CITE] method to group the languages based on language embeddings. We use the el-bow method (Thorndike, 1953) to automatically decide the optimal number of clusters K. Note that we have tried to extract the language embed-dings from multiple model checkpoints randomly chosen in the later training process, and found that the clustering results based on these language em-beddings are stable.\nFootnote or Reference text (if exists): 10 https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/corpora/\nCitation Context: MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015). [URL_CITE] It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agen-cies, and covers a wide range of topics.\nFootnote or Reference text (if exists): 4 Available at http://mpqa.cs.pitt.edu/corpora/\nSection Titles (if exists):  ['3 Task Definition', '3.1 Gold Standard Corpus: MPQA 3.0']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nCitation Context: We conduct experiments on the ACE2005 corpus. [URL_CITE] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al.\nFootnote or Reference text (if exists): 6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.pitt.edu/mpqa/\nCitation Context: For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA) [URL_CITE] corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phrase-level subjectivity and intensity. We use the expression-level boundary markings in MPQA to extract phrases.\nFootnote or Reference text (if exists): 6 http://www.cs.pitt.edu/mpqa/\nSection Titles (if exists):  ['3 Experimental Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.quora.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora [URL_CITE] , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 3 http://www.quora.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackoverflow.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora , StackOverflow [URL_CITE] etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 4 http://stackoverflow.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://answers.yahoo.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers [URL_CITE] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 1 https://answers.yahoo.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora and the StackExchange [URL_CITE] family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 3 http://stackexchange.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://quora.com\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora [URL_CITE] and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 2 http://quora.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://zhidao.baidu.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao [URL_CITE] , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 2 http://zhidao.baidu.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.conexor.com\nCitation Context: We use Conexor functional depend- t k , by estimating p(x k ). We describe three estimat-ency parser (Conexor, 2004) [URL_CITE] to analyze the struc- ing methods of p(x k ) in following sections. ture of sentences.\nFootnote or Reference text (if exists): Conexor. 2004. Conexor Functional Dependency Grammar Parser. http://www.conexor.com\nSection Titles (if exists):  ['2. Information for Term Specificity', '2.2. Contextual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PlusLabNLP/Plot-guided-Coherence-Evaluation\nCitation Context: Figure 2 demon-strates various proposed plot-level manipulations in dotted boxes. [URL_CITE] All proposed manipulations are described in the following sections. We refer this data as ManPlts.\nFootnote or Reference text (if exists): 1 Our proposed data, trained models and code is re-leased at https://github.com/PlusLabNLP/ Plot-guided-Coherence-Evaluation\nSection Titles (if exists):  ['3 Implausible Text Construction', '3.2.1 Plot Manipulations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/diffdp\nCitation Context: This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [URL_CITE] Our main contributions can be summarized as follows:\nFootnote or Reference text (if exists): 1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bilbo.cs.illinois.edu/~ctsai12/xlwikifier-wikidata.zip\nCitation Context: Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at [URL_CITE] http://bilbo.cs.illinois.edu/\u02dcctsai12/xlwikifier-wikidata.zip.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1 Wikipedia Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.unibuc.ro/resources/rosim.pdf\nCitation Context: In this subsection we present and analyze the main results drawn from our research. In Table 3 we list the output of our method for each corpus, with and without diacritics [URL_CITE] . We report the similarity between Romanian and related languages, provid-ing the average value of the three metrics used.\nFootnote or Reference text (if exists): 6 The complete ranking of similarity is available online at http://nlp.unibuc.ro/resources/rosim.pdf.\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Romanian Evolution', '3.1.2 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/tutorials/\nCitation Context: This tutorial consists of the following 3 main sections. [URL_CITE]\nFootnote or Reference text (if exists): 2 The material associated with this tutorial will be avail-able at http://statnlp.org/tutorials/.\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [URL_CITE] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017 [URL_CITE] ). Ganin et al.\nFootnote or Reference text (if exists): Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence genera-tive adversarial nets with policy gradient. In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://universaldependencies.github.io/docs/\nCitation Context: We proposed a typed dependency scheme based on the well-known and widely used Stanford typed dependencies (SD), which originated in English and has since been extended to many languages, but not to Japanese. The Universal dependencies (UD) (McDonald et al., 2013; de Marneffe et al., 2014) has been developed based on SD in order to design the cross-linguistically consistent treebank annotation [URL_CITE] . The UD for Japanese has also been discussed, but no treebanks have been provided yet.\nFootnote or Reference text (if exists): 4 http://universaldependencies.github.io/docs/.\nSection Titles (if exists):  ['2 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html\nCitation Context: Sentiment classification has been a widely explored task which received a lot of attention. The Movie Review dataset, published by Pang and Lee (2005) [URL_CITE] , has become a standard machine learning bench-mark task for binary sentence classification. Socher et al.\nFootnote or Reference text (if exists): 8 http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.2 Sentiment Polarity Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: After Dai and Le (2015) [URL_CITE] proposed the framework of semi-supervised sequence learning, there have been several attempts to extend sequence learn-ing models for different tasks to semi-supervised settings. Cheng et al.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf\nCitation Context: Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) [URL_CITE] but has been widely used for translation tasks ever since. One should note that it slightly differs from Bahdanau et al.\nFootnote or Reference text (if exists): Oriol Vinyals, \u0141 ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-mar as a foreign language. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 2773\u2013 2781. http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf.\nSection Titles (if exists):  ['3 Attention-based Models', '3.1 Soft attention']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bishanyang/EventEntityExtractor\nCitation Context: In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context. [URL_CITE] We first decompose the learn-ing problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the co-occurrence relations between events across the doc-ument, and (3) learning for entity extraction. Then we combine the learned models for these subprob-lems into a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document.\nFootnote or Reference text (if exists): 1 The code for our system is available at https://github.com/bishanyang/EventEntityExtractor.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/LASER\nCitation Context: To detect parallel sentences we use the LASER toolkit, [URL_CITE] which achieves state-of-the-art perfor-mance in mining parallel sentences (Artetxe and Schwenk, 2019). LASER uses multilingual sen-tence embeddings and a distance or margin cri-terion in the embeddings space to detect parallel sentences.\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ LASER\nSection Titles (if exists):  ['2 The MLQA corpus', '2.1 Parallel Sentence Mining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/GeoText/\nCitation Context: Geo-tagged Microblog Corpus As a second eval-uation corpus on a different domain, we use the corpus of geotagged tweets collected and used by Eisenstein et al. (2010). [URL_CITE] It contains 380,000 mes-sages from 9,500 users tweeting within the 48 states of the continental USA.\nFootnote or Reference text (if exists): 4 http://www.ark.cs.cmu.edu/GeoText/\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://goo.gl/yHWeJp\nCitation Context: In practice, exact values of percent-upvoted are not directly available; the site adds \u201cvote fuzzing\u201d to fight vote manipulation. [URL_CITE] To begin with, we first dis-card posts with fewer than 30 comments. Then, we query for the noisy percent-upvoted from each post ten times using the Reddit API, and take a mean to produce a final estimate.\nFootnote or Reference text (if exists): 6 Prior to Dec. 2016, vote information was fuzzed accord-ing to a different algorithm; however, vote statistics for all posts were recomputed according to a new algorithm that, according to a reddit moderator, can \u201cactually be trusted;\u201d https://goo.gl/yHWeJp\nSection Titles (if exists):  ['2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hlt.utdallas.edu/\u223cady\nCitation Context: However, the utilization of the ACE corpus for the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using a restricted set of event types such as LIFE , BUSI - NESS , CONFLICT , and JUSTICE . Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus [URL_CITE] to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution. One important step in the cre-ation process of this corpus consists in finding sets of related documents that describe the same semi-nal event such that the annotation of coreferential event mentions across documents is possible.\nFootnote or Reference text (if exists): 6 ECB is available at http://www.hlt.utdallas.edu/\u223cady.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PlusLabNLP/ee-wiki-bias\nCitation Context: To facilitate the study, we collect a corpus that contains demographic information, personal life de-scription, and career description from Wikipedia. [URL_CITE] We first detect events in the collected corpus using a state-of-the-art event extraction model (Han et al., 2019). Then, we extract gender-distinct events with a higher chance to occur for one group than the other.\nFootnote or Reference text (if exists): 2 https://github.com/PlusLabNLP/ ee-wiki-bias\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm\nCitation Context: Table 2 reports the performance of relation detec-tion by using SVM and LP with different sizes of labled data. For SVM, we use LIBSVM tool with linear kernel function [URL_CITE] . And the same sampled la-beled data used in LP is used to train SVM mod-els.\nFootnote or Reference text (if exists): 3 LIBSV M: a library for support vector machines. Soft-ware available at http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm.\nSection Titles (if exists):  ['3 Experiments and Results', '3.3 Experimental Evaluation 3.3.1 Relation Detection', '3.3.2 SVM vs. LP']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nCitation Context: Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 2002). Hence we apply SVM algorithm with the help of the LIBSVM [URL_CITE] tool. Almost all parameters are set to their default values except the kernel function which is changed from a polynomial kernel function to a linear one because the linear one usually performs better for text classification tasks.\nFootnote or Reference text (if exists): 4 http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bitbucket.org/melsner/browncoherence\nCitation Context: EGrid has been shown to give competitive re-sults on similar coherence tasks in other domains. Using the Brown Coherence Toolkit (Eisner and Charniak, 2011), [URL_CITE] we construct the entity transi-tion probabilities with length = 3 and salience = 2. The transition probabilities are then used as fea-tures that are fed as input to an SVM classifier with an RBF kernel and penalty parameter C = 1.5 to predict a coherence score.\nFootnote or Reference text (if exists): 13 https://bitbucket.org/melsner/browncoherence\nSection Titles (if exists):  ['5 Model Parameters and Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/cjlin/libsvm/\nCitation Context: This paper selects LIBSVM toolkit [URL_CITE] to implement the SVM classifier. The kernel is the radical basis function with the parameter \u03b3 = 0.001 in the exper-iments.\nFootnote or Reference text (if exists): 1 http://www.csie.ntu.edu.tw/cjlin/libsvm/\nSection Titles (if exists):  ['3 The U-SVM', '3.2 Classifying Question']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kelp-ml.org\nCitation Context: Learning and classification allow to apply a variety of learning algorithms on vectorial or structured data. Currently KeLP (Filice et al., 2018) [URL_CITE] is integrated in the pipeline. KeLP allows to apply a growing number of kernel-based algo-rithms and kernel functions to perform unsuper-vised, online and batch supervised kernel meth-ods.\nFootnote or Reference text (if exists): 2 http://www.kelp-ml.org\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www2.crl.go.jp/jt/a132/members/mutiyama/software.html\nCitation Context: When we use SVM, we have to choose a good linear kernel ( = 6 kernel for better performance &? [URL_CITE] C ? .)\nFootnote or Reference text (if exists): 6 http://www2.crl.go.jp/jt/a132/members/mutiyama/software. html\nSection Titles (if exists):  ['3 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (The URL citation is used to provide additional information but not explicitly introducing or using a specific tool or method)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014\nCitation Context: To experiment with a diverse set of queries and web pages, we created a new dataset, O PEN W EB , using web pages from Google search results. [URL_CITE] We use the method from Berant et al. (2013) to gen-erate search queries by performing a breadth-first search over the query space.\nFootnote or Reference text (if exists): 1 The O PEN W EB dataset and our code base are available for download at http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014.\nSection Titles (if exists):  ['2 Problem statement', '2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bookblog.net/gender/genie.php\nCitation Context: As noted before, the standard reference algorithm is Boulis and Osten-dorf (2005), and all cited relative error reductions are based on this established standard, as imple-mented in this paper. Also, as a second reference, performance is also cited for the popular \u201cGender Genie\u201d, an online gender-detector [URL_CITE] , based on the manually weighted word-level sociolinguistic fea-tures discussed in Argamon et al. (2003).\nFootnote or Reference text (if exists): 7 http://bookblog.net/gender/genie.php\nSection Titles (if exists):  ['7 Gender Classification Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flask.pocoo.org\nCitation Context: This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask [URL_CITE] micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters.\nFootnote or Reference text (if exists): 4 http://flask.pocoo.org\nSection Titles (if exists):  ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingc9/LinearTemplatesSRW\nCitation Context: Computationally, the search time increases with increasing sentence lengths. On a reasonably modern machine, our implementation generated the above sentences in about 150 seconds while using 2.2 GB of memory. [URL_CITE]\nFootnote or Reference text (if exists): 2 The code used for this research will be made available on https://github.com/tingc9/LinearTemplatesSRW\nSection Titles (if exists):  ['9 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wuch15/NRHUB\nCitation Context: We conducted experiments on a real-world news recommendation dataset [URL_CITE] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/RishabhMaheshwary/query-attack\nCitation Context: Also, all the past works evaluate their search methods only on a single search space. In this paper [URL_CITE] , we address the above discussed drawbacks through following contributions:\nFootnote or Reference text (if exists): 1 Code and adversarial examples available at: https://github.com/RishabhMaheshwary/query-attack\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, DataSource)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/pointer-generator\nCitation Context: For S EQ 2S EQ , P T G EN and P T G EN -C OVG , we used the best settings reported on the CNN and DailyMail data (See et al., 2017). [URL_CITE] All three mod-els had 256 dimensional hidden states and 128 di-mensional word embeddings. They were trained using Adagrad (Duchi et al., 2011) with learning rate 0.15 and an initial accumulator value of 0.1.\nFootnote or Reference text (if exists): 2 We used the code available at https://github.com/abisee/pointer-generator.\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://rnnlm.org\nCitation Context: In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a) [URL_CITE] to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer.\nFootnote or Reference text (if exists): Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Unlabeled tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clarkkev/deep-coref\nCitation Context: The first resolver, the Stanford neural resolver (Clark and Manning, 2016) [URL_CITE] , takes as input a set of entity mentions identified for a given document by a rule-based MD system and trains using reinforce-ment learning a simple mention ranker consisting of three hidden layers of ReLU units and a final layer that is fully-connected.\nFootnote or Reference text (if exists): 3 https://github.com/clarkkev/deep-coref\nSection Titles (if exists):  ['2 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Mantidae\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [URL_CITE] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 4 https://github.com/duyvuleo/Mantidae\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lex4all.github.io/lex4all/\nCitation Context: This is the motivation behind lex4all, [URL_CITE] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.\nFootnote or Reference text (if exists): 1 http://lex4all.github.io/lex4all/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/tools/lextool.html\nCitation Context: If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. [URL_CITE] The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited.\nFootnote or Reference text (if exists): 1 http://www.speech.cs.cmu.edu/tools/lextool.html\nSection Titles (if exists):  ['3 Target Recovery', '3.3 Phonetic Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://facebook.github.io/react\nCitation Context: The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [URL_CITE] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:\nFootnote or Reference text (if exists): 8 https://facebook.github.io/react\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/cgibin/cmudict\nCitation Context: The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998) [URL_CITE] . This dictionary has an inventory of 39 phonemes.\nFootnote or Reference text (if exists): Robert L Weide. 1998. The cmu pronouncing dictionary. URL: http://www.speech.cs.cmu.edu/cgibin/cmudict.\nSection Titles (if exists):  ['3 Target Recovery', '3.3 Phonetic Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: We propose a novel demonstration (URL: [URL_CITE] https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.w3.org/TR/pronunciation-lexicon/\nCitation Context: Once pronunciations for all terms in the vocab-ulary have been generated, the application outputs a pronunciation lexicon for the given terms as an XML file conforming to the Pronunciation Lexi-con Specification. [URL_CITE] This lexicon can then be di-rectly included in a speech recognition application built using the MSP API or a similar toolkit.\nFootnote or Reference text (if exists): 7 http://www.w3.org/TR/pronunciation-lexicon/\nSection Titles (if exists):  ['3 System overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/atcbosselut/comet-commonsense\nCitation Context: For retrieving commonsense properties of the vehi-cle, we use the pre-trained COMET model [URL_CITE] and retrieve top 5 candidates for each input. Vehicle and Overall Quality.\nFootnote or Reference text (if exists): 11 https://github.com/atcbosselut/ comet-commonsense\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rajpurkar.github.io/SQuAD-explorer/\nCitation Context: High-performance deep neural language models such as BERT (Devlin et al., 2018), XLNet (Z. Yang et al., 2019), and GPT-2 (Radford et al., 2019) have brought breakthroughs to a wide range of Natural Language Processing (NLP) tasks including text classification, sentiment analysis, textual entailment, natural language inference, machine translation, and question answering. Their immense ability in capturing various linguistic properties has led these state-of-the-art language models to master different NLP tasks, even surpassing human accuracy on some benchmarks such as SQuAD [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rajpurkar.github.io/SQuAD-explorer/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/understanding-mbr\nCitation Context: We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and documentation available at https://github.com/ZurichNLP/understanding-mbr\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bishanyang/EventEntityExtractor\nCitation Context: In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context. [URL_CITE] We first decompose the learn-ing problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the co-occurrence relations between events across the doc-ument, and (3) learning for entity extraction. Then we combine the learned models for these subprob-lems into a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document.\nFootnote or Reference text (if exists): 1 The code for our system is available at https://github.com/bishanyang/EventEntityExtractor.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/label_rationale_association\nCitation Context: Via two tests, robustness equivalence and feature importance agreement, we find that state-of-the-art T5-based joint models exhibit desir-able properties for explaining commonsense question-answering and natural language infer-ence, indicating their potential for producing faithful free-text rationales. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/allenai/label_rationale_association.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/glosstag.shtml\nCitation Context: Let L p be the set containing p and all the Wikipedia pages hav-ing an outgoing link to p, and R s be the set con-sisting of s and all other synsets that are in its di-rect neighbourhood. We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [URL_CITE] . Let B be a function mapping each WordNet synset s 0 to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise.\nFootnote or Reference text (if exists): 2 http://wordnet.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['2 Semantic Representation of Concepts', '2.1 Collecting contextual information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www/eml-research.de/nlp/download/wikirelations.php\nCitation Context: To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [URL_CITE] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links.\nFootnote or Reference text (if exists): 1 http://www/eml-research.de/nlp/download/wikirelations.php\nSection Titles (if exists):  ['4 Representation', '4.2 Selectional preference features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wiktionary.org/\nCitation Context: Contextual sentences in WiC were extracted from example usages provided for words in three lexi-cal resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) Verb-Net (Kipper-Schuler, 2005), the largest domain-independent verb-based resource; and (3) Wik-tionary [URL_CITE] , a large collaborative-constructed online dictionary. We used WordNet as our core re-source, exploiting BabelNet\u2019s mappings (Navigli and Ponzetto, 2012) as a bridge between Wik-tionary and VerbNet to WordNet. Lexicographer examples constitute a reliable base for the con-struction of the dataset, as they are curated in a way to be clearly distinguishable across different senses of a word.\nFootnote or Reference text (if exists): 3 https://www.wiktionary.org/\nSection Titles (if exists):  ['2 WiC: the Word-in-Context dataset', '2.1 Construction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://research.microsoft.com/users/silviu/WebAssistant/TestData\nCitation Context: We evaluated the system in two ways: on a set of Wikipedia articles, by comparing the system out-put with the references created by human contribu-tors, and on a set of news stories, by doing a post-hoc evaluation of the system output. The evalua-tion data can be downloaded from [URL_CITE] http://research.microsoft.com/users/silviu/WebAssistant/TestData. In both settings, we computed a disambiguation baseline in the following manner: for each surface form, if there was an entity page or redirect page whose title matches exactly the surface form then we chose the corresponding entity as the baseline disambiguation; otherwise, we chose the entity most frequently mentioned in Wikipedia using that surface form.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/nasari/\nCitation Context: N ASARI attains state-of-the-art performance on multiple standard benchmarks in word similarity as well as Wikipedia sense clustering. We release the representations obtained for all the Wikipedia pages and WordNet synsets in [URL_CITE] http://lcl.uniroma1.it/nasari/. As future work we plan to integrate N ASARI into BabelNet and apply our representation to a mul-tilingual setting, enabling the comparison of pairs of concepts across languages.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.babelnet.org/\nCitation Context: Throughout the paper, by a concept we mean a tuple b = (p, s) where p is a Wikipedia page and s is the corresponding WordNet synset. As a bridge between the two resources we use the synset-to-article mappings provided by BabelNet [URL_CITE] (Navigli and Ponzetto, 2012), a high coverage mul-tilingual encyclopedic dictionary and semantic net-work that merges, among other resources, Wikipedia and WordNet. Note that the concept b can also con-tain a Wikipedia page or a WordNet synset only, if a mapping is not provided by BabelNet.\nFootnote or Reference text (if exists): 1 http://www.babelnet.org/\nSection Titles (if exists):  ['2 Semantic Representation of Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/orenmel/context2vec\nCitation Context: One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models [URL_CITE] . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context.\nFootnote or Reference text (if exists): 7 https://github.com/orenmel/context2vec\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [URL_CITE] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 3 We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo (Peters et al., 2018) and BERT [URL_CITE] (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 4 We generate our pre-trained BERT embedding with the released model in https://github.com/google-research/bert. The model uses character-based tokenization for Chinese, which require us to maintain alignment between our input text and output text of Bert. So we take take embedding of the first word piece as the whole word representation.\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jzhou316/Unsupervised-Sentence-Summarization\nCitation Context: We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [URL_CITE] The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/jzhou316/Unsupervised-Sentence-Summarization.\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/elmo\nCitation Context: We explore if contextual word embeddings obvi-ate the need for adapting source embeddings, in the ELMo (Peters et al., 2018) setting, a contex-tualized word representation model, pre-trained on a 5.5B token corpus [URL_CITE] . We compare ELMo\u2019s contextual embeddings as-is, and also after con-catenating them with each of Tgt, SrcTune, and SrcSel embeddings in Table 8.\nFootnote or Reference text (if exists): 11 https://allennlp.org/elmo\nSection Titles (if exists):  ['Topics and tasks', '4.4 Contextual embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/hub/modules/google/elmo/1\nCitation Context: ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models [URL_CITE] for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019).\nFootnote or Reference text (if exists): 8 https://www.tensorflow.org/hub/modules/google/elmo/1\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/forkarinda/MFN\nCitation Context: The extracted ASR-output transcripts and code will be released on [URL_CITE] https://github.com/forkarinda/MFN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://x-factr.github.io\nCitation Context: We fur-ther propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effective-ness on several benchmark languages. Bench-mark data and code have be released at [URL_CITE] https: //x-factr.github.io.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/\nCitation Context: We are currently making preparations for the project to be released with an open-source license. The code will be available at [URL_CITE] http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/intersun/LightningDOT\nCitation Context: In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000\u00d7 mag-nitude of computational hours. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and pre-training checkpoints are available at https://github.com/intersun/LightningDOT.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.iso.org/iso-639-language-codes.html\nCitation Context: The language clustering based on language family is shown in Figure 2, which results in groups given our 23 languages. All the language names and their corresponding ISO-639-1 code [URL_CITE] can be found in Supplementary Materials (Section 2).\nFootnote or Reference text (if exists): 9 https://www.iso.org/iso-639-language-codes.html.\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/machelreid/afromt\nCitation Context: All code and pre-trained models will be released as further steps towards larger reproducible benchmarks for African languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code, pretrained models, and data can be found at https://github.com/machelreid/afromt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nclt.computing.dcu.ie/~jfoster/resources/\nCitation Context: Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [URL_CITE] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.\nFootnote or Reference text (if exists): 10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools [URL_CITE] ; and (c) tokenization from the parser of Charniak & Johnson (2005). Table 1 shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance (Leven-shtein, 1966) over tokens (for a total of 49,208 sen-tences and 1,173,750 gold-standard tokens).\nFootnote or Reference text (if exists): 5 See http://nlp.stanford.edu/software/corenlp.shtml, run in \u2018strictTreebank3\u2019 mode.\nSection Titles (if exists):  ['3 A Contrastive Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.bls.gov/cps/cpsaat11.htm\nCitation Context: (2018) who use statistics provided by the US Department of Labor. [URL_CITE] This metric shows that all tested sys-tems have a significant and consistently better per-formance when presented with pro-stereotypical assignments (e.g., a female nurse), while their performance deteriorates when translating anti-stereotypical roles (e.g., a male receptionist). For instance, Figure 2 depicts Google Trans-late absolute accuracies on stereotypical and non-stereotypical gender roles across all tested lan-guages.\nFootnote or Reference text (if exists): 6 https://www.bls.gov/cps/cpsaat11.htm\nSection Titles (if exists):  ['3 Evaluation', '3.2 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.pascal-network.org/Challenges/RTE,March\nCitation Context: The algorithm propagating verb arguments was tested on a set of 106 pairs of phrases with simi-lar meaning for which argument structures could be built. These phrases were selected from pairs of questions and their correct answers from the set of factoid questions in TREC 2004 and also from the pairs of scenarios and hypotheses from first edition of PASCAL RTE Challenge (Dagan et al., 2005) [URL_CITE] . Table 6 shows algorithm performance.\nFootnote or Reference text (if exists): Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. Recognising Textual Entailment Challenge, http://www.pascal-network.org/Challenges/RTE,March.\nSection Titles (if exists):  ['4 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://xjtu-intsoft.github.io/chase\nCitation Context: To understand the characteristics of C HASE , we conduct a detailed data analysis and experi-ment with three state-of-the-art (SOTA) XDTS approaches, namely, EditSQL (Zhang et al., 2019), IGSQL (Cai and Wan, 2020), and our extension of RAT-SQL (Wang et al., 2020a). The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that C HASE presents significant chal-lenges for future research. The dataset, benchmark approaches, and our annotation tools are available at [URL_CITE] https://xjtu-intsoft.github.io/chase.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/qingsongma/percentage-refBias\nCitation Context: Table 2 shows proportions of all judge pairs with significant differences in Kappa point esti-mates (non-overlapping confidence intervals) for each combination of settings (Revelle, 2014). [URL_CITE] The number of significant differences in Kappa point estimates for pairs of judges in SAME and DIFF is only 13%, or, in other words, 87% of judge pairs across SAME and DIFF have no significant difference in agreement levels. Table 2 also in-cludes proportions of significant differences for Kappa point estimates resulting from judges be-longing to a single setting (significance testing all Kappa of SAME with respect to all other Kappa be-longing to SAME , for example), revealing that the proportion of significant differences within SAME (12%) to be very similar to that of SAME \u00d7 DIFF (13%), and similarly for DIFF (12%), with only a single percentage point difference in both cases in proportions of significant differences.\nFootnote or Reference text (if exists): 7 Our re-analysis code is available at https://github.com/qingsongma/percentage-refBias\nSection Titles (if exists):  ['2 Background', '2.1 Measures of Central Tendency']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce, Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/withattardi/wikiextractor\nCitation Context: Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia [URL_CITE] for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset.\nFootnote or Reference text (if exists): 9 Downloaded https://github.com/withattardi/wikiextractor\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nCitation Context: We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [URL_CITE] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison.\nFootnote or Reference text (if exists): 10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We ran each model for 50 epochs\u2014enough for the training loss to converge. For our BERT ex-periments we use a PyTorch implementation [URL_CITE] with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam op-timizer, a batch size of 32, dropout of 0.1, and em-bedding size 768.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Method and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/ERNIE\nCitation Context: (2019) which considers the nine general entity types. [URL_CITE] Our model marks the location of a target span with the special [E] and [/E] tokens and uses the representation of the [E] token to predict the type. As shown in Table 7, KnowBert-W+W shows an improvement of 0.6% F 1 over ERNIE and 2.5% over BERT BASE .\nFootnote or Reference text (if exists): 7 Data obtained from https://github.com/thunlp/ERNIE\nSection Titles (if exists):  ['4 Experiments', '4.3 Downstream Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [URL_CITE] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1.\nFootnote or Reference text (if exists): 7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiments', '4.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/models/tree/master/official/nlp/bert\nCitation Context: (2019). [URL_CITE] Ta-ble 1 shows that using the official code to train BERT BASE could take almost 6 days under our hard-ware assumptions (Section 2), and a large model might require close to a month of non-stop compu-tation. In contrast, our recipe significantly speeds up training, allowing one to train BERT LARGE with the original number of steps (1M) in a third of the time (8 days), or converge in 2-3 days by enlarging the batch size.\nFootnote or Reference text (if exists): 3 https://github.com/tensorflow/models/tree/master/official/nlp/bert\nSection Titles (if exists):  ['3 Combining Efficient Training Methods', '3.2 Combined Speedup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FudanNLP/NeuralSentenceOrdering\nCitation Context: NSF abstract (Logeswaran et al., 2018) is from NSF Research Award abstract dataset, where abstracts in years 1990-1999/2000/2001-2003 for training/validation/testing. ArXiv abstract (Gong et al., 2016; Chen et al., 2016) is from arXiv web-site [URL_CITE] . The validation and test sets of this dataset are the first and last 10% abstracts from the shuf-fled data, and the remaining data are for training.\nFootnote or Reference text (if exists): 2 https://github.com/FudanNLP/NeuralSentenceOrdering\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/bsd/pmresources.html\nCitation Context: But how often do demonstrative nouns refer to abstract objects? We observed that from a corpus of 74,000 randomly chosen Medline [URL_CITE] abstracts, of the first 150 most fre-quently occurring distinct demonstrative nouns (fre-quency > 30), 51.3% were abstract, 41.3% were concrete, and 7.3% were discourse deictic. This shows that abstract anaphora resolution is an impor-tant component of general anaphora resolution in the biomedical domain.\nFootnote or Reference text (if exists): 2 http://www.nlm.nih.gov/bsd/pmresources. html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://manikvarma.org/downloads/XC/XMLRepository.html\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [URL_CITE] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 3 http://manikvarma.org/downloads/XC/XMLRepository.html\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://personalitycafe.com/forum\nCitation Context: We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurkovic\u0301 et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, [URL_CITE] where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts.\nFootnote or Reference text (if exists): 5 http://personalitycafe.com/forum\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/NSEG\nCitation Context: The experiments are conducted on six public datasets in different domains: NIPS abstract, AAN abstract, NSF abstract, arXiv abstract: These datasets contain ab-stracts of research papers. NIPS abstract [URL_CITE] is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018). AAN abstract (Logeswaran et al., 2018) is col-lected from ACL Anthology Network corpus.\nFootnote or Reference text (if exists): 1 https://github.com/DeepLearnXMU/NSEG\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Method & Material)\nTYPE: Code & DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~cprose/SIDE.html\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.is.cs.cmu.edu/mie\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.im2.ch\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ankara.lti.cs.cmu.edu/side/video.swf\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Support"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projects.ldc.upenn.edu/gale/data/Catalog.html\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [URL_CITE] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.\nFootnote or Reference text (if exists): 2 http://projects.ldc.upenn.edu/gale/data/Catalog.html\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.translationautomation.com/\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS. [URL_CITE]\nFootnote or Reference text (if exists): 4 http://www.translationautomation.com/\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opus.nlpl.eu/\nCitation Context: For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [URL_CITE] (Tiedemann, 2012) and (2) ParaCrawl (Espl\u00e0 et al., 2019). From OPUS, we use the JW300 corpus (Agic\u0301 and Vulic\u0301, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014).\nFootnote or Reference text (if exists): 3 http://opus.nlpl.eu/\nSection Titles (if exists):  ['2 A FRO MT benchmark', '2.2 Data Sources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tticoin/AntonymDetection\nCitation Context: In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on aver-age) and 380,873 antonymy pairs (6.5 per word). [URL_CITE] As in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (panga-sius, animal)).\nFootnote or Reference text (if exists): 5 https://github.com/tticoin/AntonymDetection\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ffzg.hr/resources/corpora/setimes/\nCitation Context: Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, [URL_CITE] United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh\u2013English and Kazakh\u2013Russian data from WMT, to be able to evaluate on Kazakh.\nFootnote or Reference text (if exists): 11 http://nlp.ffzg.hr/resources/corpora/setimes/\nSection Titles (if exists):  ['B Data Details for Replication']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014\nCitation Context: To experiment with a diverse set of queries and web pages, we created a new dataset, O PEN W EB , using web pages from Google search results. [URL_CITE] We use the method from Berant et al. (2013) to gen-erate search queries by performing a breadth-first search over the query space.\nFootnote or Reference text (if exists): 1 The O PEN W EB dataset and our code base are available for download at http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014.\nSection Titles (if exists):  ['2 Problem statement', '2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/row-column-intersection\nCitation Context: Experi-ments on recent benchmarks prove that the pro-posed methods can effectively locate cell val-ues on tables (up to \u223c98% Hit@1 accuracy on WikiSQL lookup questions). Also, the inter-action model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (T A P AS and T A B ERT ), achieving \u223c3.4% and \u223c18.86% additional pre-cision improvement on the standard WikiSQL benchmark [URL_CITE] .\nFootnote or Reference text (if exists): 1 The source code and the models we built are available at https://github.com/IBM/row-column-intersection.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nCitation Context: We use translation benchmarks IWSLT14 En\u2013De, En\u2013Fr, En\u2013It, and IWSLT15 En\u2013Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [URL_CITE] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set.\nFootnote or Reference text (if exists): 3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nSection Titles (if exists):  ['6 Experiments', '6.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://disi.unitn.it/moschitti/Tree-Kernel.htm\nCitation Context: Reranking models based on SVM and PTK have been trained with \u201cSVM-Light-TK\u201d [URL_CITE] . Kernel param-eters M and SVM parameter C have been optimized on the development set, as well as thresholds for the WRR (see section 4.2).\nFootnote or Reference text (if exists): 3 available at http://disi.unitn.it/moschitti/Tree-Kernel.htm\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/72.410363\nCitation Context: In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015); in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015) [URL_CITE] . In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value.\nFootnote or Reference text (if exists): Karl Moritz Hermann, Toma\u0301s\u030c Koc\u030cisky\u0301, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom. 2015. Teaching Ma-chines to Read and Comprehend. In Advances in Neural Information Processing Systems (NIPS 2015). pages 1\u201314. https://doi.org/10.1109/72.410363.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/tracks.html\nCitation Context: We rely on the widely-used 2009\u20132014 T REC Web Track ad-hoc task benchmarks [URL_CITE] . The benchmarks are based on the C LUE W EB 09 and C LUE W EB 12 datasets as document collections.\nFootnote or Reference text (if exists): 3 http://trec.nist.gov/tracks.html\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html\nCitation Context: Each sen-tence in texts is processed by the morphological an-alyzer JUMAN and the dependency parser KNP. [URL_CITE] JUMAN consults a hand-crafted dictionary and an automatically constructed dictionary. KNP is used to form a phrasal unit called bunsetsu by chunking morphemes.\nFootnote or Reference text (if exists): 2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html\nSection Titles (if exists):  ['3 Lexicon Acquisition', '3.2 System Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sentiwordnet.isti.cnr.it/\nCitation Context: The set of semantic orientation scores of all WordNet synsets is released by the name SentiWordNet. [URL_CITE] An evaluation of SentiWordNet by comparing orientation scores of about 1,000 WordNet glosses to scores assigned by human an-notators is presented in Esuli (2008). Our ap-proach uses a Roget-like thesaurus, and it does not use any supervised classifiers.\nFootnote or Reference text (if exists): 2 http://sentiwordnet.isti.cnr.it/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LIAAD/KeywordExtractor-Datasets\nCitation Context: To evaluate the keyword extraction methods, we consider 15 different public datasets in English. [URL_CITE] Each entry in a dataset consists of a source document and a set of gold keyphrases, where the source document is processed through the pipeline described in Section 3 and the gold keyphrase set is filtered to include only phrases which appear in its candidate set. Table 1 pro-vides high-level statistics of each dataset, including length and number of keyphrases 7 (both average and standard deviation).\nFootnote or Reference text (if exists): 6 All the datasets were fetched from a public data reposi-tory for keyword extraction data: https://github.com/LIAAD/KeywordExtractor-Datasets: KPCrowd (Marujo et al., 2013), Inspec (Hulth, 2003), Krapivin2009 (Krapivin et al., 2009), SemEval2017 (Augenstein et al., 2017), kdd (Gollapalli and Caragea, 2014), www (Gollapalli and Caragea, 2014), wiki20 (Medelyan and Witten, 2008), PubMed (Schutz et al., 2008), Schutz2008 (Schutz et al., 2008), citeulike180 (Medelyan et al., 2009), fao30 and fao780 (Medelyan and Witten, 2008), guyen2007 (Nguyen and Kan, 2007), and SemEval2010 (Kim et al., 2010).\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cl.aist-nara.ac.jp/~taku-ku/software/cabocha/\nCitation Context: In addition, we decided to use the output of ChaSen 2.2.9 and CaboCha 0.34 [URL_CITE] instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus.\nFootnote or Reference text (if exists): 4 http://cl.aist-nara.ac.jp/\u02dctaku-ku/software/cabocha/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.aist-nara.ac.jp/\nCitation Context: In addition, we decided to use the output of ChaSen 2.2.9 [URL_CITE] and CaboCha 0.34 instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus.\nFootnote or Reference text (if exists): 3 http://chasen.aist-nara.ac.jp/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freedict.com\nCitation Context: \u2022 An English-French dictionary (a total of 34,808 entries, 4,021 of which are not one-to-one). [URL_CITE] It contains morphological variants but does not include character accents. Each n-to-m entry was processed by stoplisting and then extracting all word-pairs in the remaining cross-product as in section 4.1.\nFootnote or Reference text (if exists): 9 This dictionary was generated using a dictionary de-rived from one available at http://www.freedict.com.\nSection Titles (if exists):  ['5 Experiment: English-French', '5.1 Translation Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amzn.to/2qDjNcJ\nCitation Context: The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the \u223c20 SLU domains [URL_CITE] . In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and third-party applications.\nFootnote or Reference text (if exists): 1 https://amzn.to/2qDjNcJ\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/\nCitation Context: ATIS and SNIPS, respectively), named entity recog- The results in Table 1 (above) support the observa-nition, part-of-speech tagging and semantic role tion that, in general, bidirectional models do have labeling (Weischedel et al., 2013); and four for a better non-incremental performance than LSTMs sentence classification: intent (Hemphill et al., (except for IntentATIS and ProsCons) and that there marginal difference in the evaluation metrics. We thus present 4 The code is available at [URL_CITE] https://github.com/ the results using the pre-trained model only, and leave more briemadu/inc-bidirectional. For more details on exploration of fine-tuning for future work.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11372/LRT-1632\nCitation Context: We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016 [URL_CITE] ), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.\nFootnote or Reference text (if exists): Marco Turchi, Rajen Chatterjee, and Matteo Negri. 2016. WMT16 APE shared task data. LIN-DAT/CLARIN digital library at the Institute of For-mal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.\nSection Titles (if exists):  ['4 Experiments', '4.2 Automatic MT Post-editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clinc/oos-eval\nCitation Context: We use a recently-released dataset, CLINC150, [URL_CITE] for multi-domain intent detection (Larson et al., 2019). The CLINC150 dataset defines 150 types of intents in total (i.e., N = 150), where there are 10 different domains and 15 intents for each of them. Table 2 shows the dataset statistics.\nFootnote or Reference text (if exists): 3 https://github.com/clinc/oos-eval.\nSection Titles (if exists):  ['4 Experimental Settings', '4.1 Dataset: Multi-Domain Intent Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [URL_CITE] are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk\nCitation Context: \u2022 ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [URL_CITE]\nFootnote or Reference text (if exists): 9 We used the ATIS data version from https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk.\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://purduenlp.cs.purdue.edu/projects/twittermorals\nCitation Context: In summary, this paper makes the following contributions: (1) This work is among the first to explore jointly modeling language and polit-ical framing techniques for the classification of moral foundations used in the tweets of U.S. politicians on Twitter. (2) We provide a descrip-tion of our annotation guidelines and an annotated dataset of 2,050 tweets. [URL_CITE] (3) We suggest compu-tational models which easily adapt to new policy issues, for the classification of the moral founda-tions present in tweets.\nFootnote or Reference text (if exists): 2 The data will be available at http://purduenlp.cs.purdue.edu/projects/twittermorals.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nmrksic/\nCitation Context: To evaluate the effects of spe-cialized word vectors on DST, following prior work we utilize the Neural Belief Tracker (NBT), a sta-tistical DST model that makes inferences purely based on pre-trained word vectors (Mrk\u0161ic\u0301 et al., 2017). [URL_CITE] Again, as in prior work the DST evalu-ation is based on the Wizard-of-Oz (WOZ) v2.0 dataset (Wen et al., 2017; Mrk\u0161ic\u0301 et al., 2017), com-prising 1,200 dialogues split into training (600 di-alogues), development (200), and test data (400). We report the standard DST metric: joint goal ac-curacy (JGA), the proportion of dialog turns where all the user\u2019s search goal constraints were correctly identified, computed as average over 5 NBT runs.\nFootnote or Reference text (if exists): 9 https://github.com/nmrksic/ neural-belief-tracker; For full model details, we refer the reader to the original paper.\nSection Titles (if exists):  ['4 Results and Discussion', '4.2 Downstream Tasks', '4.2.2 Dialog State Tracking']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: There are a few noteworthy differences between our neural model and the CRF of prior work. As an adapted BiLSTM, our model easily ex- 2017) trained on the [URL_CITE] Fr-En corpus (Callison-Burch et al., 2009) (Appendix A). ploits the benefits of large-scale pretraining, in the form of GloVe embeddings and MT pretrain-ing, both absent in the CRF.\nFootnote or Reference text (if exists): 10 300-dimensional, uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/; 15,533 out-of-vocabulary words across all datasets were assigned a random embedding (uniformly from [\u2212 01 01]).. , . Embeddings remained fixed during training.\nSection Titles (if exists):  ['3 \u201cNeural-Davidsonian\u201d Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: , p K independently selects a pair of answer positions (i k ,j k ), which are the begin and end of the answer from paragraph p k . In the case that p k does not support answering the question q, special NULL positions are selected (follow-ing the SQuAD 2.0 BERT implementation [URL_CITE] ). Thus, the set of possible outcomes \u2126 in the paragraph-level probability space is the set of lists of begin/end position pairs, one from each paragraph: {[(i 1 , j 1 ), .\nFootnote or Reference text (if exists): 3 https://github.com/google-research/bert\nSection Titles (if exists):  ['2 Probability Space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/pyenchant/\nCitation Context: The fifth set of features that we use are lan-guage modeling features. We use the count of words that are absent in Google News word vec-tors and misspelled words using the PyEnchant [URL_CITE] library. In order to check the grammaticality of the text, we construct a 5-gram language model, using the Brown Corpus (Francis and Kucera, 1979).\nFootnote or Reference text (if exists): 9 https://pypi.python.org/pypi/pyenchant/\nSection Titles (if exists):  ['4 Features', '4.1 Text-based Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/jacana/\nCitation Context: In the following we first provide background on the TED model, going on to evaluate our implemen-tation against prior work in the context of question answer sentence ranking (QASR), achieving state of the art in that task. We then describe how we cou-ple TED features to a linear-chain CRF for answer extraction, providing the set of features used, and fi-nally experimental results on an extraction dataset we make public (together with the software) to the community. [URL_CITE] Related prior work is interspersed throughout the paper.\nFootnote or Reference text (if exists): 3 http://code.google.com/p/jacana/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: Our classifier forms an encoder-labeler struc-ture, which consists of the above encoder and a la-beler that predicts a sentiment label from the con-text. Note that the encoder of the classifier is fine-tuned with labeled data, as in (Dai and Le, 2015) [URL_CITE] . The main difference between their approach and ours is that we examine paired (dialog) data for pretraining, while they only showed the usefulness of pretraining with unpaired data.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://rnnlm.org\nCitation Context: In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a) [URL_CITE] to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer.\nFootnote or Reference text (if exists): Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Unlabeled tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TalLinzen/rnn_agreement\nCitation Context: (2016). [URL_CITE] Word types beyond the most frequent 10,000 are converted to their respective POS tags. We summarize the corpus statistics of the dataset, along with the test set distribution of the num-ber of attractors, in Table 1.\nFootnote or Reference text (if exists): 1 The dataset and scripts are obtained from https://github.com/TalLinzen/rnn_agreement.\nSection Titles (if exists):  ['2 Number Agreement with LSTM Language Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://tizirinagh.github.io/acl2019/\nCitation Context: We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [URL_CITE] . We then evaluated the generative model and the generative + discriminative model with the Snorkel architecture on the test set with the results in Table 2.\nFootnote or Reference text (if exists): 3 https://tizirinagh.github.io/acl2019/\nSection Titles (if exists):  ['5 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/iirc\nCitation Context: We fol-low recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at [URL_CITE] https://allennlp.org/iirc.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/connorbrinton/polyai-models/releases/tag/v1.0\nCitation Context: (2020) showed that the \u201cUSE+ConveRT\u201d outperformed a BERT classifier on the CLINC150 dataset, while it was not evaluated along with the OOS detec-tion task. We modified their original code [URL_CITE] to apply the uncertainty-based OOS detection.\nFootnote or Reference text (if exists): 6 https://github.com/connorbrinton/polyai-models/releases/tag/v1.0.\nSection Titles (if exists):  ['4 Experimental Settings', '4.3 Model Training and Configurations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2015/task3/\nCitation Context: The corpus contains data from the Qatar Living forum, and is publicly available on the task\u2019s website. [URL_CITE] The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads. Each question, and each answer, consist of a short title and a more de-tailed description.\nFootnote or Reference text (if exists): 2 http://alt.qcri.org/semeval2015/task3/\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt/\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): 6 http://www.phontron.com/kftt/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/\nCitation Context: This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [URL_CITE] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems).\nFootnote or Reference text (if exists): 7 Available from http://www.statmt.org/.\nSection Titles (if exists):  ['6 Expt. 2: Predicting Pairwise Preferences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov\nCitation Context: Our first experiment evaluates the models we have proposed on a corpus with traditional an-notation on a seven-point scale, namely the NIST OpenMT 2008 corpus. [URL_CITE] The corpus contains trans-lations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur)). Each language consists of 1500\u20132800 sen-tence pairs produced by 7\u201315 MT systems.\nFootnote or Reference text (if exists): 4 Available from http://www.nist.gov.\nSection Titles (if exists):  ['5 Expt. 1: Predicting Absolute Scores']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/jdpacorpus/\nCitation Context: We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity or entity [URL_CITE] depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.\nFootnote or Reference text (if exists): 2 Available from http://verbs.colorado.edu/jdpacorpus/ \u2013 we ignore cars batch 009 where no arguments of comparative predicates are annotated.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/mate-tools/\nCitation Context: We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity [URL_CITE] or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.\nFootnote or Reference text (if exists): 1 http://code.google.com/p/mate-tools/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0\nCitation Context: Named entity counts and concreteness ratio. For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [URL_CITE] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn.\nFootnote or Reference text (if exists): 6 Version 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0. pdf Section 2.6.\nSection Titles (if exists):  ['4 Pragmatic correlations with analysts\u2019 pre-call judgments', '4.1 Pragmatic lexical features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE\nCitation Context: Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [URL_CITE] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters.\nFootnote or Reference text (if exists): 5 https://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/trangvu/alil-dream\nCitation Context: Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [URL_CITE] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/trangvu/alil-dream\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/emorynlp/reading-comprehension\nCitation Context: For the future work, we will expand the annotation for more entity types and automatically link men-tions with respect to their entities using an entity linker. All our resources including the annotated corpus and source codes of the models are avail-able at: [URL_CITE] https://github.com/emorynlp/reading-comprehension.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gucci-j/light-transformer-emnlp2021\nCitation Context: We implement our models using PyTorch (Paszke et al., 2019) and the transformers library (Wolf et al., 2020). We pretrain our models with two NVIDIA Tesla V100 (SXM2 - 32GB) and use one for fine-tuning. Our code is publicly available on GitHub: [URL_CITE] https://github.com/gucci-j/light-transformer-emnlp2021.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['C Experimental Setup', 'C.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nCitation Context: We implement our models using the jiant toolkit, [URL_CITE] which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 3 https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/higher\nCitation Context: In all the experiments, we use PyTorch (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package [URL_CITE] (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ higher\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: In all the experiments, we use PyTorch [URL_CITE] (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 2 https://pytorch.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/havakv/pycox\nCitation Context: The training, validation and test splits are 70%, 10%, 20%, respectively. We use pycox and PyTorch to implement the framework [URL_CITE] . The end-to-end training takes about 30 minutes with NVIDIA Tesla P100 16 GB GPU, mainly due to feature extraction.\nFootnote or Reference text (if exists): 2 https://github.com/havakv/pycox\nSection Titles (if exists):  ['4 Experiments', '4.3 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/docs/master/index.html\nCitation Context: We use PyTorch [URL_CITE] to implement our models.\nFootnote or Reference text (if exists): 6 http://pytorch.org/docs/master/index.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/NAACL2019-literary-entities\nCitation Context: All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: https://github.com/dbamman/ litbank; code to support this work can be found at: [URL_CITE] https://github.com/dbamman/NAACL2019-literary-entities.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/\nCitation Context: All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: [URL_CITE] https://github.com/dbamman/ litbank; code to support this work can be found at: https://github.com/dbamman/NAACL2019-literary-entities.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/bioprocess\nCitation Context: The data and code for this paper are avail-able at [URL_CITE] http://www-nlp.stanford.edu/software/bioprocess.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['b Transfer of ions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jayelm/lsl\nCitation Context: Code, data, and experiments are available at [URL_CITE] https: //github.com/jayelm/lsl and on CodaLab at https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/lsl_acl20\nCitation Context: Code, data, and experiments are available at https: //github.com/jayelm/lsl and on CodaLab at [URL_CITE] https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.allenai.org/paper-appendix/emnlp2016-p3\nCitation Context: Code, data and supplementary material for this paper are available at: [URL_CITE] http://www.allenai.org/paper-appendix/emnlp2016-p3\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/lex-parser.shtml\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [URL_CITE] . Specially, we train the parser on the GENIA Treebank 1.0 (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/software/lex-parser.shtml\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/\nCitation Context: (2004)). Our biomedi-cal data comes from the GENIA treebank [URL_CITE] (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. We downloaded additional sentences from Medline for our self-trained MEDLINE corpus.\nFootnote or Reference text (if exists): 8 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/PubMed/\nCitation Context: Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. [URL_CITE] We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE .\nFootnote or Reference text (if exists): 9 http://www.ncbi.nlm.nih.gov/PubMed/\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/lex-parser.shtml\nCitation Context: First, we create a set of candidate sentences which contains the sentence containing the this-issue anaphor and the two preceding sentences. Then, we parse every candidate sentence with the Stanford Parser [URL_CITE] . Ini-tially, the set of candidate constituents contains a list of well-defined syntactic constituents.\nFootnote or Reference text (if exists): 7 http://nlp.stanford.edu/software/lex-parser.shtml\nSection Titles (if exists):  ['4 Resolution Algorithm', '4.1 Candidate Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.geniaproject.org/genia-corpus/treebank\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser . Specially, we train the parser on the GENIA Treebank 1.0 [URL_CITE] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 3 http://www.geniaproject.org/genia-corpus/treebank\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.org/\nCitation Context: 1. Wikipedia [URL_CITE] , a large-scale online encyc-lopedia, its English version includes more than 3,000,000 concepts and new articles are added quickly and up-to-date. Wikipedia contains rich semantic knowledge in the form of hyperlinks between Wikipedia articles, such as Polysemy (disambiguation pages), Synonym (redirect pages) and Associative relation (hyperlinks between Wikipedia articles).\nFootnote or Reference text (if exists): 1 http://www.wikipedia.org/\nSection Titles (if exists):  ['2 The Structural Semantic Relatedness Measure', '2.1 Knowledge Sources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: Multi-Domain Text Classification The MDTC task was first examined by Li and Zong (2008), who proposed to fuse the training data from multi-ple domains either at the feature level or the classi-fier level. The prior art of MDTC (Wu and Huang, 2015) [URL_CITE] decomposes the text classifier into a gen-eral one and a set of domain-specific ones. How-ever, the general classifier is learned by param-eter sharing and domain-specific knowledge may sneak into it.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/adithya7/constrained-fever\nCitation Context: \u2022 We create an anonymized version of the FEVER dataset to facilitate investigation into the factual knowledge through named entities. Our datasets and code are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/adithya7/ constrained-fever\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) [URL_CITE] and semantic role la-beling (Chen and Rambow, 2003). In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance. These re-sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclanthology.coli.uni-saarland.de/pdf/W/W03/W03-1006.pdf\nCitation Context: Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) and semantic role la-beling (Chen and Rambow, 2003) [URL_CITE] . In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance. These re-sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.\nFootnote or Reference text (if exists): John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In EMNLP. pages 41\u201348. http://aclanthology.coli.uni-saarland.de/pdf/W/W03/W03-1006.pdf. Nasr, Owen Rambow, and Srinivas Bangalore. 2016. Revisiting supertagging and parsing: How to use supertags in transition-based parsing. In TAG+. pages 85\u201392. http://www.aclweb.org/anthology/W16-3309.\nSection Titles (if exists):  ['5 Downstream Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/adapt-sjtu/socvec\nCitation Context: Future di-rections include: 1) mining cross-cultural differ-ences in general concepts other than names and slang, 2) merging the mined knowledge into exist-ing knowledge bases, and 3) applying the SocVec in downstream tasks like machine translation. [URL_CITE]\nFootnote or Reference text (if exists): 14 We will make our code and data available at https: //github.com/adapt-sjtu/socvec.\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Codebase\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asahi417/kex\nCitation Context: In this section, we explain our keyword extrac-tion experimental setting. All our experiments are run on a 16-core Ubuntu computer equipped with 3.8GHz i7 core and 64GiB memory. [URL_CITE]\nFootnote or Reference text (if exists): 5 All the details to reproduce our experiments are available at https://github.com/asahi417/kex\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cond.org/schbase.html\nCitation Context: We further show how keyphrases can be placed into a semantically-meaningful \u201cphylogenetic\u201d structure and describe key features of this structure. The com-plete SCHB ASE dataset is available at: [URL_CITE] http://cond.org/schbase.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/enwiki/\nCitation Context: In this section, we describe how we obtained W I - KI CREM. Starting from English Wikipedia [URL_CITE] , we search for sentences and pairs of sentences with the following properties: at least two distinct per-sonal names appear in the text, and one of them is repeated. We do not use pieces of text with more than two sentences to collect concise examples only.\nFootnote or Reference text (if exists): 2 https://dumps.wikimedia.org/enwiki/ dump id: enwiki-20181201\nSection Titles (if exists):  ['3 The W IKI CREM Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jalammar/ecco\nCitation Context: Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco \u2013 an open-source [URL_CITE] li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models.\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/jalammar/ecco\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/\nCitation Context: In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algo-rithms, word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), based on the Oct. 2013 Wikipedi-a dump. [URL_CITE] We first compare levels of truncation of word2vec embeddings, and then evaluate the s-tochastic rounding and the auxiliary vectors based methods for training word2vec vectors.\nFootnote or Reference text (if exists): 1 https://dumps.wikimedia.org/\nSection Titles (if exists):  ['4 Experiments on Word/Phrase Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain\nCitation Context: In this section, we describe our key features for fa-cilitating a rapid understanding of linguistic struc-ture. As a companion to this section, we invite readers to use our web demonstration at [URL_CITE] http://hierplane.allenai.org/explain to in-teractively explore examples. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Key Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bilbo.cs.illinois.edu/~ctsai12/xlwikifier-wikidata.zip\nCitation Context: Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at [URL_CITE] http://bilbo.cs.illinois.edu/\u02dcctsai12/xlwikifier-wikidata.zip.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1 Wikipedia Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepset-ai/FARM\nCitation Context: We retrieved the pretrained models, fine-tuned them separately on each downstream task using the training and development sets, and tested them on the test sets. We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [URL_CITE] libraries to implement the transformer-based models. A complete list of hyperparameter values is presented in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/deepset-ai/FARM\nSection Titles (if exists):  ['3 Language models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opennlp.apache.org\nCitation Context: POS tagging was carried out using the OpenNLP POS tagger. [URL_CITE] The vandalism word list contains a hand-crafted set of around 100 vandalism and spam words from various places in the web.\nFootnote or Reference text (if exists): 5 Maxent model for English, http://opennlp.apache.org\nSection Titles (if exists):  ['3 Experiments', '3.2 Features for Edit Category Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fjoch.com/YASMET.html\nCitation Context: We use the published package YASMET [URL_CITE] to conduct parameters training and classification. YASMET requires supervised learning for the training of maximum entropy model.\nFootnote or Reference text (if exists): 1 http://www.fjoch.com/YASMET.html\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Both the MRC and RCI training was carried out using the pytorch transformers toolkit made avail-able by Huggingface [URL_CITE] . Table 9 gives the number of parameters for each introduced model.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendix']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.edikt.org.uk\nCitation Context: We would like to thank Phil Blunsom, Prachya Boonkwan, Christos Christodoulopoulos, Stephen Clark, Michael Collins, Chris Dyer, Timothy Fowler, Mark Granroth-Wilding, Philipp Koehn, Terry Koo, Tom Kwiatkowski, Andre\u0301 Martins, Matt Post, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts, and the anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility (http://www.ecdf.ed.ac.uk). The ECDF is partially supported by the eDIKT initiative ( [URL_CITE] http://www.edikt.org.uk).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://computing.open.ac.uk/coda/\nCitation Context: We would like to thank the three anonymous reviewers for their helpful comments and sug-gestions. We are also grateful to our col-leagues in the Open University\u2019s Natural Lan-guage Generation group for stimulating discussions and feedback. The research reported in this pa-per was carried out as part of the CODA re-search project ( [URL_CITE] http://computing.open.ac.uk/coda/) which was funded by the UK\u2019s Engineering and Physical Sciences Research Council under Grant EP/G020981/1.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Acknowledgements']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://osc.edu/ark:/19495/f5s1ph73\nCitation Context: The authors would like to thank the anonymous re-viewers for their helpful comments. Computations for this project were partly run on the Ohio Super-computer Center (1987) [URL_CITE] . This research was par-tially funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022.\nFootnote or Reference text (if exists): The Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. \\\\url{http://osc.edu/ark:/19495/f5s1ph73}.\nSection Titles (if exists): ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nCitation Context: The au-thors would also like to thank Thang Minh Luong for his support for this project ad Nikola Mrks\u030cic\u0301 and anonymous reviewers for their constructive feedback. The data is avail-able at [URL_CITE] http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008) [URL_CITE] , which ignores function tags and empty cate-gories. We return to this issue in the conclusion.\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['2 Overview and Example', '2.1 Creation of Dependency Representation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ecdf.ed.ac.uk\nCitation Context: We would like to thank Phil Blunsom, Prachya Boonkwan, Christos Christodoulopoulos, Stephen Clark, Michael Collins, Chris Dyer, Timothy Fowler, Mark Granroth-Wilding, Philipp Koehn, Terry Koo, Tom Kwiatkowski, Andre\u0301 Martins, Matt Post, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts, and the anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility ( [URL_CITE] http://www.ecdf.ed.ac.uk). The ECDF is partially supported by the eDIKT initiative (http://www.edikt.org.uk).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://davidmlane.com/hyperstat/A34739.html\nCitation Context: The Pearson\u2019s correlation coefficient [URL_CITE] measures the strength and direction of a linear relationship be-tween any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.\nFootnote or Reference text (if exists): 5 For a quick overview of the Pearson\u2019s coefficient, see: http://davidmlane.com/hyperstat/A34739.html.\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.mpi-inf.mpg.de/~corrogg/\nCitation Context: The score of a cluster is the sum of the indi-vidual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected. Our source code and the results of our evaluation are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 10 http://people.mpi-inf.mpg.de/\u02dccorrogg/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlds.soe.ucsc.edu/sarcasm1\nCitation Context: \u2022 Debates - We use two datasets [URL_CITE] from the In-ternet Argument Corpus (IAC) (Lukin and Walker, 2017) which have been hand anno-tated for sarcasm. This dataset, unlike the first two, is mainly concerned with long text and provides a diverse comparison from the other datasets.\nFootnote or Reference text (if exists): 4 https://nlds.soe.ucsc.edu/sarcasm1\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt{11,12}/results.html\nCitation Context: In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [URL_CITE] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt{11,12}/results.html\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://berouge.com/default.aspx\nCitation Context: Both ROUGE and BE have been implemented and included in the ROUGE/BE evaluation toolkit [URL_CITE] , which has been used as the default evaluation tool in the summarization track in the Document Un-derstanding Conference (DUC) and Text Analysis Conference (TAC). DUC and TAC also manually evaluated machine generated summaries by adopt-ing the Pyramid method. Besides evaluating with ROUGE/BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall re-sponsiveness.\nFootnote or Reference text (if exists): 1 http://berouge.com/default.aspx\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com\nCitation Context: Once post-edits had been created, DA was em-ployed in two separate runs on Amazon Mechani-cal Turk, [URL_CITE] once for GEN - REF and once for POST - EDIT . Besides employing distinct reference trans-lations in the assessment, all other set-up crite-ria were identical for both evaluation settings, in-cluding the conventional segment-level DA set-ting, where a minimum of 15 human assessments are combined into a mean DA score for a given translation, after strict quality control measures and score standardization have been applied.\nFootnote or Reference text (if exists): 11 https://www.mturk.com\nSection Titles (if exists):  ['3 Alternate Reference Bias Investigation', '3.1 Reference Bias Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Table 2: Comparison of small and base models on the GLUE dev set. The models were trained on the OpenWebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] for 1, 000, 000 and 766, 000 steps, respectively. The GLUE task scores are means of 8 runs over a set of random seeds.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments', '3.2 Efficient Pseudo-Log-Likelihood Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/berkeleyparser/\nCitation Context: The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) [URL_CITE] and score using the standard evalb program (Sekine and Collins, 2008). We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006).\nFootnote or Reference text (if exists): Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2008. The Berkeley Parser. https://code.google.com/p/berkeleyparser/.\nSection Titles (if exists):  ['5 Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) and score using the standard evalb program (Sekine and Collins, 2008) [URL_CITE] . We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006).\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['5 Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mlpc-ucsd/BERT_Convolutions\nCitation Context: Full pre-training and fine-tuning details are outlined in Appendix A.1. [URL_CITE] Evaluation Models were evaluated on the GLUE benchmark, a suite of sentence classification tasks including natural language inference (NLI), gram-maticality judgments, sentiment classification, and textual similarity (Wang et al., 2018). For each task, we ran ten fine-tuning runs and used the model with the best score on the development set.\nFootnote or Reference text (if exists): 2 Code is available at https://github.com/mlpc-ucsd/BERT_Convolutions, built upon the Huggingface Transformers library (Wolf et al., 2020).\nSection Titles (if exists):  ['3 Integrating lightweight convolutions', '3.2 Composite attention and lightweight convolution experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code and Dataset)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/timvdc/poetry\nCitation Context: In order to facilitate reproduction of the results and encourage further research, the poetry genera-tion system is made available as open source soft-ware. The current version can be downloaded at [URL_CITE] https://github.com/timvdc/poetry.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mponza/SalIE\nCitation Context: The source code and the processed datasets are publicly available [URL_CITE] to encourage further develop-ments of the fact salience task.\nFootnote or Reference text (if exists): 1 https://github.com/mponza/SalIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cis.temple.edu/~edragut/DebugSL.webm\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [URL_CITE] https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/isobs\nCitation Context: We evaluate different character recognition mod-els on self-created dataset. The results show that our implementation of prototypical network can achieve stable and competitive results. The datasets and source code can be found from [URL_CITE] https: //github.com/thunlp/isobs .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiment and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://publish.illinois.edu/yirenwang/emnlp16source/\nCitation Context: All our implementations are based on Theano (Theano De-velopment Team, 2016) and run on one K40 GPU. All the source codes and datasets can be down-loaded at [URL_CITE] https://publish.illinois.edu/yirenwang/emnlp16source/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ghpaetzold/questplusplus\nCitation Context: The source code for the framework, the datasets and extra resources can be downloaded from [URL_CITE] https://github.com/ghpaetzold/questplusplus.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Remarks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nCitation Context: Specifically, For each test or target domain, we only use 10% of the training examples to sim-ulate data scarcity. In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script [URL_CITE]\nFootnote or Reference text (if exists): 3 http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nSection Titles (if exists):  ['6 Experiments', '6.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: Results are reported using several standard mea-sures, the F 1 score, unlabeled attachment and Leaf Ancestor scores. The labeled F 1 score [F1] [URL_CITE] , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence 10 .\nFootnote or Reference text (if exists): 9 Evalb tool available at http://nlp.cs.nyu.edu/evalb/. We also used the evaluation by category implemented in the class EvalbByCat in the Stanford Parser.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: ELI5 (Fan et al., 2019b) [URL_CITE] is a collection of question-answer-evidence triples where the ques-tions are complex, and the answers are long, ex-planatory, and free-form. For dev and test, we col-lect annotations using Amazon Mechanical Turk, asking evaluators to select which supporting docu-ments from Wikipedia can be used to answer the question. We treat these as gold provenance anno-tations for evaluation (details in section 4).\nFootnote or Reference text (if exists): 8 https://yjernite.github.io/lfqa.html\nSection Titles (if exists):  ['3 Tasks', '3.4 Open Domain Question Answering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.mpi-inf.mpg.de/~corrogg/\nCitation Context: The score of a cluster is the sum of the indi-vidual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected. Our source code and the results of our evaluation are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 10 http://people.mpi-inf.mpg.de/\u02dccorrogg/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.paracrawl.eu/\nCitation Context: Unlabeled-domain For additionalData unlabeled-domain data, we use web-crawled bitext from the Paracrawl project. [URL_CITE] We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English.\nFootnote or Reference text (if exists): 4 https://www.paracrawl.eu/\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Data and Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nCitation Context: For evaluation we use labeled attachment score (LAS). Significance testing is performed using a randomization test, with the script from the CoNLL 2017 Shared Task. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jekbradbury/revtok\nCitation Context: We tokenize using revtok [URL_CITE] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al.\nFootnote or Reference text (if exists): 1 https://github.com/jekbradbury/revtok\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cocodataset.org/\nCitation Context: We train our contextual token-image matching model (in Sec. 3.1) on MS COCO image cap-tioning dataset [URL_CITE] for 20 epochs. The concatena-tion of the last 4 layers of BERT outputs (fol-lowing Devlin et al.\nFootnote or Reference text (if exists): 8 http://cocodataset.org/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2Me0Gk1\nCitation Context: BERT models. We used the pytorch huggingface implementation of BERT and GPT2 [URL_CITE] . For the two BERT models we used bert-base-uncased (12-layers, 768-hidden, 12-heads, 110M parameters).\nFootnote or Reference text (if exists): 6 https://bit.ly/2Me0Gk1\nSection Titles (if exists):  ['4 Evaluation', '4.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We ran each model for 50 epochs\u2014enough for the training loss to converge. For our BERT ex-periments we use a PyTorch implementation [URL_CITE] with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam op-timizer, a batch size of 32, dropout of 0.1, and em-bedding size 768.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Method and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: For ACMLM, we build our model based on the BERT implementation from HuggingFace. [URL_CITE] We initialize our decoder using the pre-trained \u2018Bert-base\u2019 model and set the max sequence length to 30. We train the model for 5 epochs using Adam with learning rate 2e \u22125 .\nFootnote or Reference text (if exists): 7 https://github.com/huggingface/pytorch-pretrained- BERT\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: We also calculate the answer recall after document prun-ing, which indicates the performance upper bound. Model settings We initialize our model us-ing two publicly available uncased versions of BERT [URL_CITE] : BERT BASE and BERT LARGE , and refer readers to Devlin et al. (2018) for details on model sizes.\nFootnote or Reference text (if exists): 5 https://github.com/google-research/bert\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nusnlp/nea\nCitation Context: LSTM T&N model We replicate and evaluate the LSTM model of Taghipour and Ng (2016) [URL_CITE] on ASAP and our synthetic data.\nFootnote or Reference text (if exists): 14 https://github.com/nusnlp/nea\nSection Titles (if exists):  ['5 Model Parameters and Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/GeneZC/ASGCN\nCitation Context: Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models [URL_CITE] , and further demon-strate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.\nFootnote or Reference text (if exists): 1 Code and preprocessed datasets are available at https://github.com/GeneZC/ASGCN.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lil.nlp.cornell.edu/nlvr/\nCitation Context: This paper includes four main contributions: (1) a procedure for collecting visually rich im-ages paired with semantically-diverse language descriptions; (2) NLVR2, which contains 107,292 examples of captions and image pairs, includ-ing 29,680 unique sentences and 127,502 im-ages; (3) a qualitative linguistically-driven data analysis showing that our process achieves a broader representation of linguistic phenomena compared to other resources; and (4) an evalu-ation with several baselines and state-of-the-art visual reasoning methods on NLVR2. The rel-atively low performance we observe shows that NLVR2 presents a significant challenge, even for methods that perform well on existing vi-sual reasoning tasks. NLVR2 is available at [URL_CITE] http://lil.nlp.cornell.edu/nlvr/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nanguoshun/LSR\nCitation Context: Experiments show that our model significantly outperforms the existing approaches on DocRED, a large-scale document-level relation extraction dataset with a large number of entities and re-lations, and also yields new state-of-the-art re-sults on two popular document-level relation ex-traction datasets in the biomedical domain. The code and pretrained model are available at [URL_CITE] https: //github.com/nanguoshun/LSR 1 . els in various settings.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/\nCitation Context: We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper-forms competitive baselines and other neural language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The dataset and word vectors can be downloaded at http://ai.stanford.edu/ \u223c ehhuang/.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ibm/loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: [URL_CITE] https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: In this demonstration (demo site: [URL_CITE] https://ibm.biz/acl21-loa), we present a Log-ical Optimal Actions (LOA) architecture for neuro-symbolic RL applications with LNN (Riegel et al., 2020) for text-based interaction games. While natural language-based interactive agents are the ambitious but attractive target as real-world applications of neuro-symbolic, it is not easy to provide an environment for the agent.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/alvations/pywsd\nCitation Context: DKPro WSD (Miller et al., 2013) is a modu-lar, extensible Java framework for word sense dis-ambiguation. It implements multiple WSD meth-ods and also provides an interface to evaluation datasets. PyWSD [URL_CITE] project also provides imple-mentations of popular WSD methods, but these are implemented in the Python language.\nFootnote or Reference text (if exists): 3 https://github.com/alvations/pywsd\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thu-coai/OpenMEVA\nCitation Context: We also provide an open-source toolkit which implements various metrics, and therefore supports the comparison and analysis of metrics. In addition, the toolkit provides data perturbation techniques for generating customized test cases beyond AU - TOS , which can facilitate fast development of new automatic metrics [URL_CITE] .\nFootnote or Reference text (if exists): 1 All the tools, data, and evaluation scripts are available at https://github.com/thu-coai/OpenMEVA\nSection Titles (if exists):  ['\u2217 Corresponding author']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: We propose a novel demonstration (URL: [URL_CITE] https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: [URL_CITE] https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ted.com\nCitation Context: However, the work fo-cused on a single speaker in a limited domain. Of-fline speech translation of TED [URL_CITE] talks has been ad-dressed through the IWSLT 2011 and 2012 evalua-tion tracks. The talks are from a variety of speakers with varying dialects and cover a range of topics.\nFootnote or Reference text (if exists): 1 http://www.ted.com\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor\nCitation Context: The language clustering based on language family is shown in Figure 2, which results in [URL_CITE] groups given our 23 languages. All the language names and their corresponding ISO-639-1 code can be found in Supplementary Materials (Section 2).\nFootnote or Reference text (if exists): 8 https://github.com/tensorflow/tensor2tensor\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.iso.org/iso-639-language-codes.html\nCitation Context: The language clustering based on language family is shown in Figure 2, which results in groups given our 23 languages. All the language names and their corresponding ISO-639-1 code [URL_CITE] can be found in Supplementary Materials (Section 2).\nFootnote or Reference text (if exists): 9 https://www.iso.org/iso-639-language-codes.html.\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wit3.fbk.eu/\nCitation Context: Datasets We evaluate our method on the IWSLT datasets which contain multiple languages from TED talks. We collect datasets from the IWSLT evaluation campaign [URL_CITE] from years 2011 to 2018, which consist of the translation pairs of 23 languages\u2194English. The detailed description of the training/validation/test set of the 23 transla-tion pairs can be found in Supplementary Mate-rials (Section 1).\nFootnote or Reference text (if exists): 6 https://wit3.fbk.eu/\nSection Titles (if exists):  ['4 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool\nCitation Context: Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010 [URL_CITE] ; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop).\nFootnote or Reference text (if exists): Awais Athar. 2010. Dependensee: A dependency parse visualization tool. http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlpviz.bpodgursky.com\nCitation Context: Figure 1 was generated from (Podgursky, 2015) [URL_CITE] , which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom).\nFootnote or Reference text (if exists): Ben Podgursky. 2015. Nlpviz. http://nlpviz.bpodgursky.com. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://demos.explosion.ai/\nCitation Context: Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016) [URL_CITE] provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008).\nFootnote or Reference text (if exists): Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.brenbarn.net/werewolf/\nCitation Context: There are other important roles in Were-wolf, such as seer, vigilante, etc, the goals and avail-able actions of which are not the focus of this paper. (Barnwell, 2012) [URL_CITE] provides a broader description of the game and roles.\nFootnote or Reference text (if exists): Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cordova.apache.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova [URL_CITE] \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 3 https://cordova.apache.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://demos.explosion.ai/\nCitation Context: Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016 [URL_CITE] ; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop).\nFootnote or Reference text (if exists): Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com\nCitation Context: Online conversations are increasingly significant for communication, e.g., Slack for work com-munication and Reddit [URL_CITE] for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014).\nFootnote or Reference text (if exists): 2 https://www.reddit.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\nCitation Context: Our method can be regarded as a general ver-sion of distant supervision since we assume that a reply includes the label information of the cor-responding tweet. There have been many studies about distant supervision for sentiment analysis (Read, 2005; Go et al., 2009 [URL_CITE] ; Davidov et al., 2010; Purver and Battersby, 2012; Mohammad et al., 2013; Tang et al., 2014; dos Santos and Gatti, 2014; Severyn and Moschitti, 2015; Deriu et al., 2016; Mu\u0308ller et al., 2017), but they basically fo-cused on how to use emoticons and hashtags to leverage performance. One exception is the study by (Pool and Nissim, 2016), in which Facebook reactions were used for distant supervision.\nFootnote or Reference text (if exists): Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification using Dis-tant Supervision. Technical report, Stan-ford Digital Library Technologies Project. https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://news.google.com/\nCitation Context: Online news platforms such as Google News [URL_CITE] and MSN News have attracted many users to read news online (Das et al., 2007). Massive news ar-ticles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is very important for online news platforms to target user interests and allevi-ate information overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 1 https://news.google.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: Online news platforms such as Google News and MSN News [URL_CITE] have attracted many users to read news online (Das et al., 2007). Massive news ar-ticles are generated everyday and it is impossible for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is very important for online news platforms to target user interests and allevi-ate information overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 2 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/collaborations/past-projects/ace\nCitation Context: Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 2005 [URL_CITE] , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously over-looks the arguments-scattering challenge. In con-trast, EE on financial documents, such as ChFi-nAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event.\nFootnote or Reference text (if exists): 2 https://www.ldc.upenn.edu/collaborations/past-projects/ace\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://slack.com\nCitation Context: Online conversations are increasingly significant for communication, e.g., Slack [URL_CITE] for work com-munication and Reddit for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014).\nFootnote or Reference text (if exists): 1 https://slack.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://kheafield.com/code/kenlm/\nCitation Context: For DWF, we ranked the monolingual data by word rarity (Platanios et al., 2019) of sentences and also selected the top 80M monolingual data for self-training. For S RC LM, we trained an N -gram language model (Heafield, 2011) [URL_CITE] on the source sentences in the bitext and measured the distance between each monolingual sentence to the bitext source sentences by cross-entropy. Similarly, we se-lected 8M monolingual data with the lowest cross-entropy for self-training.\nFootnote or Reference text (if exists): 9 https://kheafield.com/code/kenlm/\nSection Titles (if exists):  ['A Appendix', 'A.3 Comparison with Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.codeproject.com/KB/recipes/englishparsing.aspx\nCitation Context: Similar to SLM (Chelba and Jelinek, 2000), af-ter the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the \u201copenNLP\u201d software (Northedge, 2005) [URL_CITE] to parse a large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that contain 230 million tokens, then use them to initial-ize model parameters.\nFootnote or Reference text (if exists): R. Northedge. 2005. OpenNLP software http://www.codeproject.com/KB/recipes/englishparsing.aspx\nSection Titles (if exists):  ['4 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-2364\nCitation Context: In a language where this HPE-mean metric is low, most tokens can predict the POS of their parent even without context. We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88\u201399. Documented models at http: //hdl.handle.net/11234/1-2364.\nSection Titles (if exists):  ['5 What Correlates with Difficulty?']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~ozaidan/AOC\nCitation Context: We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. [URL_CITE] This corpus is highly dialectal\nFootnote or Reference text (if exists): 1 Available from http://www.cs.jhu.edu/~ozaidan/AOC\nSection Titles (if exists):  ['3.2. Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.infoplease.com/people.html\nCitation Context: We download from infoplease.com [URL_CITE] and biogra-phy.com two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information.\nFootnote or Reference text (if exists): 2 http://www.infoplease.com/people.html\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/first20hours/google-10000-english\nCitation Context: measuring gen-der biases of a whole corpus). Therefore, we also consider two additional, larger target word lists: 4) the top 10,000 most frequent words of Google\u2019s trillion word corpus (Google10K) [URL_CITE] and 5) the full vocabulary of each corpus (Full).\nFootnote or Reference text (if exists): 7 https://github.com/first20hours/google-10000-english.\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt19/translation-task.html\nCitation Context: To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [URL_CITE] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experiment', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/pretrained_models.html\nCitation Context: LMs examined in this paper share similar archi-tecture and pre-training setting as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), but are trained on different corpora. We provide the shortcut name of each LM in the Hugging-Face\u2019s Transformer library ( [URL_CITE] https://huggingface.co/transformers/pretrained_models.html) and their training corpora in Tab. 8, from which you can find more information.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['D Details of Pre-trained LMs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, [URL_CITE] for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/ (Wikipedia+Gigaword 5, 300d)\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.biography.com/search/index.jsp\nCitation Context: We download from infoplease.com and biogra-phy.com [URL_CITE] two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information.\nFootnote or Reference text (if exists): 3 http://www.biography.com/search/index.jsp\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://purduenlp.cs.purdue.edu/projects/twittermorals\nCitation Context: In summary, this paper makes the following contributions: (1) This work is among the first to explore jointly modeling language and polit-ical framing techniques for the classification of moral foundations used in the tweets of U.S. politicians on Twitter. (2) We provide a descrip-tion of our annotation guidelines and an annotated dataset of 2,050 tweets. [URL_CITE] (3) We suggest compu-tational models which easily adapt to new policy issues, for the classification of the moral founda-tions present in tweets.\nFootnote or Reference text (if exists): 2 The data will be available at http://purduenlp.cs.purdue.edu/projects/twittermorals.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.biu.ac.il/~nlp/downloads/\nCitation Context: To our knowledge, this is the first such in-depth study. [URL_CITE]\nFootnote or Reference text (if exists): 4 The guidelines and the dataset are available at http://www.cs.biu.ac.il/\u02dcnlp/downloads/\nSection Titles (if exists):  ['3 Motivation and Goals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Our dataset is, to the best of our knowledge, the largest noun compound dataset yet produced. We will make it available via [URL_CITE] http://www.isi.edu.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/UnsupervisedQA\nCitation Context: To summarize, this paper makes the follow-ing contributions: i) The first approach for unsu-pervised QA, reducing the problem to unsuper-vised cloze translation, using methods from unsu-pervised machine translation ii) Extensive experi-ments testing the impact of various cloze question translation algorithms and assumptions iii) Ex-periments demonstrating the application of our method for few-shot learning in EQA. [URL_CITE]\nFootnote or Reference text (if exists): 1 Synthetic EQA training data and models that generate it will be made publicly available at https://github.com/facebookresearch/UnsupervisedQA\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/data/edit-classification\nCitation Context: Additional information necessary to reproduce our results, including word lists and training, development and test data, is re-leased online. [URL_CITE] To the best of our knowledge, this is the first approach allowing to classify each single edit in Wikipedia into one or more of 21 different edit categories using a supervised machine learning approach.\nFootnote or Reference text (if exists): 2 http://www.ukp.tu-darmstadt.de/data/ edit-classification\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020\nCitation Context: To the best of our knowledge, this is the first work in attempting to generate simi-les. By framing the task as a style-transfer problem we make three contributions: [URL_CITE]\nFootnote or Reference text (if exists): 4 Code & Data at https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE\nCitation Context: Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [URL_CITE] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters.\nFootnote or Reference text (if exists): 5 https://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tdil-dc.in/index.php?lang=en\nCitation Context: Datasets: For training English-Hindi NMT sys-tems, we use the IITB English-Hindi parallel cor-pus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010) [URL_CITE] spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB par-allel corpus for validation.\nFootnote or Reference text (if exists): 1 The corpus is available on request from http://tdil-dc.in/index.php?lang=en\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mindspore.cn/\nCitation Context: We thank Mindspore [URL_CITE] which is a new deep learning computing framework for the partial support of this work.\nFootnote or Reference text (if exists): 3 https://www.mindspore.cn/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. [URL_CITE] We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set.\nFootnote or Reference text (if exists): 18 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['B Experimental Details', 'B.1 Data and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/30sGEbi\nCitation Context: We train all models on a single NVIDIA Titan X GPU, with the excep-tion of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeat-edly query the NVIDIA System Management In-terface [URL_CITE] to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel\u2019s Running Average Power Limit interface.\nFootnote or Reference text (if exists): 2 nvidia-smi: https://bit.ly/30sGEbi\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/Fast-TransX\nCitation Context: In fact, we often rely on simple personal computers for model validation. Hence, we enable OpenKE to adapt models for parallel learning on CPU [URL_CITE] be-sides employing GPU learning, which allow users to make full use of all available computing re-sources. The parallel learning method is shown in Algorithm 1.\nFootnote or Reference text (if exists): 2 https://github.com/thunlp/Fast-TransX\nSection Titles (if exists):  ['4 Implementations', '4.2 Parallel Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zapier.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www/eml-research.de/nlp/download/wikirelations.php\nCitation Context: To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [URL_CITE] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links.\nFootnote or Reference text (if exists): 1 http://www/eml-research.de/nlp/download/wikirelations.php\nSection Titles (if exists):  ['4 Representation', '4.2 Selectional preference features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/vecmap\nCitation Context: (2017). [URL_CITE] However, because the methods pre-sented in this work feature tunable hyperparame-ters, we use a portion of the training set as devel-opment data. 2 In all experiments, a single target word is predicted for each source word, and full points are awarded if it is one of the listed correct translations.\nFootnote or Reference text (if exists): 1 https://github.com/artetxem/vecmap\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/LocusLink\nCitation Context: Word, POS and shallow parse layers are sequen-tial (the latter can skip or span multiple words). The gene/protein layer assigns IDs from the LocusLink database of gene names. [URL_CITE] For a given gene there are as many LocusLink IDs as the number of organisms it is found in (e.g., 4 in the case of the gene Bcl-2).\nFootnote or Reference text (if exists): 7 http://www.ncbi.nlm.nih.gov/LocusLink\nSection Titles (if exists):  ['3 The Layered Query Language']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.github.com/festvox/datasets/CMUDoG\nCitation Context: In this paper we introduce a crowd-sourced con-versations dataset that is grounded in a predefined set of documents which is available for download [URL_CITE] . We perform multiple automatic and human judgment based analysis to understand the value the information from the document provides to the generation of responses.\nFootnote or Reference text (if exists): 4 https://www.github.com/festvox/datasets/CMU DoG\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ifttt.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/coastalcph/supersense-data-twitter\nCitation Context: We implement a window-based approach with a multi-channel multi-layer perceptron model using the Theano framework (Bastien et al., 2012). With a sliding window of size [URL_CITE] for the sequence learning setup we extract for each word the following seven feature vectors:\nFootnote or Reference text (if exists): 5 https://github.com/coastalcph/ supersense-data-twitter\nSection Titles (if exists):  ['5 Building a Supersense Tagger', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: bert-base-uncased). [URL_CITE] We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4.\nFootnote or Reference text (if exists): 2 We use the BERT implementation from https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/decomp-sem/neural-sprl\nCitation Context: achieves state-of-the-art performance for SPRL. [URL_CITE] As depicted in Figure 1, our model\u2019s architecture is an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training. We include a thorough quanti-tative analysis highlighting the contrasting errors between the proposed model and previous (non-neural) state-of-the-art.\nFootnote or Reference text (if exists): 2 Implementation available at https://github.com/decomp-sem/neural-sprl.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clarkkev/deep-coref\nCitation Context: The first resolver, the Stanford neural resolver (Clark and Manning, 2016) [URL_CITE] , takes as input a set of entity mentions identified for a given document by a rule-based MD system and trains using reinforce-ment learning a simple mention ranker consisting of three hidden layers of ReLU units and a final layer that is fully-connected.\nFootnote or Reference text (if exists): 3 https://github.com/clarkkev/deep-coref\nSection Titles (if exists):  ['2 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/fnwkde/\nCitation Context: We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexical-semantic resources. The created resource is publicly available at [URL_CITE] http://www.ukp.tu-darmstadt.de/fnwkde/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cornell.edu/~rs/sfour/\nCitation Context: \u2022 SFOUR: SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE (Sipos et al., 2012). We use the publicly available imple-mentation. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.cornell.edu/\u02dcrs/sfour/\nSection Titles (if exists):  ['4 Evaluation', '4.3 End-to-End Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ]
]